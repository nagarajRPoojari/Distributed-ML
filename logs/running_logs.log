[2025-05-20 19:13:50,178: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:13:50,178: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:13:50,179: ERROR: main: yaml file is empty]
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/utils/common.py", line 30, in read_yaml
    return ConfigBox(content)
  File "box/box.py", line 295, in box.box.Box.__init__
box.exceptions.BoxValueError: First argument must be mapping or iterable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 13, in <module>
    obj.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_01_data_ingestion.py", line 11, in main
    config = ConfigurationManager()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/config/configuration.py", line 16, in __init__
    self.config = read_yaml(config_filepath)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/ensure/main.py", line 872, in __call__
    return_val = self.f(*args, **kwargs)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/utils/common.py", line 32, in read_yaml
    raise ValueError("yaml file is empty")
ValueError: yaml file is empty
[2025-05-20 19:14:51,994: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:14:51,995: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:14:51,996: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:14:51,996: INFO: common: created directory at: artifacts]
[2025-05-20 19:14:51,996: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:14:51,996: ERROR: main: [Errno 2] No such file or directory: 'artifacts/data_ingestion/data.zip']
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 13, in <module>
    obj.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_01_data_ingestion.py", line 14, in main
    data_ingestion.download_file()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/data_ingestion.py", line 24, in download_file
    logger.info(f"File already exists of size: {get_size(Path(self.config.local_data_file))}")
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/ensure/main.py", line 872, in __call__
    return_val = self.f(*args, **kwargs)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/utils/common.py", line 63, in get_size
    size_in_kb = round(os.path.getsize(path)/1024)
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/genericpath.py", line 50, in getsize
    return os.stat(filename).st_size
FileNotFoundError: [Errno 2] No such file or directory: 'artifacts/data_ingestion/data.zip'
[2025-05-20 19:15:58,789: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:15:58,790: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:15:58,791: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:15:58,791: INFO: common: created directory at: artifacts]
[2025-05-20 19:15:58,791: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:15:58,791: INFO: data_ingestion: File already exists]
[2025-05-20 19:15:58,791: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:15:58,791: INFO: main: *******************]
[2025-05-20 19:15:58,791: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:15:58,792: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:15:58,792: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:15:58,792: INFO: common: created directory at: artifacts]
[2025-05-20 19:15:58,792: ERROR: main: "'ConfigBox' object has no attribute 'model_trainer'"]
Traceback (most recent call last):
  File "box/box.py", line 594, in box.box.Box.__getitem__
KeyError: 'model_trainer'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "box/box.py", line 633, in box.box.Box.__getattr__
  File "box/box.py", line 621, in box.box.Box.__getitem__
box.exceptions.BoxKeyError: "'model_trainer'"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "box/box.py", line 635, in box.box.Box.__getattr__
AttributeError: 'ConfigBox' object has no attribute 'model_trainer'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "box/config_box.py", line 29, in box.config_box.ConfigBox.__getattr__
  File "box/box.py", line 649, in box.box.Box.__getattr__
box.exceptions.BoxKeyError: "'ConfigBox' object has no attribute 'model_trainer'"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "box/box.py", line 594, in box.box.Box.__getitem__
KeyError: 'model_trainer'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "box/box.py", line 633, in box.box.Box.__getattr__
  File "box/box.py", line 621, in box.box.Box.__getitem__
box.exceptions.BoxKeyError: "'model_trainer'"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "box/box.py", line 635, in box.box.Box.__getattr__
AttributeError: 'ConfigBox' object has no attribute 'model_trainer'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 12, in main
    model_trainer_config = config.get_model_trainer_config()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/config/configuration.py", line 69, in get_model_trainer_config
    config = self.config.model_trainer
  File "box/config_box.py", line 31, in box.config_box.ConfigBox.__getattr__
  File "box/box.py", line 649, in box.box.Box.__getattr__
box.exceptions.BoxKeyError: "'ConfigBox' object has no attribute 'model_trainer'"
[2025-05-20 19:19:27,487: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:19:27,489: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:19:27,490: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:19:27,490: INFO: common: created directory at: artifacts]
[2025-05-20 19:19:27,490: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:19:27,490: INFO: data_ingestion: File already exists]
[2025-05-20 19:19:27,490: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:19:27,490: INFO: main: *******************]
[2025-05-20 19:19:27,490: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:19:27,492: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:19:27,492: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:19:27,492: INFO: common: created directory at: artifacts]
[2025-05-20 19:19:27,492: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:19:30,921: ERROR: main: train() missing 1 required positional argument: 'world_size']
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
TypeError: train() missing 1 required positional argument: 'world_size'
[2025-05-20 19:20:36,753: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:20:36,755: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:20:36,756: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:20:36,756: INFO: common: created directory at: artifacts]
[2025-05-20 19:20:36,756: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:20:36,756: INFO: data_ingestion: File already exists]
[2025-05-20 19:20:36,756: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:20:36,756: INFO: main: *******************]
[2025-05-20 19:20:36,756: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:20:36,758: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:20:36,758: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:20:36,758: INFO: common: created directory at: artifacts]
[2025-05-20 19:20:36,758: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:20:38,504: ERROR: main: train() takes 1 positional argument but 3 were given]
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 95, in train
    self.train(rank, world_size)
TypeError: train() takes 1 positional argument but 3 were given
[2025-05-20 19:20:54,177: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:20:54,179: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:20:54,180: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:20:54,180: INFO: common: created directory at: artifacts]
[2025-05-20 19:20:54,180: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:20:54,180: INFO: data_ingestion: File already exists]
[2025-05-20 19:20:54,180: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:20:54,180: INFO: main: *******************]
[2025-05-20 19:20:54,180: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:20:54,181: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:20:54,182: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:20:54,182: INFO: common: created directory at: artifacts]
[2025-05-20 19:20:54,182: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:20:55,902: ERROR: main: train() takes 1 positional argument but 3 were given]
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 95, in train
    self.train(rank, world_size)
TypeError: train() takes 1 positional argument but 3 were given
[2025-05-20 19:21:11,266: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:21:11,267: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:21:11,268: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:21:11,268: INFO: common: created directory at: artifacts]
[2025-05-20 19:21:11,268: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:21:11,268: INFO: data_ingestion: File already exists]
[2025-05-20 19:21:11,268: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:21:11,268: INFO: main: *******************]
[2025-05-20 19:21:11,268: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:21:11,270: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:21:11,270: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:21:11,270: INFO: common: created directory at: artifacts]
[2025-05-20 19:21:11,270: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:24:32,433: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:24:32,435: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:24:32,436: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:24:32,436: INFO: common: created directory at: artifacts]
[2025-05-20 19:24:32,436: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:24:32,436: INFO: data_ingestion: File already exists]
[2025-05-20 19:24:32,436: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:24:32,436: INFO: main: *******************]
[2025-05-20 19:24:32,436: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:24:32,437: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:24:32,438: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:24:32,438: INFO: common: created directory at: artifacts]
[2025-05-20 19:24:32,438: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:24:34,152: ERROR: main: Address already in use]
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 96, in train
    self.train_loop(rank, world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 56, in train_loop
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 576, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 229, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 157, in _create_c10d_store
    return TCPStore(
RuntimeError: Address already in use
[2025-05-20 19:26:37,741: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:26:37,744: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:26:37,745: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:26:37,745: INFO: common: created directory at: artifacts]
[2025-05-20 19:26:37,745: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:26:37,745: INFO: data_ingestion: File already exists]
[2025-05-20 19:26:37,745: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:26:37,745: INFO: main: *******************]
[2025-05-20 19:26:37,745: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:26:37,746: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:26:37,747: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:26:37,747: INFO: common: created directory at: artifacts]
[2025-05-20 19:26:37,747: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:26:39,497: ERROR: main: Address already in use]
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 96, in train
    self.train_loop(rank, world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 56, in train_loop
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 576, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 229, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 157, in _create_c10d_store
    return TCPStore(
RuntimeError: Address already in use
[2025-05-20 19:29:09,183: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:29:09,185: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:29:09,186: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:29:09,186: INFO: common: created directory at: artifacts]
[2025-05-20 19:29:09,186: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:29:09,186: INFO: data_ingestion: File already exists]
[2025-05-20 19:29:09,186: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:29:09,186: INFO: main: *******************]
[2025-05-20 19:29:09,186: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:29:09,188: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:29:09,188: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:29:09,188: INFO: common: created directory at: artifacts]
[2025-05-20 19:29:09,188: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:29:10,327: ERROR: main: Address already in use]
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 96, in train
    self.train_loop(rank, world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 56, in train_loop
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 576, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 229, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 157, in _create_c10d_store
    return TCPStore(
RuntimeError: Address already in use
[2025-05-20 19:30:03,840: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:30:03,842: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:30:03,842: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:30:03,842: INFO: common: created directory at: artifacts]
[2025-05-20 19:30:03,842: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:30:03,842: INFO: data_ingestion: File already exists]
[2025-05-20 19:30:03,843: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:30:03,843: INFO: main: *******************]
[2025-05-20 19:30:03,843: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:30:03,844: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:30:03,845: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:30:03,845: INFO: common: created directory at: artifacts]
[2025-05-20 19:30:03,845: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:32:00,378: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:32:00,380: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:32:00,381: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:32:00,381: INFO: common: created directory at: artifacts]
[2025-05-20 19:32:00,381: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:32:00,381: INFO: data_ingestion: File already exists]
[2025-05-20 19:32:00,381: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:32:00,381: INFO: main: *******************]
[2025-05-20 19:32:00,381: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:32:00,383: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:32:00,383: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:32:00,383: INFO: common: created directory at: artifacts]
[2025-05-20 19:32:00,383: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:33:07,424: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:33:07,426: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:33:07,426: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:33:07,426: INFO: common: created directory at: artifacts]
[2025-05-20 19:33:07,426: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:33:07,426: INFO: data_ingestion: File already exists]
[2025-05-20 19:33:07,427: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:33:07,427: INFO: main: *******************]
[2025-05-20 19:33:07,427: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:33:07,428: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:33:07,429: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:33:07,429: INFO: common: created directory at: artifacts]
[2025-05-20 19:33:07,429: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:33:40,362: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:33:40,364: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:33:40,365: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:33:40,365: INFO: common: created directory at: artifacts]
[2025-05-20 19:33:40,365: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:33:40,365: INFO: data_ingestion: File already exists]
[2025-05-20 19:33:40,365: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:33:40,365: INFO: main: *******************]
[2025-05-20 19:33:40,365: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:33:40,367: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:33:40,367: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:33:40,367: INFO: common: created directory at: artifacts]
[2025-05-20 19:33:40,367: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:34:11,901: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:34:11,903: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:34:11,903: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:34:11,904: INFO: common: created directory at: artifacts]
[2025-05-20 19:34:11,904: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:34:11,904: INFO: data_ingestion: File already exists]
[2025-05-20 19:34:11,904: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:34:11,904: INFO: main: *******************]
[2025-05-20 19:34:11,904: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:34:11,905: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:34:11,906: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:34:11,906: INFO: common: created directory at: artifacts]
[2025-05-20 19:34:11,906: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:34:13,036: ERROR: main: Address already in use]
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 14, in main
    model_trainer_config.train()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 106, in train
    self.train_loop(rank, world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 57, in train_loop
    dist.init_process_group("nccl", rank=rank, world_size=world_size)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 576, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 229, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/.venv/lib/python3.9/site-packages/torch/distributed/rendezvous.py", line 157, in _create_c10d_store
    return TCPStore(
RuntimeError: Address already in use
[2025-05-20 19:36:28,577: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:36:28,579: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:36:28,580: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:36:28,580: INFO: common: created directory at: artifacts]
[2025-05-20 19:36:28,580: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:36:28,580: INFO: data_ingestion: File already exists]
[2025-05-20 19:36:28,580: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:36:28,580: INFO: main: *******************]
[2025-05-20 19:36:28,580: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:36:28,582: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:36:28,582: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:36:28,582: INFO: common: created directory at: artifacts]
[2025-05-20 19:36:28,582: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 19:41:27,449: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 19:41:27,450: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:41:27,451: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:41:27,451: INFO: common: created directory at: artifacts]
[2025-05-20 19:41:27,451: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 19:41:27,451: INFO: data_ingestion: File already exists]
[2025-05-20 19:41:27,452: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 19:41:27,452: INFO: main: *******************]
[2025-05-20 19:41:27,452: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 19:41:27,453: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 19:41:27,454: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 19:41:27,454: INFO: common: created directory at: artifacts]
[2025-05-20 19:41:27,454: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 20:38:40,843: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 20:38:40,845: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 20:38:40,846: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 20:38:40,846: INFO: common: created directory at: artifacts]
[2025-05-20 20:38:40,846: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 20:38:40,846: INFO: data_ingestion: File already exists]
[2025-05-20 20:38:40,846: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<

x==========x]
[2025-05-20 20:38:40,846: INFO: main: *******************]
[2025-05-20 20:38:40,846: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 20:38:40,848: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 20:38:40,849: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 20:38:40,849: INFO: common: created directory at: artifacts]
[2025-05-20 20:38:40,849: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 20:38:47,159: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 20:38:47,161: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 20:38:47,832: INFO: distributed: Reducer buckets have been rebuilt in this iteration.]
[2025-05-20 20:39:16,048: INFO: main: >>>>>> stage Training completed <<<<<<

x==========x]
[2025-05-20 22:10:42,828: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:10:42,830: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:10:42,831: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:10:42,832: INFO: common: created directory at: artifacts]
[2025-05-20 22:10:42,832: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:10:42,832: INFO: data_ingestion: File already exists]
[2025-05-20 22:10:42,832: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:10:42,832: INFO: main: *******************]
[2025-05-20 22:10:42,832: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:10:42,833: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:10:42,834: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:10:42,834: INFO: common: created directory at: artifacts]
[2025-05-20 22:10:42,834: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:10:49,665: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:10:49,667: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:10:50,360: INFO: distributed: Reducer buckets have been rebuilt in this iteration.]
[2025-05-20 22:11:19,742: INFO: main: >>>>>> stage Training completed <<<<<<]
[2025-05-20 22:23:36,173: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:23:36,175: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:23:36,176: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:23:36,176: INFO: common: created directory at: artifacts]
[2025-05-20 22:23:36,176: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:23:36,176: INFO: data_ingestion: File already exists]
[2025-05-20 22:23:36,176: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:23:36,176: INFO: main: *******************]
[2025-05-20 22:23:36,176: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:23:36,177: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:23:36,178: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:23:36,178: INFO: common: created directory at: artifacts]
[2025-05-20 22:23:36,178: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:23:42,912: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:23:42,912: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:23:42,951: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 2.0061]
[2025-05-20 22:23:42,953: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9938]
[2025-05-20 22:23:42,955: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.8795]
[2025-05-20 22:23:42,956: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.8852]
[2025-05-20 22:23:42,958: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.7988]
[2025-05-20 22:23:42,960: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.7740]
[2025-05-20 22:23:42,963: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.7892]
[2025-05-20 22:23:42,964: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.6974]
[2025-05-20 22:23:42,965: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.7139]
[2025-05-20 22:23:42,967: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6392]
[2025-05-20 22:23:42,968: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6743]
[2025-05-20 22:23:42,970: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.6136]
[2025-05-20 22:23:42,971: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.5998]
[2025-05-20 22:23:42,973: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.5040]
[2025-05-20 22:23:42,974: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.4786]
[2025-05-20 22:23:42,976: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.4299]
[2025-05-20 22:23:42,977: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4033]
[2025-05-20 22:23:42,979: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.4458]
[2025-05-20 22:23:42,980: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.3660]
[2025-05-20 22:23:42,981: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.3539]
[2025-05-20 22:23:42,983: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.3132]
[2025-05-20 22:23:42,985: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.2816]
[2025-05-20 22:23:42,987: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.3080]
[2025-05-20 22:23:42,988: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2857]
[2025-05-20 22:23:42,989: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.1910]
[2025-05-20 22:23:42,991: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.1682]
[2025-05-20 22:23:42,992: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.1440]
[2025-05-20 22:23:42,994: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.1673]
[2025-05-20 22:23:42,995: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.1507]
[2025-05-20 22:23:42,996: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.1127]
[2025-05-20 22:23:42,998: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.0956]
[2025-05-20 22:23:42,999: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.0830]
[2025-05-20 22:23:43,000: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0244]
[2025-05-20 22:23:43,001: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 1.0643]
[2025-05-20 22:23:43,003: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 0.9584]
[2025-05-20 22:23:43,005: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 1.0274]
[2025-05-20 22:23:43,006: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.9627]
[2025-05-20 22:23:43,007: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9290]
[2025-05-20 22:23:43,008: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.8981]
[2025-05-20 22:23:43,010: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.9541]
[2025-05-20 22:23:43,012: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.9028]
[2025-05-20 22:23:43,013: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.9144]
[2025-05-20 22:23:43,014: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.8116]
[2025-05-20 22:23:43,015: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8831]
[2025-05-20 22:23:43,016: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8037]
[2025-05-20 22:23:43,017: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.8305]
[2025-05-20 22:23:43,018: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.7995]
[2025-05-20 22:23:43,020: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.8194]
[2025-05-20 22:23:43,021: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.7297]
[2025-05-20 22:23:43,022: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7449]
[2025-05-20 22:23:43,024: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.7749]
[2025-05-20 22:23:43,025: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.6944]
[2025-05-20 22:23:43,026: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.6826]
[2025-05-20 22:23:43,027: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.6371]
[2025-05-20 22:23:43,029: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.7068]
[2025-05-20 22:23:43,030: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6396]
[2025-05-20 22:23:43,031: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.6355]
[2025-05-20 22:23:43,032: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.6272]
[2025-05-20 22:23:43,033: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.6426]
[2025-05-20 22:23:43,034: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.6261]
[2025-05-20 22:23:43,036: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.5540]
[2025-05-20 22:23:43,037: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.5847]
[2025-05-20 22:23:43,038: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5483]
[2025-05-20 22:23:43,039: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.6089]
[2025-05-20 22:23:43,040: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5023]
[2025-05-20 22:23:43,042: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.5575]
[2025-05-20 22:23:43,043: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.4909]
[2025-05-20 22:23:43,045: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.5451]
[2025-05-20 22:23:43,046: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.5157]
[2025-05-20 22:23:43,047: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.4751]
[2025-05-20 22:23:43,048: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.5183]
[2025-05-20 22:23:43,049: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.4783]
[2025-05-20 22:23:43,050: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4827]
[2025-05-20 22:23:43,052: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4449]
[2025-05-20 22:23:43,053: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4627]
[2025-05-20 22:23:43,054: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4350]
[2025-05-20 22:23:43,055: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.4082]
[2025-05-20 22:23:43,056: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.4407]
[2025-05-20 22:23:43,057: INFO: model_training: Rank 0, Epoch 16, Batch 3, Loss: 0.4003]
[2025-05-20 22:23:43,058: INFO: model_training: Rank 0, Epoch 16, Batch 4, Loss: 0.3772]
[2025-05-20 22:23:43,060: INFO: model_training: Rank 0, Epoch 17, Batch 0, Loss: 0.4014]
[2025-05-20 22:23:43,062: INFO: model_training: Rank 0, Epoch 17, Batch 1, Loss: 0.3535]
[2025-05-20 22:23:43,063: INFO: model_training: Rank 0, Epoch 17, Batch 2, Loss: 0.3946]
[2025-05-20 22:23:43,064: INFO: model_training: Rank 0, Epoch 17, Batch 3, Loss: 0.3497]
[2025-05-20 22:23:43,065: INFO: model_training: Rank 0, Epoch 17, Batch 4, Loss: 0.3275]
[2025-05-20 22:23:43,067: INFO: model_training: Rank 0, Epoch 18, Batch 0, Loss: 0.3454]
[2025-05-20 22:23:43,068: INFO: model_training: Rank 0, Epoch 18, Batch 1, Loss: 0.4012]
[2025-05-20 22:23:43,069: INFO: model_training: Rank 0, Epoch 18, Batch 2, Loss: 0.3614]
[2025-05-20 22:23:43,070: INFO: model_training: Rank 0, Epoch 18, Batch 3, Loss: 0.3147]
[2025-05-20 22:23:43,072: INFO: model_training: Rank 0, Epoch 18, Batch 4, Loss: 0.2815]
[2025-05-20 22:23:43,073: INFO: model_training: Rank 0, Epoch 19, Batch 0, Loss: 0.3469]
[2025-05-20 22:23:43,074: INFO: model_training: Rank 0, Epoch 19, Batch 1, Loss: 0.3098]
[2025-05-20 22:23:43,075: INFO: model_training: Rank 0, Epoch 19, Batch 2, Loss: 0.2694]
[2025-05-20 22:23:43,076: INFO: model_training: Rank 0, Epoch 19, Batch 3, Loss: 0.2722]
[2025-05-20 22:23:43,078: INFO: model_training: Rank 0, Epoch 19, Batch 4, Loss: 0.3286]
[2025-05-20 22:23:43,079: INFO: model_training: Rank 0, Epoch 20, Batch 0, Loss: 0.3010]
[2025-05-20 22:23:43,081: INFO: model_training: Rank 0, Epoch 20, Batch 1, Loss: 0.2461]
[2025-05-20 22:23:43,082: INFO: model_training: Rank 0, Epoch 20, Batch 2, Loss: 0.3183]
[2025-05-20 22:23:43,083: INFO: model_training: Rank 0, Epoch 20, Batch 3, Loss: 0.2656]
[2025-05-20 22:23:43,084: INFO: model_training: Rank 0, Epoch 20, Batch 4, Loss: 0.2719]
[2025-05-20 22:23:43,086: INFO: model_training: Rank 0, Epoch 21, Batch 0, Loss: 0.2478]
[2025-05-20 22:23:43,088: INFO: model_training: Rank 0, Epoch 21, Batch 1, Loss: 0.2888]
[2025-05-20 22:23:43,089: INFO: model_training: Rank 0, Epoch 21, Batch 2, Loss: 0.2216]
[2025-05-20 22:23:43,090: INFO: model_training: Rank 0, Epoch 21, Batch 3, Loss: 0.2547]
[2025-05-20 22:23:43,092: INFO: model_training: Rank 0, Epoch 21, Batch 4, Loss: 0.2895]
[2025-05-20 22:23:43,093: INFO: model_training: Rank 0, Epoch 22, Batch 0, Loss: 0.2769]
[2025-05-20 22:23:43,095: INFO: model_training: Rank 0, Epoch 22, Batch 1, Loss: 0.2326]
[2025-05-20 22:23:43,097: INFO: model_training: Rank 0, Epoch 22, Batch 2, Loss: 0.2479]
[2025-05-20 22:23:43,098: INFO: model_training: Rank 0, Epoch 22, Batch 3, Loss: 0.2675]
[2025-05-20 22:23:43,099: INFO: model_training: Rank 0, Epoch 22, Batch 4, Loss: 0.2599]
[2025-05-20 22:23:43,101: INFO: model_training: Rank 0, Epoch 23, Batch 0, Loss: 0.2076]
[2025-05-20 22:23:43,102: INFO: model_training: Rank 0, Epoch 23, Batch 1, Loss: 0.2519]
[2025-05-20 22:23:43,103: INFO: model_training: Rank 0, Epoch 23, Batch 2, Loss: 0.2218]
[2025-05-20 22:23:43,105: INFO: model_training: Rank 0, Epoch 23, Batch 3, Loss: 0.1871]
[2025-05-20 22:23:43,106: INFO: model_training: Rank 0, Epoch 23, Batch 4, Loss: 0.2131]
[2025-05-20 22:23:43,107: INFO: model_training: Rank 0, Epoch 24, Batch 0, Loss: 0.1973]
[2025-05-20 22:23:43,108: INFO: model_training: Rank 0, Epoch 24, Batch 1, Loss: 0.1785]
[2025-05-20 22:23:43,110: INFO: model_training: Rank 0, Epoch 24, Batch 2, Loss: 0.2294]
[2025-05-20 22:23:43,111: INFO: model_training: Rank 0, Epoch 24, Batch 3, Loss: 0.1345]
[2025-05-20 22:23:43,112: INFO: model_training: Rank 0, Epoch 24, Batch 4, Loss: 0.1489]
[2025-05-20 22:23:43,114: INFO: model_training: Rank 0, Epoch 25, Batch 0, Loss: 0.1523]
[2025-05-20 22:23:43,115: INFO: model_training: Rank 0, Epoch 25, Batch 1, Loss: 0.2033]
[2025-05-20 22:23:43,116: INFO: model_training: Rank 0, Epoch 25, Batch 2, Loss: 0.1493]
[2025-05-20 22:23:43,117: INFO: model_training: Rank 0, Epoch 25, Batch 3, Loss: 0.1755]
[2025-05-20 22:23:43,118: INFO: model_training: Rank 0, Epoch 25, Batch 4, Loss: 0.2051]
[2025-05-20 22:23:43,120: INFO: model_training: Rank 0, Epoch 26, Batch 0, Loss: 0.1561]
[2025-05-20 22:23:43,121: INFO: model_training: Rank 0, Epoch 26, Batch 1, Loss: 0.2065]
[2025-05-20 22:23:43,122: INFO: model_training: Rank 0, Epoch 26, Batch 2, Loss: 0.1697]
[2025-05-20 22:23:43,123: INFO: model_training: Rank 0, Epoch 26, Batch 3, Loss: 0.1435]
[2025-05-20 22:23:43,124: INFO: model_training: Rank 0, Epoch 26, Batch 4, Loss: 0.1589]
[2025-05-20 22:23:43,126: INFO: model_training: Rank 0, Epoch 27, Batch 0, Loss: 0.1032]
[2025-05-20 22:23:43,127: INFO: model_training: Rank 0, Epoch 27, Batch 1, Loss: 0.0948]
[2025-05-20 22:23:43,129: INFO: model_training: Rank 0, Epoch 27, Batch 2, Loss: 0.1777]
[2025-05-20 22:23:43,130: INFO: model_training: Rank 0, Epoch 27, Batch 3, Loss: 0.1174]
[2025-05-20 22:23:43,131: INFO: model_training: Rank 0, Epoch 27, Batch 4, Loss: 0.1035]
[2025-05-20 22:23:43,132: INFO: model_training: Rank 0, Epoch 28, Batch 0, Loss: 0.1241]
[2025-05-20 22:23:43,133: INFO: model_training: Rank 0, Epoch 28, Batch 1, Loss: 0.1581]
[2025-05-20 22:23:43,134: INFO: model_training: Rank 0, Epoch 28, Batch 2, Loss: 0.1498]
[2025-05-20 22:23:43,136: INFO: model_training: Rank 0, Epoch 28, Batch 3, Loss: 0.1032]
[2025-05-20 22:23:43,137: INFO: model_training: Rank 0, Epoch 28, Batch 4, Loss: 0.1298]
[2025-05-20 22:23:43,139: INFO: model_training: Rank 0, Epoch 29, Batch 0, Loss: 0.1389]
[2025-05-20 22:23:43,140: INFO: model_training: Rank 0, Epoch 29, Batch 1, Loss: 0.0928]
[2025-05-20 22:23:43,142: INFO: model_training: Rank 0, Epoch 29, Batch 2, Loss: 0.0897]
[2025-05-20 22:23:43,143: INFO: model_training: Rank 0, Epoch 29, Batch 3, Loss: 0.1056]
[2025-05-20 22:23:43,144: INFO: model_training: Rank 0, Epoch 29, Batch 4, Loss: 0.0670]
[2025-05-20 22:23:43,146: INFO: model_training: Rank 0, Epoch 30, Batch 0, Loss: 0.0657]
[2025-05-20 22:23:43,148: INFO: model_training: Rank 0, Epoch 30, Batch 1, Loss: 0.1191]
[2025-05-20 22:23:43,149: INFO: model_training: Rank 0, Epoch 30, Batch 2, Loss: 0.1174]
[2025-05-20 22:23:43,150: INFO: model_training: Rank 0, Epoch 30, Batch 3, Loss: 0.1156]
[2025-05-20 22:23:43,152: INFO: model_training: Rank 0, Epoch 30, Batch 4, Loss: 0.1421]
[2025-05-20 22:23:43,154: INFO: model_training: Rank 0, Epoch 31, Batch 0, Loss: 0.1038]
[2025-05-20 22:23:43,156: INFO: model_training: Rank 0, Epoch 31, Batch 1, Loss: 0.1402]
[2025-05-20 22:23:43,157: INFO: model_training: Rank 0, Epoch 31, Batch 2, Loss: 0.1318]
[2025-05-20 22:23:43,159: INFO: model_training: Rank 0, Epoch 31, Batch 3, Loss: 0.0862]
[2025-05-20 22:23:43,160: INFO: model_training: Rank 0, Epoch 31, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,161: INFO: model_training: Rank 0, Epoch 32, Batch 0, Loss: 0.0569]
[2025-05-20 22:23:43,163: INFO: model_training: Rank 0, Epoch 32, Batch 1, Loss: 0.0951]
[2025-05-20 22:23:43,164: INFO: model_training: Rank 0, Epoch 32, Batch 2, Loss: 0.1192]
[2025-05-20 22:23:43,165: INFO: model_training: Rank 0, Epoch 32, Batch 3, Loss: 0.1048]
[2025-05-20 22:23:43,166: INFO: model_training: Rank 0, Epoch 32, Batch 4, Loss: 0.0935]
[2025-05-20 22:23:43,167: INFO: model_training: Rank 0, Epoch 33, Batch 0, Loss: 0.0961]
[2025-05-20 22:23:43,169: INFO: model_training: Rank 0, Epoch 33, Batch 1, Loss: 0.0529]
[2025-05-20 22:23:43,170: INFO: model_training: Rank 0, Epoch 33, Batch 2, Loss: 0.0763]
[2025-05-20 22:23:43,172: INFO: model_training: Rank 0, Epoch 33, Batch 3, Loss: 0.0744]
[2025-05-20 22:23:43,173: INFO: model_training: Rank 0, Epoch 33, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,175: INFO: model_training: Rank 0, Epoch 34, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,176: INFO: model_training: Rank 0, Epoch 34, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,177: INFO: model_training: Rank 0, Epoch 34, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,178: INFO: model_training: Rank 0, Epoch 34, Batch 3, Loss: 0.0971]
[2025-05-20 22:23:43,179: INFO: model_training: Rank 0, Epoch 34, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,181: INFO: model_training: Rank 0, Epoch 35, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,182: INFO: model_training: Rank 0, Epoch 35, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,184: INFO: model_training: Rank 0, Epoch 35, Batch 2, Loss: 0.0880]
[2025-05-20 22:23:43,185: INFO: model_training: Rank 0, Epoch 35, Batch 3, Loss: 0.0901]
[2025-05-20 22:23:43,186: INFO: model_training: Rank 0, Epoch 35, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,188: INFO: model_training: Rank 0, Epoch 36, Batch 0, Loss: 0.0887]
[2025-05-20 22:23:43,189: INFO: model_training: Rank 0, Epoch 36, Batch 1, Loss: 0.0907]
[2025-05-20 22:23:43,190: INFO: model_training: Rank 0, Epoch 36, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,191: INFO: model_training: Rank 0, Epoch 36, Batch 3, Loss: 0.0997]
[2025-05-20 22:23:43,192: INFO: model_training: Rank 0, Epoch 36, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,194: INFO: model_training: Rank 0, Epoch 37, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,196: INFO: model_training: Rank 0, Epoch 37, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,197: INFO: model_training: Rank 0, Epoch 37, Batch 2, Loss: 0.0968]
[2025-05-20 22:23:43,198: INFO: model_training: Rank 0, Epoch 37, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,199: INFO: model_training: Rank 0, Epoch 37, Batch 4, Loss: 0.0558]
[2025-05-20 22:23:43,200: INFO: model_training: Rank 0, Epoch 38, Batch 0, Loss: 0.0797]
[2025-05-20 22:23:43,201: INFO: model_training: Rank 0, Epoch 38, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,203: INFO: model_training: Rank 0, Epoch 38, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,205: INFO: model_training: Rank 0, Epoch 38, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,206: INFO: model_training: Rank 0, Epoch 38, Batch 4, Loss: 0.0759]
[2025-05-20 22:23:43,208: INFO: model_training: Rank 0, Epoch 39, Batch 0, Loss: 0.0742]
[2025-05-20 22:23:43,209: INFO: model_training: Rank 0, Epoch 39, Batch 1, Loss: 0.0693]
[2025-05-20 22:23:43,210: INFO: model_training: Rank 0, Epoch 39, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,211: INFO: model_training: Rank 0, Epoch 39, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,212: INFO: model_training: Rank 0, Epoch 39, Batch 4, Loss: 0.0565]
[2025-05-20 22:23:43,214: INFO: model_training: Rank 0, Epoch 40, Batch 0, Loss: 0.0687]
[2025-05-20 22:23:43,215: INFO: model_training: Rank 0, Epoch 40, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,216: INFO: model_training: Rank 0, Epoch 40, Batch 2, Loss: 0.0725]
[2025-05-20 22:23:43,217: INFO: model_training: Rank 0, Epoch 40, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,218: INFO: model_training: Rank 0, Epoch 40, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,220: INFO: model_training: Rank 0, Epoch 41, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,221: INFO: model_training: Rank 0, Epoch 41, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,222: INFO: model_training: Rank 0, Epoch 41, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,224: INFO: model_training: Rank 0, Epoch 41, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,225: INFO: model_training: Rank 0, Epoch 41, Batch 4, Loss: 0.0575]
[2025-05-20 22:23:43,227: INFO: model_training: Rank 0, Epoch 42, Batch 0, Loss: 0.0709]
[2025-05-20 22:23:43,228: INFO: model_training: Rank 0, Epoch 42, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,229: INFO: model_training: Rank 0, Epoch 42, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,230: INFO: model_training: Rank 0, Epoch 42, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,231: INFO: model_training: Rank 0, Epoch 42, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,233: INFO: model_training: Rank 0, Epoch 43, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,234: INFO: model_training: Rank 0, Epoch 43, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,235: INFO: model_training: Rank 0, Epoch 43, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,237: INFO: model_training: Rank 0, Epoch 43, Batch 3, Loss: 0.0596]
[2025-05-20 22:23:43,238: INFO: model_training: Rank 0, Epoch 43, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,240: INFO: model_training: Rank 0, Epoch 44, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,241: INFO: model_training: Rank 0, Epoch 44, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,242: INFO: model_training: Rank 0, Epoch 44, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,243: INFO: model_training: Rank 0, Epoch 44, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,244: INFO: model_training: Rank 0, Epoch 44, Batch 4, Loss: 0.0668]
[2025-05-20 22:23:43,246: INFO: model_training: Rank 0, Epoch 45, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,247: INFO: model_training: Rank 0, Epoch 45, Batch 1, Loss: 0.0523]
[2025-05-20 22:23:43,248: INFO: model_training: Rank 0, Epoch 45, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,249: INFO: model_training: Rank 0, Epoch 45, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,250: INFO: model_training: Rank 0, Epoch 45, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,256: INFO: model_training: Rank 0, Epoch 46, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,265: INFO: model_training: Rank 0, Epoch 46, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,269: INFO: model_training: Rank 0, Epoch 46, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,274: INFO: model_training: Rank 0, Epoch 46, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,277: INFO: model_training: Rank 0, Epoch 46, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,279: INFO: model_training: Rank 0, Epoch 47, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,281: INFO: model_training: Rank 0, Epoch 47, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,285: INFO: model_training: Rank 0, Epoch 47, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,289: INFO: model_training: Rank 0, Epoch 47, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,290: INFO: model_training: Rank 0, Epoch 47, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,292: INFO: model_training: Rank 0, Epoch 48, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,293: INFO: model_training: Rank 0, Epoch 48, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,295: INFO: model_training: Rank 0, Epoch 48, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,296: INFO: model_training: Rank 0, Epoch 48, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,297: INFO: model_training: Rank 0, Epoch 48, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,300: INFO: model_training: Rank 0, Epoch 49, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,301: INFO: model_training: Rank 0, Epoch 49, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,302: INFO: model_training: Rank 0, Epoch 49, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,304: INFO: model_training: Rank 0, Epoch 49, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,305: INFO: model_training: Rank 0, Epoch 49, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,307: INFO: model_training: Rank 0, Epoch 50, Batch 0, Loss: 0.0606]
[2025-05-20 22:23:43,308: INFO: model_training: Rank 0, Epoch 50, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,309: INFO: model_training: Rank 0, Epoch 50, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,310: INFO: model_training: Rank 0, Epoch 50, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,312: INFO: model_training: Rank 0, Epoch 50, Batch 4, Loss: 0.0613]
[2025-05-20 22:23:43,314: INFO: model_training: Rank 0, Epoch 51, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,315: INFO: model_training: Rank 0, Epoch 51, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,316: INFO: model_training: Rank 0, Epoch 51, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,317: INFO: model_training: Rank 0, Epoch 51, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,318: INFO: model_training: Rank 0, Epoch 51, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,320: INFO: model_training: Rank 0, Epoch 52, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,322: INFO: model_training: Rank 0, Epoch 52, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,323: INFO: model_training: Rank 0, Epoch 52, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,324: INFO: model_training: Rank 0, Epoch 52, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,325: INFO: model_training: Rank 0, Epoch 52, Batch 4, Loss: 0.0507]
[2025-05-20 22:23:43,327: INFO: model_training: Rank 0, Epoch 53, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,329: INFO: model_training: Rank 0, Epoch 53, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,330: INFO: model_training: Rank 0, Epoch 53, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,331: INFO: model_training: Rank 0, Epoch 53, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,332: INFO: model_training: Rank 0, Epoch 53, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,334: INFO: model_training: Rank 0, Epoch 54, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,335: INFO: model_training: Rank 0, Epoch 54, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,337: INFO: model_training: Rank 0, Epoch 54, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,338: INFO: model_training: Rank 0, Epoch 54, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,339: INFO: model_training: Rank 0, Epoch 54, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,340: INFO: model_training: Rank 0, Epoch 55, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,341: INFO: model_training: Rank 0, Epoch 55, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,342: INFO: model_training: Rank 0, Epoch 55, Batch 2, Loss: 0.0506]
[2025-05-20 22:23:43,343: INFO: model_training: Rank 0, Epoch 55, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,345: INFO: model_training: Rank 0, Epoch 55, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,346: INFO: model_training: Rank 0, Epoch 56, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,348: INFO: model_training: Rank 0, Epoch 56, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,349: INFO: model_training: Rank 0, Epoch 56, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,350: INFO: model_training: Rank 0, Epoch 56, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,351: INFO: model_training: Rank 0, Epoch 56, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,353: INFO: model_training: Rank 0, Epoch 57, Batch 0, Loss: 0.0511]
[2025-05-20 22:23:43,354: INFO: model_training: Rank 0, Epoch 57, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,355: INFO: model_training: Rank 0, Epoch 57, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,356: INFO: model_training: Rank 0, Epoch 57, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,357: INFO: model_training: Rank 0, Epoch 57, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,359: INFO: model_training: Rank 0, Epoch 58, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,360: INFO: model_training: Rank 0, Epoch 58, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,361: INFO: model_training: Rank 0, Epoch 58, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,363: INFO: model_training: Rank 0, Epoch 58, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,364: INFO: model_training: Rank 0, Epoch 58, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,365: INFO: model_training: Rank 0, Epoch 59, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,367: INFO: model_training: Rank 0, Epoch 59, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,368: INFO: model_training: Rank 0, Epoch 59, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,370: INFO: model_training: Rank 0, Epoch 59, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,371: INFO: model_training: Rank 0, Epoch 59, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,373: INFO: model_training: Rank 0, Epoch 60, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,374: INFO: model_training: Rank 0, Epoch 60, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,375: INFO: model_training: Rank 0, Epoch 60, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,376: INFO: model_training: Rank 0, Epoch 60, Batch 3, Loss: 0.0511]
[2025-05-20 22:23:43,377: INFO: model_training: Rank 0, Epoch 60, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,379: INFO: model_training: Rank 0, Epoch 61, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,380: INFO: model_training: Rank 0, Epoch 61, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,381: INFO: model_training: Rank 0, Epoch 61, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,382: INFO: model_training: Rank 0, Epoch 61, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,383: INFO: model_training: Rank 0, Epoch 61, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,385: INFO: model_training: Rank 0, Epoch 62, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,386: INFO: model_training: Rank 0, Epoch 62, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,388: INFO: model_training: Rank 0, Epoch 62, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,389: INFO: model_training: Rank 0, Epoch 62, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,390: INFO: model_training: Rank 0, Epoch 62, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,392: INFO: model_training: Rank 0, Epoch 63, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,393: INFO: model_training: Rank 0, Epoch 63, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,394: INFO: model_training: Rank 0, Epoch 63, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,395: INFO: model_training: Rank 0, Epoch 63, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,397: INFO: model_training: Rank 0, Epoch 63, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,398: INFO: model_training: Rank 0, Epoch 64, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,399: INFO: model_training: Rank 0, Epoch 64, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,400: INFO: model_training: Rank 0, Epoch 64, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,401: INFO: model_training: Rank 0, Epoch 64, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,403: INFO: model_training: Rank 0, Epoch 64, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,405: INFO: model_training: Rank 0, Epoch 65, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,406: INFO: model_training: Rank 0, Epoch 65, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,408: INFO: model_training: Rank 0, Epoch 65, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,409: INFO: model_training: Rank 0, Epoch 65, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,410: INFO: model_training: Rank 0, Epoch 65, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,412: INFO: model_training: Rank 0, Epoch 66, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,413: INFO: model_training: Rank 0, Epoch 66, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,415: INFO: model_training: Rank 0, Epoch 66, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,416: INFO: model_training: Rank 0, Epoch 66, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,417: INFO: model_training: Rank 0, Epoch 66, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,419: INFO: model_training: Rank 0, Epoch 67, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,420: INFO: model_training: Rank 0, Epoch 67, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,421: INFO: model_training: Rank 0, Epoch 67, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,423: INFO: model_training: Rank 0, Epoch 67, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,424: INFO: model_training: Rank 0, Epoch 67, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,425: INFO: model_training: Rank 0, Epoch 68, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,426: INFO: model_training: Rank 0, Epoch 68, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,427: INFO: model_training: Rank 0, Epoch 68, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,429: INFO: model_training: Rank 0, Epoch 68, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,430: INFO: model_training: Rank 0, Epoch 68, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,432: INFO: model_training: Rank 0, Epoch 69, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,433: INFO: model_training: Rank 0, Epoch 69, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,434: INFO: model_training: Rank 0, Epoch 69, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,435: INFO: model_training: Rank 0, Epoch 69, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,437: INFO: model_training: Rank 0, Epoch 69, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,438: INFO: model_training: Rank 0, Epoch 70, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,439: INFO: model_training: Rank 0, Epoch 70, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,441: INFO: model_training: Rank 0, Epoch 70, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,442: INFO: model_training: Rank 0, Epoch 70, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,443: INFO: model_training: Rank 0, Epoch 70, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,445: INFO: model_training: Rank 0, Epoch 71, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,447: INFO: model_training: Rank 0, Epoch 71, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,448: INFO: model_training: Rank 0, Epoch 71, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,449: INFO: model_training: Rank 0, Epoch 71, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,450: INFO: model_training: Rank 0, Epoch 71, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,451: INFO: model_training: Rank 0, Epoch 72, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,452: INFO: model_training: Rank 0, Epoch 72, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,454: INFO: model_training: Rank 0, Epoch 72, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,455: INFO: model_training: Rank 0, Epoch 72, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,456: INFO: model_training: Rank 0, Epoch 72, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,457: INFO: model_training: Rank 0, Epoch 73, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,458: INFO: model_training: Rank 0, Epoch 73, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,459: INFO: model_training: Rank 0, Epoch 73, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,461: INFO: model_training: Rank 0, Epoch 73, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,462: INFO: model_training: Rank 0, Epoch 73, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,464: INFO: model_training: Rank 0, Epoch 74, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,465: INFO: model_training: Rank 0, Epoch 74, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,466: INFO: model_training: Rank 0, Epoch 74, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,467: INFO: model_training: Rank 0, Epoch 74, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,468: INFO: model_training: Rank 0, Epoch 74, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,470: INFO: model_training: Rank 0, Epoch 75, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,471: INFO: model_training: Rank 0, Epoch 75, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,473: INFO: model_training: Rank 0, Epoch 75, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,474: INFO: model_training: Rank 0, Epoch 75, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,475: INFO: model_training: Rank 0, Epoch 75, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,476: INFO: model_training: Rank 0, Epoch 76, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,478: INFO: model_training: Rank 0, Epoch 76, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,479: INFO: model_training: Rank 0, Epoch 76, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,480: INFO: model_training: Rank 0, Epoch 76, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,481: INFO: model_training: Rank 0, Epoch 76, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,483: INFO: model_training: Rank 0, Epoch 77, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,484: INFO: model_training: Rank 0, Epoch 77, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,485: INFO: model_training: Rank 0, Epoch 77, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,486: INFO: model_training: Rank 0, Epoch 77, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,487: INFO: model_training: Rank 0, Epoch 77, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,489: INFO: model_training: Rank 0, Epoch 78, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,490: INFO: model_training: Rank 0, Epoch 78, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,492: INFO: model_training: Rank 0, Epoch 78, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,493: INFO: model_training: Rank 0, Epoch 78, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,494: INFO: model_training: Rank 0, Epoch 78, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,496: INFO: model_training: Rank 0, Epoch 79, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,497: INFO: model_training: Rank 0, Epoch 79, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,498: INFO: model_training: Rank 0, Epoch 79, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,499: INFO: model_training: Rank 0, Epoch 79, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,500: INFO: model_training: Rank 0, Epoch 79, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,502: INFO: model_training: Rank 0, Epoch 80, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,503: INFO: model_training: Rank 0, Epoch 80, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,505: INFO: model_training: Rank 0, Epoch 80, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,505: INFO: model_training: Rank 0, Epoch 80, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,507: INFO: model_training: Rank 0, Epoch 80, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,508: INFO: model_training: Rank 0, Epoch 81, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,509: INFO: model_training: Rank 0, Epoch 81, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,510: INFO: model_training: Rank 0, Epoch 81, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,512: INFO: model_training: Rank 0, Epoch 81, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,513: INFO: model_training: Rank 0, Epoch 81, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,515: INFO: model_training: Rank 0, Epoch 82, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,516: INFO: model_training: Rank 0, Epoch 82, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,517: INFO: model_training: Rank 0, Epoch 82, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,518: INFO: model_training: Rank 0, Epoch 82, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,519: INFO: model_training: Rank 0, Epoch 82, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,521: INFO: model_training: Rank 0, Epoch 83, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,522: INFO: model_training: Rank 0, Epoch 83, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,523: INFO: model_training: Rank 0, Epoch 83, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,525: INFO: model_training: Rank 0, Epoch 83, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,526: INFO: model_training: Rank 0, Epoch 83, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,527: INFO: model_training: Rank 0, Epoch 84, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,528: INFO: model_training: Rank 0, Epoch 84, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,530: INFO: model_training: Rank 0, Epoch 84, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,531: INFO: model_training: Rank 0, Epoch 84, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,532: INFO: model_training: Rank 0, Epoch 84, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,533: INFO: model_training: Rank 0, Epoch 85, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,534: INFO: model_training: Rank 0, Epoch 85, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,536: INFO: model_training: Rank 0, Epoch 85, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,537: INFO: model_training: Rank 0, Epoch 85, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,538: INFO: model_training: Rank 0, Epoch 85, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,540: INFO: model_training: Rank 0, Epoch 86, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,541: INFO: model_training: Rank 0, Epoch 86, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,542: INFO: model_training: Rank 0, Epoch 86, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,543: INFO: model_training: Rank 0, Epoch 86, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,545: INFO: model_training: Rank 0, Epoch 86, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,546: INFO: model_training: Rank 0, Epoch 87, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,547: INFO: model_training: Rank 0, Epoch 87, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,548: INFO: model_training: Rank 0, Epoch 87, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,550: INFO: model_training: Rank 0, Epoch 87, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,551: INFO: model_training: Rank 0, Epoch 87, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,553: INFO: model_training: Rank 0, Epoch 88, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,554: INFO: model_training: Rank 0, Epoch 88, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,555: INFO: model_training: Rank 0, Epoch 88, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,556: INFO: model_training: Rank 0, Epoch 88, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,557: INFO: model_training: Rank 0, Epoch 88, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,559: INFO: model_training: Rank 0, Epoch 89, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,560: INFO: model_training: Rank 0, Epoch 89, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,561: INFO: model_training: Rank 0, Epoch 89, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,562: INFO: model_training: Rank 0, Epoch 89, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,564: INFO: model_training: Rank 0, Epoch 89, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,565: INFO: model_training: Rank 0, Epoch 90, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,566: INFO: model_training: Rank 0, Epoch 90, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,567: INFO: model_training: Rank 0, Epoch 90, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,569: INFO: model_training: Rank 0, Epoch 90, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,570: INFO: model_training: Rank 0, Epoch 90, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,572: INFO: model_training: Rank 0, Epoch 91, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,573: INFO: model_training: Rank 0, Epoch 91, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,574: INFO: model_training: Rank 0, Epoch 91, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,575: INFO: model_training: Rank 0, Epoch 91, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,577: INFO: model_training: Rank 0, Epoch 91, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,579: INFO: model_training: Rank 0, Epoch 92, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,580: INFO: model_training: Rank 0, Epoch 92, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,581: INFO: model_training: Rank 0, Epoch 92, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,582: INFO: model_training: Rank 0, Epoch 92, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,583: INFO: model_training: Rank 0, Epoch 92, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,584: INFO: model_training: Rank 0, Epoch 93, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,586: INFO: model_training: Rank 0, Epoch 93, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,587: INFO: model_training: Rank 0, Epoch 93, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,589: INFO: model_training: Rank 0, Epoch 93, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,590: INFO: model_training: Rank 0, Epoch 93, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,591: INFO: model_training: Rank 0, Epoch 94, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,592: INFO: model_training: Rank 0, Epoch 94, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,593: INFO: model_training: Rank 0, Epoch 94, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,595: INFO: model_training: Rank 0, Epoch 94, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,596: INFO: model_training: Rank 0, Epoch 94, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,597: INFO: model_training: Rank 0, Epoch 95, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,598: INFO: model_training: Rank 0, Epoch 95, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,600: INFO: model_training: Rank 0, Epoch 95, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,601: INFO: model_training: Rank 0, Epoch 95, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,603: INFO: model_training: Rank 0, Epoch 95, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,604: INFO: model_training: Rank 0, Epoch 96, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,605: INFO: model_training: Rank 0, Epoch 96, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,606: INFO: model_training: Rank 0, Epoch 96, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,607: INFO: model_training: Rank 0, Epoch 96, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,608: INFO: model_training: Rank 0, Epoch 96, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,610: INFO: model_training: Rank 0, Epoch 97, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,612: INFO: model_training: Rank 0, Epoch 97, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,613: INFO: model_training: Rank 0, Epoch 97, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,615: INFO: model_training: Rank 0, Epoch 97, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,616: INFO: model_training: Rank 0, Epoch 97, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,617: INFO: model_training: Rank 0, Epoch 98, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,618: INFO: model_training: Rank 0, Epoch 98, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,620: INFO: model_training: Rank 0, Epoch 98, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,621: INFO: model_training: Rank 0, Epoch 98, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,622: INFO: model_training: Rank 0, Epoch 98, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,623: INFO: model_training: Rank 0, Epoch 99, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,625: INFO: model_training: Rank 0, Epoch 99, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,626: INFO: model_training: Rank 0, Epoch 99, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,627: INFO: model_training: Rank 0, Epoch 99, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,629: INFO: model_training: Rank 0, Epoch 99, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,630: INFO: model_training: Rank 0, Epoch 100, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,631: INFO: model_training: Rank 0, Epoch 100, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,633: INFO: model_training: Rank 0, Epoch 100, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,634: INFO: model_training: Rank 0, Epoch 100, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,635: INFO: model_training: Rank 0, Epoch 100, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,637: INFO: model_training: Rank 0, Epoch 101, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,638: INFO: model_training: Rank 0, Epoch 101, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,639: INFO: model_training: Rank 0, Epoch 101, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,640: INFO: model_training: Rank 0, Epoch 101, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,641: INFO: model_training: Rank 0, Epoch 101, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,643: INFO: model_training: Rank 0, Epoch 102, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,644: INFO: model_training: Rank 0, Epoch 102, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,645: INFO: model_training: Rank 0, Epoch 102, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,646: INFO: model_training: Rank 0, Epoch 102, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,647: INFO: model_training: Rank 0, Epoch 102, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,649: INFO: model_training: Rank 0, Epoch 103, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,650: INFO: model_training: Rank 0, Epoch 103, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,651: INFO: model_training: Rank 0, Epoch 103, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,653: INFO: model_training: Rank 0, Epoch 103, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,654: INFO: model_training: Rank 0, Epoch 103, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,656: INFO: model_training: Rank 0, Epoch 104, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,657: INFO: model_training: Rank 0, Epoch 104, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,658: INFO: model_training: Rank 0, Epoch 104, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,659: INFO: model_training: Rank 0, Epoch 104, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,660: INFO: model_training: Rank 0, Epoch 104, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,662: INFO: model_training: Rank 0, Epoch 105, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,663: INFO: model_training: Rank 0, Epoch 105, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,664: INFO: model_training: Rank 0, Epoch 105, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,665: INFO: model_training: Rank 0, Epoch 105, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,666: INFO: model_training: Rank 0, Epoch 105, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,668: INFO: model_training: Rank 0, Epoch 106, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,670: INFO: model_training: Rank 0, Epoch 106, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,671: INFO: model_training: Rank 0, Epoch 106, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,672: INFO: model_training: Rank 0, Epoch 106, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,674: INFO: model_training: Rank 0, Epoch 106, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,675: INFO: model_training: Rank 0, Epoch 107, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,676: INFO: model_training: Rank 0, Epoch 107, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,678: INFO: model_training: Rank 0, Epoch 107, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,679: INFO: model_training: Rank 0, Epoch 107, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,681: INFO: model_training: Rank 0, Epoch 107, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,682: INFO: model_training: Rank 0, Epoch 108, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,683: INFO: model_training: Rank 0, Epoch 108, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,684: INFO: model_training: Rank 0, Epoch 108, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,686: INFO: model_training: Rank 0, Epoch 108, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,687: INFO: model_training: Rank 0, Epoch 108, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,688: INFO: model_training: Rank 0, Epoch 109, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,689: INFO: model_training: Rank 0, Epoch 109, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,690: INFO: model_training: Rank 0, Epoch 109, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,692: INFO: model_training: Rank 0, Epoch 109, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,693: INFO: model_training: Rank 0, Epoch 109, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,695: INFO: model_training: Rank 0, Epoch 110, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,696: INFO: model_training: Rank 0, Epoch 110, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,697: INFO: model_training: Rank 0, Epoch 110, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,698: INFO: model_training: Rank 0, Epoch 110, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,699: INFO: model_training: Rank 0, Epoch 110, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,701: INFO: model_training: Rank 0, Epoch 111, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,702: INFO: model_training: Rank 0, Epoch 111, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,703: INFO: model_training: Rank 0, Epoch 111, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,705: INFO: model_training: Rank 0, Epoch 111, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,706: INFO: model_training: Rank 0, Epoch 111, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,707: INFO: model_training: Rank 0, Epoch 112, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,708: INFO: model_training: Rank 0, Epoch 112, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,710: INFO: model_training: Rank 0, Epoch 112, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,712: INFO: model_training: Rank 0, Epoch 112, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,713: INFO: model_training: Rank 0, Epoch 112, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,714: INFO: model_training: Rank 0, Epoch 113, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,716: INFO: model_training: Rank 0, Epoch 113, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,717: INFO: model_training: Rank 0, Epoch 113, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,718: INFO: model_training: Rank 0, Epoch 113, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,720: INFO: model_training: Rank 0, Epoch 113, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,722: INFO: model_training: Rank 0, Epoch 114, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,723: INFO: model_training: Rank 0, Epoch 114, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,724: INFO: model_training: Rank 0, Epoch 114, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,726: INFO: model_training: Rank 0, Epoch 114, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,727: INFO: model_training: Rank 0, Epoch 114, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,728: INFO: model_training: Rank 0, Epoch 115, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,730: INFO: model_training: Rank 0, Epoch 115, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,731: INFO: model_training: Rank 0, Epoch 115, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,732: INFO: model_training: Rank 0, Epoch 115, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,733: INFO: model_training: Rank 0, Epoch 115, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,735: INFO: model_training: Rank 0, Epoch 116, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,737: INFO: model_training: Rank 0, Epoch 116, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,738: INFO: model_training: Rank 0, Epoch 116, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,739: INFO: model_training: Rank 0, Epoch 116, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,740: INFO: model_training: Rank 0, Epoch 116, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,741: INFO: model_training: Rank 0, Epoch 117, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,742: INFO: model_training: Rank 0, Epoch 117, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,744: INFO: model_training: Rank 0, Epoch 117, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,745: INFO: model_training: Rank 0, Epoch 117, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,746: INFO: model_training: Rank 0, Epoch 117, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,748: INFO: model_training: Rank 0, Epoch 118, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,749: INFO: model_training: Rank 0, Epoch 118, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,750: INFO: model_training: Rank 0, Epoch 118, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,751: INFO: model_training: Rank 0, Epoch 118, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,753: INFO: model_training: Rank 0, Epoch 118, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,754: INFO: model_training: Rank 0, Epoch 119, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,756: INFO: model_training: Rank 0, Epoch 119, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,757: INFO: model_training: Rank 0, Epoch 119, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,758: INFO: model_training: Rank 0, Epoch 119, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,759: INFO: model_training: Rank 0, Epoch 119, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,761: INFO: model_training: Rank 0, Epoch 120, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,762: INFO: model_training: Rank 0, Epoch 120, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,763: INFO: model_training: Rank 0, Epoch 120, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,764: INFO: model_training: Rank 0, Epoch 120, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,765: INFO: model_training: Rank 0, Epoch 120, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,767: INFO: model_training: Rank 0, Epoch 121, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,768: INFO: model_training: Rank 0, Epoch 121, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,769: INFO: model_training: Rank 0, Epoch 121, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,770: INFO: model_training: Rank 0, Epoch 121, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,771: INFO: model_training: Rank 0, Epoch 121, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,773: INFO: model_training: Rank 0, Epoch 122, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,774: INFO: model_training: Rank 0, Epoch 122, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,775: INFO: model_training: Rank 0, Epoch 122, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,777: INFO: model_training: Rank 0, Epoch 122, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,778: INFO: model_training: Rank 0, Epoch 122, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,780: INFO: model_training: Rank 0, Epoch 123, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,781: INFO: model_training: Rank 0, Epoch 123, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,782: INFO: model_training: Rank 0, Epoch 123, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,782: INFO: model_training: Rank 0, Epoch 123, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,783: INFO: model_training: Rank 0, Epoch 123, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,785: INFO: model_training: Rank 0, Epoch 124, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,787: INFO: model_training: Rank 0, Epoch 124, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,788: INFO: model_training: Rank 0, Epoch 124, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,789: INFO: model_training: Rank 0, Epoch 124, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,790: INFO: model_training: Rank 0, Epoch 124, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,791: INFO: model_training: Rank 0, Epoch 125, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,793: INFO: model_training: Rank 0, Epoch 125, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,794: INFO: model_training: Rank 0, Epoch 125, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,795: INFO: model_training: Rank 0, Epoch 125, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,797: INFO: model_training: Rank 0, Epoch 125, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,799: INFO: model_training: Rank 0, Epoch 126, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,800: INFO: model_training: Rank 0, Epoch 126, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,801: INFO: model_training: Rank 0, Epoch 126, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,803: INFO: model_training: Rank 0, Epoch 126, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,804: INFO: model_training: Rank 0, Epoch 126, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,806: INFO: model_training: Rank 0, Epoch 127, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,807: INFO: model_training: Rank 0, Epoch 127, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,808: INFO: model_training: Rank 0, Epoch 127, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,809: INFO: model_training: Rank 0, Epoch 127, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,810: INFO: model_training: Rank 0, Epoch 127, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,812: INFO: model_training: Rank 0, Epoch 128, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,813: INFO: model_training: Rank 0, Epoch 128, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,814: INFO: model_training: Rank 0, Epoch 128, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,815: INFO: model_training: Rank 0, Epoch 128, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,816: INFO: model_training: Rank 0, Epoch 128, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,818: INFO: model_training: Rank 0, Epoch 129, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,819: INFO: model_training: Rank 0, Epoch 129, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,820: INFO: model_training: Rank 0, Epoch 129, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,822: INFO: model_training: Rank 0, Epoch 129, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,823: INFO: model_training: Rank 0, Epoch 129, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,824: INFO: model_training: Rank 0, Epoch 130, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,825: INFO: model_training: Rank 0, Epoch 130, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,827: INFO: model_training: Rank 0, Epoch 130, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,828: INFO: model_training: Rank 0, Epoch 130, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,829: INFO: model_training: Rank 0, Epoch 130, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,831: INFO: model_training: Rank 0, Epoch 131, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,832: INFO: model_training: Rank 0, Epoch 131, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,833: INFO: model_training: Rank 0, Epoch 131, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,834: INFO: model_training: Rank 0, Epoch 131, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,835: INFO: model_training: Rank 0, Epoch 131, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,837: INFO: model_training: Rank 0, Epoch 132, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,839: INFO: model_training: Rank 0, Epoch 132, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,840: INFO: model_training: Rank 0, Epoch 132, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,841: INFO: model_training: Rank 0, Epoch 132, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,842: INFO: model_training: Rank 0, Epoch 132, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,843: INFO: model_training: Rank 0, Epoch 133, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,845: INFO: model_training: Rank 0, Epoch 133, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,846: INFO: model_training: Rank 0, Epoch 133, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,848: INFO: model_training: Rank 0, Epoch 133, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,848: INFO: model_training: Rank 0, Epoch 133, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,850: INFO: model_training: Rank 0, Epoch 134, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,851: INFO: model_training: Rank 0, Epoch 134, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,852: INFO: model_training: Rank 0, Epoch 134, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,853: INFO: model_training: Rank 0, Epoch 134, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,855: INFO: model_training: Rank 0, Epoch 134, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,857: INFO: model_training: Rank 0, Epoch 135, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,858: INFO: model_training: Rank 0, Epoch 135, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,859: INFO: model_training: Rank 0, Epoch 135, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,860: INFO: model_training: Rank 0, Epoch 135, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,862: INFO: model_training: Rank 0, Epoch 135, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,863: INFO: model_training: Rank 0, Epoch 136, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,864: INFO: model_training: Rank 0, Epoch 136, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,865: INFO: model_training: Rank 0, Epoch 136, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,866: INFO: model_training: Rank 0, Epoch 136, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,868: INFO: model_training: Rank 0, Epoch 136, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,870: INFO: model_training: Rank 0, Epoch 137, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,871: INFO: model_training: Rank 0, Epoch 137, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,872: INFO: model_training: Rank 0, Epoch 137, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,874: INFO: model_training: Rank 0, Epoch 137, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,875: INFO: model_training: Rank 0, Epoch 137, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,877: INFO: model_training: Rank 0, Epoch 138, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,878: INFO: model_training: Rank 0, Epoch 138, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,879: INFO: model_training: Rank 0, Epoch 138, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,881: INFO: model_training: Rank 0, Epoch 138, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,882: INFO: model_training: Rank 0, Epoch 138, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,884: INFO: model_training: Rank 0, Epoch 139, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,885: INFO: model_training: Rank 0, Epoch 139, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,886: INFO: model_training: Rank 0, Epoch 139, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,888: INFO: model_training: Rank 0, Epoch 139, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,889: INFO: model_training: Rank 0, Epoch 139, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,890: INFO: model_training: Rank 0, Epoch 140, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,891: INFO: model_training: Rank 0, Epoch 140, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,893: INFO: model_training: Rank 0, Epoch 140, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,894: INFO: model_training: Rank 0, Epoch 140, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,895: INFO: model_training: Rank 0, Epoch 140, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,897: INFO: model_training: Rank 0, Epoch 141, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,898: INFO: model_training: Rank 0, Epoch 141, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,899: INFO: model_training: Rank 0, Epoch 141, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,900: INFO: model_training: Rank 0, Epoch 141, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,901: INFO: model_training: Rank 0, Epoch 141, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,903: INFO: model_training: Rank 0, Epoch 142, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,904: INFO: model_training: Rank 0, Epoch 142, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,905: INFO: model_training: Rank 0, Epoch 142, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,906: INFO: model_training: Rank 0, Epoch 142, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,908: INFO: model_training: Rank 0, Epoch 142, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,909: INFO: model_training: Rank 0, Epoch 143, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,911: INFO: model_training: Rank 0, Epoch 143, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,912: INFO: model_training: Rank 0, Epoch 143, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,913: INFO: model_training: Rank 0, Epoch 143, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,914: INFO: model_training: Rank 0, Epoch 143, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,916: INFO: model_training: Rank 0, Epoch 144, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,917: INFO: model_training: Rank 0, Epoch 144, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,919: INFO: model_training: Rank 0, Epoch 144, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,920: INFO: model_training: Rank 0, Epoch 144, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,921: INFO: model_training: Rank 0, Epoch 144, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,922: INFO: model_training: Rank 0, Epoch 145, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,924: INFO: model_training: Rank 0, Epoch 145, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,925: INFO: model_training: Rank 0, Epoch 145, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,926: INFO: model_training: Rank 0, Epoch 145, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,927: INFO: model_training: Rank 0, Epoch 145, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,929: INFO: model_training: Rank 0, Epoch 146, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,930: INFO: model_training: Rank 0, Epoch 146, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,931: INFO: model_training: Rank 0, Epoch 146, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,932: INFO: model_training: Rank 0, Epoch 146, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,933: INFO: model_training: Rank 0, Epoch 146, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,935: INFO: model_training: Rank 0, Epoch 147, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,937: INFO: model_training: Rank 0, Epoch 147, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,938: INFO: model_training: Rank 0, Epoch 147, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,939: INFO: model_training: Rank 0, Epoch 147, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,939: INFO: model_training: Rank 0, Epoch 147, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,941: INFO: model_training: Rank 0, Epoch 148, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,942: INFO: model_training: Rank 0, Epoch 148, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,943: INFO: model_training: Rank 0, Epoch 148, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,944: INFO: model_training: Rank 0, Epoch 148, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,945: INFO: model_training: Rank 0, Epoch 148, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,947: INFO: model_training: Rank 0, Epoch 149, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,948: INFO: model_training: Rank 0, Epoch 149, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,950: INFO: model_training: Rank 0, Epoch 149, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,951: INFO: model_training: Rank 0, Epoch 149, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,952: INFO: model_training: Rank 0, Epoch 149, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,954: INFO: model_training: Rank 0, Epoch 150, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,956: INFO: model_training: Rank 0, Epoch 150, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,957: INFO: model_training: Rank 0, Epoch 150, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,958: INFO: model_training: Rank 0, Epoch 150, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,959: INFO: model_training: Rank 0, Epoch 150, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,962: INFO: model_training: Rank 0, Epoch 151, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,963: INFO: model_training: Rank 0, Epoch 151, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,964: INFO: model_training: Rank 0, Epoch 151, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,965: INFO: model_training: Rank 0, Epoch 151, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,966: INFO: model_training: Rank 0, Epoch 151, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,968: INFO: model_training: Rank 0, Epoch 152, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,970: INFO: model_training: Rank 0, Epoch 152, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,971: INFO: model_training: Rank 0, Epoch 152, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,972: INFO: model_training: Rank 0, Epoch 152, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,973: INFO: model_training: Rank 0, Epoch 152, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,975: INFO: model_training: Rank 0, Epoch 153, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,976: INFO: model_training: Rank 0, Epoch 153, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,978: INFO: model_training: Rank 0, Epoch 153, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,979: INFO: model_training: Rank 0, Epoch 153, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,980: INFO: model_training: Rank 0, Epoch 153, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,982: INFO: model_training: Rank 0, Epoch 154, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,983: INFO: model_training: Rank 0, Epoch 154, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,984: INFO: model_training: Rank 0, Epoch 154, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,985: INFO: model_training: Rank 0, Epoch 154, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,986: INFO: model_training: Rank 0, Epoch 154, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,988: INFO: model_training: Rank 0, Epoch 155, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,989: INFO: model_training: Rank 0, Epoch 155, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,990: INFO: model_training: Rank 0, Epoch 155, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,991: INFO: model_training: Rank 0, Epoch 155, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,992: INFO: model_training: Rank 0, Epoch 155, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:43,994: INFO: model_training: Rank 0, Epoch 156, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:43,995: INFO: model_training: Rank 0, Epoch 156, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:43,996: INFO: model_training: Rank 0, Epoch 156, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:43,997: INFO: model_training: Rank 0, Epoch 156, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:43,999: INFO: model_training: Rank 0, Epoch 156, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,000: INFO: model_training: Rank 0, Epoch 157, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,001: INFO: model_training: Rank 0, Epoch 157, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,003: INFO: model_training: Rank 0, Epoch 157, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,005: INFO: model_training: Rank 0, Epoch 157, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,006: INFO: model_training: Rank 0, Epoch 157, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,007: INFO: model_training: Rank 0, Epoch 158, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,008: INFO: model_training: Rank 0, Epoch 158, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,009: INFO: model_training: Rank 0, Epoch 158, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,011: INFO: model_training: Rank 0, Epoch 158, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,012: INFO: model_training: Rank 0, Epoch 158, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,014: INFO: model_training: Rank 0, Epoch 159, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,015: INFO: model_training: Rank 0, Epoch 159, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,017: INFO: model_training: Rank 0, Epoch 159, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,018: INFO: model_training: Rank 0, Epoch 159, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,020: INFO: model_training: Rank 0, Epoch 159, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,022: INFO: model_training: Rank 0, Epoch 160, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,023: INFO: model_training: Rank 0, Epoch 160, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,024: INFO: model_training: Rank 0, Epoch 160, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,025: INFO: model_training: Rank 0, Epoch 160, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,027: INFO: model_training: Rank 0, Epoch 160, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,029: INFO: model_training: Rank 0, Epoch 161, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,030: INFO: model_training: Rank 0, Epoch 161, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,031: INFO: model_training: Rank 0, Epoch 161, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,032: INFO: model_training: Rank 0, Epoch 161, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,033: INFO: model_training: Rank 0, Epoch 161, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,034: INFO: model_training: Rank 0, Epoch 162, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,036: INFO: model_training: Rank 0, Epoch 162, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,037: INFO: model_training: Rank 0, Epoch 162, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,038: INFO: model_training: Rank 0, Epoch 162, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,040: INFO: model_training: Rank 0, Epoch 162, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,041: INFO: model_training: Rank 0, Epoch 163, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,043: INFO: model_training: Rank 0, Epoch 163, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,044: INFO: model_training: Rank 0, Epoch 163, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,046: INFO: model_training: Rank 0, Epoch 163, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,047: INFO: model_training: Rank 0, Epoch 163, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,048: INFO: model_training: Rank 0, Epoch 164, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,050: INFO: model_training: Rank 0, Epoch 164, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,051: INFO: model_training: Rank 0, Epoch 164, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,052: INFO: model_training: Rank 0, Epoch 164, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,054: INFO: model_training: Rank 0, Epoch 164, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,055: INFO: model_training: Rank 0, Epoch 165, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,056: INFO: model_training: Rank 0, Epoch 165, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,057: INFO: model_training: Rank 0, Epoch 165, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,059: INFO: model_training: Rank 0, Epoch 165, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,060: INFO: model_training: Rank 0, Epoch 165, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,062: INFO: model_training: Rank 0, Epoch 166, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,063: INFO: model_training: Rank 0, Epoch 166, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,065: INFO: model_training: Rank 0, Epoch 166, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,066: INFO: model_training: Rank 0, Epoch 166, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,067: INFO: model_training: Rank 0, Epoch 166, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,069: INFO: model_training: Rank 0, Epoch 167, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,071: INFO: model_training: Rank 0, Epoch 167, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,072: INFO: model_training: Rank 0, Epoch 167, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,073: INFO: model_training: Rank 0, Epoch 167, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,074: INFO: model_training: Rank 0, Epoch 167, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,076: INFO: model_training: Rank 0, Epoch 168, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,078: INFO: model_training: Rank 0, Epoch 168, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,079: INFO: model_training: Rank 0, Epoch 168, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,081: INFO: model_training: Rank 0, Epoch 168, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,082: INFO: model_training: Rank 0, Epoch 168, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,084: INFO: model_training: Rank 0, Epoch 169, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,085: INFO: model_training: Rank 0, Epoch 169, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,087: INFO: model_training: Rank 0, Epoch 169, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,088: INFO: model_training: Rank 0, Epoch 169, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,090: INFO: model_training: Rank 0, Epoch 169, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,092: INFO: model_training: Rank 0, Epoch 170, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,093: INFO: model_training: Rank 0, Epoch 170, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,095: INFO: model_training: Rank 0, Epoch 170, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,096: INFO: model_training: Rank 0, Epoch 170, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,098: INFO: model_training: Rank 0, Epoch 170, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,099: INFO: model_training: Rank 0, Epoch 171, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,101: INFO: model_training: Rank 0, Epoch 171, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,103: INFO: model_training: Rank 0, Epoch 171, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,105: INFO: model_training: Rank 0, Epoch 171, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,106: INFO: model_training: Rank 0, Epoch 171, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,108: INFO: model_training: Rank 0, Epoch 172, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,110: INFO: model_training: Rank 0, Epoch 172, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,111: INFO: model_training: Rank 0, Epoch 172, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,113: INFO: model_training: Rank 0, Epoch 172, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,114: INFO: model_training: Rank 0, Epoch 172, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,116: INFO: model_training: Rank 0, Epoch 173, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,118: INFO: model_training: Rank 0, Epoch 173, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,119: INFO: model_training: Rank 0, Epoch 173, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,121: INFO: model_training: Rank 0, Epoch 173, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,123: INFO: model_training: Rank 0, Epoch 173, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,125: INFO: model_training: Rank 0, Epoch 174, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,127: INFO: model_training: Rank 0, Epoch 174, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,129: INFO: model_training: Rank 0, Epoch 174, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,130: INFO: model_training: Rank 0, Epoch 174, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,131: INFO: model_training: Rank 0, Epoch 174, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,134: INFO: model_training: Rank 0, Epoch 175, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,135: INFO: model_training: Rank 0, Epoch 175, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,137: INFO: model_training: Rank 0, Epoch 175, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,138: INFO: model_training: Rank 0, Epoch 175, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,139: INFO: model_training: Rank 0, Epoch 175, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,141: INFO: model_training: Rank 0, Epoch 176, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,142: INFO: model_training: Rank 0, Epoch 176, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,144: INFO: model_training: Rank 0, Epoch 176, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,146: INFO: model_training: Rank 0, Epoch 176, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,147: INFO: model_training: Rank 0, Epoch 176, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,149: INFO: model_training: Rank 0, Epoch 177, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,150: INFO: model_training: Rank 0, Epoch 177, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,151: INFO: model_training: Rank 0, Epoch 177, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,152: INFO: model_training: Rank 0, Epoch 177, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,154: INFO: model_training: Rank 0, Epoch 177, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,156: INFO: model_training: Rank 0, Epoch 178, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,157: INFO: model_training: Rank 0, Epoch 178, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,158: INFO: model_training: Rank 0, Epoch 178, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,160: INFO: model_training: Rank 0, Epoch 178, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,161: INFO: model_training: Rank 0, Epoch 178, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,163: INFO: model_training: Rank 0, Epoch 179, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,164: INFO: model_training: Rank 0, Epoch 179, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,166: INFO: model_training: Rank 0, Epoch 179, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,167: INFO: model_training: Rank 0, Epoch 179, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,168: INFO: model_training: Rank 0, Epoch 179, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,170: INFO: model_training: Rank 0, Epoch 180, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,171: INFO: model_training: Rank 0, Epoch 180, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,173: INFO: model_training: Rank 0, Epoch 180, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,174: INFO: model_training: Rank 0, Epoch 180, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,175: INFO: model_training: Rank 0, Epoch 180, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,178: INFO: model_training: Rank 0, Epoch 181, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,179: INFO: model_training: Rank 0, Epoch 181, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,180: INFO: model_training: Rank 0, Epoch 181, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,182: INFO: model_training: Rank 0, Epoch 181, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,183: INFO: model_training: Rank 0, Epoch 181, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,184: INFO: model_training: Rank 0, Epoch 182, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,185: INFO: model_training: Rank 0, Epoch 182, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,187: INFO: model_training: Rank 0, Epoch 182, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,188: INFO: model_training: Rank 0, Epoch 182, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,189: INFO: model_training: Rank 0, Epoch 182, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,191: INFO: model_training: Rank 0, Epoch 183, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,192: INFO: model_training: Rank 0, Epoch 183, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,194: INFO: model_training: Rank 0, Epoch 183, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,195: INFO: model_training: Rank 0, Epoch 183, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,196: INFO: model_training: Rank 0, Epoch 183, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,198: INFO: model_training: Rank 0, Epoch 184, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,199: INFO: model_training: Rank 0, Epoch 184, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,200: INFO: model_training: Rank 0, Epoch 184, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,202: INFO: model_training: Rank 0, Epoch 184, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,203: INFO: model_training: Rank 0, Epoch 184, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,205: INFO: model_training: Rank 0, Epoch 185, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,206: INFO: model_training: Rank 0, Epoch 185, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,207: INFO: model_training: Rank 0, Epoch 185, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,209: INFO: model_training: Rank 0, Epoch 185, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,210: INFO: model_training: Rank 0, Epoch 185, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,212: INFO: model_training: Rank 0, Epoch 186, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,214: INFO: model_training: Rank 0, Epoch 186, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,215: INFO: model_training: Rank 0, Epoch 186, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,216: INFO: model_training: Rank 0, Epoch 186, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,217: INFO: model_training: Rank 0, Epoch 186, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,219: INFO: model_training: Rank 0, Epoch 187, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,220: INFO: model_training: Rank 0, Epoch 187, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,222: INFO: model_training: Rank 0, Epoch 187, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,223: INFO: model_training: Rank 0, Epoch 187, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,224: INFO: model_training: Rank 0, Epoch 187, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,226: INFO: model_training: Rank 0, Epoch 188, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,227: INFO: model_training: Rank 0, Epoch 188, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,229: INFO: model_training: Rank 0, Epoch 188, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,230: INFO: model_training: Rank 0, Epoch 188, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,231: INFO: model_training: Rank 0, Epoch 188, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,233: INFO: model_training: Rank 0, Epoch 189, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,234: INFO: model_training: Rank 0, Epoch 189, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,235: INFO: model_training: Rank 0, Epoch 189, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,237: INFO: model_training: Rank 0, Epoch 189, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,238: INFO: model_training: Rank 0, Epoch 189, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,240: INFO: model_training: Rank 0, Epoch 190, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,241: INFO: model_training: Rank 0, Epoch 190, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,242: INFO: model_training: Rank 0, Epoch 190, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,244: INFO: model_training: Rank 0, Epoch 190, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,245: INFO: model_training: Rank 0, Epoch 190, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,247: INFO: model_training: Rank 0, Epoch 191, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,248: INFO: model_training: Rank 0, Epoch 191, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,249: INFO: model_training: Rank 0, Epoch 191, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,251: INFO: model_training: Rank 0, Epoch 191, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,252: INFO: model_training: Rank 0, Epoch 191, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,254: INFO: model_training: Rank 0, Epoch 192, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,255: INFO: model_training: Rank 0, Epoch 192, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,257: INFO: model_training: Rank 0, Epoch 192, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,258: INFO: model_training: Rank 0, Epoch 192, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,259: INFO: model_training: Rank 0, Epoch 192, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,261: INFO: model_training: Rank 0, Epoch 193, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,263: INFO: model_training: Rank 0, Epoch 193, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,264: INFO: model_training: Rank 0, Epoch 193, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,265: INFO: model_training: Rank 0, Epoch 193, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,266: INFO: model_training: Rank 0, Epoch 193, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,267: INFO: model_training: Rank 0, Epoch 194, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,269: INFO: model_training: Rank 0, Epoch 194, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,270: INFO: model_training: Rank 0, Epoch 194, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,272: INFO: model_training: Rank 0, Epoch 194, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,273: INFO: model_training: Rank 0, Epoch 194, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,275: INFO: model_training: Rank 0, Epoch 195, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,276: INFO: model_training: Rank 0, Epoch 195, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,277: INFO: model_training: Rank 0, Epoch 195, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,279: INFO: model_training: Rank 0, Epoch 195, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,280: INFO: model_training: Rank 0, Epoch 195, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,282: INFO: model_training: Rank 0, Epoch 196, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,283: INFO: model_training: Rank 0, Epoch 196, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,284: INFO: model_training: Rank 0, Epoch 196, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,285: INFO: model_training: Rank 0, Epoch 196, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,287: INFO: model_training: Rank 0, Epoch 196, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,288: INFO: model_training: Rank 0, Epoch 197, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,290: INFO: model_training: Rank 0, Epoch 197, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,291: INFO: model_training: Rank 0, Epoch 197, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,291: INFO: model_training: Rank 0, Epoch 197, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,293: INFO: model_training: Rank 0, Epoch 197, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,295: INFO: model_training: Rank 0, Epoch 198, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,296: INFO: model_training: Rank 0, Epoch 198, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,298: INFO: model_training: Rank 0, Epoch 198, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,299: INFO: model_training: Rank 0, Epoch 198, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,300: INFO: model_training: Rank 0, Epoch 198, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,303: INFO: model_training: Rank 0, Epoch 199, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,304: INFO: model_training: Rank 0, Epoch 199, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,305: INFO: model_training: Rank 0, Epoch 199, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,306: INFO: model_training: Rank 0, Epoch 199, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,307: INFO: model_training: Rank 0, Epoch 199, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,309: INFO: model_training: Rank 0, Epoch 200, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,310: INFO: model_training: Rank 0, Epoch 200, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,312: INFO: model_training: Rank 0, Epoch 200, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,313: INFO: model_training: Rank 0, Epoch 200, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,314: INFO: model_training: Rank 0, Epoch 200, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,316: INFO: model_training: Rank 0, Epoch 201, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,317: INFO: model_training: Rank 0, Epoch 201, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,318: INFO: model_training: Rank 0, Epoch 201, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,319: INFO: model_training: Rank 0, Epoch 201, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,321: INFO: model_training: Rank 0, Epoch 201, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,322: INFO: model_training: Rank 0, Epoch 202, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,324: INFO: model_training: Rank 0, Epoch 202, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,325: INFO: model_training: Rank 0, Epoch 202, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,326: INFO: model_training: Rank 0, Epoch 202, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,327: INFO: model_training: Rank 0, Epoch 202, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,329: INFO: model_training: Rank 0, Epoch 203, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,330: INFO: model_training: Rank 0, Epoch 203, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,331: INFO: model_training: Rank 0, Epoch 203, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,333: INFO: model_training: Rank 0, Epoch 203, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,334: INFO: model_training: Rank 0, Epoch 203, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,336: INFO: model_training: Rank 0, Epoch 204, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,337: INFO: model_training: Rank 0, Epoch 204, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,338: INFO: model_training: Rank 0, Epoch 204, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,339: INFO: model_training: Rank 0, Epoch 204, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,340: INFO: model_training: Rank 0, Epoch 204, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,342: INFO: model_training: Rank 0, Epoch 205, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,343: INFO: model_training: Rank 0, Epoch 205, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,344: INFO: model_training: Rank 0, Epoch 205, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,345: INFO: model_training: Rank 0, Epoch 205, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,347: INFO: model_training: Rank 0, Epoch 205, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,349: INFO: model_training: Rank 0, Epoch 206, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,350: INFO: model_training: Rank 0, Epoch 206, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,351: INFO: model_training: Rank 0, Epoch 206, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,352: INFO: model_training: Rank 0, Epoch 206, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,353: INFO: model_training: Rank 0, Epoch 206, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,355: INFO: model_training: Rank 0, Epoch 207, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,356: INFO: model_training: Rank 0, Epoch 207, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,357: INFO: model_training: Rank 0, Epoch 207, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,359: INFO: model_training: Rank 0, Epoch 207, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,360: INFO: model_training: Rank 0, Epoch 207, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,362: INFO: model_training: Rank 0, Epoch 208, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,363: INFO: model_training: Rank 0, Epoch 208, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,365: INFO: model_training: Rank 0, Epoch 208, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,366: INFO: model_training: Rank 0, Epoch 208, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,367: INFO: model_training: Rank 0, Epoch 208, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,369: INFO: model_training: Rank 0, Epoch 209, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,370: INFO: model_training: Rank 0, Epoch 209, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,372: INFO: model_training: Rank 0, Epoch 209, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,373: INFO: model_training: Rank 0, Epoch 209, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,374: INFO: model_training: Rank 0, Epoch 209, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,376: INFO: model_training: Rank 0, Epoch 210, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,377: INFO: model_training: Rank 0, Epoch 210, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,378: INFO: model_training: Rank 0, Epoch 210, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,380: INFO: model_training: Rank 0, Epoch 210, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,381: INFO: model_training: Rank 0, Epoch 210, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,382: INFO: model_training: Rank 0, Epoch 211, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,384: INFO: model_training: Rank 0, Epoch 211, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,385: INFO: model_training: Rank 0, Epoch 211, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,386: INFO: model_training: Rank 0, Epoch 211, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,387: INFO: model_training: Rank 0, Epoch 211, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,389: INFO: model_training: Rank 0, Epoch 212, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,390: INFO: model_training: Rank 0, Epoch 212, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,392: INFO: model_training: Rank 0, Epoch 212, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,393: INFO: model_training: Rank 0, Epoch 212, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,394: INFO: model_training: Rank 0, Epoch 212, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,396: INFO: model_training: Rank 0, Epoch 213, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,397: INFO: model_training: Rank 0, Epoch 213, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,399: INFO: model_training: Rank 0, Epoch 213, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,400: INFO: model_training: Rank 0, Epoch 213, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,401: INFO: model_training: Rank 0, Epoch 213, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,403: INFO: model_training: Rank 0, Epoch 214, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,405: INFO: model_training: Rank 0, Epoch 214, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,406: INFO: model_training: Rank 0, Epoch 214, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,407: INFO: model_training: Rank 0, Epoch 214, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,408: INFO: model_training: Rank 0, Epoch 214, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,409: INFO: model_training: Rank 0, Epoch 215, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,411: INFO: model_training: Rank 0, Epoch 215, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,413: INFO: model_training: Rank 0, Epoch 215, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,414: INFO: model_training: Rank 0, Epoch 215, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,416: INFO: model_training: Rank 0, Epoch 215, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,417: INFO: model_training: Rank 0, Epoch 216, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,419: INFO: model_training: Rank 0, Epoch 216, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,420: INFO: model_training: Rank 0, Epoch 216, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,421: INFO: model_training: Rank 0, Epoch 216, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,422: INFO: model_training: Rank 0, Epoch 216, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,424: INFO: model_training: Rank 0, Epoch 217, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,425: INFO: model_training: Rank 0, Epoch 217, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,426: INFO: model_training: Rank 0, Epoch 217, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,428: INFO: model_training: Rank 0, Epoch 217, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,430: INFO: model_training: Rank 0, Epoch 217, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,431: INFO: model_training: Rank 0, Epoch 218, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,433: INFO: model_training: Rank 0, Epoch 218, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,434: INFO: model_training: Rank 0, Epoch 218, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,435: INFO: model_training: Rank 0, Epoch 218, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,436: INFO: model_training: Rank 0, Epoch 218, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,438: INFO: model_training: Rank 0, Epoch 219, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,439: INFO: model_training: Rank 0, Epoch 219, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,440: INFO: model_training: Rank 0, Epoch 219, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,441: INFO: model_training: Rank 0, Epoch 219, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,443: INFO: model_training: Rank 0, Epoch 219, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,445: INFO: model_training: Rank 0, Epoch 220, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,446: INFO: model_training: Rank 0, Epoch 220, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,447: INFO: model_training: Rank 0, Epoch 220, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,448: INFO: model_training: Rank 0, Epoch 220, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,449: INFO: model_training: Rank 0, Epoch 220, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,451: INFO: model_training: Rank 0, Epoch 221, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,452: INFO: model_training: Rank 0, Epoch 221, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,455: INFO: model_training: Rank 0, Epoch 221, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,456: INFO: model_training: Rank 0, Epoch 221, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,457: INFO: model_training: Rank 0, Epoch 221, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,458: INFO: model_training: Rank 0, Epoch 222, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,459: INFO: model_training: Rank 0, Epoch 222, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,461: INFO: model_training: Rank 0, Epoch 222, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,463: INFO: model_training: Rank 0, Epoch 222, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,464: INFO: model_training: Rank 0, Epoch 222, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,465: INFO: model_training: Rank 0, Epoch 223, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,466: INFO: model_training: Rank 0, Epoch 223, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,468: INFO: model_training: Rank 0, Epoch 223, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,469: INFO: model_training: Rank 0, Epoch 223, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,471: INFO: model_training: Rank 0, Epoch 223, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,473: INFO: model_training: Rank 0, Epoch 224, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,474: INFO: model_training: Rank 0, Epoch 224, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,475: INFO: model_training: Rank 0, Epoch 224, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,476: INFO: model_training: Rank 0, Epoch 224, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,477: INFO: model_training: Rank 0, Epoch 224, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,479: INFO: model_training: Rank 0, Epoch 225, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,481: INFO: model_training: Rank 0, Epoch 225, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,482: INFO: model_training: Rank 0, Epoch 225, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,483: INFO: model_training: Rank 0, Epoch 225, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,484: INFO: model_training: Rank 0, Epoch 225, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,487: INFO: model_training: Rank 0, Epoch 226, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,489: INFO: model_training: Rank 0, Epoch 226, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,490: INFO: model_training: Rank 0, Epoch 226, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,491: INFO: model_training: Rank 0, Epoch 226, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,492: INFO: model_training: Rank 0, Epoch 226, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,494: INFO: model_training: Rank 0, Epoch 227, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,496: INFO: model_training: Rank 0, Epoch 227, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,497: INFO: model_training: Rank 0, Epoch 227, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,498: INFO: model_training: Rank 0, Epoch 227, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,498: INFO: model_training: Rank 0, Epoch 227, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,500: INFO: model_training: Rank 0, Epoch 228, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,501: INFO: model_training: Rank 0, Epoch 228, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,503: INFO: model_training: Rank 0, Epoch 228, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,504: INFO: model_training: Rank 0, Epoch 228, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,505: INFO: model_training: Rank 0, Epoch 228, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,507: INFO: model_training: Rank 0, Epoch 229, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,508: INFO: model_training: Rank 0, Epoch 229, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,509: INFO: model_training: Rank 0, Epoch 229, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,511: INFO: model_training: Rank 0, Epoch 229, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,513: INFO: model_training: Rank 0, Epoch 229, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,527: INFO: model_training: Rank 0, Epoch 230, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,531: INFO: model_training: Rank 0, Epoch 230, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,536: INFO: model_training: Rank 0, Epoch 230, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,541: INFO: model_training: Rank 0, Epoch 230, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,542: INFO: model_training: Rank 0, Epoch 230, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,544: INFO: model_training: Rank 0, Epoch 231, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,547: INFO: model_training: Rank 0, Epoch 231, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,550: INFO: model_training: Rank 0, Epoch 231, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,553: INFO: model_training: Rank 0, Epoch 231, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,555: INFO: model_training: Rank 0, Epoch 231, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,557: INFO: model_training: Rank 0, Epoch 232, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,558: INFO: model_training: Rank 0, Epoch 232, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,559: INFO: model_training: Rank 0, Epoch 232, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,561: INFO: model_training: Rank 0, Epoch 232, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,562: INFO: model_training: Rank 0, Epoch 232, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,564: INFO: model_training: Rank 0, Epoch 233, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,565: INFO: model_training: Rank 0, Epoch 233, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,566: INFO: model_training: Rank 0, Epoch 233, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,567: INFO: model_training: Rank 0, Epoch 233, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,569: INFO: model_training: Rank 0, Epoch 233, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,571: INFO: model_training: Rank 0, Epoch 234, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,572: INFO: model_training: Rank 0, Epoch 234, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,574: INFO: model_training: Rank 0, Epoch 234, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,575: INFO: model_training: Rank 0, Epoch 234, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,576: INFO: model_training: Rank 0, Epoch 234, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,578: INFO: model_training: Rank 0, Epoch 235, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,579: INFO: model_training: Rank 0, Epoch 235, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,580: INFO: model_training: Rank 0, Epoch 235, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,582: INFO: model_training: Rank 0, Epoch 235, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,582: INFO: model_training: Rank 0, Epoch 235, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,584: INFO: model_training: Rank 0, Epoch 236, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,586: INFO: model_training: Rank 0, Epoch 236, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,587: INFO: model_training: Rank 0, Epoch 236, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,588: INFO: model_training: Rank 0, Epoch 236, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,589: INFO: model_training: Rank 0, Epoch 236, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,591: INFO: model_training: Rank 0, Epoch 237, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,592: INFO: model_training: Rank 0, Epoch 237, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,593: INFO: model_training: Rank 0, Epoch 237, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,595: INFO: model_training: Rank 0, Epoch 237, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,596: INFO: model_training: Rank 0, Epoch 237, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,598: INFO: model_training: Rank 0, Epoch 238, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,599: INFO: model_training: Rank 0, Epoch 238, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,600: INFO: model_training: Rank 0, Epoch 238, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,602: INFO: model_training: Rank 0, Epoch 238, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,603: INFO: model_training: Rank 0, Epoch 238, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,605: INFO: model_training: Rank 0, Epoch 239, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,606: INFO: model_training: Rank 0, Epoch 239, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,608: INFO: model_training: Rank 0, Epoch 239, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,609: INFO: model_training: Rank 0, Epoch 239, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,610: INFO: model_training: Rank 0, Epoch 239, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,612: INFO: model_training: Rank 0, Epoch 240, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,614: INFO: model_training: Rank 0, Epoch 240, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,615: INFO: model_training: Rank 0, Epoch 240, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,616: INFO: model_training: Rank 0, Epoch 240, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,617: INFO: model_training: Rank 0, Epoch 240, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,619: INFO: model_training: Rank 0, Epoch 241, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,620: INFO: model_training: Rank 0, Epoch 241, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,622: INFO: model_training: Rank 0, Epoch 241, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,623: INFO: model_training: Rank 0, Epoch 241, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,624: INFO: model_training: Rank 0, Epoch 241, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,625: INFO: model_training: Rank 0, Epoch 242, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,626: INFO: model_training: Rank 0, Epoch 242, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,628: INFO: model_training: Rank 0, Epoch 242, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,629: INFO: model_training: Rank 0, Epoch 242, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,630: INFO: model_training: Rank 0, Epoch 242, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,632: INFO: model_training: Rank 0, Epoch 243, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,633: INFO: model_training: Rank 0, Epoch 243, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,635: INFO: model_training: Rank 0, Epoch 243, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,636: INFO: model_training: Rank 0, Epoch 243, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,637: INFO: model_training: Rank 0, Epoch 243, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,639: INFO: model_training: Rank 0, Epoch 244, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,640: INFO: model_training: Rank 0, Epoch 244, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,641: INFO: model_training: Rank 0, Epoch 244, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,642: INFO: model_training: Rank 0, Epoch 244, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,644: INFO: model_training: Rank 0, Epoch 244, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,645: INFO: model_training: Rank 0, Epoch 245, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,646: INFO: model_training: Rank 0, Epoch 245, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,647: INFO: model_training: Rank 0, Epoch 245, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,649: INFO: model_training: Rank 0, Epoch 245, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,650: INFO: model_training: Rank 0, Epoch 245, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,651: INFO: model_training: Rank 0, Epoch 246, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,652: INFO: model_training: Rank 0, Epoch 246, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,654: INFO: model_training: Rank 0, Epoch 246, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,655: INFO: model_training: Rank 0, Epoch 246, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,656: INFO: model_training: Rank 0, Epoch 246, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,658: INFO: model_training: Rank 0, Epoch 247, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,659: INFO: model_training: Rank 0, Epoch 247, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,660: INFO: model_training: Rank 0, Epoch 247, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,662: INFO: model_training: Rank 0, Epoch 247, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,663: INFO: model_training: Rank 0, Epoch 247, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,664: INFO: model_training: Rank 0, Epoch 248, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,665: INFO: model_training: Rank 0, Epoch 248, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,666: INFO: model_training: Rank 0, Epoch 248, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,668: INFO: model_training: Rank 0, Epoch 248, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,669: INFO: model_training: Rank 0, Epoch 248, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,671: INFO: model_training: Rank 0, Epoch 249, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,672: INFO: model_training: Rank 0, Epoch 249, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,673: INFO: model_training: Rank 0, Epoch 249, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,674: INFO: model_training: Rank 0, Epoch 249, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,676: INFO: model_training: Rank 0, Epoch 249, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,678: INFO: model_training: Rank 0, Epoch 250, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,679: INFO: model_training: Rank 0, Epoch 250, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,680: INFO: model_training: Rank 0, Epoch 250, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,681: INFO: model_training: Rank 0, Epoch 250, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,683: INFO: model_training: Rank 0, Epoch 250, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,684: INFO: model_training: Rank 0, Epoch 251, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,685: INFO: model_training: Rank 0, Epoch 251, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,687: INFO: model_training: Rank 0, Epoch 251, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,688: INFO: model_training: Rank 0, Epoch 251, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,689: INFO: model_training: Rank 0, Epoch 251, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,690: INFO: model_training: Rank 0, Epoch 252, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,691: INFO: model_training: Rank 0, Epoch 252, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,693: INFO: model_training: Rank 0, Epoch 252, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,694: INFO: model_training: Rank 0, Epoch 252, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,696: INFO: model_training: Rank 0, Epoch 252, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,697: INFO: model_training: Rank 0, Epoch 253, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,698: INFO: model_training: Rank 0, Epoch 253, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,699: INFO: model_training: Rank 0, Epoch 253, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,701: INFO: model_training: Rank 0, Epoch 253, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,701: INFO: model_training: Rank 0, Epoch 253, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,703: INFO: model_training: Rank 0, Epoch 254, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,705: INFO: model_training: Rank 0, Epoch 254, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,706: INFO: model_training: Rank 0, Epoch 254, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,707: INFO: model_training: Rank 0, Epoch 254, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,708: INFO: model_training: Rank 0, Epoch 254, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,709: INFO: model_training: Rank 0, Epoch 255, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,711: INFO: model_training: Rank 0, Epoch 255, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,713: INFO: model_training: Rank 0, Epoch 255, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,714: INFO: model_training: Rank 0, Epoch 255, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,715: INFO: model_training: Rank 0, Epoch 255, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,716: INFO: model_training: Rank 0, Epoch 256, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,717: INFO: model_training: Rank 0, Epoch 256, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,719: INFO: model_training: Rank 0, Epoch 256, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,720: INFO: model_training: Rank 0, Epoch 256, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,721: INFO: model_training: Rank 0, Epoch 256, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,723: INFO: model_training: Rank 0, Epoch 257, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,724: INFO: model_training: Rank 0, Epoch 257, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,725: INFO: model_training: Rank 0, Epoch 257, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,726: INFO: model_training: Rank 0, Epoch 257, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,728: INFO: model_training: Rank 0, Epoch 257, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,729: INFO: model_training: Rank 0, Epoch 258, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,730: INFO: model_training: Rank 0, Epoch 258, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,731: INFO: model_training: Rank 0, Epoch 258, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,732: INFO: model_training: Rank 0, Epoch 258, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,734: INFO: model_training: Rank 0, Epoch 258, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,735: INFO: model_training: Rank 0, Epoch 259, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,737: INFO: model_training: Rank 0, Epoch 259, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,738: INFO: model_training: Rank 0, Epoch 259, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,739: INFO: model_training: Rank 0, Epoch 259, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,740: INFO: model_training: Rank 0, Epoch 259, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,742: INFO: model_training: Rank 0, Epoch 260, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,743: INFO: model_training: Rank 0, Epoch 260, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,745: INFO: model_training: Rank 0, Epoch 260, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,746: INFO: model_training: Rank 0, Epoch 260, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,747: INFO: model_training: Rank 0, Epoch 260, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,749: INFO: model_training: Rank 0, Epoch 261, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,750: INFO: model_training: Rank 0, Epoch 261, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,752: INFO: model_training: Rank 0, Epoch 261, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,753: INFO: model_training: Rank 0, Epoch 261, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,754: INFO: model_training: Rank 0, Epoch 261, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,756: INFO: model_training: Rank 0, Epoch 262, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,757: INFO: model_training: Rank 0, Epoch 262, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,759: INFO: model_training: Rank 0, Epoch 262, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,760: INFO: model_training: Rank 0, Epoch 262, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,761: INFO: model_training: Rank 0, Epoch 262, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,763: INFO: model_training: Rank 0, Epoch 263, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,764: INFO: model_training: Rank 0, Epoch 263, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,765: INFO: model_training: Rank 0, Epoch 263, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,766: INFO: model_training: Rank 0, Epoch 263, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,767: INFO: model_training: Rank 0, Epoch 263, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,769: INFO: model_training: Rank 0, Epoch 264, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,770: INFO: model_training: Rank 0, Epoch 264, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,771: INFO: model_training: Rank 0, Epoch 264, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,772: INFO: model_training: Rank 0, Epoch 264, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,773: INFO: model_training: Rank 0, Epoch 264, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,775: INFO: model_training: Rank 0, Epoch 265, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,776: INFO: model_training: Rank 0, Epoch 265, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,777: INFO: model_training: Rank 0, Epoch 265, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,779: INFO: model_training: Rank 0, Epoch 265, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,780: INFO: model_training: Rank 0, Epoch 265, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,782: INFO: model_training: Rank 0, Epoch 266, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,783: INFO: model_training: Rank 0, Epoch 266, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,784: INFO: model_training: Rank 0, Epoch 266, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,785: INFO: model_training: Rank 0, Epoch 266, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,787: INFO: model_training: Rank 0, Epoch 266, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,788: INFO: model_training: Rank 0, Epoch 267, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,789: INFO: model_training: Rank 0, Epoch 267, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,790: INFO: model_training: Rank 0, Epoch 267, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,792: INFO: model_training: Rank 0, Epoch 267, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,793: INFO: model_training: Rank 0, Epoch 267, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,795: INFO: model_training: Rank 0, Epoch 268, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,796: INFO: model_training: Rank 0, Epoch 268, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,797: INFO: model_training: Rank 0, Epoch 268, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,798: INFO: model_training: Rank 0, Epoch 268, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,799: INFO: model_training: Rank 0, Epoch 268, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,801: INFO: model_training: Rank 0, Epoch 269, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,802: INFO: model_training: Rank 0, Epoch 269, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,804: INFO: model_training: Rank 0, Epoch 269, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,805: INFO: model_training: Rank 0, Epoch 269, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,806: INFO: model_training: Rank 0, Epoch 269, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,808: INFO: model_training: Rank 0, Epoch 270, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,809: INFO: model_training: Rank 0, Epoch 270, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,810: INFO: model_training: Rank 0, Epoch 270, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,811: INFO: model_training: Rank 0, Epoch 270, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,813: INFO: model_training: Rank 0, Epoch 270, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,814: INFO: model_training: Rank 0, Epoch 271, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,815: INFO: model_training: Rank 0, Epoch 271, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,816: INFO: model_training: Rank 0, Epoch 271, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,818: INFO: model_training: Rank 0, Epoch 271, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,819: INFO: model_training: Rank 0, Epoch 271, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,820: INFO: model_training: Rank 0, Epoch 272, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,822: INFO: model_training: Rank 0, Epoch 272, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,823: INFO: model_training: Rank 0, Epoch 272, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,824: INFO: model_training: Rank 0, Epoch 272, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,825: INFO: model_training: Rank 0, Epoch 272, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,827: INFO: model_training: Rank 0, Epoch 273, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,828: INFO: model_training: Rank 0, Epoch 273, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,830: INFO: model_training: Rank 0, Epoch 273, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,831: INFO: model_training: Rank 0, Epoch 273, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,832: INFO: model_training: Rank 0, Epoch 273, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,833: INFO: model_training: Rank 0, Epoch 274, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,835: INFO: model_training: Rank 0, Epoch 274, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,836: INFO: model_training: Rank 0, Epoch 274, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,837: INFO: model_training: Rank 0, Epoch 274, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,839: INFO: model_training: Rank 0, Epoch 274, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,840: INFO: model_training: Rank 0, Epoch 275, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,841: INFO: model_training: Rank 0, Epoch 275, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,842: INFO: model_training: Rank 0, Epoch 275, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,844: INFO: model_training: Rank 0, Epoch 275, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,845: INFO: model_training: Rank 0, Epoch 275, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,847: INFO: model_training: Rank 0, Epoch 276, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,848: INFO: model_training: Rank 0, Epoch 276, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,849: INFO: model_training: Rank 0, Epoch 276, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,850: INFO: model_training: Rank 0, Epoch 276, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,851: INFO: model_training: Rank 0, Epoch 276, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,853: INFO: model_training: Rank 0, Epoch 277, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,854: INFO: model_training: Rank 0, Epoch 277, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,855: INFO: model_training: Rank 0, Epoch 277, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,856: INFO: model_training: Rank 0, Epoch 277, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,857: INFO: model_training: Rank 0, Epoch 277, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,859: INFO: model_training: Rank 0, Epoch 278, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,860: INFO: model_training: Rank 0, Epoch 278, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,862: INFO: model_training: Rank 0, Epoch 278, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,863: INFO: model_training: Rank 0, Epoch 278, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,864: INFO: model_training: Rank 0, Epoch 278, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,866: INFO: model_training: Rank 0, Epoch 279, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,867: INFO: model_training: Rank 0, Epoch 279, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,868: INFO: model_training: Rank 0, Epoch 279, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,870: INFO: model_training: Rank 0, Epoch 279, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,871: INFO: model_training: Rank 0, Epoch 279, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,872: INFO: model_training: Rank 0, Epoch 280, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,873: INFO: model_training: Rank 0, Epoch 280, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,874: INFO: model_training: Rank 0, Epoch 280, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,876: INFO: model_training: Rank 0, Epoch 280, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,877: INFO: model_training: Rank 0, Epoch 280, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,879: INFO: model_training: Rank 0, Epoch 281, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,880: INFO: model_training: Rank 0, Epoch 281, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,881: INFO: model_training: Rank 0, Epoch 281, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,882: INFO: model_training: Rank 0, Epoch 281, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,883: INFO: model_training: Rank 0, Epoch 281, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,885: INFO: model_training: Rank 0, Epoch 282, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,886: INFO: model_training: Rank 0, Epoch 282, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,887: INFO: model_training: Rank 0, Epoch 282, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,888: INFO: model_training: Rank 0, Epoch 282, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,890: INFO: model_training: Rank 0, Epoch 282, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,891: INFO: model_training: Rank 0, Epoch 283, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,892: INFO: model_training: Rank 0, Epoch 283, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,894: INFO: model_training: Rank 0, Epoch 283, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,895: INFO: model_training: Rank 0, Epoch 283, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,896: INFO: model_training: Rank 0, Epoch 283, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,898: INFO: model_training: Rank 0, Epoch 284, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,899: INFO: model_training: Rank 0, Epoch 284, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,899: INFO: model_training: Rank 0, Epoch 284, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,901: INFO: model_training: Rank 0, Epoch 284, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,902: INFO: model_training: Rank 0, Epoch 284, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,904: INFO: model_training: Rank 0, Epoch 285, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,905: INFO: model_training: Rank 0, Epoch 285, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,906: INFO: model_training: Rank 0, Epoch 285, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,907: INFO: model_training: Rank 0, Epoch 285, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,908: INFO: model_training: Rank 0, Epoch 285, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,910: INFO: model_training: Rank 0, Epoch 286, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,911: INFO: model_training: Rank 0, Epoch 286, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,912: INFO: model_training: Rank 0, Epoch 286, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,913: INFO: model_training: Rank 0, Epoch 286, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,915: INFO: model_training: Rank 0, Epoch 286, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,917: INFO: model_training: Rank 0, Epoch 287, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,918: INFO: model_training: Rank 0, Epoch 287, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,919: INFO: model_training: Rank 0, Epoch 287, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,920: INFO: model_training: Rank 0, Epoch 287, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,922: INFO: model_training: Rank 0, Epoch 287, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,924: INFO: model_training: Rank 0, Epoch 288, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,925: INFO: model_training: Rank 0, Epoch 288, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,926: INFO: model_training: Rank 0, Epoch 288, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,927: INFO: model_training: Rank 0, Epoch 288, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,928: INFO: model_training: Rank 0, Epoch 288, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,930: INFO: model_training: Rank 0, Epoch 289, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,931: INFO: model_training: Rank 0, Epoch 289, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,932: INFO: model_training: Rank 0, Epoch 289, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,933: INFO: model_training: Rank 0, Epoch 289, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,935: INFO: model_training: Rank 0, Epoch 289, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,937: INFO: model_training: Rank 0, Epoch 290, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,938: INFO: model_training: Rank 0, Epoch 290, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,939: INFO: model_training: Rank 0, Epoch 290, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,940: INFO: model_training: Rank 0, Epoch 290, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,941: INFO: model_training: Rank 0, Epoch 290, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,942: INFO: model_training: Rank 0, Epoch 291, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,944: INFO: model_training: Rank 0, Epoch 291, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,945: INFO: model_training: Rank 0, Epoch 291, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,946: INFO: model_training: Rank 0, Epoch 291, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,948: INFO: model_training: Rank 0, Epoch 291, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,949: INFO: model_training: Rank 0, Epoch 292, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,950: INFO: model_training: Rank 0, Epoch 292, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,951: INFO: model_training: Rank 0, Epoch 292, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,953: INFO: model_training: Rank 0, Epoch 292, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,954: INFO: model_training: Rank 0, Epoch 292, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,955: INFO: model_training: Rank 0, Epoch 293, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,956: INFO: model_training: Rank 0, Epoch 293, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,957: INFO: model_training: Rank 0, Epoch 293, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,959: INFO: model_training: Rank 0, Epoch 293, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,960: INFO: model_training: Rank 0, Epoch 293, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,961: INFO: model_training: Rank 0, Epoch 294, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,963: INFO: model_training: Rank 0, Epoch 294, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,964: INFO: model_training: Rank 0, Epoch 294, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,965: INFO: model_training: Rank 0, Epoch 294, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,966: INFO: model_training: Rank 0, Epoch 294, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,968: INFO: model_training: Rank 0, Epoch 295, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,969: INFO: model_training: Rank 0, Epoch 295, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,971: INFO: model_training: Rank 0, Epoch 295, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,972: INFO: model_training: Rank 0, Epoch 295, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,973: INFO: model_training: Rank 0, Epoch 295, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,975: INFO: model_training: Rank 0, Epoch 296, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,976: INFO: model_training: Rank 0, Epoch 296, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,977: INFO: model_training: Rank 0, Epoch 296, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,979: INFO: model_training: Rank 0, Epoch 296, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,980: INFO: model_training: Rank 0, Epoch 296, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,982: INFO: model_training: Rank 0, Epoch 297, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,983: INFO: model_training: Rank 0, Epoch 297, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,984: INFO: model_training: Rank 0, Epoch 297, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,986: INFO: model_training: Rank 0, Epoch 297, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,987: INFO: model_training: Rank 0, Epoch 297, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,989: INFO: model_training: Rank 0, Epoch 298, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,990: INFO: model_training: Rank 0, Epoch 298, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,991: INFO: model_training: Rank 0, Epoch 298, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,992: INFO: model_training: Rank 0, Epoch 298, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:44,994: INFO: model_training: Rank 0, Epoch 298, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:44,996: INFO: model_training: Rank 0, Epoch 299, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:44,997: INFO: model_training: Rank 0, Epoch 299, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:44,998: INFO: model_training: Rank 0, Epoch 299, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:44,999: INFO: model_training: Rank 0, Epoch 299, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,000: INFO: model_training: Rank 0, Epoch 299, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,002: INFO: model_training: Rank 0, Epoch 300, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,004: INFO: model_training: Rank 0, Epoch 300, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,005: INFO: model_training: Rank 0, Epoch 300, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,006: INFO: model_training: Rank 0, Epoch 300, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,008: INFO: model_training: Rank 0, Epoch 300, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,009: INFO: model_training: Rank 0, Epoch 301, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,011: INFO: model_training: Rank 0, Epoch 301, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,012: INFO: model_training: Rank 0, Epoch 301, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,014: INFO: model_training: Rank 0, Epoch 301, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,015: INFO: model_training: Rank 0, Epoch 301, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,017: INFO: model_training: Rank 0, Epoch 302, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,018: INFO: model_training: Rank 0, Epoch 302, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,020: INFO: model_training: Rank 0, Epoch 302, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,021: INFO: model_training: Rank 0, Epoch 302, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,024: INFO: model_training: Rank 0, Epoch 302, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,025: INFO: model_training: Rank 0, Epoch 303, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,027: INFO: model_training: Rank 0, Epoch 303, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,029: INFO: model_training: Rank 0, Epoch 303, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,030: INFO: model_training: Rank 0, Epoch 303, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,032: INFO: model_training: Rank 0, Epoch 303, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,034: INFO: model_training: Rank 0, Epoch 304, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,036: INFO: model_training: Rank 0, Epoch 304, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,038: INFO: model_training: Rank 0, Epoch 304, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,039: INFO: model_training: Rank 0, Epoch 304, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,040: INFO: model_training: Rank 0, Epoch 304, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,042: INFO: model_training: Rank 0, Epoch 305, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,043: INFO: model_training: Rank 0, Epoch 305, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,044: INFO: model_training: Rank 0, Epoch 305, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,046: INFO: model_training: Rank 0, Epoch 305, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,047: INFO: model_training: Rank 0, Epoch 305, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,049: INFO: model_training: Rank 0, Epoch 306, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,050: INFO: model_training: Rank 0, Epoch 306, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,052: INFO: model_training: Rank 0, Epoch 306, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,054: INFO: model_training: Rank 0, Epoch 306, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,055: INFO: model_training: Rank 0, Epoch 306, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,057: INFO: model_training: Rank 0, Epoch 307, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,057: INFO: model_training: Rank 0, Epoch 307, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,059: INFO: model_training: Rank 0, Epoch 307, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,061: INFO: model_training: Rank 0, Epoch 307, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,062: INFO: model_training: Rank 0, Epoch 307, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,064: INFO: model_training: Rank 0, Epoch 308, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,065: INFO: model_training: Rank 0, Epoch 308, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,066: INFO: model_training: Rank 0, Epoch 308, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,068: INFO: model_training: Rank 0, Epoch 308, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,069: INFO: model_training: Rank 0, Epoch 308, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,071: INFO: model_training: Rank 0, Epoch 309, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,072: INFO: model_training: Rank 0, Epoch 309, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,073: INFO: model_training: Rank 0, Epoch 309, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,075: INFO: model_training: Rank 0, Epoch 309, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,076: INFO: model_training: Rank 0, Epoch 309, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,078: INFO: model_training: Rank 0, Epoch 310, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,079: INFO: model_training: Rank 0, Epoch 310, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,080: INFO: model_training: Rank 0, Epoch 310, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,082: INFO: model_training: Rank 0, Epoch 310, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,083: INFO: model_training: Rank 0, Epoch 310, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,084: INFO: model_training: Rank 0, Epoch 311, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,086: INFO: model_training: Rank 0, Epoch 311, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,088: INFO: model_training: Rank 0, Epoch 311, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,089: INFO: model_training: Rank 0, Epoch 311, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,090: INFO: model_training: Rank 0, Epoch 311, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,091: INFO: model_training: Rank 0, Epoch 312, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,093: INFO: model_training: Rank 0, Epoch 312, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,095: INFO: model_training: Rank 0, Epoch 312, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,096: INFO: model_training: Rank 0, Epoch 312, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,097: INFO: model_training: Rank 0, Epoch 312, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,099: INFO: model_training: Rank 0, Epoch 313, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,100: INFO: model_training: Rank 0, Epoch 313, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,102: INFO: model_training: Rank 0, Epoch 313, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,103: INFO: model_training: Rank 0, Epoch 313, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,105: INFO: model_training: Rank 0, Epoch 313, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,106: INFO: model_training: Rank 0, Epoch 314, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,107: INFO: model_training: Rank 0, Epoch 314, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,108: INFO: model_training: Rank 0, Epoch 314, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,110: INFO: model_training: Rank 0, Epoch 314, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,112: INFO: model_training: Rank 0, Epoch 314, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,113: INFO: model_training: Rank 0, Epoch 315, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,115: INFO: model_training: Rank 0, Epoch 315, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,116: INFO: model_training: Rank 0, Epoch 315, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,117: INFO: model_training: Rank 0, Epoch 315, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,118: INFO: model_training: Rank 0, Epoch 315, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,120: INFO: model_training: Rank 0, Epoch 316, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,121: INFO: model_training: Rank 0, Epoch 316, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,122: INFO: model_training: Rank 0, Epoch 316, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,124: INFO: model_training: Rank 0, Epoch 316, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,125: INFO: model_training: Rank 0, Epoch 316, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,126: INFO: model_training: Rank 0, Epoch 317, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,128: INFO: model_training: Rank 0, Epoch 317, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,129: INFO: model_training: Rank 0, Epoch 317, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,130: INFO: model_training: Rank 0, Epoch 317, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,132: INFO: model_training: Rank 0, Epoch 317, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,134: INFO: model_training: Rank 0, Epoch 318, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,135: INFO: model_training: Rank 0, Epoch 318, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,137: INFO: model_training: Rank 0, Epoch 318, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,138: INFO: model_training: Rank 0, Epoch 318, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,139: INFO: model_training: Rank 0, Epoch 318, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,141: INFO: model_training: Rank 0, Epoch 319, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,142: INFO: model_training: Rank 0, Epoch 319, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,144: INFO: model_training: Rank 0, Epoch 319, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,145: INFO: model_training: Rank 0, Epoch 319, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,147: INFO: model_training: Rank 0, Epoch 319, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,148: INFO: model_training: Rank 0, Epoch 320, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,149: INFO: model_training: Rank 0, Epoch 320, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,150: INFO: model_training: Rank 0, Epoch 320, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,152: INFO: model_training: Rank 0, Epoch 320, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,153: INFO: model_training: Rank 0, Epoch 320, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,155: INFO: model_training: Rank 0, Epoch 321, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,156: INFO: model_training: Rank 0, Epoch 321, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,157: INFO: model_training: Rank 0, Epoch 321, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,159: INFO: model_training: Rank 0, Epoch 321, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,160: INFO: model_training: Rank 0, Epoch 321, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,162: INFO: model_training: Rank 0, Epoch 322, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,164: INFO: model_training: Rank 0, Epoch 322, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,165: INFO: model_training: Rank 0, Epoch 322, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,166: INFO: model_training: Rank 0, Epoch 322, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,167: INFO: model_training: Rank 0, Epoch 322, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,168: INFO: model_training: Rank 0, Epoch 323, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,170: INFO: model_training: Rank 0, Epoch 323, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,171: INFO: model_training: Rank 0, Epoch 323, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,172: INFO: model_training: Rank 0, Epoch 323, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,173: INFO: model_training: Rank 0, Epoch 323, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,175: INFO: model_training: Rank 0, Epoch 324, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,176: INFO: model_training: Rank 0, Epoch 324, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,178: INFO: model_training: Rank 0, Epoch 324, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,180: INFO: model_training: Rank 0, Epoch 324, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,181: INFO: model_training: Rank 0, Epoch 324, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,182: INFO: model_training: Rank 0, Epoch 325, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,184: INFO: model_training: Rank 0, Epoch 325, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,185: INFO: model_training: Rank 0, Epoch 325, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,187: INFO: model_training: Rank 0, Epoch 325, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,188: INFO: model_training: Rank 0, Epoch 325, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,190: INFO: model_training: Rank 0, Epoch 326, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,191: INFO: model_training: Rank 0, Epoch 326, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,193: INFO: model_training: Rank 0, Epoch 326, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,194: INFO: model_training: Rank 0, Epoch 326, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,197: INFO: model_training: Rank 0, Epoch 326, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,199: INFO: model_training: Rank 0, Epoch 327, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,202: INFO: model_training: Rank 0, Epoch 327, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,203: INFO: model_training: Rank 0, Epoch 327, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,205: INFO: model_training: Rank 0, Epoch 327, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,207: INFO: model_training: Rank 0, Epoch 327, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,209: INFO: model_training: Rank 0, Epoch 328, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,211: INFO: model_training: Rank 0, Epoch 328, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,213: INFO: model_training: Rank 0, Epoch 328, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,215: INFO: model_training: Rank 0, Epoch 328, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,216: INFO: model_training: Rank 0, Epoch 328, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,218: INFO: model_training: Rank 0, Epoch 329, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,220: INFO: model_training: Rank 0, Epoch 329, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,222: INFO: model_training: Rank 0, Epoch 329, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,225: INFO: model_training: Rank 0, Epoch 329, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,227: INFO: model_training: Rank 0, Epoch 329, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,239: INFO: model_training: Rank 0, Epoch 330, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,244: INFO: model_training: Rank 0, Epoch 330, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,257: INFO: model_training: Rank 0, Epoch 330, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,264: INFO: model_training: Rank 0, Epoch 330, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,266: INFO: model_training: Rank 0, Epoch 330, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,269: INFO: model_training: Rank 0, Epoch 331, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,271: INFO: model_training: Rank 0, Epoch 331, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,273: INFO: model_training: Rank 0, Epoch 331, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,275: INFO: model_training: Rank 0, Epoch 331, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,277: INFO: model_training: Rank 0, Epoch 331, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,279: INFO: model_training: Rank 0, Epoch 332, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,281: INFO: model_training: Rank 0, Epoch 332, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,284: INFO: model_training: Rank 0, Epoch 332, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,287: INFO: model_training: Rank 0, Epoch 332, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,292: INFO: model_training: Rank 0, Epoch 332, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,294: INFO: model_training: Rank 0, Epoch 333, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,296: INFO: model_training: Rank 0, Epoch 333, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,298: INFO: model_training: Rank 0, Epoch 333, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,300: INFO: model_training: Rank 0, Epoch 333, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,302: INFO: model_training: Rank 0, Epoch 333, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,304: INFO: model_training: Rank 0, Epoch 334, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,305: INFO: model_training: Rank 0, Epoch 334, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,307: INFO: model_training: Rank 0, Epoch 334, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,309: INFO: model_training: Rank 0, Epoch 334, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,311: INFO: model_training: Rank 0, Epoch 334, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,320: INFO: model_training: Rank 0, Epoch 335, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,329: INFO: model_training: Rank 0, Epoch 335, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,334: INFO: model_training: Rank 0, Epoch 335, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,346: INFO: model_training: Rank 0, Epoch 335, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,349: INFO: model_training: Rank 0, Epoch 335, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,351: INFO: model_training: Rank 0, Epoch 336, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,354: INFO: model_training: Rank 0, Epoch 336, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,357: INFO: model_training: Rank 0, Epoch 336, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,359: INFO: model_training: Rank 0, Epoch 336, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,360: INFO: model_training: Rank 0, Epoch 336, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,362: INFO: model_training: Rank 0, Epoch 337, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,364: INFO: model_training: Rank 0, Epoch 337, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,366: INFO: model_training: Rank 0, Epoch 337, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,367: INFO: model_training: Rank 0, Epoch 337, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,369: INFO: model_training: Rank 0, Epoch 337, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,371: INFO: model_training: Rank 0, Epoch 338, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,373: INFO: model_training: Rank 0, Epoch 338, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,376: INFO: model_training: Rank 0, Epoch 338, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,378: INFO: model_training: Rank 0, Epoch 338, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,380: INFO: model_training: Rank 0, Epoch 338, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,383: INFO: model_training: Rank 0, Epoch 339, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,385: INFO: model_training: Rank 0, Epoch 339, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,387: INFO: model_training: Rank 0, Epoch 339, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,388: INFO: model_training: Rank 0, Epoch 339, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,390: INFO: model_training: Rank 0, Epoch 339, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,393: INFO: model_training: Rank 0, Epoch 340, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,395: INFO: model_training: Rank 0, Epoch 340, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,399: INFO: model_training: Rank 0, Epoch 340, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,402: INFO: model_training: Rank 0, Epoch 340, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,403: INFO: model_training: Rank 0, Epoch 340, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,405: INFO: model_training: Rank 0, Epoch 341, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,407: INFO: model_training: Rank 0, Epoch 341, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,409: INFO: model_training: Rank 0, Epoch 341, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,411: INFO: model_training: Rank 0, Epoch 341, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,413: INFO: model_training: Rank 0, Epoch 341, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,416: INFO: model_training: Rank 0, Epoch 342, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,418: INFO: model_training: Rank 0, Epoch 342, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,420: INFO: model_training: Rank 0, Epoch 342, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,422: INFO: model_training: Rank 0, Epoch 342, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,424: INFO: model_training: Rank 0, Epoch 342, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,431: INFO: model_training: Rank 0, Epoch 343, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,435: INFO: model_training: Rank 0, Epoch 343, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,443: INFO: model_training: Rank 0, Epoch 343, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,448: INFO: model_training: Rank 0, Epoch 343, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,460: INFO: model_training: Rank 0, Epoch 343, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,463: INFO: model_training: Rank 0, Epoch 344, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,465: INFO: model_training: Rank 0, Epoch 344, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,467: INFO: model_training: Rank 0, Epoch 344, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,469: INFO: model_training: Rank 0, Epoch 344, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,471: INFO: model_training: Rank 0, Epoch 344, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,473: INFO: model_training: Rank 0, Epoch 345, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,474: INFO: model_training: Rank 0, Epoch 345, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,477: INFO: model_training: Rank 0, Epoch 345, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,479: INFO: model_training: Rank 0, Epoch 345, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,482: INFO: model_training: Rank 0, Epoch 345, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,487: INFO: model_training: Rank 0, Epoch 346, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,489: INFO: model_training: Rank 0, Epoch 346, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,491: INFO: model_training: Rank 0, Epoch 346, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,493: INFO: model_training: Rank 0, Epoch 346, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,495: INFO: model_training: Rank 0, Epoch 346, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,497: INFO: model_training: Rank 0, Epoch 347, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,499: INFO: model_training: Rank 0, Epoch 347, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,501: INFO: model_training: Rank 0, Epoch 347, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,503: INFO: model_training: Rank 0, Epoch 347, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,506: INFO: model_training: Rank 0, Epoch 347, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,508: INFO: model_training: Rank 0, Epoch 348, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,510: INFO: model_training: Rank 0, Epoch 348, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,518: INFO: model_training: Rank 0, Epoch 348, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,525: INFO: model_training: Rank 0, Epoch 348, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,541: INFO: model_training: Rank 0, Epoch 348, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,545: INFO: model_training: Rank 0, Epoch 349, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,548: INFO: model_training: Rank 0, Epoch 349, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,551: INFO: model_training: Rank 0, Epoch 349, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,553: INFO: model_training: Rank 0, Epoch 349, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,557: INFO: model_training: Rank 0, Epoch 349, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,561: INFO: model_training: Rank 0, Epoch 350, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,563: INFO: model_training: Rank 0, Epoch 350, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,565: INFO: model_training: Rank 0, Epoch 350, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,567: INFO: model_training: Rank 0, Epoch 350, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,570: INFO: model_training: Rank 0, Epoch 350, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,572: INFO: model_training: Rank 0, Epoch 351, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,573: INFO: model_training: Rank 0, Epoch 351, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,574: INFO: model_training: Rank 0, Epoch 351, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,575: INFO: model_training: Rank 0, Epoch 351, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,577: INFO: model_training: Rank 0, Epoch 351, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,579: INFO: model_training: Rank 0, Epoch 352, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,580: INFO: model_training: Rank 0, Epoch 352, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,582: INFO: model_training: Rank 0, Epoch 352, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,583: INFO: model_training: Rank 0, Epoch 352, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,584: INFO: model_training: Rank 0, Epoch 352, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,586: INFO: model_training: Rank 0, Epoch 353, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,588: INFO: model_training: Rank 0, Epoch 353, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,589: INFO: model_training: Rank 0, Epoch 353, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,591: INFO: model_training: Rank 0, Epoch 353, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,592: INFO: model_training: Rank 0, Epoch 353, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,594: INFO: model_training: Rank 0, Epoch 354, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,595: INFO: model_training: Rank 0, Epoch 354, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,597: INFO: model_training: Rank 0, Epoch 354, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,598: INFO: model_training: Rank 0, Epoch 354, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,599: INFO: model_training: Rank 0, Epoch 354, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,600: INFO: model_training: Rank 0, Epoch 355, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,602: INFO: model_training: Rank 0, Epoch 355, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,604: INFO: model_training: Rank 0, Epoch 355, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,605: INFO: model_training: Rank 0, Epoch 355, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,606: INFO: model_training: Rank 0, Epoch 355, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,608: INFO: model_training: Rank 0, Epoch 356, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,609: INFO: model_training: Rank 0, Epoch 356, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,611: INFO: model_training: Rank 0, Epoch 356, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,612: INFO: model_training: Rank 0, Epoch 356, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,613: INFO: model_training: Rank 0, Epoch 356, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,615: INFO: model_training: Rank 0, Epoch 357, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,616: INFO: model_training: Rank 0, Epoch 357, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,617: INFO: model_training: Rank 0, Epoch 357, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,618: INFO: model_training: Rank 0, Epoch 357, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,620: INFO: model_training: Rank 0, Epoch 357, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,622: INFO: model_training: Rank 0, Epoch 358, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,623: INFO: model_training: Rank 0, Epoch 358, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,624: INFO: model_training: Rank 0, Epoch 358, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,625: INFO: model_training: Rank 0, Epoch 358, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,626: INFO: model_training: Rank 0, Epoch 358, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,628: INFO: model_training: Rank 0, Epoch 359, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,630: INFO: model_training: Rank 0, Epoch 359, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,631: INFO: model_training: Rank 0, Epoch 359, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,632: INFO: model_training: Rank 0, Epoch 359, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,633: INFO: model_training: Rank 0, Epoch 359, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,635: INFO: model_training: Rank 0, Epoch 360, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,637: INFO: model_training: Rank 0, Epoch 360, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,638: INFO: model_training: Rank 0, Epoch 360, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,639: INFO: model_training: Rank 0, Epoch 360, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,640: INFO: model_training: Rank 0, Epoch 360, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,641: INFO: model_training: Rank 0, Epoch 361, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,643: INFO: model_training: Rank 0, Epoch 361, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,644: INFO: model_training: Rank 0, Epoch 361, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,645: INFO: model_training: Rank 0, Epoch 361, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,647: INFO: model_training: Rank 0, Epoch 361, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,648: INFO: model_training: Rank 0, Epoch 362, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,649: INFO: model_training: Rank 0, Epoch 362, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,651: INFO: model_training: Rank 0, Epoch 362, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,652: INFO: model_training: Rank 0, Epoch 362, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,654: INFO: model_training: Rank 0, Epoch 362, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,655: INFO: model_training: Rank 0, Epoch 363, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,656: INFO: model_training: Rank 0, Epoch 363, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,657: INFO: model_training: Rank 0, Epoch 363, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,659: INFO: model_training: Rank 0, Epoch 363, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,660: INFO: model_training: Rank 0, Epoch 363, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,662: INFO: model_training: Rank 0, Epoch 364, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,663: INFO: model_training: Rank 0, Epoch 364, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,665: INFO: model_training: Rank 0, Epoch 364, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,666: INFO: model_training: Rank 0, Epoch 364, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,667: INFO: model_training: Rank 0, Epoch 364, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,668: INFO: model_training: Rank 0, Epoch 365, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,670: INFO: model_training: Rank 0, Epoch 365, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,671: INFO: model_training: Rank 0, Epoch 365, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,672: INFO: model_training: Rank 0, Epoch 365, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,673: INFO: model_training: Rank 0, Epoch 365, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,675: INFO: model_training: Rank 0, Epoch 366, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,677: INFO: model_training: Rank 0, Epoch 366, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,678: INFO: model_training: Rank 0, Epoch 366, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,679: INFO: model_training: Rank 0, Epoch 366, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,680: INFO: model_training: Rank 0, Epoch 366, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,682: INFO: model_training: Rank 0, Epoch 367, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,683: INFO: model_training: Rank 0, Epoch 367, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,684: INFO: model_training: Rank 0, Epoch 367, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,685: INFO: model_training: Rank 0, Epoch 367, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,687: INFO: model_training: Rank 0, Epoch 367, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,688: INFO: model_training: Rank 0, Epoch 368, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,690: INFO: model_training: Rank 0, Epoch 368, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,691: INFO: model_training: Rank 0, Epoch 368, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,692: INFO: model_training: Rank 0, Epoch 368, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,693: INFO: model_training: Rank 0, Epoch 368, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,695: INFO: model_training: Rank 0, Epoch 369, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,697: INFO: model_training: Rank 0, Epoch 369, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,698: INFO: model_training: Rank 0, Epoch 369, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,698: INFO: model_training: Rank 0, Epoch 369, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,700: INFO: model_training: Rank 0, Epoch 369, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,701: INFO: model_training: Rank 0, Epoch 370, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,703: INFO: model_training: Rank 0, Epoch 370, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,704: INFO: model_training: Rank 0, Epoch 370, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,706: INFO: model_training: Rank 0, Epoch 370, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,707: INFO: model_training: Rank 0, Epoch 370, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,708: INFO: model_training: Rank 0, Epoch 371, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,709: INFO: model_training: Rank 0, Epoch 371, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,710: INFO: model_training: Rank 0, Epoch 371, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,712: INFO: model_training: Rank 0, Epoch 371, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,713: INFO: model_training: Rank 0, Epoch 371, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,715: INFO: model_training: Rank 0, Epoch 372, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,716: INFO: model_training: Rank 0, Epoch 372, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,717: INFO: model_training: Rank 0, Epoch 372, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,719: INFO: model_training: Rank 0, Epoch 372, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,720: INFO: model_training: Rank 0, Epoch 372, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,722: INFO: model_training: Rank 0, Epoch 373, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,723: INFO: model_training: Rank 0, Epoch 373, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,724: INFO: model_training: Rank 0, Epoch 373, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,725: INFO: model_training: Rank 0, Epoch 373, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,727: INFO: model_training: Rank 0, Epoch 373, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,729: INFO: model_training: Rank 0, Epoch 374, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,730: INFO: model_training: Rank 0, Epoch 374, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,731: INFO: model_training: Rank 0, Epoch 374, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,732: INFO: model_training: Rank 0, Epoch 374, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,733: INFO: model_training: Rank 0, Epoch 374, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,734: INFO: model_training: Rank 0, Epoch 375, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,736: INFO: model_training: Rank 0, Epoch 375, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,737: INFO: model_training: Rank 0, Epoch 375, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,738: INFO: model_training: Rank 0, Epoch 375, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,739: INFO: model_training: Rank 0, Epoch 375, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,741: INFO: model_training: Rank 0, Epoch 376, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,742: INFO: model_training: Rank 0, Epoch 376, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,743: INFO: model_training: Rank 0, Epoch 376, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,744: INFO: model_training: Rank 0, Epoch 376, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,746: INFO: model_training: Rank 0, Epoch 376, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,747: INFO: model_training: Rank 0, Epoch 377, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,748: INFO: model_training: Rank 0, Epoch 377, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,749: INFO: model_training: Rank 0, Epoch 377, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,750: INFO: model_training: Rank 0, Epoch 377, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,752: INFO: model_training: Rank 0, Epoch 377, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,754: INFO: model_training: Rank 0, Epoch 378, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,755: INFO: model_training: Rank 0, Epoch 378, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,756: INFO: model_training: Rank 0, Epoch 378, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,757: INFO: model_training: Rank 0, Epoch 378, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,758: INFO: model_training: Rank 0, Epoch 378, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,759: INFO: model_training: Rank 0, Epoch 379, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,761: INFO: model_training: Rank 0, Epoch 379, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,763: INFO: model_training: Rank 0, Epoch 379, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,765: INFO: model_training: Rank 0, Epoch 379, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,766: INFO: model_training: Rank 0, Epoch 379, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,767: INFO: model_training: Rank 0, Epoch 380, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,769: INFO: model_training: Rank 0, Epoch 380, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,770: INFO: model_training: Rank 0, Epoch 380, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,772: INFO: model_training: Rank 0, Epoch 380, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,773: INFO: model_training: Rank 0, Epoch 380, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,774: INFO: model_training: Rank 0, Epoch 381, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,775: INFO: model_training: Rank 0, Epoch 381, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,777: INFO: model_training: Rank 0, Epoch 381, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,779: INFO: model_training: Rank 0, Epoch 381, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,780: INFO: model_training: Rank 0, Epoch 381, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,781: INFO: model_training: Rank 0, Epoch 382, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,783: INFO: model_training: Rank 0, Epoch 382, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,784: INFO: model_training: Rank 0, Epoch 382, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,786: INFO: model_training: Rank 0, Epoch 382, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,787: INFO: model_training: Rank 0, Epoch 382, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,789: INFO: model_training: Rank 0, Epoch 383, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,790: INFO: model_training: Rank 0, Epoch 383, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,791: INFO: model_training: Rank 0, Epoch 383, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,792: INFO: model_training: Rank 0, Epoch 383, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,794: INFO: model_training: Rank 0, Epoch 383, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,796: INFO: model_training: Rank 0, Epoch 384, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,797: INFO: model_training: Rank 0, Epoch 384, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,799: INFO: model_training: Rank 0, Epoch 384, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,800: INFO: model_training: Rank 0, Epoch 384, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,801: INFO: model_training: Rank 0, Epoch 384, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,804: INFO: model_training: Rank 0, Epoch 385, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,805: INFO: model_training: Rank 0, Epoch 385, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,806: INFO: model_training: Rank 0, Epoch 385, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,807: INFO: model_training: Rank 0, Epoch 385, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,809: INFO: model_training: Rank 0, Epoch 385, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,811: INFO: model_training: Rank 0, Epoch 386, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,812: INFO: model_training: Rank 0, Epoch 386, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,814: INFO: model_training: Rank 0, Epoch 386, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,815: INFO: model_training: Rank 0, Epoch 386, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,816: INFO: model_training: Rank 0, Epoch 386, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,818: INFO: model_training: Rank 0, Epoch 387, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,820: INFO: model_training: Rank 0, Epoch 387, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,821: INFO: model_training: Rank 0, Epoch 387, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,822: INFO: model_training: Rank 0, Epoch 387, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,823: INFO: model_training: Rank 0, Epoch 387, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,825: INFO: model_training: Rank 0, Epoch 388, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,826: INFO: model_training: Rank 0, Epoch 388, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,828: INFO: model_training: Rank 0, Epoch 388, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,829: INFO: model_training: Rank 0, Epoch 388, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,830: INFO: model_training: Rank 0, Epoch 388, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,832: INFO: model_training: Rank 0, Epoch 389, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,834: INFO: model_training: Rank 0, Epoch 389, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,835: INFO: model_training: Rank 0, Epoch 389, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,837: INFO: model_training: Rank 0, Epoch 389, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,839: INFO: model_training: Rank 0, Epoch 389, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,840: INFO: model_training: Rank 0, Epoch 390, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,841: INFO: model_training: Rank 0, Epoch 390, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,843: INFO: model_training: Rank 0, Epoch 390, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,845: INFO: model_training: Rank 0, Epoch 390, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,846: INFO: model_training: Rank 0, Epoch 390, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,848: INFO: model_training: Rank 0, Epoch 391, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,849: INFO: model_training: Rank 0, Epoch 391, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,850: INFO: model_training: Rank 0, Epoch 391, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,851: INFO: model_training: Rank 0, Epoch 391, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,853: INFO: model_training: Rank 0, Epoch 391, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,855: INFO: model_training: Rank 0, Epoch 392, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,856: INFO: model_training: Rank 0, Epoch 392, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,857: INFO: model_training: Rank 0, Epoch 392, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,859: INFO: model_training: Rank 0, Epoch 392, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,861: INFO: model_training: Rank 0, Epoch 392, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,863: INFO: model_training: Rank 0, Epoch 393, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,864: INFO: model_training: Rank 0, Epoch 393, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,866: INFO: model_training: Rank 0, Epoch 393, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,867: INFO: model_training: Rank 0, Epoch 393, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,868: INFO: model_training: Rank 0, Epoch 393, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,870: INFO: model_training: Rank 0, Epoch 394, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,872: INFO: model_training: Rank 0, Epoch 394, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,873: INFO: model_training: Rank 0, Epoch 394, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,874: INFO: model_training: Rank 0, Epoch 394, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,876: INFO: model_training: Rank 0, Epoch 394, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,878: INFO: model_training: Rank 0, Epoch 395, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,879: INFO: model_training: Rank 0, Epoch 395, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,881: INFO: model_training: Rank 0, Epoch 395, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,882: INFO: model_training: Rank 0, Epoch 395, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,884: INFO: model_training: Rank 0, Epoch 395, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,887: INFO: model_training: Rank 0, Epoch 396, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,888: INFO: model_training: Rank 0, Epoch 396, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,889: INFO: model_training: Rank 0, Epoch 396, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,890: INFO: model_training: Rank 0, Epoch 396, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,891: INFO: model_training: Rank 0, Epoch 396, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,893: INFO: model_training: Rank 0, Epoch 397, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,894: INFO: model_training: Rank 0, Epoch 397, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,896: INFO: model_training: Rank 0, Epoch 397, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,897: INFO: model_training: Rank 0, Epoch 397, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,898: INFO: model_training: Rank 0, Epoch 397, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,900: INFO: model_training: Rank 0, Epoch 398, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,902: INFO: model_training: Rank 0, Epoch 398, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,903: INFO: model_training: Rank 0, Epoch 398, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,904: INFO: model_training: Rank 0, Epoch 398, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,906: INFO: model_training: Rank 0, Epoch 398, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,907: INFO: model_training: Rank 0, Epoch 399, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,908: INFO: model_training: Rank 0, Epoch 399, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,910: INFO: model_training: Rank 0, Epoch 399, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,911: INFO: model_training: Rank 0, Epoch 399, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,913: INFO: model_training: Rank 0, Epoch 399, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,915: INFO: model_training: Rank 0, Epoch 400, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,916: INFO: model_training: Rank 0, Epoch 400, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,917: INFO: model_training: Rank 0, Epoch 400, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,918: INFO: model_training: Rank 0, Epoch 400, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,920: INFO: model_training: Rank 0, Epoch 400, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,921: INFO: model_training: Rank 0, Epoch 401, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,922: INFO: model_training: Rank 0, Epoch 401, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,923: INFO: model_training: Rank 0, Epoch 401, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,925: INFO: model_training: Rank 0, Epoch 401, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,926: INFO: model_training: Rank 0, Epoch 401, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,928: INFO: model_training: Rank 0, Epoch 402, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,929: INFO: model_training: Rank 0, Epoch 402, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,930: INFO: model_training: Rank 0, Epoch 402, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,932: INFO: model_training: Rank 0, Epoch 402, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,933: INFO: model_training: Rank 0, Epoch 402, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,935: INFO: model_training: Rank 0, Epoch 403, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,936: INFO: model_training: Rank 0, Epoch 403, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,938: INFO: model_training: Rank 0, Epoch 403, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,939: INFO: model_training: Rank 0, Epoch 403, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,940: INFO: model_training: Rank 0, Epoch 403, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,941: INFO: model_training: Rank 0, Epoch 404, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,943: INFO: model_training: Rank 0, Epoch 404, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,945: INFO: model_training: Rank 0, Epoch 404, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,946: INFO: model_training: Rank 0, Epoch 404, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,947: INFO: model_training: Rank 0, Epoch 404, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,949: INFO: model_training: Rank 0, Epoch 405, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,950: INFO: model_training: Rank 0, Epoch 405, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,951: INFO: model_training: Rank 0, Epoch 405, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,953: INFO: model_training: Rank 0, Epoch 405, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,954: INFO: model_training: Rank 0, Epoch 405, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,956: INFO: model_training: Rank 0, Epoch 406, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,957: INFO: model_training: Rank 0, Epoch 406, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,958: INFO: model_training: Rank 0, Epoch 406, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,960: INFO: model_training: Rank 0, Epoch 406, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,962: INFO: model_training: Rank 0, Epoch 406, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,963: INFO: model_training: Rank 0, Epoch 407, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,965: INFO: model_training: Rank 0, Epoch 407, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,966: INFO: model_training: Rank 0, Epoch 407, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,967: INFO: model_training: Rank 0, Epoch 407, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,968: INFO: model_training: Rank 0, Epoch 407, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,970: INFO: model_training: Rank 0, Epoch 408, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,971: INFO: model_training: Rank 0, Epoch 408, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,972: INFO: model_training: Rank 0, Epoch 408, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,974: INFO: model_training: Rank 0, Epoch 408, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,975: INFO: model_training: Rank 0, Epoch 408, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,976: INFO: model_training: Rank 0, Epoch 409, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,978: INFO: model_training: Rank 0, Epoch 409, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,980: INFO: model_training: Rank 0, Epoch 409, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,981: INFO: model_training: Rank 0, Epoch 409, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,982: INFO: model_training: Rank 0, Epoch 409, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,984: INFO: model_training: Rank 0, Epoch 410, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,985: INFO: model_training: Rank 0, Epoch 410, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,987: INFO: model_training: Rank 0, Epoch 410, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,988: INFO: model_training: Rank 0, Epoch 410, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,990: INFO: model_training: Rank 0, Epoch 410, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,991: INFO: model_training: Rank 0, Epoch 411, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:45,992: INFO: model_training: Rank 0, Epoch 411, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:45,994: INFO: model_training: Rank 0, Epoch 411, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:45,995: INFO: model_training: Rank 0, Epoch 411, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:45,997: INFO: model_training: Rank 0, Epoch 411, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:45,998: INFO: model_training: Rank 0, Epoch 412, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,000: INFO: model_training: Rank 0, Epoch 412, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,001: INFO: model_training: Rank 0, Epoch 412, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,003: INFO: model_training: Rank 0, Epoch 412, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,004: INFO: model_training: Rank 0, Epoch 412, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,006: INFO: model_training: Rank 0, Epoch 413, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,007: INFO: model_training: Rank 0, Epoch 413, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,009: INFO: model_training: Rank 0, Epoch 413, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,010: INFO: model_training: Rank 0, Epoch 413, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,011: INFO: model_training: Rank 0, Epoch 413, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,013: INFO: model_training: Rank 0, Epoch 414, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,014: INFO: model_training: Rank 0, Epoch 414, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,015: INFO: model_training: Rank 0, Epoch 414, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,017: INFO: model_training: Rank 0, Epoch 414, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,018: INFO: model_training: Rank 0, Epoch 414, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,020: INFO: model_training: Rank 0, Epoch 415, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,022: INFO: model_training: Rank 0, Epoch 415, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,023: INFO: model_training: Rank 0, Epoch 415, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,024: INFO: model_training: Rank 0, Epoch 415, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,025: INFO: model_training: Rank 0, Epoch 415, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,026: INFO: model_training: Rank 0, Epoch 416, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,028: INFO: model_training: Rank 0, Epoch 416, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,030: INFO: model_training: Rank 0, Epoch 416, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,031: INFO: model_training: Rank 0, Epoch 416, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,032: INFO: model_training: Rank 0, Epoch 416, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,033: INFO: model_training: Rank 0, Epoch 417, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,035: INFO: model_training: Rank 0, Epoch 417, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,037: INFO: model_training: Rank 0, Epoch 417, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,038: INFO: model_training: Rank 0, Epoch 417, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,039: INFO: model_training: Rank 0, Epoch 417, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,044: INFO: model_training: Rank 0, Epoch 418, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,052: INFO: model_training: Rank 0, Epoch 418, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,067: INFO: model_training: Rank 0, Epoch 418, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,069: INFO: model_training: Rank 0, Epoch 418, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,071: INFO: model_training: Rank 0, Epoch 418, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,078: INFO: model_training: Rank 0, Epoch 419, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,080: INFO: model_training: Rank 0, Epoch 419, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,081: INFO: model_training: Rank 0, Epoch 419, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,082: INFO: model_training: Rank 0, Epoch 419, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,084: INFO: model_training: Rank 0, Epoch 419, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,086: INFO: model_training: Rank 0, Epoch 420, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,087: INFO: model_training: Rank 0, Epoch 420, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,088: INFO: model_training: Rank 0, Epoch 420, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,090: INFO: model_training: Rank 0, Epoch 420, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,091: INFO: model_training: Rank 0, Epoch 420, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,093: INFO: model_training: Rank 0, Epoch 421, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,094: INFO: model_training: Rank 0, Epoch 421, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,096: INFO: model_training: Rank 0, Epoch 421, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,097: INFO: model_training: Rank 0, Epoch 421, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,098: INFO: model_training: Rank 0, Epoch 421, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,099: INFO: model_training: Rank 0, Epoch 422, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,100: INFO: model_training: Rank 0, Epoch 422, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,102: INFO: model_training: Rank 0, Epoch 422, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,103: INFO: model_training: Rank 0, Epoch 422, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,105: INFO: model_training: Rank 0, Epoch 422, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,106: INFO: model_training: Rank 0, Epoch 423, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,108: INFO: model_training: Rank 0, Epoch 423, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,109: INFO: model_training: Rank 0, Epoch 423, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,110: INFO: model_training: Rank 0, Epoch 423, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,112: INFO: model_training: Rank 0, Epoch 423, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,113: INFO: model_training: Rank 0, Epoch 424, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,115: INFO: model_training: Rank 0, Epoch 424, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,116: INFO: model_training: Rank 0, Epoch 424, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,117: INFO: model_training: Rank 0, Epoch 424, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,118: INFO: model_training: Rank 0, Epoch 424, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,120: INFO: model_training: Rank 0, Epoch 425, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,121: INFO: model_training: Rank 0, Epoch 425, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,123: INFO: model_training: Rank 0, Epoch 425, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,124: INFO: model_training: Rank 0, Epoch 425, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,125: INFO: model_training: Rank 0, Epoch 425, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,126: INFO: model_training: Rank 0, Epoch 426, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,128: INFO: model_training: Rank 0, Epoch 426, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,129: INFO: model_training: Rank 0, Epoch 426, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,130: INFO: model_training: Rank 0, Epoch 426, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,131: INFO: model_training: Rank 0, Epoch 426, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,132: INFO: model_training: Rank 0, Epoch 427, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,134: INFO: model_training: Rank 0, Epoch 427, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,135: INFO: model_training: Rank 0, Epoch 427, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,136: INFO: model_training: Rank 0, Epoch 427, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,137: INFO: model_training: Rank 0, Epoch 427, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,139: INFO: model_training: Rank 0, Epoch 428, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,140: INFO: model_training: Rank 0, Epoch 428, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,141: INFO: model_training: Rank 0, Epoch 428, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,142: INFO: model_training: Rank 0, Epoch 428, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,144: INFO: model_training: Rank 0, Epoch 428, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,146: INFO: model_training: Rank 0, Epoch 429, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,147: INFO: model_training: Rank 0, Epoch 429, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,148: INFO: model_training: Rank 0, Epoch 429, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,149: INFO: model_training: Rank 0, Epoch 429, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,150: INFO: model_training: Rank 0, Epoch 429, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,151: INFO: model_training: Rank 0, Epoch 430, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,153: INFO: model_training: Rank 0, Epoch 430, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,154: INFO: model_training: Rank 0, Epoch 430, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,155: INFO: model_training: Rank 0, Epoch 430, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,156: INFO: model_training: Rank 0, Epoch 430, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,157: INFO: model_training: Rank 0, Epoch 431, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,158: INFO: model_training: Rank 0, Epoch 431, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,159: INFO: model_training: Rank 0, Epoch 431, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,161: INFO: model_training: Rank 0, Epoch 431, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,163: INFO: model_training: Rank 0, Epoch 431, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,164: INFO: model_training: Rank 0, Epoch 432, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,165: INFO: model_training: Rank 0, Epoch 432, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,166: INFO: model_training: Rank 0, Epoch 432, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,167: INFO: model_training: Rank 0, Epoch 432, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,169: INFO: model_training: Rank 0, Epoch 432, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,171: INFO: model_training: Rank 0, Epoch 433, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,172: INFO: model_training: Rank 0, Epoch 433, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,173: INFO: model_training: Rank 0, Epoch 433, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,174: INFO: model_training: Rank 0, Epoch 433, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,175: INFO: model_training: Rank 0, Epoch 433, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,176: INFO: model_training: Rank 0, Epoch 434, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,178: INFO: model_training: Rank 0, Epoch 434, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,179: INFO: model_training: Rank 0, Epoch 434, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,180: INFO: model_training: Rank 0, Epoch 434, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,181: INFO: model_training: Rank 0, Epoch 434, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,183: INFO: model_training: Rank 0, Epoch 435, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,184: INFO: model_training: Rank 0, Epoch 435, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,185: INFO: model_training: Rank 0, Epoch 435, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,186: INFO: model_training: Rank 0, Epoch 435, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,188: INFO: model_training: Rank 0, Epoch 435, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,189: INFO: model_training: Rank 0, Epoch 436, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,190: INFO: model_training: Rank 0, Epoch 436, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,191: INFO: model_training: Rank 0, Epoch 436, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,192: INFO: model_training: Rank 0, Epoch 436, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,194: INFO: model_training: Rank 0, Epoch 436, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,195: INFO: model_training: Rank 0, Epoch 437, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,197: INFO: model_training: Rank 0, Epoch 437, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,198: INFO: model_training: Rank 0, Epoch 437, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,199: INFO: model_training: Rank 0, Epoch 437, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,200: INFO: model_training: Rank 0, Epoch 437, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,202: INFO: model_training: Rank 0, Epoch 438, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,203: INFO: model_training: Rank 0, Epoch 438, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,204: INFO: model_training: Rank 0, Epoch 438, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,205: INFO: model_training: Rank 0, Epoch 438, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,206: INFO: model_training: Rank 0, Epoch 438, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,208: INFO: model_training: Rank 0, Epoch 439, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,209: INFO: model_training: Rank 0, Epoch 439, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,210: INFO: model_training: Rank 0, Epoch 439, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,211: INFO: model_training: Rank 0, Epoch 439, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,213: INFO: model_training: Rank 0, Epoch 439, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,214: INFO: model_training: Rank 0, Epoch 440, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,215: INFO: model_training: Rank 0, Epoch 440, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,216: INFO: model_training: Rank 0, Epoch 440, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,217: INFO: model_training: Rank 0, Epoch 440, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,219: INFO: model_training: Rank 0, Epoch 440, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,220: INFO: model_training: Rank 0, Epoch 441, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,221: INFO: model_training: Rank 0, Epoch 441, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,223: INFO: model_training: Rank 0, Epoch 441, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,224: INFO: model_training: Rank 0, Epoch 441, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,225: INFO: model_training: Rank 0, Epoch 441, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,226: INFO: model_training: Rank 0, Epoch 442, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,228: INFO: model_training: Rank 0, Epoch 442, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,229: INFO: model_training: Rank 0, Epoch 442, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,230: INFO: model_training: Rank 0, Epoch 442, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,231: INFO: model_training: Rank 0, Epoch 442, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,232: INFO: model_training: Rank 0, Epoch 443, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,234: INFO: model_training: Rank 0, Epoch 443, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,235: INFO: model_training: Rank 0, Epoch 443, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,237: INFO: model_training: Rank 0, Epoch 443, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,238: INFO: model_training: Rank 0, Epoch 443, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,239: INFO: model_training: Rank 0, Epoch 444, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,240: INFO: model_training: Rank 0, Epoch 444, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,241: INFO: model_training: Rank 0, Epoch 444, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,244: INFO: model_training: Rank 0, Epoch 444, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,245: INFO: model_training: Rank 0, Epoch 444, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,247: INFO: model_training: Rank 0, Epoch 445, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,248: INFO: model_training: Rank 0, Epoch 445, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,249: INFO: model_training: Rank 0, Epoch 445, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,250: INFO: model_training: Rank 0, Epoch 445, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,251: INFO: model_training: Rank 0, Epoch 445, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,253: INFO: model_training: Rank 0, Epoch 446, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,254: INFO: model_training: Rank 0, Epoch 446, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,255: INFO: model_training: Rank 0, Epoch 446, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,256: INFO: model_training: Rank 0, Epoch 446, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,257: INFO: model_training: Rank 0, Epoch 446, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,259: INFO: model_training: Rank 0, Epoch 447, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,260: INFO: model_training: Rank 0, Epoch 447, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,261: INFO: model_training: Rank 0, Epoch 447, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,262: INFO: model_training: Rank 0, Epoch 447, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,264: INFO: model_training: Rank 0, Epoch 447, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,265: INFO: model_training: Rank 0, Epoch 448, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,266: INFO: model_training: Rank 0, Epoch 448, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,267: INFO: model_training: Rank 0, Epoch 448, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,269: INFO: model_training: Rank 0, Epoch 448, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,270: INFO: model_training: Rank 0, Epoch 448, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,272: INFO: model_training: Rank 0, Epoch 449, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,273: INFO: model_training: Rank 0, Epoch 449, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,274: INFO: model_training: Rank 0, Epoch 449, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,275: INFO: model_training: Rank 0, Epoch 449, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,276: INFO: model_training: Rank 0, Epoch 449, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,278: INFO: model_training: Rank 0, Epoch 450, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,279: INFO: model_training: Rank 0, Epoch 450, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,281: INFO: model_training: Rank 0, Epoch 450, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,282: INFO: model_training: Rank 0, Epoch 450, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,284: INFO: model_training: Rank 0, Epoch 450, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,286: INFO: model_training: Rank 0, Epoch 451, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,287: INFO: model_training: Rank 0, Epoch 451, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,288: INFO: model_training: Rank 0, Epoch 451, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,289: INFO: model_training: Rank 0, Epoch 451, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,291: INFO: model_training: Rank 0, Epoch 451, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,292: INFO: model_training: Rank 0, Epoch 452, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,294: INFO: model_training: Rank 0, Epoch 452, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,295: INFO: model_training: Rank 0, Epoch 452, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,297: INFO: model_training: Rank 0, Epoch 452, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,298: INFO: model_training: Rank 0, Epoch 452, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,299: INFO: model_training: Rank 0, Epoch 453, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,300: INFO: model_training: Rank 0, Epoch 453, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,302: INFO: model_training: Rank 0, Epoch 453, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,303: INFO: model_training: Rank 0, Epoch 453, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,304: INFO: model_training: Rank 0, Epoch 453, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,306: INFO: model_training: Rank 0, Epoch 454, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,307: INFO: model_training: Rank 0, Epoch 454, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,308: INFO: model_training: Rank 0, Epoch 454, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,309: INFO: model_training: Rank 0, Epoch 454, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,310: INFO: model_training: Rank 0, Epoch 454, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,312: INFO: model_training: Rank 0, Epoch 455, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,313: INFO: model_training: Rank 0, Epoch 455, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,314: INFO: model_training: Rank 0, Epoch 455, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,315: INFO: model_training: Rank 0, Epoch 455, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,316: INFO: model_training: Rank 0, Epoch 455, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,318: INFO: model_training: Rank 0, Epoch 456, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,320: INFO: model_training: Rank 0, Epoch 456, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,321: INFO: model_training: Rank 0, Epoch 456, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,323: INFO: model_training: Rank 0, Epoch 456, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,324: INFO: model_training: Rank 0, Epoch 456, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,326: INFO: model_training: Rank 0, Epoch 457, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,327: INFO: model_training: Rank 0, Epoch 457, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,329: INFO: model_training: Rank 0, Epoch 457, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,330: INFO: model_training: Rank 0, Epoch 457, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,331: INFO: model_training: Rank 0, Epoch 457, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,332: INFO: model_training: Rank 0, Epoch 458, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,333: INFO: model_training: Rank 0, Epoch 458, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,335: INFO: model_training: Rank 0, Epoch 458, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,336: INFO: model_training: Rank 0, Epoch 458, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,338: INFO: model_training: Rank 0, Epoch 458, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,339: INFO: model_training: Rank 0, Epoch 459, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,341: INFO: model_training: Rank 0, Epoch 459, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,342: INFO: model_training: Rank 0, Epoch 459, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,343: INFO: model_training: Rank 0, Epoch 459, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,344: INFO: model_training: Rank 0, Epoch 459, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,346: INFO: model_training: Rank 0, Epoch 460, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,347: INFO: model_training: Rank 0, Epoch 460, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,348: INFO: model_training: Rank 0, Epoch 460, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,349: INFO: model_training: Rank 0, Epoch 460, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,351: INFO: model_training: Rank 0, Epoch 460, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,353: INFO: model_training: Rank 0, Epoch 461, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,354: INFO: model_training: Rank 0, Epoch 461, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,355: INFO: model_training: Rank 0, Epoch 461, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,356: INFO: model_training: Rank 0, Epoch 461, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,357: INFO: model_training: Rank 0, Epoch 461, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,359: INFO: model_training: Rank 0, Epoch 462, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,360: INFO: model_training: Rank 0, Epoch 462, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,362: INFO: model_training: Rank 0, Epoch 462, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,364: INFO: model_training: Rank 0, Epoch 462, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,365: INFO: model_training: Rank 0, Epoch 462, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,366: INFO: model_training: Rank 0, Epoch 463, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,367: INFO: model_training: Rank 0, Epoch 463, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,369: INFO: model_training: Rank 0, Epoch 463, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,370: INFO: model_training: Rank 0, Epoch 463, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,371: INFO: model_training: Rank 0, Epoch 463, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,372: INFO: model_training: Rank 0, Epoch 464, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,373: INFO: model_training: Rank 0, Epoch 464, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,375: INFO: model_training: Rank 0, Epoch 464, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,376: INFO: model_training: Rank 0, Epoch 464, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,378: INFO: model_training: Rank 0, Epoch 464, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,379: INFO: model_training: Rank 0, Epoch 465, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,380: INFO: model_training: Rank 0, Epoch 465, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,381: INFO: model_training: Rank 0, Epoch 465, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,382: INFO: model_training: Rank 0, Epoch 465, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,384: INFO: model_training: Rank 0, Epoch 465, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,385: INFO: model_training: Rank 0, Epoch 466, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,387: INFO: model_training: Rank 0, Epoch 466, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,388: INFO: model_training: Rank 0, Epoch 466, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,389: INFO: model_training: Rank 0, Epoch 466, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,390: INFO: model_training: Rank 0, Epoch 466, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,391: INFO: model_training: Rank 0, Epoch 467, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,393: INFO: model_training: Rank 0, Epoch 467, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,394: INFO: model_training: Rank 0, Epoch 467, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,396: INFO: model_training: Rank 0, Epoch 467, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,397: INFO: model_training: Rank 0, Epoch 467, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,398: INFO: model_training: Rank 0, Epoch 468, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,400: INFO: model_training: Rank 0, Epoch 468, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,401: INFO: model_training: Rank 0, Epoch 468, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,403: INFO: model_training: Rank 0, Epoch 468, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,404: INFO: model_training: Rank 0, Epoch 468, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,406: INFO: model_training: Rank 0, Epoch 469, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,407: INFO: model_training: Rank 0, Epoch 469, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,408: INFO: model_training: Rank 0, Epoch 469, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,409: INFO: model_training: Rank 0, Epoch 469, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,411: INFO: model_training: Rank 0, Epoch 469, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,413: INFO: model_training: Rank 0, Epoch 470, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,414: INFO: model_training: Rank 0, Epoch 470, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,415: INFO: model_training: Rank 0, Epoch 470, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,416: INFO: model_training: Rank 0, Epoch 470, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,417: INFO: model_training: Rank 0, Epoch 470, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,419: INFO: model_training: Rank 0, Epoch 471, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,420: INFO: model_training: Rank 0, Epoch 471, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,421: INFO: model_training: Rank 0, Epoch 471, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,423: INFO: model_training: Rank 0, Epoch 471, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,424: INFO: model_training: Rank 0, Epoch 471, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,426: INFO: model_training: Rank 0, Epoch 472, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,427: INFO: model_training: Rank 0, Epoch 472, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,428: INFO: model_training: Rank 0, Epoch 472, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,429: INFO: model_training: Rank 0, Epoch 472, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,431: INFO: model_training: Rank 0, Epoch 472, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,433: INFO: model_training: Rank 0, Epoch 473, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,434: INFO: model_training: Rank 0, Epoch 473, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,435: INFO: model_training: Rank 0, Epoch 473, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,437: INFO: model_training: Rank 0, Epoch 473, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,438: INFO: model_training: Rank 0, Epoch 473, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,440: INFO: model_training: Rank 0, Epoch 474, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,441: INFO: model_training: Rank 0, Epoch 474, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,442: INFO: model_training: Rank 0, Epoch 474, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,443: INFO: model_training: Rank 0, Epoch 474, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,445: INFO: model_training: Rank 0, Epoch 474, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,446: INFO: model_training: Rank 0, Epoch 475, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,447: INFO: model_training: Rank 0, Epoch 475, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,448: INFO: model_training: Rank 0, Epoch 475, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,449: INFO: model_training: Rank 0, Epoch 475, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,451: INFO: model_training: Rank 0, Epoch 475, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,453: INFO: model_training: Rank 0, Epoch 476, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,454: INFO: model_training: Rank 0, Epoch 476, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,455: INFO: model_training: Rank 0, Epoch 476, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,456: INFO: model_training: Rank 0, Epoch 476, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,457: INFO: model_training: Rank 0, Epoch 476, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,459: INFO: model_training: Rank 0, Epoch 477, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,461: INFO: model_training: Rank 0, Epoch 477, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,462: INFO: model_training: Rank 0, Epoch 477, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,463: INFO: model_training: Rank 0, Epoch 477, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,464: INFO: model_training: Rank 0, Epoch 477, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,465: INFO: model_training: Rank 0, Epoch 478, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,467: INFO: model_training: Rank 0, Epoch 478, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,468: INFO: model_training: Rank 0, Epoch 478, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,470: INFO: model_training: Rank 0, Epoch 478, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,471: INFO: model_training: Rank 0, Epoch 478, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,473: INFO: model_training: Rank 0, Epoch 479, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,474: INFO: model_training: Rank 0, Epoch 479, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,475: INFO: model_training: Rank 0, Epoch 479, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,477: INFO: model_training: Rank 0, Epoch 479, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,478: INFO: model_training: Rank 0, Epoch 479, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,480: INFO: model_training: Rank 0, Epoch 480, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,481: INFO: model_training: Rank 0, Epoch 480, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,482: INFO: model_training: Rank 0, Epoch 480, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,483: INFO: model_training: Rank 0, Epoch 480, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,484: INFO: model_training: Rank 0, Epoch 480, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,487: INFO: model_training: Rank 0, Epoch 481, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,488: INFO: model_training: Rank 0, Epoch 481, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,489: INFO: model_training: Rank 0, Epoch 481, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,490: INFO: model_training: Rank 0, Epoch 481, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,491: INFO: model_training: Rank 0, Epoch 481, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,493: INFO: model_training: Rank 0, Epoch 482, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,494: INFO: model_training: Rank 0, Epoch 482, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,495: INFO: model_training: Rank 0, Epoch 482, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,496: INFO: model_training: Rank 0, Epoch 482, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,498: INFO: model_training: Rank 0, Epoch 482, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,499: INFO: model_training: Rank 0, Epoch 483, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,500: INFO: model_training: Rank 0, Epoch 483, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,502: INFO: model_training: Rank 0, Epoch 483, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,503: INFO: model_training: Rank 0, Epoch 483, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,504: INFO: model_training: Rank 0, Epoch 483, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,506: INFO: model_training: Rank 0, Epoch 484, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,507: INFO: model_training: Rank 0, Epoch 484, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,508: INFO: model_training: Rank 0, Epoch 484, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,510: INFO: model_training: Rank 0, Epoch 484, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,511: INFO: model_training: Rank 0, Epoch 484, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,513: INFO: model_training: Rank 0, Epoch 485, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,514: INFO: model_training: Rank 0, Epoch 485, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,516: INFO: model_training: Rank 0, Epoch 485, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,517: INFO: model_training: Rank 0, Epoch 485, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,518: INFO: model_training: Rank 0, Epoch 485, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,520: INFO: model_training: Rank 0, Epoch 486, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,521: INFO: model_training: Rank 0, Epoch 486, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,522: INFO: model_training: Rank 0, Epoch 486, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,523: INFO: model_training: Rank 0, Epoch 486, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,524: INFO: model_training: Rank 0, Epoch 486, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,526: INFO: model_training: Rank 0, Epoch 487, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,528: INFO: model_training: Rank 0, Epoch 487, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,529: INFO: model_training: Rank 0, Epoch 487, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,530: INFO: model_training: Rank 0, Epoch 487, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,531: INFO: model_training: Rank 0, Epoch 487, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,532: INFO: model_training: Rank 0, Epoch 488, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,534: INFO: model_training: Rank 0, Epoch 488, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,535: INFO: model_training: Rank 0, Epoch 488, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,537: INFO: model_training: Rank 0, Epoch 488, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,538: INFO: model_training: Rank 0, Epoch 488, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,539: INFO: model_training: Rank 0, Epoch 489, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,541: INFO: model_training: Rank 0, Epoch 489, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,542: INFO: model_training: Rank 0, Epoch 489, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,543: INFO: model_training: Rank 0, Epoch 489, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,545: INFO: model_training: Rank 0, Epoch 489, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,547: INFO: model_training: Rank 0, Epoch 490, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,548: INFO: model_training: Rank 0, Epoch 490, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,550: INFO: model_training: Rank 0, Epoch 490, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,551: INFO: model_training: Rank 0, Epoch 490, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,553: INFO: model_training: Rank 0, Epoch 490, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,555: INFO: model_training: Rank 0, Epoch 491, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,556: INFO: model_training: Rank 0, Epoch 491, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,557: INFO: model_training: Rank 0, Epoch 491, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,558: INFO: model_training: Rank 0, Epoch 491, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,560: INFO: model_training: Rank 0, Epoch 491, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,562: INFO: model_training: Rank 0, Epoch 492, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,563: INFO: model_training: Rank 0, Epoch 492, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,564: INFO: model_training: Rank 0, Epoch 492, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,565: INFO: model_training: Rank 0, Epoch 492, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,566: INFO: model_training: Rank 0, Epoch 492, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,568: INFO: model_training: Rank 0, Epoch 493, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,570: INFO: model_training: Rank 0, Epoch 493, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,571: INFO: model_training: Rank 0, Epoch 493, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,572: INFO: model_training: Rank 0, Epoch 493, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,573: INFO: model_training: Rank 0, Epoch 493, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,575: INFO: model_training: Rank 0, Epoch 494, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,576: INFO: model_training: Rank 0, Epoch 494, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,578: INFO: model_training: Rank 0, Epoch 494, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,579: INFO: model_training: Rank 0, Epoch 494, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,580: INFO: model_training: Rank 0, Epoch 494, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,582: INFO: model_training: Rank 0, Epoch 495, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,583: INFO: model_training: Rank 0, Epoch 495, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,584: INFO: model_training: Rank 0, Epoch 495, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,586: INFO: model_training: Rank 0, Epoch 495, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,587: INFO: model_training: Rank 0, Epoch 495, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,589: INFO: model_training: Rank 0, Epoch 496, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,590: INFO: model_training: Rank 0, Epoch 496, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,591: INFO: model_training: Rank 0, Epoch 496, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,592: INFO: model_training: Rank 0, Epoch 496, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,594: INFO: model_training: Rank 0, Epoch 496, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,596: INFO: model_training: Rank 0, Epoch 497, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,597: INFO: model_training: Rank 0, Epoch 497, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,598: INFO: model_training: Rank 0, Epoch 497, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,599: INFO: model_training: Rank 0, Epoch 497, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,601: INFO: model_training: Rank 0, Epoch 497, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,602: INFO: model_training: Rank 0, Epoch 498, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,604: INFO: model_training: Rank 0, Epoch 498, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,605: INFO: model_training: Rank 0, Epoch 498, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,607: INFO: model_training: Rank 0, Epoch 498, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,608: INFO: model_training: Rank 0, Epoch 498, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,609: INFO: model_training: Rank 0, Epoch 499, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,610: INFO: model_training: Rank 0, Epoch 499, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,612: INFO: model_training: Rank 0, Epoch 499, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,614: INFO: model_training: Rank 0, Epoch 499, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,615: INFO: model_training: Rank 0, Epoch 499, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,617: INFO: model_training: Rank 0, Epoch 500, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,618: INFO: model_training: Rank 0, Epoch 500, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,619: INFO: model_training: Rank 0, Epoch 500, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,621: INFO: model_training: Rank 0, Epoch 500, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,623: INFO: model_training: Rank 0, Epoch 500, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,625: INFO: model_training: Rank 0, Epoch 501, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,627: INFO: model_training: Rank 0, Epoch 501, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,628: INFO: model_training: Rank 0, Epoch 501, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,631: INFO: model_training: Rank 0, Epoch 501, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,632: INFO: model_training: Rank 0, Epoch 501, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,634: INFO: model_training: Rank 0, Epoch 502, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,636: INFO: model_training: Rank 0, Epoch 502, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,637: INFO: model_training: Rank 0, Epoch 502, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,639: INFO: model_training: Rank 0, Epoch 502, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,640: INFO: model_training: Rank 0, Epoch 502, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,642: INFO: model_training: Rank 0, Epoch 503, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,644: INFO: model_training: Rank 0, Epoch 503, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,645: INFO: model_training: Rank 0, Epoch 503, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,647: INFO: model_training: Rank 0, Epoch 503, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,648: INFO: model_training: Rank 0, Epoch 503, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,649: INFO: model_training: Rank 0, Epoch 504, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,651: INFO: model_training: Rank 0, Epoch 504, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,652: INFO: model_training: Rank 0, Epoch 504, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,654: INFO: model_training: Rank 0, Epoch 504, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,655: INFO: model_training: Rank 0, Epoch 504, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,656: INFO: model_training: Rank 0, Epoch 505, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,658: INFO: model_training: Rank 0, Epoch 505, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,659: INFO: model_training: Rank 0, Epoch 505, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,660: INFO: model_training: Rank 0, Epoch 505, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,662: INFO: model_training: Rank 0, Epoch 505, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,664: INFO: model_training: Rank 0, Epoch 506, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,665: INFO: model_training: Rank 0, Epoch 506, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,666: INFO: model_training: Rank 0, Epoch 506, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,667: INFO: model_training: Rank 0, Epoch 506, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,669: INFO: model_training: Rank 0, Epoch 506, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,671: INFO: model_training: Rank 0, Epoch 507, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,672: INFO: model_training: Rank 0, Epoch 507, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,673: INFO: model_training: Rank 0, Epoch 507, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,675: INFO: model_training: Rank 0, Epoch 507, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,676: INFO: model_training: Rank 0, Epoch 507, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,678: INFO: model_training: Rank 0, Epoch 508, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,679: INFO: model_training: Rank 0, Epoch 508, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,681: INFO: model_training: Rank 0, Epoch 508, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,682: INFO: model_training: Rank 0, Epoch 508, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,683: INFO: model_training: Rank 0, Epoch 508, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,685: INFO: model_training: Rank 0, Epoch 509, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,686: INFO: model_training: Rank 0, Epoch 509, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,687: INFO: model_training: Rank 0, Epoch 509, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,690: INFO: model_training: Rank 0, Epoch 509, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,691: INFO: model_training: Rank 0, Epoch 509, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,693: INFO: model_training: Rank 0, Epoch 510, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,694: INFO: model_training: Rank 0, Epoch 510, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,696: INFO: model_training: Rank 0, Epoch 510, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,697: INFO: model_training: Rank 0, Epoch 510, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,698: INFO: model_training: Rank 0, Epoch 510, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,700: INFO: model_training: Rank 0, Epoch 511, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,701: INFO: model_training: Rank 0, Epoch 511, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,702: INFO: model_training: Rank 0, Epoch 511, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,704: INFO: model_training: Rank 0, Epoch 511, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,705: INFO: model_training: Rank 0, Epoch 511, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,707: INFO: model_training: Rank 0, Epoch 512, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,708: INFO: model_training: Rank 0, Epoch 512, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,709: INFO: model_training: Rank 0, Epoch 512, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,710: INFO: model_training: Rank 0, Epoch 512, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,711: INFO: model_training: Rank 0, Epoch 512, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,714: INFO: model_training: Rank 0, Epoch 513, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,715: INFO: model_training: Rank 0, Epoch 513, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,716: INFO: model_training: Rank 0, Epoch 513, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,717: INFO: model_training: Rank 0, Epoch 513, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,718: INFO: model_training: Rank 0, Epoch 513, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,720: INFO: model_training: Rank 0, Epoch 514, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,721: INFO: model_training: Rank 0, Epoch 514, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,723: INFO: model_training: Rank 0, Epoch 514, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,724: INFO: model_training: Rank 0, Epoch 514, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,725: INFO: model_training: Rank 0, Epoch 514, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,727: INFO: model_training: Rank 0, Epoch 515, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,729: INFO: model_training: Rank 0, Epoch 515, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,730: INFO: model_training: Rank 0, Epoch 515, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,731: INFO: model_training: Rank 0, Epoch 515, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,732: INFO: model_training: Rank 0, Epoch 515, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,734: INFO: model_training: Rank 0, Epoch 516, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,735: INFO: model_training: Rank 0, Epoch 516, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,736: INFO: model_training: Rank 0, Epoch 516, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,737: INFO: model_training: Rank 0, Epoch 516, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,739: INFO: model_training: Rank 0, Epoch 516, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,740: INFO: model_training: Rank 0, Epoch 517, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,741: INFO: model_training: Rank 0, Epoch 517, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,742: INFO: model_training: Rank 0, Epoch 517, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,744: INFO: model_training: Rank 0, Epoch 517, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,745: INFO: model_training: Rank 0, Epoch 517, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,747: INFO: model_training: Rank 0, Epoch 518, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,748: INFO: model_training: Rank 0, Epoch 518, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,749: INFO: model_training: Rank 0, Epoch 518, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,750: INFO: model_training: Rank 0, Epoch 518, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,751: INFO: model_training: Rank 0, Epoch 518, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,754: INFO: model_training: Rank 0, Epoch 519, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,755: INFO: model_training: Rank 0, Epoch 519, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,756: INFO: model_training: Rank 0, Epoch 519, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,757: INFO: model_training: Rank 0, Epoch 519, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,758: INFO: model_training: Rank 0, Epoch 519, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,760: INFO: model_training: Rank 0, Epoch 520, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,762: INFO: model_training: Rank 0, Epoch 520, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,763: INFO: model_training: Rank 0, Epoch 520, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,764: INFO: model_training: Rank 0, Epoch 520, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,765: INFO: model_training: Rank 0, Epoch 520, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,766: INFO: model_training: Rank 0, Epoch 521, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,767: INFO: model_training: Rank 0, Epoch 521, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,769: INFO: model_training: Rank 0, Epoch 521, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,770: INFO: model_training: Rank 0, Epoch 521, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,771: INFO: model_training: Rank 0, Epoch 521, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,774: INFO: model_training: Rank 0, Epoch 522, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,775: INFO: model_training: Rank 0, Epoch 522, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,776: INFO: model_training: Rank 0, Epoch 522, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,777: INFO: model_training: Rank 0, Epoch 522, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,779: INFO: model_training: Rank 0, Epoch 522, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,780: INFO: model_training: Rank 0, Epoch 523, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,782: INFO: model_training: Rank 0, Epoch 523, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,783: INFO: model_training: Rank 0, Epoch 523, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,785: INFO: model_training: Rank 0, Epoch 523, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,786: INFO: model_training: Rank 0, Epoch 523, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,788: INFO: model_training: Rank 0, Epoch 524, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,789: INFO: model_training: Rank 0, Epoch 524, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,790: INFO: model_training: Rank 0, Epoch 524, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,791: INFO: model_training: Rank 0, Epoch 524, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,792: INFO: model_training: Rank 0, Epoch 524, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,794: INFO: model_training: Rank 0, Epoch 525, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,796: INFO: model_training: Rank 0, Epoch 525, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,797: INFO: model_training: Rank 0, Epoch 525, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,798: INFO: model_training: Rank 0, Epoch 525, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,799: INFO: model_training: Rank 0, Epoch 525, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,801: INFO: model_training: Rank 0, Epoch 526, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,802: INFO: model_training: Rank 0, Epoch 526, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,804: INFO: model_training: Rank 0, Epoch 526, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,805: INFO: model_training: Rank 0, Epoch 526, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,806: INFO: model_training: Rank 0, Epoch 526, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,807: INFO: model_training: Rank 0, Epoch 527, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,809: INFO: model_training: Rank 0, Epoch 527, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,810: INFO: model_training: Rank 0, Epoch 527, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,813: INFO: model_training: Rank 0, Epoch 527, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,814: INFO: model_training: Rank 0, Epoch 527, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,816: INFO: model_training: Rank 0, Epoch 528, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,817: INFO: model_training: Rank 0, Epoch 528, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,818: INFO: model_training: Rank 0, Epoch 528, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,820: INFO: model_training: Rank 0, Epoch 528, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,822: INFO: model_training: Rank 0, Epoch 528, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,823: INFO: model_training: Rank 0, Epoch 529, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,824: INFO: model_training: Rank 0, Epoch 529, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,825: INFO: model_training: Rank 0, Epoch 529, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,826: INFO: model_training: Rank 0, Epoch 529, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,828: INFO: model_training: Rank 0, Epoch 529, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,830: INFO: model_training: Rank 0, Epoch 530, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,832: INFO: model_training: Rank 0, Epoch 530, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,833: INFO: model_training: Rank 0, Epoch 530, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,834: INFO: model_training: Rank 0, Epoch 530, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,835: INFO: model_training: Rank 0, Epoch 530, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,837: INFO: model_training: Rank 0, Epoch 531, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,838: INFO: model_training: Rank 0, Epoch 531, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,840: INFO: model_training: Rank 0, Epoch 531, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,841: INFO: model_training: Rank 0, Epoch 531, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,842: INFO: model_training: Rank 0, Epoch 531, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,845: INFO: model_training: Rank 0, Epoch 532, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,846: INFO: model_training: Rank 0, Epoch 532, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,847: INFO: model_training: Rank 0, Epoch 532, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,849: INFO: model_training: Rank 0, Epoch 532, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,850: INFO: model_training: Rank 0, Epoch 532, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,852: INFO: model_training: Rank 0, Epoch 533, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,853: INFO: model_training: Rank 0, Epoch 533, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,854: INFO: model_training: Rank 0, Epoch 533, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,856: INFO: model_training: Rank 0, Epoch 533, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,857: INFO: model_training: Rank 0, Epoch 533, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,859: INFO: model_training: Rank 0, Epoch 534, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,860: INFO: model_training: Rank 0, Epoch 534, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,862: INFO: model_training: Rank 0, Epoch 534, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,863: INFO: model_training: Rank 0, Epoch 534, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,865: INFO: model_training: Rank 0, Epoch 534, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,866: INFO: model_training: Rank 0, Epoch 535, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,867: INFO: model_training: Rank 0, Epoch 535, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,868: INFO: model_training: Rank 0, Epoch 535, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,870: INFO: model_training: Rank 0, Epoch 535, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,872: INFO: model_training: Rank 0, Epoch 535, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,873: INFO: model_training: Rank 0, Epoch 536, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,875: INFO: model_training: Rank 0, Epoch 536, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,877: INFO: model_training: Rank 0, Epoch 536, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,878: INFO: model_training: Rank 0, Epoch 536, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,880: INFO: model_training: Rank 0, Epoch 536, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,882: INFO: model_training: Rank 0, Epoch 537, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,883: INFO: model_training: Rank 0, Epoch 537, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,884: INFO: model_training: Rank 0, Epoch 537, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,885: INFO: model_training: Rank 0, Epoch 537, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,887: INFO: model_training: Rank 0, Epoch 537, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,889: INFO: model_training: Rank 0, Epoch 538, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,890: INFO: model_training: Rank 0, Epoch 538, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,891: INFO: model_training: Rank 0, Epoch 538, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,892: INFO: model_training: Rank 0, Epoch 538, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,894: INFO: model_training: Rank 0, Epoch 538, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,896: INFO: model_training: Rank 0, Epoch 539, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,897: INFO: model_training: Rank 0, Epoch 539, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,898: INFO: model_training: Rank 0, Epoch 539, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,899: INFO: model_training: Rank 0, Epoch 539, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,900: INFO: model_training: Rank 0, Epoch 539, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,902: INFO: model_training: Rank 0, Epoch 540, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,904: INFO: model_training: Rank 0, Epoch 540, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,906: INFO: model_training: Rank 0, Epoch 540, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,908: INFO: model_training: Rank 0, Epoch 540, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,909: INFO: model_training: Rank 0, Epoch 540, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,912: INFO: model_training: Rank 0, Epoch 541, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,914: INFO: model_training: Rank 0, Epoch 541, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,915: INFO: model_training: Rank 0, Epoch 541, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,916: INFO: model_training: Rank 0, Epoch 541, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,917: INFO: model_training: Rank 0, Epoch 541, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,919: INFO: model_training: Rank 0, Epoch 542, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,920: INFO: model_training: Rank 0, Epoch 542, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,922: INFO: model_training: Rank 0, Epoch 542, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,924: INFO: model_training: Rank 0, Epoch 542, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,925: INFO: model_training: Rank 0, Epoch 542, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,927: INFO: model_training: Rank 0, Epoch 543, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,928: INFO: model_training: Rank 0, Epoch 543, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,930: INFO: model_training: Rank 0, Epoch 543, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,931: INFO: model_training: Rank 0, Epoch 543, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,932: INFO: model_training: Rank 0, Epoch 543, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,934: INFO: model_training: Rank 0, Epoch 544, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,936: INFO: model_training: Rank 0, Epoch 544, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,938: INFO: model_training: Rank 0, Epoch 544, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,939: INFO: model_training: Rank 0, Epoch 544, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,940: INFO: model_training: Rank 0, Epoch 544, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,942: INFO: model_training: Rank 0, Epoch 545, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,944: INFO: model_training: Rank 0, Epoch 545, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,946: INFO: model_training: Rank 0, Epoch 545, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,947: INFO: model_training: Rank 0, Epoch 545, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,948: INFO: model_training: Rank 0, Epoch 545, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,950: INFO: model_training: Rank 0, Epoch 546, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,951: INFO: model_training: Rank 0, Epoch 546, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,953: INFO: model_training: Rank 0, Epoch 546, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,955: INFO: model_training: Rank 0, Epoch 546, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,956: INFO: model_training: Rank 0, Epoch 546, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,958: INFO: model_training: Rank 0, Epoch 547, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,959: INFO: model_training: Rank 0, Epoch 547, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,961: INFO: model_training: Rank 0, Epoch 547, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,962: INFO: model_training: Rank 0, Epoch 547, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,964: INFO: model_training: Rank 0, Epoch 547, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,966: INFO: model_training: Rank 0, Epoch 548, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,967: INFO: model_training: Rank 0, Epoch 548, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,969: INFO: model_training: Rank 0, Epoch 548, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,971: INFO: model_training: Rank 0, Epoch 548, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,972: INFO: model_training: Rank 0, Epoch 548, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,974: INFO: model_training: Rank 0, Epoch 549, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,976: INFO: model_training: Rank 0, Epoch 549, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,977: INFO: model_training: Rank 0, Epoch 549, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,979: INFO: model_training: Rank 0, Epoch 549, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,981: INFO: model_training: Rank 0, Epoch 549, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,983: INFO: model_training: Rank 0, Epoch 550, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,984: INFO: model_training: Rank 0, Epoch 550, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,985: INFO: model_training: Rank 0, Epoch 550, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,987: INFO: model_training: Rank 0, Epoch 550, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,989: INFO: model_training: Rank 0, Epoch 550, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,990: INFO: model_training: Rank 0, Epoch 551, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,992: INFO: model_training: Rank 0, Epoch 551, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:46,993: INFO: model_training: Rank 0, Epoch 551, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:46,994: INFO: model_training: Rank 0, Epoch 551, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:46,996: INFO: model_training: Rank 0, Epoch 551, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:46,998: INFO: model_training: Rank 0, Epoch 552, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:46,999: INFO: model_training: Rank 0, Epoch 552, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,000: INFO: model_training: Rank 0, Epoch 552, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,002: INFO: model_training: Rank 0, Epoch 552, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,004: INFO: model_training: Rank 0, Epoch 552, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,005: INFO: model_training: Rank 0, Epoch 553, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,007: INFO: model_training: Rank 0, Epoch 553, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,008: INFO: model_training: Rank 0, Epoch 553, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,009: INFO: model_training: Rank 0, Epoch 553, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,011: INFO: model_training: Rank 0, Epoch 553, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,013: INFO: model_training: Rank 0, Epoch 554, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,015: INFO: model_training: Rank 0, Epoch 554, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,016: INFO: model_training: Rank 0, Epoch 554, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,017: INFO: model_training: Rank 0, Epoch 554, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,019: INFO: model_training: Rank 0, Epoch 554, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,021: INFO: model_training: Rank 0, Epoch 555, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,022: INFO: model_training: Rank 0, Epoch 555, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,024: INFO: model_training: Rank 0, Epoch 555, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,025: INFO: model_training: Rank 0, Epoch 555, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,027: INFO: model_training: Rank 0, Epoch 555, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,029: INFO: model_training: Rank 0, Epoch 556, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,031: INFO: model_training: Rank 0, Epoch 556, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,032: INFO: model_training: Rank 0, Epoch 556, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,034: INFO: model_training: Rank 0, Epoch 556, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,035: INFO: model_training: Rank 0, Epoch 556, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,037: INFO: model_training: Rank 0, Epoch 557, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,039: INFO: model_training: Rank 0, Epoch 557, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,040: INFO: model_training: Rank 0, Epoch 557, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,041: INFO: model_training: Rank 0, Epoch 557, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,043: INFO: model_training: Rank 0, Epoch 557, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,045: INFO: model_training: Rank 0, Epoch 558, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,047: INFO: model_training: Rank 0, Epoch 558, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,048: INFO: model_training: Rank 0, Epoch 558, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,050: INFO: model_training: Rank 0, Epoch 558, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,051: INFO: model_training: Rank 0, Epoch 558, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,053: INFO: model_training: Rank 0, Epoch 559, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,055: INFO: model_training: Rank 0, Epoch 559, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,056: INFO: model_training: Rank 0, Epoch 559, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,057: INFO: model_training: Rank 0, Epoch 559, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,059: INFO: model_training: Rank 0, Epoch 559, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,061: INFO: model_training: Rank 0, Epoch 560, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,063: INFO: model_training: Rank 0, Epoch 560, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,064: INFO: model_training: Rank 0, Epoch 560, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,065: INFO: model_training: Rank 0, Epoch 560, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,066: INFO: model_training: Rank 0, Epoch 560, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,068: INFO: model_training: Rank 0, Epoch 561, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,070: INFO: model_training: Rank 0, Epoch 561, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,071: INFO: model_training: Rank 0, Epoch 561, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,073: INFO: model_training: Rank 0, Epoch 561, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,074: INFO: model_training: Rank 0, Epoch 561, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,076: INFO: model_training: Rank 0, Epoch 562, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,077: INFO: model_training: Rank 0, Epoch 562, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,079: INFO: model_training: Rank 0, Epoch 562, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,080: INFO: model_training: Rank 0, Epoch 562, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,081: INFO: model_training: Rank 0, Epoch 562, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,084: INFO: model_training: Rank 0, Epoch 563, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,086: INFO: model_training: Rank 0, Epoch 563, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,087: INFO: model_training: Rank 0, Epoch 563, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,089: INFO: model_training: Rank 0, Epoch 563, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,090: INFO: model_training: Rank 0, Epoch 563, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,092: INFO: model_training: Rank 0, Epoch 564, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,093: INFO: model_training: Rank 0, Epoch 564, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,095: INFO: model_training: Rank 0, Epoch 564, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,097: INFO: model_training: Rank 0, Epoch 564, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,098: INFO: model_training: Rank 0, Epoch 564, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,099: INFO: model_training: Rank 0, Epoch 565, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,100: INFO: model_training: Rank 0, Epoch 565, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,102: INFO: model_training: Rank 0, Epoch 565, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,103: INFO: model_training: Rank 0, Epoch 565, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,105: INFO: model_training: Rank 0, Epoch 565, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,106: INFO: model_training: Rank 0, Epoch 566, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,107: INFO: model_training: Rank 0, Epoch 566, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,109: INFO: model_training: Rank 0, Epoch 566, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,110: INFO: model_training: Rank 0, Epoch 566, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,111: INFO: model_training: Rank 0, Epoch 566, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,113: INFO: model_training: Rank 0, Epoch 567, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,114: INFO: model_training: Rank 0, Epoch 567, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,115: INFO: model_training: Rank 0, Epoch 567, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,116: INFO: model_training: Rank 0, Epoch 567, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,117: INFO: model_training: Rank 0, Epoch 567, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,118: INFO: model_training: Rank 0, Epoch 568, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,120: INFO: model_training: Rank 0, Epoch 568, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,121: INFO: model_training: Rank 0, Epoch 568, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,123: INFO: model_training: Rank 0, Epoch 568, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,124: INFO: model_training: Rank 0, Epoch 568, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,126: INFO: model_training: Rank 0, Epoch 569, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,127: INFO: model_training: Rank 0, Epoch 569, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,129: INFO: model_training: Rank 0, Epoch 569, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,130: INFO: model_training: Rank 0, Epoch 569, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,131: INFO: model_training: Rank 0, Epoch 569, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,132: INFO: model_training: Rank 0, Epoch 570, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,134: INFO: model_training: Rank 0, Epoch 570, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,135: INFO: model_training: Rank 0, Epoch 570, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,137: INFO: model_training: Rank 0, Epoch 570, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,138: INFO: model_training: Rank 0, Epoch 570, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,139: INFO: model_training: Rank 0, Epoch 571, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,141: INFO: model_training: Rank 0, Epoch 571, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,142: INFO: model_training: Rank 0, Epoch 571, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,144: INFO: model_training: Rank 0, Epoch 571, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,145: INFO: model_training: Rank 0, Epoch 571, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,147: INFO: model_training: Rank 0, Epoch 572, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,148: INFO: model_training: Rank 0, Epoch 572, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,149: INFO: model_training: Rank 0, Epoch 572, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,150: INFO: model_training: Rank 0, Epoch 572, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,152: INFO: model_training: Rank 0, Epoch 572, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,154: INFO: model_training: Rank 0, Epoch 573, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,155: INFO: model_training: Rank 0, Epoch 573, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,156: INFO: model_training: Rank 0, Epoch 573, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,157: INFO: model_training: Rank 0, Epoch 573, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,159: INFO: model_training: Rank 0, Epoch 573, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,161: INFO: model_training: Rank 0, Epoch 574, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,163: INFO: model_training: Rank 0, Epoch 574, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,164: INFO: model_training: Rank 0, Epoch 574, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,166: INFO: model_training: Rank 0, Epoch 574, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,167: INFO: model_training: Rank 0, Epoch 574, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,169: INFO: model_training: Rank 0, Epoch 575, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,171: INFO: model_training: Rank 0, Epoch 575, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,172: INFO: model_training: Rank 0, Epoch 575, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,174: INFO: model_training: Rank 0, Epoch 575, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,175: INFO: model_training: Rank 0, Epoch 575, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,176: INFO: model_training: Rank 0, Epoch 576, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,177: INFO: model_training: Rank 0, Epoch 576, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,179: INFO: model_training: Rank 0, Epoch 576, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,180: INFO: model_training: Rank 0, Epoch 576, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,181: INFO: model_training: Rank 0, Epoch 576, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,183: INFO: model_training: Rank 0, Epoch 577, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,184: INFO: model_training: Rank 0, Epoch 577, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,185: INFO: model_training: Rank 0, Epoch 577, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,187: INFO: model_training: Rank 0, Epoch 577, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,189: INFO: model_training: Rank 0, Epoch 577, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,191: INFO: model_training: Rank 0, Epoch 578, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,192: INFO: model_training: Rank 0, Epoch 578, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,193: INFO: model_training: Rank 0, Epoch 578, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,194: INFO: model_training: Rank 0, Epoch 578, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,196: INFO: model_training: Rank 0, Epoch 578, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,197: INFO: model_training: Rank 0, Epoch 579, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,198: INFO: model_training: Rank 0, Epoch 579, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,199: INFO: model_training: Rank 0, Epoch 579, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,201: INFO: model_training: Rank 0, Epoch 579, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,202: INFO: model_training: Rank 0, Epoch 579, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,204: INFO: model_training: Rank 0, Epoch 580, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,205: INFO: model_training: Rank 0, Epoch 580, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,207: INFO: model_training: Rank 0, Epoch 580, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,207: INFO: model_training: Rank 0, Epoch 580, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,208: INFO: model_training: Rank 0, Epoch 580, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,210: INFO: model_training: Rank 0, Epoch 581, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,212: INFO: model_training: Rank 0, Epoch 581, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,214: INFO: model_training: Rank 0, Epoch 581, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,215: INFO: model_training: Rank 0, Epoch 581, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,216: INFO: model_training: Rank 0, Epoch 581, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,217: INFO: model_training: Rank 0, Epoch 582, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,220: INFO: model_training: Rank 0, Epoch 582, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,221: INFO: model_training: Rank 0, Epoch 582, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,222: INFO: model_training: Rank 0, Epoch 582, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,223: INFO: model_training: Rank 0, Epoch 582, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,225: INFO: model_training: Rank 0, Epoch 583, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,227: INFO: model_training: Rank 0, Epoch 583, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,228: INFO: model_training: Rank 0, Epoch 583, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,229: INFO: model_training: Rank 0, Epoch 583, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,230: INFO: model_training: Rank 0, Epoch 583, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,231: INFO: model_training: Rank 0, Epoch 584, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,233: INFO: model_training: Rank 0, Epoch 584, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,234: INFO: model_training: Rank 0, Epoch 584, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,235: INFO: model_training: Rank 0, Epoch 584, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,237: INFO: model_training: Rank 0, Epoch 584, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,238: INFO: model_training: Rank 0, Epoch 585, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,239: INFO: model_training: Rank 0, Epoch 585, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,241: INFO: model_training: Rank 0, Epoch 585, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,242: INFO: model_training: Rank 0, Epoch 585, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,243: INFO: model_training: Rank 0, Epoch 585, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,245: INFO: model_training: Rank 0, Epoch 586, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,247: INFO: model_training: Rank 0, Epoch 586, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,248: INFO: model_training: Rank 0, Epoch 586, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,249: INFO: model_training: Rank 0, Epoch 586, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,251: INFO: model_training: Rank 0, Epoch 586, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,253: INFO: model_training: Rank 0, Epoch 587, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,254: INFO: model_training: Rank 0, Epoch 587, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,255: INFO: model_training: Rank 0, Epoch 587, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,256: INFO: model_training: Rank 0, Epoch 587, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,257: INFO: model_training: Rank 0, Epoch 587, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,259: INFO: model_training: Rank 0, Epoch 588, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,260: INFO: model_training: Rank 0, Epoch 588, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,262: INFO: model_training: Rank 0, Epoch 588, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,263: INFO: model_training: Rank 0, Epoch 588, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,264: INFO: model_training: Rank 0, Epoch 588, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,266: INFO: model_training: Rank 0, Epoch 589, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,267: INFO: model_training: Rank 0, Epoch 589, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,269: INFO: model_training: Rank 0, Epoch 589, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,271: INFO: model_training: Rank 0, Epoch 589, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,272: INFO: model_training: Rank 0, Epoch 589, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,275: INFO: model_training: Rank 0, Epoch 590, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,277: INFO: model_training: Rank 0, Epoch 590, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,279: INFO: model_training: Rank 0, Epoch 590, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,281: INFO: model_training: Rank 0, Epoch 590, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,283: INFO: model_training: Rank 0, Epoch 590, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,285: INFO: model_training: Rank 0, Epoch 591, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,286: INFO: model_training: Rank 0, Epoch 591, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,288: INFO: model_training: Rank 0, Epoch 591, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,290: INFO: model_training: Rank 0, Epoch 591, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,292: INFO: model_training: Rank 0, Epoch 591, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,294: INFO: model_training: Rank 0, Epoch 592, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,296: INFO: model_training: Rank 0, Epoch 592, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,298: INFO: model_training: Rank 0, Epoch 592, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,300: INFO: model_training: Rank 0, Epoch 592, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,301: INFO: model_training: Rank 0, Epoch 592, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,304: INFO: model_training: Rank 0, Epoch 593, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,306: INFO: model_training: Rank 0, Epoch 593, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,308: INFO: model_training: Rank 0, Epoch 593, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,310: INFO: model_training: Rank 0, Epoch 593, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,312: INFO: model_training: Rank 0, Epoch 593, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,314: INFO: model_training: Rank 0, Epoch 594, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,316: INFO: model_training: Rank 0, Epoch 594, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,317: INFO: model_training: Rank 0, Epoch 594, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,319: INFO: model_training: Rank 0, Epoch 594, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,321: INFO: model_training: Rank 0, Epoch 594, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,323: INFO: model_training: Rank 0, Epoch 595, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,325: INFO: model_training: Rank 0, Epoch 595, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,326: INFO: model_training: Rank 0, Epoch 595, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,328: INFO: model_training: Rank 0, Epoch 595, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,330: INFO: model_training: Rank 0, Epoch 595, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,332: INFO: model_training: Rank 0, Epoch 596, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,334: INFO: model_training: Rank 0, Epoch 596, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,336: INFO: model_training: Rank 0, Epoch 596, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,338: INFO: model_training: Rank 0, Epoch 596, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,340: INFO: model_training: Rank 0, Epoch 596, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,342: INFO: model_training: Rank 0, Epoch 597, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,344: INFO: model_training: Rank 0, Epoch 597, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,346: INFO: model_training: Rank 0, Epoch 597, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,349: INFO: model_training: Rank 0, Epoch 597, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,350: INFO: model_training: Rank 0, Epoch 597, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,352: INFO: model_training: Rank 0, Epoch 598, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,354: INFO: model_training: Rank 0, Epoch 598, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,356: INFO: model_training: Rank 0, Epoch 598, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,358: INFO: model_training: Rank 0, Epoch 598, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,359: INFO: model_training: Rank 0, Epoch 598, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,361: INFO: model_training: Rank 0, Epoch 599, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,363: INFO: model_training: Rank 0, Epoch 599, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,365: INFO: model_training: Rank 0, Epoch 599, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,366: INFO: model_training: Rank 0, Epoch 599, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,367: INFO: model_training: Rank 0, Epoch 599, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,369: INFO: model_training: Rank 0, Epoch 600, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,370: INFO: model_training: Rank 0, Epoch 600, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,371: INFO: model_training: Rank 0, Epoch 600, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,373: INFO: model_training: Rank 0, Epoch 600, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,375: INFO: model_training: Rank 0, Epoch 600, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,377: INFO: model_training: Rank 0, Epoch 601, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,378: INFO: model_training: Rank 0, Epoch 601, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,380: INFO: model_training: Rank 0, Epoch 601, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,382: INFO: model_training: Rank 0, Epoch 601, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,383: INFO: model_training: Rank 0, Epoch 601, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,385: INFO: model_training: Rank 0, Epoch 602, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,386: INFO: model_training: Rank 0, Epoch 602, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,388: INFO: model_training: Rank 0, Epoch 602, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,389: INFO: model_training: Rank 0, Epoch 602, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,390: INFO: model_training: Rank 0, Epoch 602, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,392: INFO: model_training: Rank 0, Epoch 603, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,394: INFO: model_training: Rank 0, Epoch 603, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,395: INFO: model_training: Rank 0, Epoch 603, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,397: INFO: model_training: Rank 0, Epoch 603, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,399: INFO: model_training: Rank 0, Epoch 603, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,400: INFO: model_training: Rank 0, Epoch 604, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,401: INFO: model_training: Rank 0, Epoch 604, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,403: INFO: model_training: Rank 0, Epoch 604, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,405: INFO: model_training: Rank 0, Epoch 604, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,419: INFO: model_training: Rank 0, Epoch 604, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,427: INFO: model_training: Rank 0, Epoch 605, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,432: INFO: model_training: Rank 0, Epoch 605, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,435: INFO: model_training: Rank 0, Epoch 605, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,437: INFO: model_training: Rank 0, Epoch 605, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,440: INFO: model_training: Rank 0, Epoch 605, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,445: INFO: model_training: Rank 0, Epoch 606, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,447: INFO: model_training: Rank 0, Epoch 606, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,449: INFO: model_training: Rank 0, Epoch 606, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,450: INFO: model_training: Rank 0, Epoch 606, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,451: INFO: model_training: Rank 0, Epoch 606, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,453: INFO: model_training: Rank 0, Epoch 607, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,455: INFO: model_training: Rank 0, Epoch 607, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,456: INFO: model_training: Rank 0, Epoch 607, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,458: INFO: model_training: Rank 0, Epoch 607, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,459: INFO: model_training: Rank 0, Epoch 607, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,461: INFO: model_training: Rank 0, Epoch 608, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,462: INFO: model_training: Rank 0, Epoch 608, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,463: INFO: model_training: Rank 0, Epoch 608, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,465: INFO: model_training: Rank 0, Epoch 608, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,466: INFO: model_training: Rank 0, Epoch 608, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,468: INFO: model_training: Rank 0, Epoch 609, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,469: INFO: model_training: Rank 0, Epoch 609, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,471: INFO: model_training: Rank 0, Epoch 609, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,472: INFO: model_training: Rank 0, Epoch 609, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,474: INFO: model_training: Rank 0, Epoch 609, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,475: INFO: model_training: Rank 0, Epoch 610, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,478: INFO: model_training: Rank 0, Epoch 610, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,479: INFO: model_training: Rank 0, Epoch 610, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,481: INFO: model_training: Rank 0, Epoch 610, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,482: INFO: model_training: Rank 0, Epoch 610, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,484: INFO: model_training: Rank 0, Epoch 611, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,485: INFO: model_training: Rank 0, Epoch 611, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,487: INFO: model_training: Rank 0, Epoch 611, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,488: INFO: model_training: Rank 0, Epoch 611, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,490: INFO: model_training: Rank 0, Epoch 611, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,492: INFO: model_training: Rank 0, Epoch 612, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,494: INFO: model_training: Rank 0, Epoch 612, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,495: INFO: model_training: Rank 0, Epoch 612, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,497: INFO: model_training: Rank 0, Epoch 612, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,498: INFO: model_training: Rank 0, Epoch 612, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,500: INFO: model_training: Rank 0, Epoch 613, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,502: INFO: model_training: Rank 0, Epoch 613, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,504: INFO: model_training: Rank 0, Epoch 613, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,506: INFO: model_training: Rank 0, Epoch 613, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,507: INFO: model_training: Rank 0, Epoch 613, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,509: INFO: model_training: Rank 0, Epoch 614, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,511: INFO: model_training: Rank 0, Epoch 614, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,512: INFO: model_training: Rank 0, Epoch 614, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,514: INFO: model_training: Rank 0, Epoch 614, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,516: INFO: model_training: Rank 0, Epoch 614, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,518: INFO: model_training: Rank 0, Epoch 615, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,520: INFO: model_training: Rank 0, Epoch 615, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,521: INFO: model_training: Rank 0, Epoch 615, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,523: INFO: model_training: Rank 0, Epoch 615, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,525: INFO: model_training: Rank 0, Epoch 615, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,526: INFO: model_training: Rank 0, Epoch 616, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,528: INFO: model_training: Rank 0, Epoch 616, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,529: INFO: model_training: Rank 0, Epoch 616, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,531: INFO: model_training: Rank 0, Epoch 616, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,533: INFO: model_training: Rank 0, Epoch 616, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,534: INFO: model_training: Rank 0, Epoch 617, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,536: INFO: model_training: Rank 0, Epoch 617, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,537: INFO: model_training: Rank 0, Epoch 617, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,539: INFO: model_training: Rank 0, Epoch 617, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,540: INFO: model_training: Rank 0, Epoch 617, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,542: INFO: model_training: Rank 0, Epoch 618, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,544: INFO: model_training: Rank 0, Epoch 618, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,546: INFO: model_training: Rank 0, Epoch 618, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,549: INFO: model_training: Rank 0, Epoch 618, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,550: INFO: model_training: Rank 0, Epoch 618, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,552: INFO: model_training: Rank 0, Epoch 619, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,553: INFO: model_training: Rank 0, Epoch 619, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,557: INFO: model_training: Rank 0, Epoch 619, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,558: INFO: model_training: Rank 0, Epoch 619, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,559: INFO: model_training: Rank 0, Epoch 619, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,562: INFO: model_training: Rank 0, Epoch 620, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,563: INFO: model_training: Rank 0, Epoch 620, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,565: INFO: model_training: Rank 0, Epoch 620, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,566: INFO: model_training: Rank 0, Epoch 620, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,567: INFO: model_training: Rank 0, Epoch 620, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,570: INFO: model_training: Rank 0, Epoch 621, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,571: INFO: model_training: Rank 0, Epoch 621, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,573: INFO: model_training: Rank 0, Epoch 621, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,574: INFO: model_training: Rank 0, Epoch 621, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,576: INFO: model_training: Rank 0, Epoch 621, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,578: INFO: model_training: Rank 0, Epoch 622, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,579: INFO: model_training: Rank 0, Epoch 622, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,581: INFO: model_training: Rank 0, Epoch 622, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,583: INFO: model_training: Rank 0, Epoch 622, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,584: INFO: model_training: Rank 0, Epoch 622, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,586: INFO: model_training: Rank 0, Epoch 623, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,587: INFO: model_training: Rank 0, Epoch 623, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,589: INFO: model_training: Rank 0, Epoch 623, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,590: INFO: model_training: Rank 0, Epoch 623, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,591: INFO: model_training: Rank 0, Epoch 623, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,594: INFO: model_training: Rank 0, Epoch 624, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,596: INFO: model_training: Rank 0, Epoch 624, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,597: INFO: model_training: Rank 0, Epoch 624, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,598: INFO: model_training: Rank 0, Epoch 624, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,599: INFO: model_training: Rank 0, Epoch 624, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,601: INFO: model_training: Rank 0, Epoch 625, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,603: INFO: model_training: Rank 0, Epoch 625, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,604: INFO: model_training: Rank 0, Epoch 625, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,605: INFO: model_training: Rank 0, Epoch 625, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,607: INFO: model_training: Rank 0, Epoch 625, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,609: INFO: model_training: Rank 0, Epoch 626, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,610: INFO: model_training: Rank 0, Epoch 626, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,612: INFO: model_training: Rank 0, Epoch 626, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,613: INFO: model_training: Rank 0, Epoch 626, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,614: INFO: model_training: Rank 0, Epoch 626, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,616: INFO: model_training: Rank 0, Epoch 627, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,617: INFO: model_training: Rank 0, Epoch 627, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,619: INFO: model_training: Rank 0, Epoch 627, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,621: INFO: model_training: Rank 0, Epoch 627, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,622: INFO: model_training: Rank 0, Epoch 627, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,623: INFO: model_training: Rank 0, Epoch 628, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,625: INFO: model_training: Rank 0, Epoch 628, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,626: INFO: model_training: Rank 0, Epoch 628, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,628: INFO: model_training: Rank 0, Epoch 628, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,630: INFO: model_training: Rank 0, Epoch 628, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,632: INFO: model_training: Rank 0, Epoch 629, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,633: INFO: model_training: Rank 0, Epoch 629, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,635: INFO: model_training: Rank 0, Epoch 629, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,637: INFO: model_training: Rank 0, Epoch 629, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,638: INFO: model_training: Rank 0, Epoch 629, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,639: INFO: model_training: Rank 0, Epoch 630, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,641: INFO: model_training: Rank 0, Epoch 630, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,642: INFO: model_training: Rank 0, Epoch 630, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,643: INFO: model_training: Rank 0, Epoch 630, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,645: INFO: model_training: Rank 0, Epoch 630, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,646: INFO: model_training: Rank 0, Epoch 631, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,648: INFO: model_training: Rank 0, Epoch 631, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,649: INFO: model_training: Rank 0, Epoch 631, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,651: INFO: model_training: Rank 0, Epoch 631, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,652: INFO: model_training: Rank 0, Epoch 631, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,654: INFO: model_training: Rank 0, Epoch 632, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,655: INFO: model_training: Rank 0, Epoch 632, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,656: INFO: model_training: Rank 0, Epoch 632, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,658: INFO: model_training: Rank 0, Epoch 632, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,659: INFO: model_training: Rank 0, Epoch 632, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,661: INFO: model_training: Rank 0, Epoch 633, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,662: INFO: model_training: Rank 0, Epoch 633, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,663: INFO: model_training: Rank 0, Epoch 633, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,665: INFO: model_training: Rank 0, Epoch 633, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,667: INFO: model_training: Rank 0, Epoch 633, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,668: INFO: model_training: Rank 0, Epoch 634, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,670: INFO: model_training: Rank 0, Epoch 634, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,671: INFO: model_training: Rank 0, Epoch 634, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,672: INFO: model_training: Rank 0, Epoch 634, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,673: INFO: model_training: Rank 0, Epoch 634, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,675: INFO: model_training: Rank 0, Epoch 635, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,676: INFO: model_training: Rank 0, Epoch 635, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,678: INFO: model_training: Rank 0, Epoch 635, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,679: INFO: model_training: Rank 0, Epoch 635, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,680: INFO: model_training: Rank 0, Epoch 635, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,682: INFO: model_training: Rank 0, Epoch 636, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,683: INFO: model_training: Rank 0, Epoch 636, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,684: INFO: model_training: Rank 0, Epoch 636, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,685: INFO: model_training: Rank 0, Epoch 636, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,687: INFO: model_training: Rank 0, Epoch 636, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,688: INFO: model_training: Rank 0, Epoch 637, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,689: INFO: model_training: Rank 0, Epoch 637, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,690: INFO: model_training: Rank 0, Epoch 637, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,691: INFO: model_training: Rank 0, Epoch 637, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,692: INFO: model_training: Rank 0, Epoch 637, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,695: INFO: model_training: Rank 0, Epoch 638, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,696: INFO: model_training: Rank 0, Epoch 638, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,698: INFO: model_training: Rank 0, Epoch 638, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,699: INFO: model_training: Rank 0, Epoch 638, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,700: INFO: model_training: Rank 0, Epoch 638, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,702: INFO: model_training: Rank 0, Epoch 639, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,703: INFO: model_training: Rank 0, Epoch 639, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,704: INFO: model_training: Rank 0, Epoch 639, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,706: INFO: model_training: Rank 0, Epoch 639, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,707: INFO: model_training: Rank 0, Epoch 639, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,708: INFO: model_training: Rank 0, Epoch 640, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,710: INFO: model_training: Rank 0, Epoch 640, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,711: INFO: model_training: Rank 0, Epoch 640, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,713: INFO: model_training: Rank 0, Epoch 640, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,714: INFO: model_training: Rank 0, Epoch 640, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,716: INFO: model_training: Rank 0, Epoch 641, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,717: INFO: model_training: Rank 0, Epoch 641, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,718: INFO: model_training: Rank 0, Epoch 641, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,720: INFO: model_training: Rank 0, Epoch 641, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,721: INFO: model_training: Rank 0, Epoch 641, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,722: INFO: model_training: Rank 0, Epoch 642, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,723: INFO: model_training: Rank 0, Epoch 642, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,725: INFO: model_training: Rank 0, Epoch 642, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,726: INFO: model_training: Rank 0, Epoch 642, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,728: INFO: model_training: Rank 0, Epoch 642, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,729: INFO: model_training: Rank 0, Epoch 643, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,731: INFO: model_training: Rank 0, Epoch 643, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,733: INFO: model_training: Rank 0, Epoch 643, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,734: INFO: model_training: Rank 0, Epoch 643, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,735: INFO: model_training: Rank 0, Epoch 643, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,736: INFO: model_training: Rank 0, Epoch 644, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,737: INFO: model_training: Rank 0, Epoch 644, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,739: INFO: model_training: Rank 0, Epoch 644, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,740: INFO: model_training: Rank 0, Epoch 644, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,741: INFO: model_training: Rank 0, Epoch 644, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,742: INFO: model_training: Rank 0, Epoch 645, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,743: INFO: model_training: Rank 0, Epoch 645, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,745: INFO: model_training: Rank 0, Epoch 645, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,746: INFO: model_training: Rank 0, Epoch 645, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,747: INFO: model_training: Rank 0, Epoch 645, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,749: INFO: model_training: Rank 0, Epoch 646, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,750: INFO: model_training: Rank 0, Epoch 646, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,751: INFO: model_training: Rank 0, Epoch 646, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,753: INFO: model_training: Rank 0, Epoch 646, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,754: INFO: model_training: Rank 0, Epoch 646, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,755: INFO: model_training: Rank 0, Epoch 647, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,757: INFO: model_training: Rank 0, Epoch 647, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,758: INFO: model_training: Rank 0, Epoch 647, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,760: INFO: model_training: Rank 0, Epoch 647, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,761: INFO: model_training: Rank 0, Epoch 647, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,763: INFO: model_training: Rank 0, Epoch 648, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,765: INFO: model_training: Rank 0, Epoch 648, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,766: INFO: model_training: Rank 0, Epoch 648, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,767: INFO: model_training: Rank 0, Epoch 648, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,769: INFO: model_training: Rank 0, Epoch 648, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,770: INFO: model_training: Rank 0, Epoch 649, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,772: INFO: model_training: Rank 0, Epoch 649, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,773: INFO: model_training: Rank 0, Epoch 649, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,774: INFO: model_training: Rank 0, Epoch 649, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,775: INFO: model_training: Rank 0, Epoch 649, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,778: INFO: model_training: Rank 0, Epoch 650, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,779: INFO: model_training: Rank 0, Epoch 650, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,780: INFO: model_training: Rank 0, Epoch 650, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,781: INFO: model_training: Rank 0, Epoch 650, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,782: INFO: model_training: Rank 0, Epoch 650, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,784: INFO: model_training: Rank 0, Epoch 651, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,785: INFO: model_training: Rank 0, Epoch 651, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,786: INFO: model_training: Rank 0, Epoch 651, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,787: INFO: model_training: Rank 0, Epoch 651, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,789: INFO: model_training: Rank 0, Epoch 651, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,790: INFO: model_training: Rank 0, Epoch 652, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,791: INFO: model_training: Rank 0, Epoch 652, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,792: INFO: model_training: Rank 0, Epoch 652, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,794: INFO: model_training: Rank 0, Epoch 652, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,795: INFO: model_training: Rank 0, Epoch 652, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,797: INFO: model_training: Rank 0, Epoch 653, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,798: INFO: model_training: Rank 0, Epoch 653, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,800: INFO: model_training: Rank 0, Epoch 653, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,801: INFO: model_training: Rank 0, Epoch 653, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,802: INFO: model_training: Rank 0, Epoch 653, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,804: INFO: model_training: Rank 0, Epoch 654, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,805: INFO: model_training: Rank 0, Epoch 654, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,806: INFO: model_training: Rank 0, Epoch 654, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,807: INFO: model_training: Rank 0, Epoch 654, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,809: INFO: model_training: Rank 0, Epoch 654, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,811: INFO: model_training: Rank 0, Epoch 655, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,812: INFO: model_training: Rank 0, Epoch 655, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,813: INFO: model_training: Rank 0, Epoch 655, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,814: INFO: model_training: Rank 0, Epoch 655, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,815: INFO: model_training: Rank 0, Epoch 655, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,817: INFO: model_training: Rank 0, Epoch 656, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,818: INFO: model_training: Rank 0, Epoch 656, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,820: INFO: model_training: Rank 0, Epoch 656, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,821: INFO: model_training: Rank 0, Epoch 656, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,822: INFO: model_training: Rank 0, Epoch 656, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,823: INFO: model_training: Rank 0, Epoch 657, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,825: INFO: model_training: Rank 0, Epoch 657, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,826: INFO: model_training: Rank 0, Epoch 657, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,827: INFO: model_training: Rank 0, Epoch 657, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,829: INFO: model_training: Rank 0, Epoch 657, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,830: INFO: model_training: Rank 0, Epoch 658, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,831: INFO: model_training: Rank 0, Epoch 658, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,833: INFO: model_training: Rank 0, Epoch 658, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,834: INFO: model_training: Rank 0, Epoch 658, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,836: INFO: model_training: Rank 0, Epoch 658, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,837: INFO: model_training: Rank 0, Epoch 659, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,838: INFO: model_training: Rank 0, Epoch 659, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,839: INFO: model_training: Rank 0, Epoch 659, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,840: INFO: model_training: Rank 0, Epoch 659, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,841: INFO: model_training: Rank 0, Epoch 659, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,843: INFO: model_training: Rank 0, Epoch 660, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,845: INFO: model_training: Rank 0, Epoch 660, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,846: INFO: model_training: Rank 0, Epoch 660, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,848: INFO: model_training: Rank 0, Epoch 660, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,849: INFO: model_training: Rank 0, Epoch 660, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,850: INFO: model_training: Rank 0, Epoch 661, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,852: INFO: model_training: Rank 0, Epoch 661, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,853: INFO: model_training: Rank 0, Epoch 661, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,854: INFO: model_training: Rank 0, Epoch 661, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,856: INFO: model_training: Rank 0, Epoch 661, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,857: INFO: model_training: Rank 0, Epoch 662, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,858: INFO: model_training: Rank 0, Epoch 662, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,860: INFO: model_training: Rank 0, Epoch 662, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,861: INFO: model_training: Rank 0, Epoch 662, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,863: INFO: model_training: Rank 0, Epoch 662, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,864: INFO: model_training: Rank 0, Epoch 663, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,866: INFO: model_training: Rank 0, Epoch 663, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,867: INFO: model_training: Rank 0, Epoch 663, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,868: INFO: model_training: Rank 0, Epoch 663, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,869: INFO: model_training: Rank 0, Epoch 663, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,871: INFO: model_training: Rank 0, Epoch 664, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,872: INFO: model_training: Rank 0, Epoch 664, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,873: INFO: model_training: Rank 0, Epoch 664, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,874: INFO: model_training: Rank 0, Epoch 664, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,876: INFO: model_training: Rank 0, Epoch 664, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,877: INFO: model_training: Rank 0, Epoch 665, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,878: INFO: model_training: Rank 0, Epoch 665, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,880: INFO: model_training: Rank 0, Epoch 665, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,880: INFO: model_training: Rank 0, Epoch 665, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,882: INFO: model_training: Rank 0, Epoch 665, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,884: INFO: model_training: Rank 0, Epoch 666, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,885: INFO: model_training: Rank 0, Epoch 666, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,887: INFO: model_training: Rank 0, Epoch 666, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,888: INFO: model_training: Rank 0, Epoch 666, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,889: INFO: model_training: Rank 0, Epoch 666, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,891: INFO: model_training: Rank 0, Epoch 667, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,892: INFO: model_training: Rank 0, Epoch 667, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,893: INFO: model_training: Rank 0, Epoch 667, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,895: INFO: model_training: Rank 0, Epoch 667, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,896: INFO: model_training: Rank 0, Epoch 667, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,898: INFO: model_training: Rank 0, Epoch 668, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,899: INFO: model_training: Rank 0, Epoch 668, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,900: INFO: model_training: Rank 0, Epoch 668, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,902: INFO: model_training: Rank 0, Epoch 668, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,903: INFO: model_training: Rank 0, Epoch 668, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,904: INFO: model_training: Rank 0, Epoch 669, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,906: INFO: model_training: Rank 0, Epoch 669, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,907: INFO: model_training: Rank 0, Epoch 669, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,908: INFO: model_training: Rank 0, Epoch 669, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,909: INFO: model_training: Rank 0, Epoch 669, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,911: INFO: model_training: Rank 0, Epoch 670, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,912: INFO: model_training: Rank 0, Epoch 670, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,914: INFO: model_training: Rank 0, Epoch 670, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,915: INFO: model_training: Rank 0, Epoch 670, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,916: INFO: model_training: Rank 0, Epoch 670, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,917: INFO: model_training: Rank 0, Epoch 671, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,919: INFO: model_training: Rank 0, Epoch 671, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,920: INFO: model_training: Rank 0, Epoch 671, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,921: INFO: model_training: Rank 0, Epoch 671, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,922: INFO: model_training: Rank 0, Epoch 671, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,924: INFO: model_training: Rank 0, Epoch 672, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,926: INFO: model_training: Rank 0, Epoch 672, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,927: INFO: model_training: Rank 0, Epoch 672, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,929: INFO: model_training: Rank 0, Epoch 672, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,930: INFO: model_training: Rank 0, Epoch 672, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,931: INFO: model_training: Rank 0, Epoch 673, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,932: INFO: model_training: Rank 0, Epoch 673, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,933: INFO: model_training: Rank 0, Epoch 673, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,935: INFO: model_training: Rank 0, Epoch 673, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,936: INFO: model_training: Rank 0, Epoch 673, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,938: INFO: model_training: Rank 0, Epoch 674, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,939: INFO: model_training: Rank 0, Epoch 674, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,940: INFO: model_training: Rank 0, Epoch 674, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,941: INFO: model_training: Rank 0, Epoch 674, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,942: INFO: model_training: Rank 0, Epoch 674, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,944: INFO: model_training: Rank 0, Epoch 675, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,945: INFO: model_training: Rank 0, Epoch 675, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,947: INFO: model_training: Rank 0, Epoch 675, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,948: INFO: model_training: Rank 0, Epoch 675, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,949: INFO: model_training: Rank 0, Epoch 675, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,950: INFO: model_training: Rank 0, Epoch 676, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,952: INFO: model_training: Rank 0, Epoch 676, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,954: INFO: model_training: Rank 0, Epoch 676, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,955: INFO: model_training: Rank 0, Epoch 676, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,956: INFO: model_training: Rank 0, Epoch 676, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,957: INFO: model_training: Rank 0, Epoch 677, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,959: INFO: model_training: Rank 0, Epoch 677, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,960: INFO: model_training: Rank 0, Epoch 677, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,962: INFO: model_training: Rank 0, Epoch 677, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,963: INFO: model_training: Rank 0, Epoch 677, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,964: INFO: model_training: Rank 0, Epoch 678, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,966: INFO: model_training: Rank 0, Epoch 678, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,967: INFO: model_training: Rank 0, Epoch 678, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,968: INFO: model_training: Rank 0, Epoch 678, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,969: INFO: model_training: Rank 0, Epoch 678, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,971: INFO: model_training: Rank 0, Epoch 679, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,973: INFO: model_training: Rank 0, Epoch 679, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,974: INFO: model_training: Rank 0, Epoch 679, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,975: INFO: model_training: Rank 0, Epoch 679, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,976: INFO: model_training: Rank 0, Epoch 679, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,978: INFO: model_training: Rank 0, Epoch 680, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,979: INFO: model_training: Rank 0, Epoch 680, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,981: INFO: model_training: Rank 0, Epoch 680, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,982: INFO: model_training: Rank 0, Epoch 680, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,983: INFO: model_training: Rank 0, Epoch 680, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,986: INFO: model_training: Rank 0, Epoch 681, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,987: INFO: model_training: Rank 0, Epoch 681, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,988: INFO: model_training: Rank 0, Epoch 681, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,989: INFO: model_training: Rank 0, Epoch 681, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,991: INFO: model_training: Rank 0, Epoch 681, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:47,993: INFO: model_training: Rank 0, Epoch 682, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:47,995: INFO: model_training: Rank 0, Epoch 682, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:47,996: INFO: model_training: Rank 0, Epoch 682, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:47,997: INFO: model_training: Rank 0, Epoch 682, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:47,998: INFO: model_training: Rank 0, Epoch 682, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,000: INFO: model_training: Rank 0, Epoch 683, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,001: INFO: model_training: Rank 0, Epoch 683, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,002: INFO: model_training: Rank 0, Epoch 683, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,003: INFO: model_training: Rank 0, Epoch 683, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,004: INFO: model_training: Rank 0, Epoch 683, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,006: INFO: model_training: Rank 0, Epoch 684, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,007: INFO: model_training: Rank 0, Epoch 684, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,008: INFO: model_training: Rank 0, Epoch 684, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,010: INFO: model_training: Rank 0, Epoch 684, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,011: INFO: model_training: Rank 0, Epoch 684, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,013: INFO: model_training: Rank 0, Epoch 685, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,014: INFO: model_training: Rank 0, Epoch 685, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,015: INFO: model_training: Rank 0, Epoch 685, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,016: INFO: model_training: Rank 0, Epoch 685, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,018: INFO: model_training: Rank 0, Epoch 685, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,020: INFO: model_training: Rank 0, Epoch 686, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,021: INFO: model_training: Rank 0, Epoch 686, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,023: INFO: model_training: Rank 0, Epoch 686, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,024: INFO: model_training: Rank 0, Epoch 686, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,025: INFO: model_training: Rank 0, Epoch 686, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,027: INFO: model_training: Rank 0, Epoch 687, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,029: INFO: model_training: Rank 0, Epoch 687, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,030: INFO: model_training: Rank 0, Epoch 687, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,031: INFO: model_training: Rank 0, Epoch 687, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,032: INFO: model_training: Rank 0, Epoch 687, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,034: INFO: model_training: Rank 0, Epoch 688, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,035: INFO: model_training: Rank 0, Epoch 688, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,036: INFO: model_training: Rank 0, Epoch 688, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,037: INFO: model_training: Rank 0, Epoch 688, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,039: INFO: model_training: Rank 0, Epoch 688, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,040: INFO: model_training: Rank 0, Epoch 689, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,042: INFO: model_training: Rank 0, Epoch 689, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,043: INFO: model_training: Rank 0, Epoch 689, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,044: INFO: model_training: Rank 0, Epoch 689, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,046: INFO: model_training: Rank 0, Epoch 689, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,047: INFO: model_training: Rank 0, Epoch 690, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,048: INFO: model_training: Rank 0, Epoch 690, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,049: INFO: model_training: Rank 0, Epoch 690, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,050: INFO: model_training: Rank 0, Epoch 690, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,052: INFO: model_training: Rank 0, Epoch 690, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,054: INFO: model_training: Rank 0, Epoch 691, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,055: INFO: model_training: Rank 0, Epoch 691, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,056: INFO: model_training: Rank 0, Epoch 691, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,058: INFO: model_training: Rank 0, Epoch 691, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,059: INFO: model_training: Rank 0, Epoch 691, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,061: INFO: model_training: Rank 0, Epoch 692, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,062: INFO: model_training: Rank 0, Epoch 692, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,064: INFO: model_training: Rank 0, Epoch 692, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,065: INFO: model_training: Rank 0, Epoch 692, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,066: INFO: model_training: Rank 0, Epoch 692, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,067: INFO: model_training: Rank 0, Epoch 693, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,069: INFO: model_training: Rank 0, Epoch 693, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,070: INFO: model_training: Rank 0, Epoch 693, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,071: INFO: model_training: Rank 0, Epoch 693, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,073: INFO: model_training: Rank 0, Epoch 693, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,074: INFO: model_training: Rank 0, Epoch 694, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,075: INFO: model_training: Rank 0, Epoch 694, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,077: INFO: model_training: Rank 0, Epoch 694, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,078: INFO: model_training: Rank 0, Epoch 694, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,079: INFO: model_training: Rank 0, Epoch 694, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,080: INFO: model_training: Rank 0, Epoch 695, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,082: INFO: model_training: Rank 0, Epoch 695, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,083: INFO: model_training: Rank 0, Epoch 695, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,084: INFO: model_training: Rank 0, Epoch 695, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,086: INFO: model_training: Rank 0, Epoch 695, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,088: INFO: model_training: Rank 0, Epoch 696, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,089: INFO: model_training: Rank 0, Epoch 696, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,091: INFO: model_training: Rank 0, Epoch 696, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,092: INFO: model_training: Rank 0, Epoch 696, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,093: INFO: model_training: Rank 0, Epoch 696, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,095: INFO: model_training: Rank 0, Epoch 697, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,096: INFO: model_training: Rank 0, Epoch 697, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,098: INFO: model_training: Rank 0, Epoch 697, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,099: INFO: model_training: Rank 0, Epoch 697, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,100: INFO: model_training: Rank 0, Epoch 697, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,102: INFO: model_training: Rank 0, Epoch 698, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,103: INFO: model_training: Rank 0, Epoch 698, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,104: INFO: model_training: Rank 0, Epoch 698, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,106: INFO: model_training: Rank 0, Epoch 698, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,107: INFO: model_training: Rank 0, Epoch 698, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,108: INFO: model_training: Rank 0, Epoch 699, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,109: INFO: model_training: Rank 0, Epoch 699, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,111: INFO: model_training: Rank 0, Epoch 699, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,113: INFO: model_training: Rank 0, Epoch 699, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,114: INFO: model_training: Rank 0, Epoch 699, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,116: INFO: model_training: Rank 0, Epoch 700, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,117: INFO: model_training: Rank 0, Epoch 700, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,118: INFO: model_training: Rank 0, Epoch 700, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,120: INFO: model_training: Rank 0, Epoch 700, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,121: INFO: model_training: Rank 0, Epoch 700, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,123: INFO: model_training: Rank 0, Epoch 701, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,124: INFO: model_training: Rank 0, Epoch 701, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,125: INFO: model_training: Rank 0, Epoch 701, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,126: INFO: model_training: Rank 0, Epoch 701, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,128: INFO: model_training: Rank 0, Epoch 701, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,130: INFO: model_training: Rank 0, Epoch 702, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,131: INFO: model_training: Rank 0, Epoch 702, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,132: INFO: model_training: Rank 0, Epoch 702, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,133: INFO: model_training: Rank 0, Epoch 702, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,135: INFO: model_training: Rank 0, Epoch 702, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,137: INFO: model_training: Rank 0, Epoch 703, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,138: INFO: model_training: Rank 0, Epoch 703, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,139: INFO: model_training: Rank 0, Epoch 703, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,140: INFO: model_training: Rank 0, Epoch 703, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,141: INFO: model_training: Rank 0, Epoch 703, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,143: INFO: model_training: Rank 0, Epoch 704, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,144: INFO: model_training: Rank 0, Epoch 704, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,146: INFO: model_training: Rank 0, Epoch 704, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,147: INFO: model_training: Rank 0, Epoch 704, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,148: INFO: model_training: Rank 0, Epoch 704, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,150: INFO: model_training: Rank 0, Epoch 705, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,150: INFO: model_training: Rank 0, Epoch 705, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,152: INFO: model_training: Rank 0, Epoch 705, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,153: INFO: model_training: Rank 0, Epoch 705, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,155: INFO: model_training: Rank 0, Epoch 705, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,156: INFO: model_training: Rank 0, Epoch 706, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,158: INFO: model_training: Rank 0, Epoch 706, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,159: INFO: model_training: Rank 0, Epoch 706, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,160: INFO: model_training: Rank 0, Epoch 706, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,162: INFO: model_training: Rank 0, Epoch 706, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,163: INFO: model_training: Rank 0, Epoch 707, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,164: INFO: model_training: Rank 0, Epoch 707, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,166: INFO: model_training: Rank 0, Epoch 707, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,167: INFO: model_training: Rank 0, Epoch 707, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,169: INFO: model_training: Rank 0, Epoch 707, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,170: INFO: model_training: Rank 0, Epoch 708, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,171: INFO: model_training: Rank 0, Epoch 708, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,173: INFO: model_training: Rank 0, Epoch 708, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,174: INFO: model_training: Rank 0, Epoch 708, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,175: INFO: model_training: Rank 0, Epoch 708, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,176: INFO: model_training: Rank 0, Epoch 709, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,178: INFO: model_training: Rank 0, Epoch 709, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,180: INFO: model_training: Rank 0, Epoch 709, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,181: INFO: model_training: Rank 0, Epoch 709, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,182: INFO: model_training: Rank 0, Epoch 709, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,184: INFO: model_training: Rank 0, Epoch 710, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,185: INFO: model_training: Rank 0, Epoch 710, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,187: INFO: model_training: Rank 0, Epoch 710, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,188: INFO: model_training: Rank 0, Epoch 710, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,189: INFO: model_training: Rank 0, Epoch 710, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,190: INFO: model_training: Rank 0, Epoch 711, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,191: INFO: model_training: Rank 0, Epoch 711, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,193: INFO: model_training: Rank 0, Epoch 711, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,194: INFO: model_training: Rank 0, Epoch 711, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,195: INFO: model_training: Rank 0, Epoch 711, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,197: INFO: model_training: Rank 0, Epoch 712, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,198: INFO: model_training: Rank 0, Epoch 712, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,199: INFO: model_training: Rank 0, Epoch 712, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,200: INFO: model_training: Rank 0, Epoch 712, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,202: INFO: model_training: Rank 0, Epoch 712, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,204: INFO: model_training: Rank 0, Epoch 713, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,205: INFO: model_training: Rank 0, Epoch 713, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,206: INFO: model_training: Rank 0, Epoch 713, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,208: INFO: model_training: Rank 0, Epoch 713, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,209: INFO: model_training: Rank 0, Epoch 713, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,212: INFO: model_training: Rank 0, Epoch 714, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,213: INFO: model_training: Rank 0, Epoch 714, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,214: INFO: model_training: Rank 0, Epoch 714, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,215: INFO: model_training: Rank 0, Epoch 714, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,216: INFO: model_training: Rank 0, Epoch 714, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,218: INFO: model_training: Rank 0, Epoch 715, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,220: INFO: model_training: Rank 0, Epoch 715, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,221: INFO: model_training: Rank 0, Epoch 715, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,222: INFO: model_training: Rank 0, Epoch 715, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,224: INFO: model_training: Rank 0, Epoch 715, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,225: INFO: model_training: Rank 0, Epoch 716, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,227: INFO: model_training: Rank 0, Epoch 716, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,228: INFO: model_training: Rank 0, Epoch 716, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,229: INFO: model_training: Rank 0, Epoch 716, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,230: INFO: model_training: Rank 0, Epoch 716, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,232: INFO: model_training: Rank 0, Epoch 717, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,233: INFO: model_training: Rank 0, Epoch 717, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,234: INFO: model_training: Rank 0, Epoch 717, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,236: INFO: model_training: Rank 0, Epoch 717, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,237: INFO: model_training: Rank 0, Epoch 717, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,238: INFO: model_training: Rank 0, Epoch 718, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,239: INFO: model_training: Rank 0, Epoch 718, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,241: INFO: model_training: Rank 0, Epoch 718, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,242: INFO: model_training: Rank 0, Epoch 718, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,244: INFO: model_training: Rank 0, Epoch 718, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,246: INFO: model_training: Rank 0, Epoch 719, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,247: INFO: model_training: Rank 0, Epoch 719, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,248: INFO: model_training: Rank 0, Epoch 719, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,249: INFO: model_training: Rank 0, Epoch 719, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,250: INFO: model_training: Rank 0, Epoch 719, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,252: INFO: model_training: Rank 0, Epoch 720, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,253: INFO: model_training: Rank 0, Epoch 720, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,254: INFO: model_training: Rank 0, Epoch 720, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,255: INFO: model_training: Rank 0, Epoch 720, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,257: INFO: model_training: Rank 0, Epoch 720, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,258: INFO: model_training: Rank 0, Epoch 721, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,260: INFO: model_training: Rank 0, Epoch 721, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,261: INFO: model_training: Rank 0, Epoch 721, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,263: INFO: model_training: Rank 0, Epoch 721, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,264: INFO: model_training: Rank 0, Epoch 721, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,266: INFO: model_training: Rank 0, Epoch 722, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,267: INFO: model_training: Rank 0, Epoch 722, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,268: INFO: model_training: Rank 0, Epoch 722, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,270: INFO: model_training: Rank 0, Epoch 722, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,271: INFO: model_training: Rank 0, Epoch 722, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,272: INFO: model_training: Rank 0, Epoch 723, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,274: INFO: model_training: Rank 0, Epoch 723, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,275: INFO: model_training: Rank 0, Epoch 723, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,276: INFO: model_training: Rank 0, Epoch 723, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,277: INFO: model_training: Rank 0, Epoch 723, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,279: INFO: model_training: Rank 0, Epoch 724, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,280: INFO: model_training: Rank 0, Epoch 724, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,281: INFO: model_training: Rank 0, Epoch 724, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,283: INFO: model_training: Rank 0, Epoch 724, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,284: INFO: model_training: Rank 0, Epoch 724, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,286: INFO: model_training: Rank 0, Epoch 725, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,287: INFO: model_training: Rank 0, Epoch 725, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,288: INFO: model_training: Rank 0, Epoch 725, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,290: INFO: model_training: Rank 0, Epoch 725, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,291: INFO: model_training: Rank 0, Epoch 725, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,292: INFO: model_training: Rank 0, Epoch 726, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,294: INFO: model_training: Rank 0, Epoch 726, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,296: INFO: model_training: Rank 0, Epoch 726, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,297: INFO: model_training: Rank 0, Epoch 726, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,298: INFO: model_training: Rank 0, Epoch 726, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,299: INFO: model_training: Rank 0, Epoch 727, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,301: INFO: model_training: Rank 0, Epoch 727, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,302: INFO: model_training: Rank 0, Epoch 727, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,304: INFO: model_training: Rank 0, Epoch 727, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,305: INFO: model_training: Rank 0, Epoch 727, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,306: INFO: model_training: Rank 0, Epoch 728, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,307: INFO: model_training: Rank 0, Epoch 728, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,309: INFO: model_training: Rank 0, Epoch 728, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,310: INFO: model_training: Rank 0, Epoch 728, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,311: INFO: model_training: Rank 0, Epoch 728, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,313: INFO: model_training: Rank 0, Epoch 729, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,315: INFO: model_training: Rank 0, Epoch 729, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,316: INFO: model_training: Rank 0, Epoch 729, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,317: INFO: model_training: Rank 0, Epoch 729, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,318: INFO: model_training: Rank 0, Epoch 729, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,320: INFO: model_training: Rank 0, Epoch 730, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,321: INFO: model_training: Rank 0, Epoch 730, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,322: INFO: model_training: Rank 0, Epoch 730, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,324: INFO: model_training: Rank 0, Epoch 730, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,325: INFO: model_training: Rank 0, Epoch 730, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,327: INFO: model_training: Rank 0, Epoch 731, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,328: INFO: model_training: Rank 0, Epoch 731, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,329: INFO: model_training: Rank 0, Epoch 731, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,330: INFO: model_training: Rank 0, Epoch 731, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,332: INFO: model_training: Rank 0, Epoch 731, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,333: INFO: model_training: Rank 0, Epoch 732, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,335: INFO: model_training: Rank 0, Epoch 732, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,336: INFO: model_training: Rank 0, Epoch 732, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,338: INFO: model_training: Rank 0, Epoch 732, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,339: INFO: model_training: Rank 0, Epoch 732, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,341: INFO: model_training: Rank 0, Epoch 733, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,342: INFO: model_training: Rank 0, Epoch 733, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,344: INFO: model_training: Rank 0, Epoch 733, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,345: INFO: model_training: Rank 0, Epoch 733, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,346: INFO: model_training: Rank 0, Epoch 733, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,347: INFO: model_training: Rank 0, Epoch 734, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,349: INFO: model_training: Rank 0, Epoch 734, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,350: INFO: model_training: Rank 0, Epoch 734, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,352: INFO: model_training: Rank 0, Epoch 734, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,353: INFO: model_training: Rank 0, Epoch 734, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,354: INFO: model_training: Rank 0, Epoch 735, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,356: INFO: model_training: Rank 0, Epoch 735, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,357: INFO: model_training: Rank 0, Epoch 735, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,359: INFO: model_training: Rank 0, Epoch 735, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,360: INFO: model_training: Rank 0, Epoch 735, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,362: INFO: model_training: Rank 0, Epoch 736, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,364: INFO: model_training: Rank 0, Epoch 736, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,365: INFO: model_training: Rank 0, Epoch 736, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,366: INFO: model_training: Rank 0, Epoch 736, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,367: INFO: model_training: Rank 0, Epoch 736, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,370: INFO: model_training: Rank 0, Epoch 737, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,371: INFO: model_training: Rank 0, Epoch 737, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,372: INFO: model_training: Rank 0, Epoch 737, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,373: INFO: model_training: Rank 0, Epoch 737, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,375: INFO: model_training: Rank 0, Epoch 737, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,376: INFO: model_training: Rank 0, Epoch 738, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,378: INFO: model_training: Rank 0, Epoch 738, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,380: INFO: model_training: Rank 0, Epoch 738, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,381: INFO: model_training: Rank 0, Epoch 738, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,382: INFO: model_training: Rank 0, Epoch 738, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,384: INFO: model_training: Rank 0, Epoch 739, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,385: INFO: model_training: Rank 0, Epoch 739, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,386: INFO: model_training: Rank 0, Epoch 739, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,387: INFO: model_training: Rank 0, Epoch 739, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,389: INFO: model_training: Rank 0, Epoch 739, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,391: INFO: model_training: Rank 0, Epoch 740, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,392: INFO: model_training: Rank 0, Epoch 740, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,394: INFO: model_training: Rank 0, Epoch 740, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,395: INFO: model_training: Rank 0, Epoch 740, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,396: INFO: model_training: Rank 0, Epoch 740, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,398: INFO: model_training: Rank 0, Epoch 741, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,399: INFO: model_training: Rank 0, Epoch 741, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,400: INFO: model_training: Rank 0, Epoch 741, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,401: INFO: model_training: Rank 0, Epoch 741, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,403: INFO: model_training: Rank 0, Epoch 741, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,405: INFO: model_training: Rank 0, Epoch 742, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,406: INFO: model_training: Rank 0, Epoch 742, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,407: INFO: model_training: Rank 0, Epoch 742, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,409: INFO: model_training: Rank 0, Epoch 742, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,410: INFO: model_training: Rank 0, Epoch 742, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,412: INFO: model_training: Rank 0, Epoch 743, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,413: INFO: model_training: Rank 0, Epoch 743, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,415: INFO: model_training: Rank 0, Epoch 743, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,416: INFO: model_training: Rank 0, Epoch 743, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,417: INFO: model_training: Rank 0, Epoch 743, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,419: INFO: model_training: Rank 0, Epoch 744, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,421: INFO: model_training: Rank 0, Epoch 744, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,422: INFO: model_training: Rank 0, Epoch 744, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,423: INFO: model_training: Rank 0, Epoch 744, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,424: INFO: model_training: Rank 0, Epoch 744, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,425: INFO: model_training: Rank 0, Epoch 745, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,428: INFO: model_training: Rank 0, Epoch 745, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,429: INFO: model_training: Rank 0, Epoch 745, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,430: INFO: model_training: Rank 0, Epoch 745, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,431: INFO: model_training: Rank 0, Epoch 745, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,433: INFO: model_training: Rank 0, Epoch 746, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,434: INFO: model_training: Rank 0, Epoch 746, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,435: INFO: model_training: Rank 0, Epoch 746, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,437: INFO: model_training: Rank 0, Epoch 746, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,438: INFO: model_training: Rank 0, Epoch 746, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,440: INFO: model_training: Rank 0, Epoch 747, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,441: INFO: model_training: Rank 0, Epoch 747, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,442: INFO: model_training: Rank 0, Epoch 747, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,444: INFO: model_training: Rank 0, Epoch 747, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,446: INFO: model_training: Rank 0, Epoch 747, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,447: INFO: model_training: Rank 0, Epoch 748, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,448: INFO: model_training: Rank 0, Epoch 748, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,450: INFO: model_training: Rank 0, Epoch 748, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,451: INFO: model_training: Rank 0, Epoch 748, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,452: INFO: model_training: Rank 0, Epoch 748, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,454: INFO: model_training: Rank 0, Epoch 749, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,455: INFO: model_training: Rank 0, Epoch 749, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,457: INFO: model_training: Rank 0, Epoch 749, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,458: INFO: model_training: Rank 0, Epoch 749, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,459: INFO: model_training: Rank 0, Epoch 749, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,461: INFO: model_training: Rank 0, Epoch 750, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,463: INFO: model_training: Rank 0, Epoch 750, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,464: INFO: model_training: Rank 0, Epoch 750, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,465: INFO: model_training: Rank 0, Epoch 750, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,467: INFO: model_training: Rank 0, Epoch 750, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,469: INFO: model_training: Rank 0, Epoch 751, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,471: INFO: model_training: Rank 0, Epoch 751, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,472: INFO: model_training: Rank 0, Epoch 751, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,473: INFO: model_training: Rank 0, Epoch 751, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,474: INFO: model_training: Rank 0, Epoch 751, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,475: INFO: model_training: Rank 0, Epoch 752, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,477: INFO: model_training: Rank 0, Epoch 752, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,479: INFO: model_training: Rank 0, Epoch 752, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,480: INFO: model_training: Rank 0, Epoch 752, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,481: INFO: model_training: Rank 0, Epoch 752, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,483: INFO: model_training: Rank 0, Epoch 753, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,484: INFO: model_training: Rank 0, Epoch 753, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,485: INFO: model_training: Rank 0, Epoch 753, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,486: INFO: model_training: Rank 0, Epoch 753, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,488: INFO: model_training: Rank 0, Epoch 753, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,490: INFO: model_training: Rank 0, Epoch 754, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,492: INFO: model_training: Rank 0, Epoch 754, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,493: INFO: model_training: Rank 0, Epoch 754, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,495: INFO: model_training: Rank 0, Epoch 754, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,496: INFO: model_training: Rank 0, Epoch 754, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,498: INFO: model_training: Rank 0, Epoch 755, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,499: INFO: model_training: Rank 0, Epoch 755, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,500: INFO: model_training: Rank 0, Epoch 755, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,501: INFO: model_training: Rank 0, Epoch 755, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,503: INFO: model_training: Rank 0, Epoch 755, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,505: INFO: model_training: Rank 0, Epoch 756, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,506: INFO: model_training: Rank 0, Epoch 756, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,508: INFO: model_training: Rank 0, Epoch 756, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,509: INFO: model_training: Rank 0, Epoch 756, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,510: INFO: model_training: Rank 0, Epoch 756, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,513: INFO: model_training: Rank 0, Epoch 757, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,515: INFO: model_training: Rank 0, Epoch 757, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,516: INFO: model_training: Rank 0, Epoch 757, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,518: INFO: model_training: Rank 0, Epoch 757, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,520: INFO: model_training: Rank 0, Epoch 757, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,522: INFO: model_training: Rank 0, Epoch 758, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,524: INFO: model_training: Rank 0, Epoch 758, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,525: INFO: model_training: Rank 0, Epoch 758, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,526: INFO: model_training: Rank 0, Epoch 758, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,528: INFO: model_training: Rank 0, Epoch 758, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,530: INFO: model_training: Rank 0, Epoch 759, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,531: INFO: model_training: Rank 0, Epoch 759, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,532: INFO: model_training: Rank 0, Epoch 759, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,533: INFO: model_training: Rank 0, Epoch 759, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,534: INFO: model_training: Rank 0, Epoch 759, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,537: INFO: model_training: Rank 0, Epoch 760, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,538: INFO: model_training: Rank 0, Epoch 760, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,539: INFO: model_training: Rank 0, Epoch 760, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,540: INFO: model_training: Rank 0, Epoch 760, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,542: INFO: model_training: Rank 0, Epoch 760, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,544: INFO: model_training: Rank 0, Epoch 761, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,546: INFO: model_training: Rank 0, Epoch 761, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,547: INFO: model_training: Rank 0, Epoch 761, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,548: INFO: model_training: Rank 0, Epoch 761, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,549: INFO: model_training: Rank 0, Epoch 761, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,550: INFO: model_training: Rank 0, Epoch 762, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,552: INFO: model_training: Rank 0, Epoch 762, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,554: INFO: model_training: Rank 0, Epoch 762, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,555: INFO: model_training: Rank 0, Epoch 762, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,556: INFO: model_training: Rank 0, Epoch 762, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,558: INFO: model_training: Rank 0, Epoch 763, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,559: INFO: model_training: Rank 0, Epoch 763, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,560: INFO: model_training: Rank 0, Epoch 763, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,562: INFO: model_training: Rank 0, Epoch 763, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,564: INFO: model_training: Rank 0, Epoch 763, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,565: INFO: model_training: Rank 0, Epoch 764, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,567: INFO: model_training: Rank 0, Epoch 764, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,569: INFO: model_training: Rank 0, Epoch 764, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,570: INFO: model_training: Rank 0, Epoch 764, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,571: INFO: model_training: Rank 0, Epoch 764, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,573: INFO: model_training: Rank 0, Epoch 765, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,574: INFO: model_training: Rank 0, Epoch 765, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,575: INFO: model_training: Rank 0, Epoch 765, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,577: INFO: model_training: Rank 0, Epoch 765, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,579: INFO: model_training: Rank 0, Epoch 765, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,580: INFO: model_training: Rank 0, Epoch 766, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,581: INFO: model_training: Rank 0, Epoch 766, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,582: INFO: model_training: Rank 0, Epoch 766, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,584: INFO: model_training: Rank 0, Epoch 766, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,585: INFO: model_training: Rank 0, Epoch 766, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,587: INFO: model_training: Rank 0, Epoch 767, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,589: INFO: model_training: Rank 0, Epoch 767, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,590: INFO: model_training: Rank 0, Epoch 767, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,591: INFO: model_training: Rank 0, Epoch 767, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,593: INFO: model_training: Rank 0, Epoch 767, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,595: INFO: model_training: Rank 0, Epoch 768, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,596: INFO: model_training: Rank 0, Epoch 768, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,597: INFO: model_training: Rank 0, Epoch 768, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,599: INFO: model_training: Rank 0, Epoch 768, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,600: INFO: model_training: Rank 0, Epoch 768, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,602: INFO: model_training: Rank 0, Epoch 769, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,604: INFO: model_training: Rank 0, Epoch 769, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,605: INFO: model_training: Rank 0, Epoch 769, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,606: INFO: model_training: Rank 0, Epoch 769, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,608: INFO: model_training: Rank 0, Epoch 769, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,609: INFO: model_training: Rank 0, Epoch 770, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,611: INFO: model_training: Rank 0, Epoch 770, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,612: INFO: model_training: Rank 0, Epoch 770, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,613: INFO: model_training: Rank 0, Epoch 770, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,615: INFO: model_training: Rank 0, Epoch 770, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,617: INFO: model_training: Rank 0, Epoch 771, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,619: INFO: model_training: Rank 0, Epoch 771, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,620: INFO: model_training: Rank 0, Epoch 771, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,621: INFO: model_training: Rank 0, Epoch 771, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,623: INFO: model_training: Rank 0, Epoch 771, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,624: INFO: model_training: Rank 0, Epoch 772, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,625: INFO: model_training: Rank 0, Epoch 772, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,627: INFO: model_training: Rank 0, Epoch 772, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,628: INFO: model_training: Rank 0, Epoch 772, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,630: INFO: model_training: Rank 0, Epoch 772, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,631: INFO: model_training: Rank 0, Epoch 773, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,633: INFO: model_training: Rank 0, Epoch 773, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,634: INFO: model_training: Rank 0, Epoch 773, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,636: INFO: model_training: Rank 0, Epoch 773, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,637: INFO: model_training: Rank 0, Epoch 773, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,639: INFO: model_training: Rank 0, Epoch 774, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,640: INFO: model_training: Rank 0, Epoch 774, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,642: INFO: model_training: Rank 0, Epoch 774, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,644: INFO: model_training: Rank 0, Epoch 774, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,645: INFO: model_training: Rank 0, Epoch 774, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,647: INFO: model_training: Rank 0, Epoch 775, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,648: INFO: model_training: Rank 0, Epoch 775, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,650: INFO: model_training: Rank 0, Epoch 775, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,651: INFO: model_training: Rank 0, Epoch 775, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,652: INFO: model_training: Rank 0, Epoch 775, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,654: INFO: model_training: Rank 0, Epoch 776, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,655: INFO: model_training: Rank 0, Epoch 776, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,657: INFO: model_training: Rank 0, Epoch 776, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,658: INFO: model_training: Rank 0, Epoch 776, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,659: INFO: model_training: Rank 0, Epoch 776, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,661: INFO: model_training: Rank 0, Epoch 777, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,663: INFO: model_training: Rank 0, Epoch 777, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,664: INFO: model_training: Rank 0, Epoch 777, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,665: INFO: model_training: Rank 0, Epoch 777, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,666: INFO: model_training: Rank 0, Epoch 777, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,668: INFO: model_training: Rank 0, Epoch 778, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,670: INFO: model_training: Rank 0, Epoch 778, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,671: INFO: model_training: Rank 0, Epoch 778, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,672: INFO: model_training: Rank 0, Epoch 778, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,673: INFO: model_training: Rank 0, Epoch 778, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,675: INFO: model_training: Rank 0, Epoch 779, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,677: INFO: model_training: Rank 0, Epoch 779, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,678: INFO: model_training: Rank 0, Epoch 779, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,679: INFO: model_training: Rank 0, Epoch 779, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,680: INFO: model_training: Rank 0, Epoch 779, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,682: INFO: model_training: Rank 0, Epoch 780, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,684: INFO: model_training: Rank 0, Epoch 780, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,684: INFO: model_training: Rank 0, Epoch 780, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,686: INFO: model_training: Rank 0, Epoch 780, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,688: INFO: model_training: Rank 0, Epoch 780, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,689: INFO: model_training: Rank 0, Epoch 781, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,690: INFO: model_training: Rank 0, Epoch 781, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,692: INFO: model_training: Rank 0, Epoch 781, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,693: INFO: model_training: Rank 0, Epoch 781, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,695: INFO: model_training: Rank 0, Epoch 781, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,696: INFO: model_training: Rank 0, Epoch 782, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,698: INFO: model_training: Rank 0, Epoch 782, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,699: INFO: model_training: Rank 0, Epoch 782, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,700: INFO: model_training: Rank 0, Epoch 782, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,702: INFO: model_training: Rank 0, Epoch 782, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,703: INFO: model_training: Rank 0, Epoch 783, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,705: INFO: model_training: Rank 0, Epoch 783, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,705: INFO: model_training: Rank 0, Epoch 783, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,707: INFO: model_training: Rank 0, Epoch 783, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,708: INFO: model_training: Rank 0, Epoch 783, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,710: INFO: model_training: Rank 0, Epoch 784, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,711: INFO: model_training: Rank 0, Epoch 784, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,713: INFO: model_training: Rank 0, Epoch 784, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,714: INFO: model_training: Rank 0, Epoch 784, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,716: INFO: model_training: Rank 0, Epoch 784, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,717: INFO: model_training: Rank 0, Epoch 785, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,719: INFO: model_training: Rank 0, Epoch 785, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,720: INFO: model_training: Rank 0, Epoch 785, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,721: INFO: model_training: Rank 0, Epoch 785, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,722: INFO: model_training: Rank 0, Epoch 785, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,724: INFO: model_training: Rank 0, Epoch 786, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,725: INFO: model_training: Rank 0, Epoch 786, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,726: INFO: model_training: Rank 0, Epoch 786, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,728: INFO: model_training: Rank 0, Epoch 786, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,748: INFO: model_training: Rank 0, Epoch 786, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,752: INFO: model_training: Rank 0, Epoch 787, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,754: INFO: model_training: Rank 0, Epoch 787, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,756: INFO: model_training: Rank 0, Epoch 787, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,758: INFO: model_training: Rank 0, Epoch 787, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,763: INFO: model_training: Rank 0, Epoch 787, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,766: INFO: model_training: Rank 0, Epoch 788, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,767: INFO: model_training: Rank 0, Epoch 788, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,768: INFO: model_training: Rank 0, Epoch 788, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,770: INFO: model_training: Rank 0, Epoch 788, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,771: INFO: model_training: Rank 0, Epoch 788, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,773: INFO: model_training: Rank 0, Epoch 789, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,774: INFO: model_training: Rank 0, Epoch 789, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,775: INFO: model_training: Rank 0, Epoch 789, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,777: INFO: model_training: Rank 0, Epoch 789, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,778: INFO: model_training: Rank 0, Epoch 789, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,780: INFO: model_training: Rank 0, Epoch 790, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,781: INFO: model_training: Rank 0, Epoch 790, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,782: INFO: model_training: Rank 0, Epoch 790, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,784: INFO: model_training: Rank 0, Epoch 790, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,785: INFO: model_training: Rank 0, Epoch 790, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,787: INFO: model_training: Rank 0, Epoch 791, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,789: INFO: model_training: Rank 0, Epoch 791, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,790: INFO: model_training: Rank 0, Epoch 791, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,791: INFO: model_training: Rank 0, Epoch 791, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,792: INFO: model_training: Rank 0, Epoch 791, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,794: INFO: model_training: Rank 0, Epoch 792, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,796: INFO: model_training: Rank 0, Epoch 792, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,797: INFO: model_training: Rank 0, Epoch 792, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,798: INFO: model_training: Rank 0, Epoch 792, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,799: INFO: model_training: Rank 0, Epoch 792, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,801: INFO: model_training: Rank 0, Epoch 793, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,802: INFO: model_training: Rank 0, Epoch 793, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,804: INFO: model_training: Rank 0, Epoch 793, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,805: INFO: model_training: Rank 0, Epoch 793, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,806: INFO: model_training: Rank 0, Epoch 793, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,808: INFO: model_training: Rank 0, Epoch 794, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,809: INFO: model_training: Rank 0, Epoch 794, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,810: INFO: model_training: Rank 0, Epoch 794, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,813: INFO: model_training: Rank 0, Epoch 794, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,814: INFO: model_training: Rank 0, Epoch 794, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,815: INFO: model_training: Rank 0, Epoch 795, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,817: INFO: model_training: Rank 0, Epoch 795, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,818: INFO: model_training: Rank 0, Epoch 795, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,820: INFO: model_training: Rank 0, Epoch 795, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,821: INFO: model_training: Rank 0, Epoch 795, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,823: INFO: model_training: Rank 0, Epoch 796, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,824: INFO: model_training: Rank 0, Epoch 796, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,825: INFO: model_training: Rank 0, Epoch 796, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,826: INFO: model_training: Rank 0, Epoch 796, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,827: INFO: model_training: Rank 0, Epoch 796, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,829: INFO: model_training: Rank 0, Epoch 797, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,830: INFO: model_training: Rank 0, Epoch 797, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,831: INFO: model_training: Rank 0, Epoch 797, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,833: INFO: model_training: Rank 0, Epoch 797, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,834: INFO: model_training: Rank 0, Epoch 797, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,836: INFO: model_training: Rank 0, Epoch 798, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,838: INFO: model_training: Rank 0, Epoch 798, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,839: INFO: model_training: Rank 0, Epoch 798, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,840: INFO: model_training: Rank 0, Epoch 798, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,841: INFO: model_training: Rank 0, Epoch 798, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,843: INFO: model_training: Rank 0, Epoch 799, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,844: INFO: model_training: Rank 0, Epoch 799, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,846: INFO: model_training: Rank 0, Epoch 799, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,847: INFO: model_training: Rank 0, Epoch 799, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,848: INFO: model_training: Rank 0, Epoch 799, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,849: INFO: model_training: Rank 0, Epoch 800, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,851: INFO: model_training: Rank 0, Epoch 800, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,852: INFO: model_training: Rank 0, Epoch 800, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,854: INFO: model_training: Rank 0, Epoch 800, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,855: INFO: model_training: Rank 0, Epoch 800, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,856: INFO: model_training: Rank 0, Epoch 801, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,858: INFO: model_training: Rank 0, Epoch 801, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,859: INFO: model_training: Rank 0, Epoch 801, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,860: INFO: model_training: Rank 0, Epoch 801, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,862: INFO: model_training: Rank 0, Epoch 801, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,864: INFO: model_training: Rank 0, Epoch 802, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,865: INFO: model_training: Rank 0, Epoch 802, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,866: INFO: model_training: Rank 0, Epoch 802, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,868: INFO: model_training: Rank 0, Epoch 802, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,869: INFO: model_training: Rank 0, Epoch 802, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,871: INFO: model_training: Rank 0, Epoch 803, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,872: INFO: model_training: Rank 0, Epoch 803, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,873: INFO: model_training: Rank 0, Epoch 803, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,875: INFO: model_training: Rank 0, Epoch 803, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,876: INFO: model_training: Rank 0, Epoch 803, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,878: INFO: model_training: Rank 0, Epoch 804, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,879: INFO: model_training: Rank 0, Epoch 804, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,881: INFO: model_training: Rank 0, Epoch 804, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,882: INFO: model_training: Rank 0, Epoch 804, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,883: INFO: model_training: Rank 0, Epoch 804, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,885: INFO: model_training: Rank 0, Epoch 805, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,887: INFO: model_training: Rank 0, Epoch 805, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,889: INFO: model_training: Rank 0, Epoch 805, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,890: INFO: model_training: Rank 0, Epoch 805, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,892: INFO: model_training: Rank 0, Epoch 805, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,893: INFO: model_training: Rank 0, Epoch 806, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,895: INFO: model_training: Rank 0, Epoch 806, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,896: INFO: model_training: Rank 0, Epoch 806, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,897: INFO: model_training: Rank 0, Epoch 806, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,898: INFO: model_training: Rank 0, Epoch 806, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,899: INFO: model_training: Rank 0, Epoch 807, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,901: INFO: model_training: Rank 0, Epoch 807, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,903: INFO: model_training: Rank 0, Epoch 807, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,904: INFO: model_training: Rank 0, Epoch 807, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,905: INFO: model_training: Rank 0, Epoch 807, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,907: INFO: model_training: Rank 0, Epoch 808, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,908: INFO: model_training: Rank 0, Epoch 808, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,909: INFO: model_training: Rank 0, Epoch 808, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,910: INFO: model_training: Rank 0, Epoch 808, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,912: INFO: model_training: Rank 0, Epoch 808, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,914: INFO: model_training: Rank 0, Epoch 809, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,915: INFO: model_training: Rank 0, Epoch 809, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,916: INFO: model_training: Rank 0, Epoch 809, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,917: INFO: model_training: Rank 0, Epoch 809, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,918: INFO: model_training: Rank 0, Epoch 809, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,921: INFO: model_training: Rank 0, Epoch 810, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,922: INFO: model_training: Rank 0, Epoch 810, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,923: INFO: model_training: Rank 0, Epoch 810, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,924: INFO: model_training: Rank 0, Epoch 810, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,925: INFO: model_training: Rank 0, Epoch 810, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,927: INFO: model_training: Rank 0, Epoch 811, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,928: INFO: model_training: Rank 0, Epoch 811, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,930: INFO: model_training: Rank 0, Epoch 811, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,931: INFO: model_training: Rank 0, Epoch 811, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,932: INFO: model_training: Rank 0, Epoch 811, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,934: INFO: model_training: Rank 0, Epoch 812, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,935: INFO: model_training: Rank 0, Epoch 812, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,937: INFO: model_training: Rank 0, Epoch 812, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,938: INFO: model_training: Rank 0, Epoch 812, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,940: INFO: model_training: Rank 0, Epoch 812, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,941: INFO: model_training: Rank 0, Epoch 813, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,942: INFO: model_training: Rank 0, Epoch 813, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,944: INFO: model_training: Rank 0, Epoch 813, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,946: INFO: model_training: Rank 0, Epoch 813, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,947: INFO: model_training: Rank 0, Epoch 813, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,949: INFO: model_training: Rank 0, Epoch 814, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,950: INFO: model_training: Rank 0, Epoch 814, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,951: INFO: model_training: Rank 0, Epoch 814, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,953: INFO: model_training: Rank 0, Epoch 814, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,954: INFO: model_training: Rank 0, Epoch 814, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,956: INFO: model_training: Rank 0, Epoch 815, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,958: INFO: model_training: Rank 0, Epoch 815, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,959: INFO: model_training: Rank 0, Epoch 815, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,960: INFO: model_training: Rank 0, Epoch 815, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,962: INFO: model_training: Rank 0, Epoch 815, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,965: INFO: model_training: Rank 0, Epoch 816, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,966: INFO: model_training: Rank 0, Epoch 816, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,968: INFO: model_training: Rank 0, Epoch 816, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,969: INFO: model_training: Rank 0, Epoch 816, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,971: INFO: model_training: Rank 0, Epoch 816, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,973: INFO: model_training: Rank 0, Epoch 817, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,974: INFO: model_training: Rank 0, Epoch 817, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,975: INFO: model_training: Rank 0, Epoch 817, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,976: INFO: model_training: Rank 0, Epoch 817, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,977: INFO: model_training: Rank 0, Epoch 817, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,979: INFO: model_training: Rank 0, Epoch 818, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,980: INFO: model_training: Rank 0, Epoch 818, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,981: INFO: model_training: Rank 0, Epoch 818, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,983: INFO: model_training: Rank 0, Epoch 818, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,984: INFO: model_training: Rank 0, Epoch 818, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,986: INFO: model_training: Rank 0, Epoch 819, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,987: INFO: model_training: Rank 0, Epoch 819, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,988: INFO: model_training: Rank 0, Epoch 819, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,990: INFO: model_training: Rank 0, Epoch 819, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,991: INFO: model_training: Rank 0, Epoch 819, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,992: INFO: model_training: Rank 0, Epoch 820, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:48,994: INFO: model_training: Rank 0, Epoch 820, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:48,995: INFO: model_training: Rank 0, Epoch 820, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:48,996: INFO: model_training: Rank 0, Epoch 820, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:48,997: INFO: model_training: Rank 0, Epoch 820, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:48,999: INFO: model_training: Rank 0, Epoch 821, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,000: INFO: model_training: Rank 0, Epoch 821, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,002: INFO: model_training: Rank 0, Epoch 821, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,003: INFO: model_training: Rank 0, Epoch 821, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,004: INFO: model_training: Rank 0, Epoch 821, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,006: INFO: model_training: Rank 0, Epoch 822, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,007: INFO: model_training: Rank 0, Epoch 822, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,008: INFO: model_training: Rank 0, Epoch 822, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,009: INFO: model_training: Rank 0, Epoch 822, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,011: INFO: model_training: Rank 0, Epoch 822, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,013: INFO: model_training: Rank 0, Epoch 823, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,014: INFO: model_training: Rank 0, Epoch 823, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,016: INFO: model_training: Rank 0, Epoch 823, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,017: INFO: model_training: Rank 0, Epoch 823, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,018: INFO: model_training: Rank 0, Epoch 823, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,021: INFO: model_training: Rank 0, Epoch 824, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,022: INFO: model_training: Rank 0, Epoch 824, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,023: INFO: model_training: Rank 0, Epoch 824, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,025: INFO: model_training: Rank 0, Epoch 824, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,026: INFO: model_training: Rank 0, Epoch 824, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,029: INFO: model_training: Rank 0, Epoch 825, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,030: INFO: model_training: Rank 0, Epoch 825, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,031: INFO: model_training: Rank 0, Epoch 825, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,033: INFO: model_training: Rank 0, Epoch 825, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,034: INFO: model_training: Rank 0, Epoch 825, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,036: INFO: model_training: Rank 0, Epoch 826, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,038: INFO: model_training: Rank 0, Epoch 826, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,039: INFO: model_training: Rank 0, Epoch 826, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,041: INFO: model_training: Rank 0, Epoch 826, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,042: INFO: model_training: Rank 0, Epoch 826, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,045: INFO: model_training: Rank 0, Epoch 827, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,046: INFO: model_training: Rank 0, Epoch 827, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,047: INFO: model_training: Rank 0, Epoch 827, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,049: INFO: model_training: Rank 0, Epoch 827, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,050: INFO: model_training: Rank 0, Epoch 827, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,052: INFO: model_training: Rank 0, Epoch 828, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,053: INFO: model_training: Rank 0, Epoch 828, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,054: INFO: model_training: Rank 0, Epoch 828, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,056: INFO: model_training: Rank 0, Epoch 828, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,057: INFO: model_training: Rank 0, Epoch 828, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,059: INFO: model_training: Rank 0, Epoch 829, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,060: INFO: model_training: Rank 0, Epoch 829, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,061: INFO: model_training: Rank 0, Epoch 829, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,063: INFO: model_training: Rank 0, Epoch 829, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,064: INFO: model_training: Rank 0, Epoch 829, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,066: INFO: model_training: Rank 0, Epoch 830, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,067: INFO: model_training: Rank 0, Epoch 830, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,068: INFO: model_training: Rank 0, Epoch 830, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,070: INFO: model_training: Rank 0, Epoch 830, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,071: INFO: model_training: Rank 0, Epoch 830, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,073: INFO: model_training: Rank 0, Epoch 831, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,074: INFO: model_training: Rank 0, Epoch 831, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,076: INFO: model_training: Rank 0, Epoch 831, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,077: INFO: model_training: Rank 0, Epoch 831, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,079: INFO: model_training: Rank 0, Epoch 831, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,080: INFO: model_training: Rank 0, Epoch 832, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,082: INFO: model_training: Rank 0, Epoch 832, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,083: INFO: model_training: Rank 0, Epoch 832, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,084: INFO: model_training: Rank 0, Epoch 832, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,085: INFO: model_training: Rank 0, Epoch 832, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,087: INFO: model_training: Rank 0, Epoch 833, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,089: INFO: model_training: Rank 0, Epoch 833, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,091: INFO: model_training: Rank 0, Epoch 833, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,093: INFO: model_training: Rank 0, Epoch 833, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,095: INFO: model_training: Rank 0, Epoch 833, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,097: INFO: model_training: Rank 0, Epoch 834, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,099: INFO: model_training: Rank 0, Epoch 834, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,100: INFO: model_training: Rank 0, Epoch 834, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,102: INFO: model_training: Rank 0, Epoch 834, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,104: INFO: model_training: Rank 0, Epoch 834, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,106: INFO: model_training: Rank 0, Epoch 835, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,108: INFO: model_training: Rank 0, Epoch 835, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,109: INFO: model_training: Rank 0, Epoch 835, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,110: INFO: model_training: Rank 0, Epoch 835, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,113: INFO: model_training: Rank 0, Epoch 835, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,114: INFO: model_training: Rank 0, Epoch 836, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,116: INFO: model_training: Rank 0, Epoch 836, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,117: INFO: model_training: Rank 0, Epoch 836, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,119: INFO: model_training: Rank 0, Epoch 836, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,121: INFO: model_training: Rank 0, Epoch 836, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,123: INFO: model_training: Rank 0, Epoch 837, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,125: INFO: model_training: Rank 0, Epoch 837, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,126: INFO: model_training: Rank 0, Epoch 837, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,128: INFO: model_training: Rank 0, Epoch 837, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,130: INFO: model_training: Rank 0, Epoch 837, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,132: INFO: model_training: Rank 0, Epoch 838, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,133: INFO: model_training: Rank 0, Epoch 838, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,134: INFO: model_training: Rank 0, Epoch 838, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,136: INFO: model_training: Rank 0, Epoch 838, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,137: INFO: model_training: Rank 0, Epoch 838, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,138: INFO: model_training: Rank 0, Epoch 839, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,139: INFO: model_training: Rank 0, Epoch 839, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,141: INFO: model_training: Rank 0, Epoch 839, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,142: INFO: model_training: Rank 0, Epoch 839, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,144: INFO: model_training: Rank 0, Epoch 839, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,145: INFO: model_training: Rank 0, Epoch 840, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,147: INFO: model_training: Rank 0, Epoch 840, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,148: INFO: model_training: Rank 0, Epoch 840, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,149: INFO: model_training: Rank 0, Epoch 840, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,150: INFO: model_training: Rank 0, Epoch 840, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,152: INFO: model_training: Rank 0, Epoch 841, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,154: INFO: model_training: Rank 0, Epoch 841, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,155: INFO: model_training: Rank 0, Epoch 841, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,156: INFO: model_training: Rank 0, Epoch 841, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,157: INFO: model_training: Rank 0, Epoch 841, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,158: INFO: model_training: Rank 0, Epoch 842, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,159: INFO: model_training: Rank 0, Epoch 842, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,161: INFO: model_training: Rank 0, Epoch 842, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,163: INFO: model_training: Rank 0, Epoch 842, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,164: INFO: model_training: Rank 0, Epoch 842, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,165: INFO: model_training: Rank 0, Epoch 843, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,166: INFO: model_training: Rank 0, Epoch 843, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,167: INFO: model_training: Rank 0, Epoch 843, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,169: INFO: model_training: Rank 0, Epoch 843, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,170: INFO: model_training: Rank 0, Epoch 843, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,172: INFO: model_training: Rank 0, Epoch 844, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,173: INFO: model_training: Rank 0, Epoch 844, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,175: INFO: model_training: Rank 0, Epoch 844, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,176: INFO: model_training: Rank 0, Epoch 844, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,177: INFO: model_training: Rank 0, Epoch 844, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,179: INFO: model_training: Rank 0, Epoch 845, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,180: INFO: model_training: Rank 0, Epoch 845, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,181: INFO: model_training: Rank 0, Epoch 845, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,183: INFO: model_training: Rank 0, Epoch 845, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,184: INFO: model_training: Rank 0, Epoch 845, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,186: INFO: model_training: Rank 0, Epoch 846, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,187: INFO: model_training: Rank 0, Epoch 846, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,188: INFO: model_training: Rank 0, Epoch 846, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,189: INFO: model_training: Rank 0, Epoch 846, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,190: INFO: model_training: Rank 0, Epoch 846, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,192: INFO: model_training: Rank 0, Epoch 847, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,193: INFO: model_training: Rank 0, Epoch 847, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,194: INFO: model_training: Rank 0, Epoch 847, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,196: INFO: model_training: Rank 0, Epoch 847, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,197: INFO: model_training: Rank 0, Epoch 847, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,198: INFO: model_training: Rank 0, Epoch 848, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,199: INFO: model_training: Rank 0, Epoch 848, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,201: INFO: model_training: Rank 0, Epoch 848, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,203: INFO: model_training: Rank 0, Epoch 848, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,204: INFO: model_training: Rank 0, Epoch 848, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,205: INFO: model_training: Rank 0, Epoch 849, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,206: INFO: model_training: Rank 0, Epoch 849, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,208: INFO: model_training: Rank 0, Epoch 849, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,209: INFO: model_training: Rank 0, Epoch 849, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,210: INFO: model_training: Rank 0, Epoch 849, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,212: INFO: model_training: Rank 0, Epoch 850, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,213: INFO: model_training: Rank 0, Epoch 850, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,214: INFO: model_training: Rank 0, Epoch 850, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,215: INFO: model_training: Rank 0, Epoch 850, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,216: INFO: model_training: Rank 0, Epoch 850, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,218: INFO: model_training: Rank 0, Epoch 851, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,219: INFO: model_training: Rank 0, Epoch 851, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,221: INFO: model_training: Rank 0, Epoch 851, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,222: INFO: model_training: Rank 0, Epoch 851, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,223: INFO: model_training: Rank 0, Epoch 851, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,225: INFO: model_training: Rank 0, Epoch 852, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,226: INFO: model_training: Rank 0, Epoch 852, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,228: INFO: model_training: Rank 0, Epoch 852, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,229: INFO: model_training: Rank 0, Epoch 852, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,230: INFO: model_training: Rank 0, Epoch 852, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,231: INFO: model_training: Rank 0, Epoch 853, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,233: INFO: model_training: Rank 0, Epoch 853, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,234: INFO: model_training: Rank 0, Epoch 853, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,235: INFO: model_training: Rank 0, Epoch 853, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,236: INFO: model_training: Rank 0, Epoch 853, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,238: INFO: model_training: Rank 0, Epoch 854, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,239: INFO: model_training: Rank 0, Epoch 854, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,240: INFO: model_training: Rank 0, Epoch 854, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,241: INFO: model_training: Rank 0, Epoch 854, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,243: INFO: model_training: Rank 0, Epoch 854, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,245: INFO: model_training: Rank 0, Epoch 855, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,246: INFO: model_training: Rank 0, Epoch 855, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,247: INFO: model_training: Rank 0, Epoch 855, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,248: INFO: model_training: Rank 0, Epoch 855, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,250: INFO: model_training: Rank 0, Epoch 855, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,251: INFO: model_training: Rank 0, Epoch 856, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,253: INFO: model_training: Rank 0, Epoch 856, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,254: INFO: model_training: Rank 0, Epoch 856, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,255: INFO: model_training: Rank 0, Epoch 856, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,257: INFO: model_training: Rank 0, Epoch 856, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,258: INFO: model_training: Rank 0, Epoch 857, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,260: INFO: model_training: Rank 0, Epoch 857, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,261: INFO: model_training: Rank 0, Epoch 857, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,263: INFO: model_training: Rank 0, Epoch 857, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,264: INFO: model_training: Rank 0, Epoch 857, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,265: INFO: model_training: Rank 0, Epoch 858, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,267: INFO: model_training: Rank 0, Epoch 858, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,268: INFO: model_training: Rank 0, Epoch 858, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,269: INFO: model_training: Rank 0, Epoch 858, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,271: INFO: model_training: Rank 0, Epoch 858, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,272: INFO: model_training: Rank 0, Epoch 859, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,274: INFO: model_training: Rank 0, Epoch 859, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,275: INFO: model_training: Rank 0, Epoch 859, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,277: INFO: model_training: Rank 0, Epoch 859, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,278: INFO: model_training: Rank 0, Epoch 859, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,280: INFO: model_training: Rank 0, Epoch 860, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,281: INFO: model_training: Rank 0, Epoch 860, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,282: INFO: model_training: Rank 0, Epoch 860, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,284: INFO: model_training: Rank 0, Epoch 860, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,285: INFO: model_training: Rank 0, Epoch 860, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,287: INFO: model_training: Rank 0, Epoch 861, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,288: INFO: model_training: Rank 0, Epoch 861, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,290: INFO: model_training: Rank 0, Epoch 861, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,291: INFO: model_training: Rank 0, Epoch 861, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,292: INFO: model_training: Rank 0, Epoch 861, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,295: INFO: model_training: Rank 0, Epoch 862, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,296: INFO: model_training: Rank 0, Epoch 862, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,297: INFO: model_training: Rank 0, Epoch 862, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,298: INFO: model_training: Rank 0, Epoch 862, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,299: INFO: model_training: Rank 0, Epoch 862, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,301: INFO: model_training: Rank 0, Epoch 863, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,302: INFO: model_training: Rank 0, Epoch 863, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,304: INFO: model_training: Rank 0, Epoch 863, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,305: INFO: model_training: Rank 0, Epoch 863, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,307: INFO: model_training: Rank 0, Epoch 863, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,308: INFO: model_training: Rank 0, Epoch 864, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,309: INFO: model_training: Rank 0, Epoch 864, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,311: INFO: model_training: Rank 0, Epoch 864, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,313: INFO: model_training: Rank 0, Epoch 864, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,314: INFO: model_training: Rank 0, Epoch 864, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,315: INFO: model_training: Rank 0, Epoch 865, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,316: INFO: model_training: Rank 0, Epoch 865, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,317: INFO: model_training: Rank 0, Epoch 865, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,319: INFO: model_training: Rank 0, Epoch 865, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,321: INFO: model_training: Rank 0, Epoch 865, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,322: INFO: model_training: Rank 0, Epoch 866, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,324: INFO: model_training: Rank 0, Epoch 866, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,325: INFO: model_training: Rank 0, Epoch 866, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,326: INFO: model_training: Rank 0, Epoch 866, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,329: INFO: model_training: Rank 0, Epoch 866, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,331: INFO: model_training: Rank 0, Epoch 867, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,332: INFO: model_training: Rank 0, Epoch 867, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,333: INFO: model_training: Rank 0, Epoch 867, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,334: INFO: model_training: Rank 0, Epoch 867, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,336: INFO: model_training: Rank 0, Epoch 867, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,338: INFO: model_training: Rank 0, Epoch 868, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,339: INFO: model_training: Rank 0, Epoch 868, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,340: INFO: model_training: Rank 0, Epoch 868, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,342: INFO: model_training: Rank 0, Epoch 868, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,343: INFO: model_training: Rank 0, Epoch 868, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,345: INFO: model_training: Rank 0, Epoch 869, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,347: INFO: model_training: Rank 0, Epoch 869, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,348: INFO: model_training: Rank 0, Epoch 869, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,349: INFO: model_training: Rank 0, Epoch 869, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,350: INFO: model_training: Rank 0, Epoch 869, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,353: INFO: model_training: Rank 0, Epoch 870, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,355: INFO: model_training: Rank 0, Epoch 870, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,356: INFO: model_training: Rank 0, Epoch 870, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,357: INFO: model_training: Rank 0, Epoch 870, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,359: INFO: model_training: Rank 0, Epoch 870, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,361: INFO: model_training: Rank 0, Epoch 871, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,363: INFO: model_training: Rank 0, Epoch 871, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,364: INFO: model_training: Rank 0, Epoch 871, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,365: INFO: model_training: Rank 0, Epoch 871, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,366: INFO: model_training: Rank 0, Epoch 871, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,368: INFO: model_training: Rank 0, Epoch 872, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,369: INFO: model_training: Rank 0, Epoch 872, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,370: INFO: model_training: Rank 0, Epoch 872, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,372: INFO: model_training: Rank 0, Epoch 872, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,373: INFO: model_training: Rank 0, Epoch 872, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,375: INFO: model_training: Rank 0, Epoch 873, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,377: INFO: model_training: Rank 0, Epoch 873, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,378: INFO: model_training: Rank 0, Epoch 873, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,380: INFO: model_training: Rank 0, Epoch 873, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,381: INFO: model_training: Rank 0, Epoch 873, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,383: INFO: model_training: Rank 0, Epoch 874, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,384: INFO: model_training: Rank 0, Epoch 874, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,385: INFO: model_training: Rank 0, Epoch 874, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,387: INFO: model_training: Rank 0, Epoch 874, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,388: INFO: model_training: Rank 0, Epoch 874, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,390: INFO: model_training: Rank 0, Epoch 875, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,391: INFO: model_training: Rank 0, Epoch 875, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,392: INFO: model_training: Rank 0, Epoch 875, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,394: INFO: model_training: Rank 0, Epoch 875, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,395: INFO: model_training: Rank 0, Epoch 875, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,396: INFO: model_training: Rank 0, Epoch 876, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,397: INFO: model_training: Rank 0, Epoch 876, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,399: INFO: model_training: Rank 0, Epoch 876, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,401: INFO: model_training: Rank 0, Epoch 876, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,402: INFO: model_training: Rank 0, Epoch 876, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,404: INFO: model_training: Rank 0, Epoch 877, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,405: INFO: model_training: Rank 0, Epoch 877, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,406: INFO: model_training: Rank 0, Epoch 877, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,407: INFO: model_training: Rank 0, Epoch 877, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,408: INFO: model_training: Rank 0, Epoch 877, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,410: INFO: model_training: Rank 0, Epoch 878, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,411: INFO: model_training: Rank 0, Epoch 878, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,412: INFO: model_training: Rank 0, Epoch 878, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,414: INFO: model_training: Rank 0, Epoch 878, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,415: INFO: model_training: Rank 0, Epoch 878, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,417: INFO: model_training: Rank 0, Epoch 879, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,419: INFO: model_training: Rank 0, Epoch 879, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,420: INFO: model_training: Rank 0, Epoch 879, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,421: INFO: model_training: Rank 0, Epoch 879, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,423: INFO: model_training: Rank 0, Epoch 879, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,425: INFO: model_training: Rank 0, Epoch 880, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,426: INFO: model_training: Rank 0, Epoch 880, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,428: INFO: model_training: Rank 0, Epoch 880, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,430: INFO: model_training: Rank 0, Epoch 880, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,431: INFO: model_training: Rank 0, Epoch 880, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,433: INFO: model_training: Rank 0, Epoch 881, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,434: INFO: model_training: Rank 0, Epoch 881, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,436: INFO: model_training: Rank 0, Epoch 881, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,437: INFO: model_training: Rank 0, Epoch 881, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,439: INFO: model_training: Rank 0, Epoch 881, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,441: INFO: model_training: Rank 0, Epoch 882, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,442: INFO: model_training: Rank 0, Epoch 882, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,444: INFO: model_training: Rank 0, Epoch 882, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,445: INFO: model_training: Rank 0, Epoch 882, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,447: INFO: model_training: Rank 0, Epoch 882, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,448: INFO: model_training: Rank 0, Epoch 883, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,449: INFO: model_training: Rank 0, Epoch 883, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,451: INFO: model_training: Rank 0, Epoch 883, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,453: INFO: model_training: Rank 0, Epoch 883, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,454: INFO: model_training: Rank 0, Epoch 883, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,456: INFO: model_training: Rank 0, Epoch 884, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,457: INFO: model_training: Rank 0, Epoch 884, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,458: INFO: model_training: Rank 0, Epoch 884, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,460: INFO: model_training: Rank 0, Epoch 884, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,461: INFO: model_training: Rank 0, Epoch 884, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,464: INFO: model_training: Rank 0, Epoch 885, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,465: INFO: model_training: Rank 0, Epoch 885, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,466: INFO: model_training: Rank 0, Epoch 885, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,467: INFO: model_training: Rank 0, Epoch 885, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,468: INFO: model_training: Rank 0, Epoch 885, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,470: INFO: model_training: Rank 0, Epoch 886, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,472: INFO: model_training: Rank 0, Epoch 886, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,473: INFO: model_training: Rank 0, Epoch 886, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,474: INFO: model_training: Rank 0, Epoch 886, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,475: INFO: model_training: Rank 0, Epoch 886, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,478: INFO: model_training: Rank 0, Epoch 887, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,480: INFO: model_training: Rank 0, Epoch 887, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,481: INFO: model_training: Rank 0, Epoch 887, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,482: INFO: model_training: Rank 0, Epoch 887, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,484: INFO: model_training: Rank 0, Epoch 887, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,485: INFO: model_training: Rank 0, Epoch 888, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,487: INFO: model_training: Rank 0, Epoch 888, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,489: INFO: model_training: Rank 0, Epoch 888, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,490: INFO: model_training: Rank 0, Epoch 888, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,492: INFO: model_training: Rank 0, Epoch 888, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,494: INFO: model_training: Rank 0, Epoch 889, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,496: INFO: model_training: Rank 0, Epoch 889, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,497: INFO: model_training: Rank 0, Epoch 889, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,499: INFO: model_training: Rank 0, Epoch 889, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,500: INFO: model_training: Rank 0, Epoch 889, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,502: INFO: model_training: Rank 0, Epoch 890, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,504: INFO: model_training: Rank 0, Epoch 890, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,505: INFO: model_training: Rank 0, Epoch 890, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,507: INFO: model_training: Rank 0, Epoch 890, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,509: INFO: model_training: Rank 0, Epoch 890, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,511: INFO: model_training: Rank 0, Epoch 891, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,513: INFO: model_training: Rank 0, Epoch 891, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,514: INFO: model_training: Rank 0, Epoch 891, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,516: INFO: model_training: Rank 0, Epoch 891, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,517: INFO: model_training: Rank 0, Epoch 891, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,520: INFO: model_training: Rank 0, Epoch 892, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,522: INFO: model_training: Rank 0, Epoch 892, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,523: INFO: model_training: Rank 0, Epoch 892, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,525: INFO: model_training: Rank 0, Epoch 892, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,526: INFO: model_training: Rank 0, Epoch 892, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,528: INFO: model_training: Rank 0, Epoch 893, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,530: INFO: model_training: Rank 0, Epoch 893, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,531: INFO: model_training: Rank 0, Epoch 893, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,533: INFO: model_training: Rank 0, Epoch 893, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,534: INFO: model_training: Rank 0, Epoch 893, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,536: INFO: model_training: Rank 0, Epoch 894, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,538: INFO: model_training: Rank 0, Epoch 894, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,539: INFO: model_training: Rank 0, Epoch 894, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,540: INFO: model_training: Rank 0, Epoch 894, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,542: INFO: model_training: Rank 0, Epoch 894, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,543: INFO: model_training: Rank 0, Epoch 895, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,545: INFO: model_training: Rank 0, Epoch 895, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,546: INFO: model_training: Rank 0, Epoch 895, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,547: INFO: model_training: Rank 0, Epoch 895, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,548: INFO: model_training: Rank 0, Epoch 895, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,550: INFO: model_training: Rank 0, Epoch 896, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,551: INFO: model_training: Rank 0, Epoch 896, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,553: INFO: model_training: Rank 0, Epoch 896, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,554: INFO: model_training: Rank 0, Epoch 896, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,556: INFO: model_training: Rank 0, Epoch 896, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,557: INFO: model_training: Rank 0, Epoch 897, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,558: INFO: model_training: Rank 0, Epoch 897, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,560: INFO: model_training: Rank 0, Epoch 897, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,561: INFO: model_training: Rank 0, Epoch 897, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,563: INFO: model_training: Rank 0, Epoch 897, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,565: INFO: model_training: Rank 0, Epoch 898, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,566: INFO: model_training: Rank 0, Epoch 898, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,568: INFO: model_training: Rank 0, Epoch 898, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,569: INFO: model_training: Rank 0, Epoch 898, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,571: INFO: model_training: Rank 0, Epoch 898, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,573: INFO: model_training: Rank 0, Epoch 899, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,574: INFO: model_training: Rank 0, Epoch 899, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,576: INFO: model_training: Rank 0, Epoch 899, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,577: INFO: model_training: Rank 0, Epoch 899, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,579: INFO: model_training: Rank 0, Epoch 899, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,580: INFO: model_training: Rank 0, Epoch 900, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,582: INFO: model_training: Rank 0, Epoch 900, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,583: INFO: model_training: Rank 0, Epoch 900, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,584: INFO: model_training: Rank 0, Epoch 900, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,586: INFO: model_training: Rank 0, Epoch 900, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,587: INFO: model_training: Rank 0, Epoch 901, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,589: INFO: model_training: Rank 0, Epoch 901, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,590: INFO: model_training: Rank 0, Epoch 901, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,593: INFO: model_training: Rank 0, Epoch 901, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,594: INFO: model_training: Rank 0, Epoch 901, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,596: INFO: model_training: Rank 0, Epoch 902, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,597: INFO: model_training: Rank 0, Epoch 902, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,599: INFO: model_training: Rank 0, Epoch 902, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,600: INFO: model_training: Rank 0, Epoch 902, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,601: INFO: model_training: Rank 0, Epoch 902, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,603: INFO: model_training: Rank 0, Epoch 903, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,605: INFO: model_training: Rank 0, Epoch 903, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,606: INFO: model_training: Rank 0, Epoch 903, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,607: INFO: model_training: Rank 0, Epoch 903, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,609: INFO: model_training: Rank 0, Epoch 903, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,611: INFO: model_training: Rank 0, Epoch 904, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,612: INFO: model_training: Rank 0, Epoch 904, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,613: INFO: model_training: Rank 0, Epoch 904, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,614: INFO: model_training: Rank 0, Epoch 904, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,616: INFO: model_training: Rank 0, Epoch 904, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,618: INFO: model_training: Rank 0, Epoch 905, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,619: INFO: model_training: Rank 0, Epoch 905, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,621: INFO: model_training: Rank 0, Epoch 905, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,622: INFO: model_training: Rank 0, Epoch 905, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,624: INFO: model_training: Rank 0, Epoch 905, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,625: INFO: model_training: Rank 0, Epoch 906, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,626: INFO: model_training: Rank 0, Epoch 906, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,627: INFO: model_training: Rank 0, Epoch 906, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,629: INFO: model_training: Rank 0, Epoch 906, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,631: INFO: model_training: Rank 0, Epoch 906, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,632: INFO: model_training: Rank 0, Epoch 907, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,634: INFO: model_training: Rank 0, Epoch 907, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,635: INFO: model_training: Rank 0, Epoch 907, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,636: INFO: model_training: Rank 0, Epoch 907, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,638: INFO: model_training: Rank 0, Epoch 907, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,639: INFO: model_training: Rank 0, Epoch 908, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,641: INFO: model_training: Rank 0, Epoch 908, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,642: INFO: model_training: Rank 0, Epoch 908, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,644: INFO: model_training: Rank 0, Epoch 908, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,645: INFO: model_training: Rank 0, Epoch 908, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,647: INFO: model_training: Rank 0, Epoch 909, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,648: INFO: model_training: Rank 0, Epoch 909, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,649: INFO: model_training: Rank 0, Epoch 909, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,650: INFO: model_training: Rank 0, Epoch 909, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,651: INFO: model_training: Rank 0, Epoch 909, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,653: INFO: model_training: Rank 0, Epoch 910, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,655: INFO: model_training: Rank 0, Epoch 910, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,656: INFO: model_training: Rank 0, Epoch 910, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,657: INFO: model_training: Rank 0, Epoch 910, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,658: INFO: model_training: Rank 0, Epoch 910, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,660: INFO: model_training: Rank 0, Epoch 911, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,662: INFO: model_training: Rank 0, Epoch 911, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,664: INFO: model_training: Rank 0, Epoch 911, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,665: INFO: model_training: Rank 0, Epoch 911, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,666: INFO: model_training: Rank 0, Epoch 911, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,668: INFO: model_training: Rank 0, Epoch 912, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,669: INFO: model_training: Rank 0, Epoch 912, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,670: INFO: model_training: Rank 0, Epoch 912, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,672: INFO: model_training: Rank 0, Epoch 912, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,673: INFO: model_training: Rank 0, Epoch 912, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,675: INFO: model_training: Rank 0, Epoch 913, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,676: INFO: model_training: Rank 0, Epoch 913, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,678: INFO: model_training: Rank 0, Epoch 913, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,679: INFO: model_training: Rank 0, Epoch 913, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,680: INFO: model_training: Rank 0, Epoch 913, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,682: INFO: model_training: Rank 0, Epoch 914, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,684: INFO: model_training: Rank 0, Epoch 914, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,685: INFO: model_training: Rank 0, Epoch 914, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,686: INFO: model_training: Rank 0, Epoch 914, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,688: INFO: model_training: Rank 0, Epoch 914, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,690: INFO: model_training: Rank 0, Epoch 915, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,691: INFO: model_training: Rank 0, Epoch 915, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,692: INFO: model_training: Rank 0, Epoch 915, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,693: INFO: model_training: Rank 0, Epoch 915, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,695: INFO: model_training: Rank 0, Epoch 915, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,697: INFO: model_training: Rank 0, Epoch 916, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,698: INFO: model_training: Rank 0, Epoch 916, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,699: INFO: model_training: Rank 0, Epoch 916, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,700: INFO: model_training: Rank 0, Epoch 916, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,701: INFO: model_training: Rank 0, Epoch 916, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,703: INFO: model_training: Rank 0, Epoch 917, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,705: INFO: model_training: Rank 0, Epoch 917, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,706: INFO: model_training: Rank 0, Epoch 917, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,707: INFO: model_training: Rank 0, Epoch 917, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,708: INFO: model_training: Rank 0, Epoch 917, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,710: INFO: model_training: Rank 0, Epoch 918, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,711: INFO: model_training: Rank 0, Epoch 918, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,713: INFO: model_training: Rank 0, Epoch 918, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,714: INFO: model_training: Rank 0, Epoch 918, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,716: INFO: model_training: Rank 0, Epoch 918, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,717: INFO: model_training: Rank 0, Epoch 919, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,719: INFO: model_training: Rank 0, Epoch 919, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,720: INFO: model_training: Rank 0, Epoch 919, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,722: INFO: model_training: Rank 0, Epoch 919, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,723: INFO: model_training: Rank 0, Epoch 919, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,724: INFO: model_training: Rank 0, Epoch 920, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,725: INFO: model_training: Rank 0, Epoch 920, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,727: INFO: model_training: Rank 0, Epoch 920, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,728: INFO: model_training: Rank 0, Epoch 920, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,730: INFO: model_training: Rank 0, Epoch 920, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,731: INFO: model_training: Rank 0, Epoch 921, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,732: INFO: model_training: Rank 0, Epoch 921, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,733: INFO: model_training: Rank 0, Epoch 921, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,735: INFO: model_training: Rank 0, Epoch 921, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,736: INFO: model_training: Rank 0, Epoch 921, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,738: INFO: model_training: Rank 0, Epoch 922, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,740: INFO: model_training: Rank 0, Epoch 922, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,741: INFO: model_training: Rank 0, Epoch 922, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,742: INFO: model_training: Rank 0, Epoch 922, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,744: INFO: model_training: Rank 0, Epoch 922, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,746: INFO: model_training: Rank 0, Epoch 923, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,748: INFO: model_training: Rank 0, Epoch 923, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,749: INFO: model_training: Rank 0, Epoch 923, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,750: INFO: model_training: Rank 0, Epoch 923, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,751: INFO: model_training: Rank 0, Epoch 923, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,753: INFO: model_training: Rank 0, Epoch 924, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,755: INFO: model_training: Rank 0, Epoch 924, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,756: INFO: model_training: Rank 0, Epoch 924, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,758: INFO: model_training: Rank 0, Epoch 924, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,759: INFO: model_training: Rank 0, Epoch 924, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,760: INFO: model_training: Rank 0, Epoch 925, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,762: INFO: model_training: Rank 0, Epoch 925, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,764: INFO: model_training: Rank 0, Epoch 925, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,765: INFO: model_training: Rank 0, Epoch 925, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,767: INFO: model_training: Rank 0, Epoch 925, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,768: INFO: model_training: Rank 0, Epoch 926, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,770: INFO: model_training: Rank 0, Epoch 926, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,771: INFO: model_training: Rank 0, Epoch 926, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,772: INFO: model_training: Rank 0, Epoch 926, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,774: INFO: model_training: Rank 0, Epoch 926, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,775: INFO: model_training: Rank 0, Epoch 927, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,777: INFO: model_training: Rank 0, Epoch 927, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,778: INFO: model_training: Rank 0, Epoch 927, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,780: INFO: model_training: Rank 0, Epoch 927, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,781: INFO: model_training: Rank 0, Epoch 927, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,782: INFO: model_training: Rank 0, Epoch 928, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,784: INFO: model_training: Rank 0, Epoch 928, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,785: INFO: model_training: Rank 0, Epoch 928, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,787: INFO: model_training: Rank 0, Epoch 928, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,788: INFO: model_training: Rank 0, Epoch 928, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,790: INFO: model_training: Rank 0, Epoch 929, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,791: INFO: model_training: Rank 0, Epoch 929, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,792: INFO: model_training: Rank 0, Epoch 929, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,794: INFO: model_training: Rank 0, Epoch 929, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,796: INFO: model_training: Rank 0, Epoch 929, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,798: INFO: model_training: Rank 0, Epoch 930, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,799: INFO: model_training: Rank 0, Epoch 930, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,800: INFO: model_training: Rank 0, Epoch 930, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,802: INFO: model_training: Rank 0, Epoch 930, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,803: INFO: model_training: Rank 0, Epoch 930, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,805: INFO: model_training: Rank 0, Epoch 931, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,806: INFO: model_training: Rank 0, Epoch 931, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,807: INFO: model_training: Rank 0, Epoch 931, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,809: INFO: model_training: Rank 0, Epoch 931, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,810: INFO: model_training: Rank 0, Epoch 931, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,812: INFO: model_training: Rank 0, Epoch 932, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,813: INFO: model_training: Rank 0, Epoch 932, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,814: INFO: model_training: Rank 0, Epoch 932, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,816: INFO: model_training: Rank 0, Epoch 932, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,816: INFO: model_training: Rank 0, Epoch 932, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,818: INFO: model_training: Rank 0, Epoch 933, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,819: INFO: model_training: Rank 0, Epoch 933, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,821: INFO: model_training: Rank 0, Epoch 933, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,823: INFO: model_training: Rank 0, Epoch 933, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,824: INFO: model_training: Rank 0, Epoch 933, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,826: INFO: model_training: Rank 0, Epoch 934, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,827: INFO: model_training: Rank 0, Epoch 934, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,828: INFO: model_training: Rank 0, Epoch 934, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,830: INFO: model_training: Rank 0, Epoch 934, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,831: INFO: model_training: Rank 0, Epoch 934, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,833: INFO: model_training: Rank 0, Epoch 935, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,834: INFO: model_training: Rank 0, Epoch 935, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,836: INFO: model_training: Rank 0, Epoch 935, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,837: INFO: model_training: Rank 0, Epoch 935, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,838: INFO: model_training: Rank 0, Epoch 935, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,840: INFO: model_training: Rank 0, Epoch 936, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,842: INFO: model_training: Rank 0, Epoch 936, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,843: INFO: model_training: Rank 0, Epoch 936, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,845: INFO: model_training: Rank 0, Epoch 936, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,846: INFO: model_training: Rank 0, Epoch 936, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,848: INFO: model_training: Rank 0, Epoch 937, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,850: INFO: model_training: Rank 0, Epoch 937, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,851: INFO: model_training: Rank 0, Epoch 937, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,852: INFO: model_training: Rank 0, Epoch 937, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,854: INFO: model_training: Rank 0, Epoch 937, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,856: INFO: model_training: Rank 0, Epoch 938, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,857: INFO: model_training: Rank 0, Epoch 938, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,859: INFO: model_training: Rank 0, Epoch 938, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,860: INFO: model_training: Rank 0, Epoch 938, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,861: INFO: model_training: Rank 0, Epoch 938, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,863: INFO: model_training: Rank 0, Epoch 939, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,865: INFO: model_training: Rank 0, Epoch 939, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,866: INFO: model_training: Rank 0, Epoch 939, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,867: INFO: model_training: Rank 0, Epoch 939, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,868: INFO: model_training: Rank 0, Epoch 939, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,871: INFO: model_training: Rank 0, Epoch 940, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,872: INFO: model_training: Rank 0, Epoch 940, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,873: INFO: model_training: Rank 0, Epoch 940, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,875: INFO: model_training: Rank 0, Epoch 940, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,876: INFO: model_training: Rank 0, Epoch 940, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,878: INFO: model_training: Rank 0, Epoch 941, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,879: INFO: model_training: Rank 0, Epoch 941, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,880: INFO: model_training: Rank 0, Epoch 941, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,882: INFO: model_training: Rank 0, Epoch 941, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,883: INFO: model_training: Rank 0, Epoch 941, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,885: INFO: model_training: Rank 0, Epoch 942, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,886: INFO: model_training: Rank 0, Epoch 942, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,888: INFO: model_training: Rank 0, Epoch 942, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,889: INFO: model_training: Rank 0, Epoch 942, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,890: INFO: model_training: Rank 0, Epoch 942, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,891: INFO: model_training: Rank 0, Epoch 943, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,893: INFO: model_training: Rank 0, Epoch 943, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,894: INFO: model_training: Rank 0, Epoch 943, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,899: INFO: model_training: Rank 0, Epoch 943, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,911: INFO: model_training: Rank 0, Epoch 943, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,918: INFO: model_training: Rank 0, Epoch 944, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,923: INFO: model_training: Rank 0, Epoch 944, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,925: INFO: model_training: Rank 0, Epoch 944, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,926: INFO: model_training: Rank 0, Epoch 944, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,931: INFO: model_training: Rank 0, Epoch 944, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,934: INFO: model_training: Rank 0, Epoch 945, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,936: INFO: model_training: Rank 0, Epoch 945, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,938: INFO: model_training: Rank 0, Epoch 945, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,939: INFO: model_training: Rank 0, Epoch 945, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,940: INFO: model_training: Rank 0, Epoch 945, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,942: INFO: model_training: Rank 0, Epoch 946, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,943: INFO: model_training: Rank 0, Epoch 946, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,945: INFO: model_training: Rank 0, Epoch 946, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,946: INFO: model_training: Rank 0, Epoch 946, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,947: INFO: model_training: Rank 0, Epoch 946, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,949: INFO: model_training: Rank 0, Epoch 947, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,950: INFO: model_training: Rank 0, Epoch 947, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,951: INFO: model_training: Rank 0, Epoch 947, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,953: INFO: model_training: Rank 0, Epoch 947, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,955: INFO: model_training: Rank 0, Epoch 947, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,957: INFO: model_training: Rank 0, Epoch 948, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,958: INFO: model_training: Rank 0, Epoch 948, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,959: INFO: model_training: Rank 0, Epoch 948, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,961: INFO: model_training: Rank 0, Epoch 948, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,962: INFO: model_training: Rank 0, Epoch 948, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,964: INFO: model_training: Rank 0, Epoch 949, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,965: INFO: model_training: Rank 0, Epoch 949, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,966: INFO: model_training: Rank 0, Epoch 949, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,967: INFO: model_training: Rank 0, Epoch 949, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,969: INFO: model_training: Rank 0, Epoch 949, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,970: INFO: model_training: Rank 0, Epoch 950, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,972: INFO: model_training: Rank 0, Epoch 950, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,973: INFO: model_training: Rank 0, Epoch 950, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,974: INFO: model_training: Rank 0, Epoch 950, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,975: INFO: model_training: Rank 0, Epoch 950, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,977: INFO: model_training: Rank 0, Epoch 951, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,979: INFO: model_training: Rank 0, Epoch 951, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,980: INFO: model_training: Rank 0, Epoch 951, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,982: INFO: model_training: Rank 0, Epoch 951, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,983: INFO: model_training: Rank 0, Epoch 951, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,984: INFO: model_training: Rank 0, Epoch 952, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,986: INFO: model_training: Rank 0, Epoch 952, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,987: INFO: model_training: Rank 0, Epoch 952, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,989: INFO: model_training: Rank 0, Epoch 952, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,990: INFO: model_training: Rank 0, Epoch 952, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,991: INFO: model_training: Rank 0, Epoch 953, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:49,993: INFO: model_training: Rank 0, Epoch 953, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:49,994: INFO: model_training: Rank 0, Epoch 953, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:49,995: INFO: model_training: Rank 0, Epoch 953, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:49,997: INFO: model_training: Rank 0, Epoch 953, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:49,999: INFO: model_training: Rank 0, Epoch 954, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,000: INFO: model_training: Rank 0, Epoch 954, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,001: INFO: model_training: Rank 0, Epoch 954, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,002: INFO: model_training: Rank 0, Epoch 954, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,004: INFO: model_training: Rank 0, Epoch 954, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,006: INFO: model_training: Rank 0, Epoch 955, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,008: INFO: model_training: Rank 0, Epoch 955, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,009: INFO: model_training: Rank 0, Epoch 955, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,010: INFO: model_training: Rank 0, Epoch 955, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,011: INFO: model_training: Rank 0, Epoch 955, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,013: INFO: model_training: Rank 0, Epoch 956, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,014: INFO: model_training: Rank 0, Epoch 956, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,016: INFO: model_training: Rank 0, Epoch 956, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,017: INFO: model_training: Rank 0, Epoch 956, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,018: INFO: model_training: Rank 0, Epoch 956, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,020: INFO: model_training: Rank 0, Epoch 957, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,022: INFO: model_training: Rank 0, Epoch 957, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,023: INFO: model_training: Rank 0, Epoch 957, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,024: INFO: model_training: Rank 0, Epoch 957, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,025: INFO: model_training: Rank 0, Epoch 957, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,027: INFO: model_training: Rank 0, Epoch 958, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,028: INFO: model_training: Rank 0, Epoch 958, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,030: INFO: model_training: Rank 0, Epoch 958, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,031: INFO: model_training: Rank 0, Epoch 958, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,033: INFO: model_training: Rank 0, Epoch 958, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,035: INFO: model_training: Rank 0, Epoch 959, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,036: INFO: model_training: Rank 0, Epoch 959, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,037: INFO: model_training: Rank 0, Epoch 959, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,039: INFO: model_training: Rank 0, Epoch 959, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,040: INFO: model_training: Rank 0, Epoch 959, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,042: INFO: model_training: Rank 0, Epoch 960, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,043: INFO: model_training: Rank 0, Epoch 960, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,045: INFO: model_training: Rank 0, Epoch 960, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,046: INFO: model_training: Rank 0, Epoch 960, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,047: INFO: model_training: Rank 0, Epoch 960, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,049: INFO: model_training: Rank 0, Epoch 961, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,050: INFO: model_training: Rank 0, Epoch 961, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,051: INFO: model_training: Rank 0, Epoch 961, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,052: INFO: model_training: Rank 0, Epoch 961, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,053: INFO: model_training: Rank 0, Epoch 961, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,055: INFO: model_training: Rank 0, Epoch 962, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,056: INFO: model_training: Rank 0, Epoch 962, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,057: INFO: model_training: Rank 0, Epoch 962, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,059: INFO: model_training: Rank 0, Epoch 962, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,061: INFO: model_training: Rank 0, Epoch 962, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,063: INFO: model_training: Rank 0, Epoch 963, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,064: INFO: model_training: Rank 0, Epoch 963, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,065: INFO: model_training: Rank 0, Epoch 963, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,066: INFO: model_training: Rank 0, Epoch 963, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,067: INFO: model_training: Rank 0, Epoch 963, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,069: INFO: model_training: Rank 0, Epoch 964, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,070: INFO: model_training: Rank 0, Epoch 964, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,073: INFO: model_training: Rank 0, Epoch 964, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,074: INFO: model_training: Rank 0, Epoch 964, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,075: INFO: model_training: Rank 0, Epoch 964, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,076: INFO: model_training: Rank 0, Epoch 965, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,078: INFO: model_training: Rank 0, Epoch 965, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,079: INFO: model_training: Rank 0, Epoch 965, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,081: INFO: model_training: Rank 0, Epoch 965, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,082: INFO: model_training: Rank 0, Epoch 965, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,084: INFO: model_training: Rank 0, Epoch 966, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,085: INFO: model_training: Rank 0, Epoch 966, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,086: INFO: model_training: Rank 0, Epoch 966, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,088: INFO: model_training: Rank 0, Epoch 966, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,090: INFO: model_training: Rank 0, Epoch 966, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,091: INFO: model_training: Rank 0, Epoch 967, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,093: INFO: model_training: Rank 0, Epoch 967, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,094: INFO: model_training: Rank 0, Epoch 967, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,095: INFO: model_training: Rank 0, Epoch 967, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,097: INFO: model_training: Rank 0, Epoch 967, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,099: INFO: model_training: Rank 0, Epoch 968, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,100: INFO: model_training: Rank 0, Epoch 968, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,101: INFO: model_training: Rank 0, Epoch 968, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,102: INFO: model_training: Rank 0, Epoch 968, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,104: INFO: model_training: Rank 0, Epoch 968, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,106: INFO: model_training: Rank 0, Epoch 969, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,107: INFO: model_training: Rank 0, Epoch 969, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,108: INFO: model_training: Rank 0, Epoch 969, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,109: INFO: model_training: Rank 0, Epoch 969, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,111: INFO: model_training: Rank 0, Epoch 969, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,113: INFO: model_training: Rank 0, Epoch 970, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,115: INFO: model_training: Rank 0, Epoch 970, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,116: INFO: model_training: Rank 0, Epoch 970, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,117: INFO: model_training: Rank 0, Epoch 970, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,118: INFO: model_training: Rank 0, Epoch 970, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,120: INFO: model_training: Rank 0, Epoch 971, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,122: INFO: model_training: Rank 0, Epoch 971, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,123: INFO: model_training: Rank 0, Epoch 971, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,124: INFO: model_training: Rank 0, Epoch 971, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,125: INFO: model_training: Rank 0, Epoch 971, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,126: INFO: model_training: Rank 0, Epoch 972, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,128: INFO: model_training: Rank 0, Epoch 972, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,129: INFO: model_training: Rank 0, Epoch 972, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,131: INFO: model_training: Rank 0, Epoch 972, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,132: INFO: model_training: Rank 0, Epoch 972, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,134: INFO: model_training: Rank 0, Epoch 973, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,135: INFO: model_training: Rank 0, Epoch 973, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,136: INFO: model_training: Rank 0, Epoch 973, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,138: INFO: model_training: Rank 0, Epoch 973, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,140: INFO: model_training: Rank 0, Epoch 973, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,142: INFO: model_training: Rank 0, Epoch 974, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,143: INFO: model_training: Rank 0, Epoch 974, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,144: INFO: model_training: Rank 0, Epoch 974, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,146: INFO: model_training: Rank 0, Epoch 974, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,147: INFO: model_training: Rank 0, Epoch 974, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,149: INFO: model_training: Rank 0, Epoch 975, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,150: INFO: model_training: Rank 0, Epoch 975, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,151: INFO: model_training: Rank 0, Epoch 975, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,153: INFO: model_training: Rank 0, Epoch 975, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,154: INFO: model_training: Rank 0, Epoch 975, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,156: INFO: model_training: Rank 0, Epoch 976, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,157: INFO: model_training: Rank 0, Epoch 976, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,158: INFO: model_training: Rank 0, Epoch 976, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,160: INFO: model_training: Rank 0, Epoch 976, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,161: INFO: model_training: Rank 0, Epoch 976, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,163: INFO: model_training: Rank 0, Epoch 977, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,165: INFO: model_training: Rank 0, Epoch 977, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,166: INFO: model_training: Rank 0, Epoch 977, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,167: INFO: model_training: Rank 0, Epoch 977, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,168: INFO: model_training: Rank 0, Epoch 977, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,170: INFO: model_training: Rank 0, Epoch 978, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,172: INFO: model_training: Rank 0, Epoch 978, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,173: INFO: model_training: Rank 0, Epoch 978, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,174: INFO: model_training: Rank 0, Epoch 978, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,175: INFO: model_training: Rank 0, Epoch 978, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,177: INFO: model_training: Rank 0, Epoch 979, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,178: INFO: model_training: Rank 0, Epoch 979, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,179: INFO: model_training: Rank 0, Epoch 979, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,181: INFO: model_training: Rank 0, Epoch 979, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,182: INFO: model_training: Rank 0, Epoch 979, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,183: INFO: model_training: Rank 0, Epoch 980, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,185: INFO: model_training: Rank 0, Epoch 980, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,186: INFO: model_training: Rank 0, Epoch 980, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,187: INFO: model_training: Rank 0, Epoch 980, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,189: INFO: model_training: Rank 0, Epoch 980, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,191: INFO: model_training: Rank 0, Epoch 981, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,192: INFO: model_training: Rank 0, Epoch 981, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,194: INFO: model_training: Rank 0, Epoch 981, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,195: INFO: model_training: Rank 0, Epoch 981, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,197: INFO: model_training: Rank 0, Epoch 981, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,198: INFO: model_training: Rank 0, Epoch 982, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,199: INFO: model_training: Rank 0, Epoch 982, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,200: INFO: model_training: Rank 0, Epoch 982, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,202: INFO: model_training: Rank 0, Epoch 982, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,204: INFO: model_training: Rank 0, Epoch 982, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,205: INFO: model_training: Rank 0, Epoch 983, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,206: INFO: model_training: Rank 0, Epoch 983, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,207: INFO: model_training: Rank 0, Epoch 983, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,208: INFO: model_training: Rank 0, Epoch 983, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,209: INFO: model_training: Rank 0, Epoch 983, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,211: INFO: model_training: Rank 0, Epoch 984, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,213: INFO: model_training: Rank 0, Epoch 984, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,215: INFO: model_training: Rank 0, Epoch 984, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,216: INFO: model_training: Rank 0, Epoch 984, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,217: INFO: model_training: Rank 0, Epoch 984, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,219: INFO: model_training: Rank 0, Epoch 985, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,220: INFO: model_training: Rank 0, Epoch 985, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,222: INFO: model_training: Rank 0, Epoch 985, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,223: INFO: model_training: Rank 0, Epoch 985, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,224: INFO: model_training: Rank 0, Epoch 985, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,226: INFO: model_training: Rank 0, Epoch 986, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,227: INFO: model_training: Rank 0, Epoch 986, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,229: INFO: model_training: Rank 0, Epoch 986, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,230: INFO: model_training: Rank 0, Epoch 986, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,231: INFO: model_training: Rank 0, Epoch 986, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,233: INFO: model_training: Rank 0, Epoch 987, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,235: INFO: model_training: Rank 0, Epoch 987, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,236: INFO: model_training: Rank 0, Epoch 987, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,238: INFO: model_training: Rank 0, Epoch 987, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,240: INFO: model_training: Rank 0, Epoch 987, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,241: INFO: model_training: Rank 0, Epoch 988, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,242: INFO: model_training: Rank 0, Epoch 988, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,245: INFO: model_training: Rank 0, Epoch 988, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,247: INFO: model_training: Rank 0, Epoch 988, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,248: INFO: model_training: Rank 0, Epoch 988, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,249: INFO: model_training: Rank 0, Epoch 989, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,251: INFO: model_training: Rank 0, Epoch 989, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,252: INFO: model_training: Rank 0, Epoch 989, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,254: INFO: model_training: Rank 0, Epoch 989, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,256: INFO: model_training: Rank 0, Epoch 989, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,257: INFO: model_training: Rank 0, Epoch 990, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,259: INFO: model_training: Rank 0, Epoch 990, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,260: INFO: model_training: Rank 0, Epoch 990, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,262: INFO: model_training: Rank 0, Epoch 990, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,263: INFO: model_training: Rank 0, Epoch 990, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,265: INFO: model_training: Rank 0, Epoch 991, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,266: INFO: model_training: Rank 0, Epoch 991, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,268: INFO: model_training: Rank 0, Epoch 991, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,269: INFO: model_training: Rank 0, Epoch 991, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,271: INFO: model_training: Rank 0, Epoch 991, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,273: INFO: model_training: Rank 0, Epoch 992, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,275: INFO: model_training: Rank 0, Epoch 992, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,276: INFO: model_training: Rank 0, Epoch 992, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,277: INFO: model_training: Rank 0, Epoch 992, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,278: INFO: model_training: Rank 0, Epoch 992, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,280: INFO: model_training: Rank 0, Epoch 993, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,282: INFO: model_training: Rank 0, Epoch 993, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,283: INFO: model_training: Rank 0, Epoch 993, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,284: INFO: model_training: Rank 0, Epoch 993, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,285: INFO: model_training: Rank 0, Epoch 993, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,288: INFO: model_training: Rank 0, Epoch 994, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,289: INFO: model_training: Rank 0, Epoch 994, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,290: INFO: model_training: Rank 0, Epoch 994, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,291: INFO: model_training: Rank 0, Epoch 994, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,292: INFO: model_training: Rank 0, Epoch 994, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,294: INFO: model_training: Rank 0, Epoch 995, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,296: INFO: model_training: Rank 0, Epoch 995, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,297: INFO: model_training: Rank 0, Epoch 995, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,298: INFO: model_training: Rank 0, Epoch 995, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,300: INFO: model_training: Rank 0, Epoch 995, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,302: INFO: model_training: Rank 0, Epoch 996, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,303: INFO: model_training: Rank 0, Epoch 996, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,305: INFO: model_training: Rank 0, Epoch 996, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,306: INFO: model_training: Rank 0, Epoch 996, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,307: INFO: model_training: Rank 0, Epoch 996, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,308: INFO: model_training: Rank 0, Epoch 997, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,310: INFO: model_training: Rank 0, Epoch 997, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,311: INFO: model_training: Rank 0, Epoch 997, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,313: INFO: model_training: Rank 0, Epoch 997, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,314: INFO: model_training: Rank 0, Epoch 997, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,316: INFO: model_training: Rank 0, Epoch 998, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,317: INFO: model_training: Rank 0, Epoch 998, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,319: INFO: model_training: Rank 0, Epoch 998, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,320: INFO: model_training: Rank 0, Epoch 998, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,322: INFO: model_training: Rank 0, Epoch 998, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,323: INFO: model_training: Rank 0, Epoch 999, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,325: INFO: model_training: Rank 0, Epoch 999, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,327: INFO: model_training: Rank 0, Epoch 999, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,328: INFO: model_training: Rank 0, Epoch 999, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,330: INFO: model_training: Rank 0, Epoch 999, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,332: INFO: model_training: Rank 0, Epoch 1000, Batch 0, Loss: 0.0500]
[2025-05-20 22:23:50,333: INFO: model_training: Rank 0, Epoch 1000, Batch 1, Loss: 0.0500]
[2025-05-20 22:23:50,334: INFO: model_training: Rank 0, Epoch 1000, Batch 2, Loss: 0.0500]
[2025-05-20 22:23:50,335: INFO: model_training: Rank 0, Epoch 1000, Batch 3, Loss: 0.0500]
[2025-05-20 22:23:50,337: INFO: model_training: Rank 0, Epoch 1000, Batch 4, Loss: 0.0500]
[2025-05-20 22:23:50,337: INFO: model_training: Training completed on rank 0.]
[2025-05-20 22:23:53,281: INFO: main: >>>>>> stage Training completed <<<<<<]
[2025-05-20 22:25:33,838: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:25:33,841: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:25:33,842: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:25:33,842: INFO: common: created directory at: artifacts]
[2025-05-20 22:25:33,842: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:25:33,842: INFO: data_ingestion: File already exists]
[2025-05-20 22:25:33,842: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:25:33,842: INFO: main: *******************]
[2025-05-20 22:25:33,842: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:25:33,843: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:25:33,844: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:25:33,844: INFO: common: created directory at: artifacts]
[2025-05-20 22:25:33,844: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:25:40,578: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:25:40,579: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:25:41,307: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 0.7292482256889343]
[2025-05-20 22:25:41,310: INFO: distributed: Reducer buckets have been rebuilt in this iteration.]
[2025-05-20 22:25:41,869: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 0.7226725816726685]
[2025-05-20 22:25:42,414: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 0.7144083380699158]
[2025-05-20 22:25:42,932: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 0.709667980670929]
[2025-05-20 22:25:43,441: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 0.7046673893928528]
[2025-05-20 22:25:43,959: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 0.6975193619728088]
[2025-05-20 22:25:44,479: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 0.6974540948867798]
[2025-05-20 22:25:44,999: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 0.6974772810935974]
[2025-05-20 22:25:45,514: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 0.6959035396575928]
[2025-05-20 22:25:46,032: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 0.6958770751953125]
[2025-05-20 22:25:46,546: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 0.6951355338096619]
[2025-05-20 22:25:47,067: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 0.695212721824646]
[2025-05-20 22:25:47,595: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 0.694632887840271]
[2025-05-20 22:25:48,107: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 0.6945813298225403]
[2025-05-20 22:25:48,631: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 0.6946595907211304]
[2025-05-20 22:25:49,159: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 0.6938291788101196]
[2025-05-20 22:25:49,684: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 0.6937746405601501]
[2025-05-20 22:25:50,217: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 0.693598210811615]
[2025-05-20 22:25:50,745: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 0.6934626698493958]
[2025-05-20 22:25:51,275: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 0.6932039856910706]
[2025-05-20 22:25:51,815: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 0.6928472518920898]
[2025-05-20 22:25:52,346: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 0.6923555135726929]
[2025-05-20 22:25:52,888: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 0.6924337148666382]
[2025-05-20 22:25:53,420: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 0.6924482583999634]
[2025-05-20 22:25:53,954: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 0.6921467185020447]
[2025-05-20 22:25:54,482: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 0.6917836666107178]
[2025-05-20 22:25:55,018: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 0.6914348602294922]
[2025-05-20 22:25:55,576: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 0.6912514567375183]
[2025-05-20 22:25:56,152: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 0.6908165216445923]
[2025-05-20 22:25:56,703: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 0.6908521056175232]
[2025-05-20 22:25:57,257: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 0.6897597908973694]
[2025-05-20 22:25:57,811: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 0.690170407295227]
[2025-05-20 22:25:58,371: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 0.6889041662216187]
[2025-05-20 22:25:58,915: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 0.6894038319587708]
[2025-05-20 22:25:59,464: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 0.6896616220474243]
[2025-05-20 22:26:00,013: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 0.687970757484436]
[2025-05-20 22:26:00,548: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.6873685717582703]
[2025-05-20 22:26:01,103: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.6875017285346985]
[2025-05-20 22:26:01,664: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.6865677833557129]
[2025-05-20 22:26:02,220: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.6880056262016296]
[2025-05-20 22:26:02,772: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.6850924491882324]
[2025-05-20 22:26:03,346: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.6853851079940796]
[2025-05-20 22:26:03,917: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.6841282844543457]
[2025-05-20 22:26:04,482: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.6842735409736633]
[2025-05-20 22:26:05,083: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.6856000423431396]
[2025-05-20 22:26:05,730: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.6828362941741943]
[2025-05-20 22:26:06,352: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.6811449527740479]
[2025-05-20 22:26:06,896: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.681879997253418]
[2025-05-20 22:26:07,460: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.6815367341041565]
[2025-05-20 22:26:08,077: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.6814374327659607]
[2025-05-20 22:26:08,308: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 1.9658]
[2025-05-20 22:26:08,660: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9496]
[2025-05-20 22:26:08,907: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.9326]
[2025-05-20 22:26:09,154: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.9034]
[2025-05-20 22:26:09,408: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8883]
[2025-05-20 22:26:09,651: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.8041]
[2025-05-20 22:26:09,883: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.7361]
[2025-05-20 22:26:10,122: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.7391]
[2025-05-20 22:26:10,345: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.7136]
[2025-05-20 22:26:10,566: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6265]
[2025-05-20 22:26:10,788: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6137]
[2025-05-20 22:26:11,014: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.6061]
[2025-05-20 22:26:11,236: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.6103]
[2025-05-20 22:26:11,450: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.5736]
[2025-05-20 22:26:11,682: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.5515]
[2025-05-20 22:26:11,907: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.4321]
[2025-05-20 22:26:12,123: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4464]
[2025-05-20 22:26:12,345: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.3813]
[2025-05-20 22:26:12,578: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.4389]
[2025-05-20 22:26:12,806: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.3247]
[2025-05-20 22:26:13,019: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.2914]
[2025-05-20 22:26:13,231: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.2749]
[2025-05-20 22:26:13,442: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.2348]
[2025-05-20 22:26:13,672: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2903]
[2025-05-20 22:26:13,892: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.2117]
[2025-05-20 22:26:14,124: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.1706]
[2025-05-20 22:26:14,336: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.2014]
[2025-05-20 22:26:14,556: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.2085]
[2025-05-20 22:26:14,768: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.0969]
[2025-05-20 22:26:14,983: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.1553]
[2025-05-20 22:26:15,212: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.0437]
[2025-05-20 22:26:15,435: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.0413]
[2025-05-20 22:26:15,666: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0852]
[2025-05-20 22:26:15,912: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 1.0613]
[2025-05-20 22:26:16,132: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 0.9605]
[2025-05-20 22:26:16,361: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 0.9946]
[2025-05-20 22:26:16,568: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.9939]
[2025-05-20 22:26:16,781: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9261]
[2025-05-20 22:26:17,007: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.9358]
[2025-05-20 22:26:17,211: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.8709]
[2025-05-20 22:26:17,416: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.8855]
[2025-05-20 22:26:17,624: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.8736]
[2025-05-20 22:26:17,835: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.8328]
[2025-05-20 22:26:18,054: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8300]
[2025-05-20 22:26:18,263: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8257]
[2025-05-20 22:26:18,476: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.8376]
[2025-05-20 22:26:18,688: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.8298]
[2025-05-20 22:26:18,899: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.7679]
[2025-05-20 22:26:19,249: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.7813]
[2025-05-20 22:26:19,463: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7188]
[2025-05-20 22:26:19,678: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.7772]
[2025-05-20 22:26:19,896: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.7565]
[2025-05-20 22:26:20,122: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.6676]
[2025-05-20 22:26:20,332: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.6966]
[2025-05-20 22:26:20,542: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.6877]
[2025-05-20 22:26:20,755: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6486]
[2025-05-20 22:26:20,967: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.6370]
[2025-05-20 22:26:21,173: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.5978]
[2025-05-20 22:26:21,380: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.6261]
[2025-05-20 22:26:21,585: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.5680]
[2025-05-20 22:26:21,795: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.6413]
[2025-05-20 22:26:22,006: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.5840]
[2025-05-20 22:26:22,213: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5997]
[2025-05-20 22:26:22,422: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.5203]
[2025-05-20 22:26:22,643: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5450]
[2025-05-20 22:26:22,863: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.5407]
[2025-05-20 22:26:23,074: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.5464]
[2025-05-20 22:26:23,284: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.5651]
[2025-05-20 22:26:23,505: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.4740]
[2025-05-20 22:26:23,713: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.5229]
[2025-05-20 22:26:23,926: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.4821]
[2025-05-20 22:26:24,167: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.4504]
[2025-05-20 22:26:24,377: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4641]
[2025-05-20 22:26:24,588: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4213]
[2025-05-20 22:26:24,798: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4197]
[2025-05-20 22:26:25,015: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4039]
[2025-05-20 22:26:25,233: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.4802]
[2025-05-20 22:26:25,443: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.4169]
[2025-05-20 22:26:25,650: INFO: model_training: Rank 0, Epoch 16, Batch 3, Loss: 0.4346]
[2025-05-20 22:26:25,859: INFO: model_training: Rank 0, Epoch 16, Batch 4, Loss: 0.3678]
[2025-05-20 22:26:26,065: INFO: model_training: Rank 0, Epoch 17, Batch 0, Loss: 0.4258]
[2025-05-20 22:26:26,273: INFO: model_training: Rank 0, Epoch 17, Batch 1, Loss: 0.3848]
[2025-05-20 22:26:26,481: INFO: model_training: Rank 0, Epoch 17, Batch 2, Loss: 0.3842]
[2025-05-20 22:26:26,690: INFO: model_training: Rank 0, Epoch 17, Batch 3, Loss: 0.4051]
[2025-05-20 22:26:26,900: INFO: model_training: Rank 0, Epoch 17, Batch 4, Loss: 0.3504]
[2025-05-20 22:26:27,112: INFO: model_training: Rank 0, Epoch 18, Batch 0, Loss: 0.4067]
[2025-05-20 22:26:27,323: INFO: model_training: Rank 0, Epoch 18, Batch 1, Loss: 0.3401]
[2025-05-20 22:26:27,526: INFO: model_training: Rank 0, Epoch 18, Batch 2, Loss: 0.3128]
[2025-05-20 22:26:27,738: INFO: model_training: Rank 0, Epoch 18, Batch 3, Loss: 0.3219]
[2025-05-20 22:26:27,953: INFO: model_training: Rank 0, Epoch 18, Batch 4, Loss: 0.3372]
[2025-05-20 22:26:28,168: INFO: model_training: Rank 0, Epoch 19, Batch 0, Loss: 0.2954]
[2025-05-20 22:26:28,391: INFO: model_training: Rank 0, Epoch 19, Batch 1, Loss: 0.3017]
[2025-05-20 22:26:28,608: INFO: model_training: Rank 0, Epoch 19, Batch 2, Loss: 0.3492]
[2025-05-20 22:26:28,826: INFO: model_training: Rank 0, Epoch 19, Batch 3, Loss: 0.3371]
[2025-05-20 22:26:29,047: INFO: model_training: Rank 0, Epoch 19, Batch 4, Loss: 0.3363]
[2025-05-20 22:26:29,274: INFO: model_training: Rank 0, Epoch 20, Batch 0, Loss: 0.3176]
[2025-05-20 22:26:29,486: INFO: model_training: Rank 0, Epoch 20, Batch 1, Loss: 0.2508]
[2025-05-20 22:26:29,697: INFO: model_training: Rank 0, Epoch 20, Batch 2, Loss: 0.2360]
[2025-05-20 22:26:29,911: INFO: model_training: Rank 0, Epoch 20, Batch 3, Loss: 0.3144]
[2025-05-20 22:26:30,127: INFO: model_training: Rank 0, Epoch 20, Batch 4, Loss: 0.2713]
[2025-05-20 22:26:30,344: INFO: model_training: Rank 0, Epoch 21, Batch 0, Loss: 0.2276]
[2025-05-20 22:26:30,574: INFO: model_training: Rank 0, Epoch 21, Batch 1, Loss: 0.2495]
[2025-05-20 22:26:30,790: INFO: model_training: Rank 0, Epoch 21, Batch 2, Loss: 0.2120]
[2025-05-20 22:26:31,004: INFO: model_training: Rank 0, Epoch 21, Batch 3, Loss: 0.2358]
[2025-05-20 22:26:31,212: INFO: model_training: Rank 0, Epoch 21, Batch 4, Loss: 0.2103]
[2025-05-20 22:26:31,436: INFO: model_training: Rank 0, Epoch 22, Batch 0, Loss: 0.2163]
[2025-05-20 22:26:31,666: INFO: model_training: Rank 0, Epoch 22, Batch 1, Loss: 0.2073]
[2025-05-20 22:26:31,890: INFO: model_training: Rank 0, Epoch 22, Batch 2, Loss: 0.2628]
[2025-05-20 22:26:32,102: INFO: model_training: Rank 0, Epoch 22, Batch 3, Loss: 0.1870]
[2025-05-20 22:26:32,312: INFO: model_training: Rank 0, Epoch 22, Batch 4, Loss: 0.1829]
[2025-05-20 22:26:32,520: INFO: model_training: Rank 0, Epoch 23, Batch 0, Loss: 0.1965]
[2025-05-20 22:26:32,738: INFO: model_training: Rank 0, Epoch 23, Batch 1, Loss: 0.1680]
[2025-05-20 22:26:32,946: INFO: model_training: Rank 0, Epoch 23, Batch 2, Loss: 0.2323]
[2025-05-20 22:26:33,155: INFO: model_training: Rank 0, Epoch 23, Batch 3, Loss: 0.2522]
[2025-05-20 22:26:33,365: INFO: model_training: Rank 0, Epoch 23, Batch 4, Loss: 0.2195]
[2025-05-20 22:26:33,575: INFO: model_training: Rank 0, Epoch 24, Batch 0, Loss: 0.2177]
[2025-05-20 22:26:33,798: INFO: model_training: Rank 0, Epoch 24, Batch 1, Loss: 0.1500]
[2025-05-20 22:26:34,003: INFO: model_training: Rank 0, Epoch 24, Batch 2, Loss: 0.1640]
[2025-05-20 22:26:34,200: INFO: model_training: Rank 0, Epoch 24, Batch 3, Loss: 0.1877]
[2025-05-20 22:26:34,397: INFO: model_training: Rank 0, Epoch 24, Batch 4, Loss: 0.1731]
[2025-05-20 22:26:34,599: INFO: model_training: Rank 0, Epoch 25, Batch 0, Loss: 0.1588]
[2025-05-20 22:26:34,804: INFO: model_training: Rank 0, Epoch 25, Batch 1, Loss: 0.1343]
[2025-05-20 22:26:35,005: INFO: model_training: Rank 0, Epoch 25, Batch 2, Loss: 0.1911]
[2025-05-20 22:26:35,203: INFO: model_training: Rank 0, Epoch 25, Batch 3, Loss: 0.2151]
[2025-05-20 22:26:35,402: INFO: model_training: Rank 0, Epoch 25, Batch 4, Loss: 0.1378]
[2025-05-20 22:26:35,604: INFO: model_training: Rank 0, Epoch 26, Batch 0, Loss: 0.1601]
[2025-05-20 22:26:35,804: INFO: model_training: Rank 0, Epoch 26, Batch 1, Loss: 0.1201]
[2025-05-20 22:26:36,032: INFO: model_training: Rank 0, Epoch 26, Batch 2, Loss: 0.1367]
[2025-05-20 22:26:36,239: INFO: model_training: Rank 0, Epoch 26, Batch 3, Loss: 0.1502]
[2025-05-20 22:26:36,458: INFO: model_training: Rank 0, Epoch 26, Batch 4, Loss: 0.1180]
[2025-05-20 22:26:36,664: INFO: model_training: Rank 0, Epoch 27, Batch 0, Loss: 0.1633]
[2025-05-20 22:26:36,891: INFO: model_training: Rank 0, Epoch 27, Batch 1, Loss: 0.1457]
[2025-05-20 22:26:37,100: INFO: model_training: Rank 0, Epoch 27, Batch 2, Loss: 0.1058]
[2025-05-20 22:26:37,300: INFO: model_training: Rank 0, Epoch 27, Batch 3, Loss: 0.1617]
[2025-05-20 22:26:37,497: INFO: model_training: Rank 0, Epoch 27, Batch 4, Loss: 0.1167]
[2025-05-20 22:26:37,706: INFO: model_training: Rank 0, Epoch 28, Batch 0, Loss: 0.1698]
[2025-05-20 22:26:37,910: INFO: model_training: Rank 0, Epoch 28, Batch 1, Loss: 0.1484]
[2025-05-20 22:26:38,120: INFO: model_training: Rank 0, Epoch 28, Batch 2, Loss: 0.1477]
[2025-05-20 22:26:38,334: INFO: model_training: Rank 0, Epoch 28, Batch 3, Loss: 0.1141]
[2025-05-20 22:26:38,527: INFO: model_training: Rank 0, Epoch 28, Batch 4, Loss: 0.1557]
[2025-05-20 22:26:38,791: INFO: model_training: Rank 0, Epoch 29, Batch 0, Loss: 0.0755]
[2025-05-20 22:26:39,025: INFO: model_training: Rank 0, Epoch 29, Batch 1, Loss: 0.1611]
[2025-05-20 22:26:39,234: INFO: model_training: Rank 0, Epoch 29, Batch 2, Loss: 0.0966]
[2025-05-20 22:26:39,430: INFO: model_training: Rank 0, Epoch 29, Batch 3, Loss: 0.0930]
[2025-05-20 22:26:39,643: INFO: model_training: Rank 0, Epoch 29, Batch 4, Loss: 0.0698]
[2025-05-20 22:26:39,852: INFO: model_training: Rank 0, Epoch 30, Batch 0, Loss: 0.1520]
[2025-05-20 22:26:40,079: INFO: model_training: Rank 0, Epoch 30, Batch 1, Loss: 0.0806]
[2025-05-20 22:26:40,312: INFO: model_training: Rank 0, Epoch 30, Batch 2, Loss: 0.1112]
[2025-05-20 22:26:40,539: INFO: model_training: Rank 0, Epoch 30, Batch 3, Loss: 0.0790]
[2025-05-20 22:26:40,784: INFO: model_training: Rank 0, Epoch 30, Batch 4, Loss: 0.1422]
[2025-05-20 22:26:41,007: INFO: model_training: Rank 0, Epoch 31, Batch 0, Loss: 0.0596]
[2025-05-20 22:26:41,225: INFO: model_training: Rank 0, Epoch 31, Batch 1, Loss: 0.0752]
[2025-05-20 22:26:41,456: INFO: model_training: Rank 0, Epoch 31, Batch 2, Loss: 0.1326]
[2025-05-20 22:26:41,682: INFO: model_training: Rank 0, Epoch 31, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:41,901: INFO: model_training: Rank 0, Epoch 31, Batch 4, Loss: 0.1292]
[2025-05-20 22:26:42,127: INFO: model_training: Rank 0, Epoch 32, Batch 0, Loss: 0.1122]
[2025-05-20 22:26:42,372: INFO: model_training: Rank 0, Epoch 32, Batch 1, Loss: 0.1306]
[2025-05-20 22:26:42,607: INFO: model_training: Rank 0, Epoch 32, Batch 2, Loss: 0.1240]
[2025-05-20 22:26:42,823: INFO: model_training: Rank 0, Epoch 32, Batch 3, Loss: 0.1234]
[2025-05-20 22:26:43,040: INFO: model_training: Rank 0, Epoch 32, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:43,248: INFO: model_training: Rank 0, Epoch 33, Batch 0, Loss: 0.1057]
[2025-05-20 22:26:43,461: INFO: model_training: Rank 0, Epoch 33, Batch 1, Loss: 0.1247]
[2025-05-20 22:26:43,674: INFO: model_training: Rank 0, Epoch 33, Batch 2, Loss: 0.1108]
[2025-05-20 22:26:43,884: INFO: model_training: Rank 0, Epoch 33, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:44,115: INFO: model_training: Rank 0, Epoch 33, Batch 4, Loss: 0.0577]
[2025-05-20 22:26:44,328: INFO: model_training: Rank 0, Epoch 34, Batch 0, Loss: 0.1088]
[2025-05-20 22:26:44,545: INFO: model_training: Rank 0, Epoch 34, Batch 1, Loss: 0.0500]
[2025-05-20 22:26:44,768: INFO: model_training: Rank 0, Epoch 34, Batch 2, Loss: 0.1066]
[2025-05-20 22:26:44,985: INFO: model_training: Rank 0, Epoch 34, Batch 3, Loss: 0.0661]
[2025-05-20 22:26:45,192: INFO: model_training: Rank 0, Epoch 34, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:45,404: INFO: model_training: Rank 0, Epoch 35, Batch 0, Loss: 0.0680]
[2025-05-20 22:26:45,634: INFO: model_training: Rank 0, Epoch 35, Batch 1, Loss: 0.0929]
[2025-05-20 22:26:45,841: INFO: model_training: Rank 0, Epoch 35, Batch 2, Loss: 0.1020]
[2025-05-20 22:26:46,049: INFO: model_training: Rank 0, Epoch 35, Batch 3, Loss: 0.0869]
[2025-05-20 22:26:46,320: INFO: model_training: Rank 0, Epoch 35, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:46,541: INFO: model_training: Rank 0, Epoch 36, Batch 0, Loss: 0.0556]
[2025-05-20 22:26:46,750: INFO: model_training: Rank 0, Epoch 36, Batch 1, Loss: 0.1005]
[2025-05-20 22:26:46,969: INFO: model_training: Rank 0, Epoch 36, Batch 2, Loss: 0.0662]
[2025-05-20 22:26:47,185: INFO: model_training: Rank 0, Epoch 36, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:47,397: INFO: model_training: Rank 0, Epoch 36, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:47,609: INFO: model_training: Rank 0, Epoch 37, Batch 0, Loss: 0.0600]
[2025-05-20 22:26:47,821: INFO: model_training: Rank 0, Epoch 37, Batch 1, Loss: 0.0596]
[2025-05-20 22:26:48,048: INFO: model_training: Rank 0, Epoch 37, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:48,267: INFO: model_training: Rank 0, Epoch 37, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:48,473: INFO: model_training: Rank 0, Epoch 37, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:48,689: INFO: model_training: Rank 0, Epoch 38, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:48,903: INFO: model_training: Rank 0, Epoch 38, Batch 1, Loss: 0.0500]
[2025-05-20 22:26:49,112: INFO: model_training: Rank 0, Epoch 38, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:49,318: INFO: model_training: Rank 0, Epoch 38, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:49,523: INFO: model_training: Rank 0, Epoch 38, Batch 4, Loss: 0.0668]
[2025-05-20 22:26:49,743: INFO: model_training: Rank 0, Epoch 39, Batch 0, Loss: 0.0604]
[2025-05-20 22:26:49,956: INFO: model_training: Rank 0, Epoch 39, Batch 1, Loss: 0.0760]
[2025-05-20 22:26:50,163: INFO: model_training: Rank 0, Epoch 39, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:50,368: INFO: model_training: Rank 0, Epoch 39, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:50,581: INFO: model_training: Rank 0, Epoch 39, Batch 4, Loss: 0.0753]
[2025-05-20 22:26:50,799: INFO: model_training: Rank 0, Epoch 40, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:51,010: INFO: model_training: Rank 0, Epoch 40, Batch 1, Loss: 0.0793]
[2025-05-20 22:26:51,220: INFO: model_training: Rank 0, Epoch 40, Batch 2, Loss: 0.0816]
[2025-05-20 22:26:51,434: INFO: model_training: Rank 0, Epoch 40, Batch 3, Loss: 0.0658]
[2025-05-20 22:26:51,646: INFO: model_training: Rank 0, Epoch 40, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:51,860: INFO: model_training: Rank 0, Epoch 41, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:52,071: INFO: model_training: Rank 0, Epoch 41, Batch 1, Loss: 0.0500]
[2025-05-20 22:26:52,288: INFO: model_training: Rank 0, Epoch 41, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:52,522: INFO: model_training: Rank 0, Epoch 41, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:52,736: INFO: model_training: Rank 0, Epoch 41, Batch 4, Loss: 0.0713]
[2025-05-20 22:26:52,945: INFO: model_training: Rank 0, Epoch 42, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:53,156: INFO: model_training: Rank 0, Epoch 42, Batch 1, Loss: 0.0500]
[2025-05-20 22:26:53,364: INFO: model_training: Rank 0, Epoch 42, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:53,579: INFO: model_training: Rank 0, Epoch 42, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:53,794: INFO: model_training: Rank 0, Epoch 42, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:54,006: INFO: model_training: Rank 0, Epoch 43, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:54,218: INFO: model_training: Rank 0, Epoch 43, Batch 1, Loss: 0.0500]
[2025-05-20 22:26:54,427: INFO: model_training: Rank 0, Epoch 43, Batch 2, Loss: 0.0664]
[2025-05-20 22:26:54,641: INFO: model_training: Rank 0, Epoch 43, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:54,854: INFO: model_training: Rank 0, Epoch 43, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:55,073: INFO: model_training: Rank 0, Epoch 44, Batch 0, Loss: 0.0687]
[2025-05-20 22:26:55,310: INFO: model_training: Rank 0, Epoch 44, Batch 1, Loss: 0.0500]
[2025-05-20 22:26:55,532: INFO: model_training: Rank 0, Epoch 44, Batch 2, Loss: 0.0741]
[2025-05-20 22:26:55,745: INFO: model_training: Rank 0, Epoch 44, Batch 3, Loss: 0.0637]
[2025-05-20 22:26:55,977: INFO: model_training: Rank 0, Epoch 44, Batch 4, Loss: 0.0552]
[2025-05-20 22:26:56,199: INFO: model_training: Rank 0, Epoch 45, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:56,411: INFO: model_training: Rank 0, Epoch 45, Batch 1, Loss: 0.0694]
[2025-05-20 22:26:56,616: INFO: model_training: Rank 0, Epoch 45, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:56,829: INFO: model_training: Rank 0, Epoch 45, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:57,039: INFO: model_training: Rank 0, Epoch 45, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:57,253: INFO: model_training: Rank 0, Epoch 46, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:57,478: INFO: model_training: Rank 0, Epoch 46, Batch 1, Loss: 0.0643]
[2025-05-20 22:26:57,688: INFO: model_training: Rank 0, Epoch 46, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:57,903: INFO: model_training: Rank 0, Epoch 46, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:58,118: INFO: model_training: Rank 0, Epoch 46, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:58,328: INFO: model_training: Rank 0, Epoch 47, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:58,550: INFO: model_training: Rank 0, Epoch 47, Batch 1, Loss: 0.0500]
[2025-05-20 22:26:58,764: INFO: model_training: Rank 0, Epoch 47, Batch 2, Loss: 0.0500]
[2025-05-20 22:26:58,980: INFO: model_training: Rank 0, Epoch 47, Batch 3, Loss: 0.0500]
[2025-05-20 22:26:59,203: INFO: model_training: Rank 0, Epoch 47, Batch 4, Loss: 0.0500]
[2025-05-20 22:26:59,422: INFO: model_training: Rank 0, Epoch 48, Batch 0, Loss: 0.0500]
[2025-05-20 22:26:59,640: INFO: model_training: Rank 0, Epoch 48, Batch 1, Loss: 0.0522]
[2025-05-20 22:26:59,854: INFO: model_training: Rank 0, Epoch 48, Batch 2, Loss: 0.0500]
[2025-05-20 22:27:08,412: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:27:08,414: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:27:08,415: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:27:08,415: INFO: common: created directory at: artifacts]
[2025-05-20 22:27:08,415: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:27:08,415: INFO: data_ingestion: File already exists]
[2025-05-20 22:27:08,415: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:27:08,415: INFO: main: *******************]
[2025-05-20 22:27:08,415: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:27:08,416: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:27:08,417: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:27:08,417: INFO: common: created directory at: artifacts]
[2025-05-20 22:27:08,417: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:27:15,184: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:27:15,185: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:27:15,477: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 1.9776]
[2025-05-20 22:27:15,744: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9424]
[2025-05-20 22:27:15,957: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.8875]
[2025-05-20 22:27:16,169: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.9311]
[2025-05-20 22:27:16,380: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8706]
[2025-05-20 22:27:16,612: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.8020]
[2025-05-20 22:27:16,820: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.7715]
[2025-05-20 22:27:17,029: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.7414]
[2025-05-20 22:27:17,240: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.6917]
[2025-05-20 22:27:17,448: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6254]
[2025-05-20 22:27:17,655: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6474]
[2025-05-20 22:27:17,860: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.5830]
[2025-05-20 22:27:18,065: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.6000]
[2025-05-20 22:27:18,271: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.4967]
[2025-05-20 22:27:18,476: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.5107]
[2025-05-20 22:27:18,679: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.4403]
[2025-05-20 22:27:18,881: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4330]
[2025-05-20 22:27:19,085: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.3908]
[2025-05-20 22:27:19,290: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.3676]
[2025-05-20 22:27:19,494: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.3181]
[2025-05-20 22:27:19,699: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.2958]
[2025-05-20 22:27:19,902: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.3144]
[2025-05-20 22:27:20,106: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.2692]
[2025-05-20 22:27:20,308: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2890]
[2025-05-20 22:27:20,513: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.1903]
[2025-05-20 22:27:20,722: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.1786]
[2025-05-20 22:27:20,937: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.1751]
[2025-05-20 22:27:21,138: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.1304]
[2025-05-20 22:27:21,336: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.1248]
[2025-05-20 22:27:21,532: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.1370]
[2025-05-20 22:27:21,726: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.0719]
[2025-05-20 22:27:21,922: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.0868]
[2025-05-20 22:27:22,129: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0405]
[2025-05-20 22:27:22,333: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 1.0258]
[2025-05-20 22:27:22,540: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 1.0262]
[2025-05-20 22:27:22,742: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 0.9823]
[2025-05-20 22:27:22,945: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.9322]
[2025-05-20 22:27:23,147: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9614]
[2025-05-20 22:27:23,352: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.9266]
[2025-05-20 22:27:23,559: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.9480]
[2025-05-20 22:27:23,759: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.8795]
[2025-05-20 22:27:23,962: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.8704]
[2025-05-20 22:27:24,170: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.8167]
[2025-05-20 22:27:24,373: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8676]
[2025-05-20 22:27:24,577: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8709]
[2025-05-20 22:27:24,781: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.7706]
[2025-05-20 22:27:24,984: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.8057]
[2025-05-20 22:27:25,189: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.7526]
[2025-05-20 22:27:25,398: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.7400]
[2025-05-20 22:27:25,600: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7559]
[2025-05-20 22:27:25,802: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.7551]
[2025-05-20 22:27:26,004: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.7311]
[2025-05-20 22:27:26,207: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.6502]
[2025-05-20 22:27:26,412: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.7329]
[2025-05-20 22:27:26,621: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.7121]
[2025-05-20 22:27:26,834: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6617]
[2025-05-20 22:27:27,041: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.6017]
[2025-05-20 22:27:27,242: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.5864]
[2025-05-20 22:27:27,448: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.6056]
[2025-05-20 22:27:27,657: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.5662]
[2025-05-20 22:27:27,865: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.5562]
[2025-05-20 22:27:28,070: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.5627]
[2025-05-20 22:27:28,274: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5564]
[2025-05-20 22:27:28,481: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.5258]
[2025-05-20 22:27:28,683: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5264]
[2025-05-20 22:27:28,886: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.4981]
[2025-05-20 22:27:29,089: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.5620]
[2025-05-20 22:27:29,292: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.5234]
[2025-05-20 22:27:29,489: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.5246]
[2025-05-20 22:27:29,691: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.5207]
[2025-05-20 22:27:29,893: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.5097]
[2025-05-20 22:27:30,089: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.4782]
[2025-05-20 22:27:30,288: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4552]
[2025-05-20 22:27:30,488: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4083]
[2025-05-20 22:27:30,688: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4287]
[2025-05-20 22:27:30,887: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4737]
[2025-05-20 22:27:31,087: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.4731]
[2025-05-20 22:27:31,285: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.4180]
[2025-05-20 22:27:31,491: INFO: model_training: Rank 0, Epoch 16, Batch 3, Loss: 0.3751]
[2025-05-20 22:27:31,691: INFO: model_training: Rank 0, Epoch 16, Batch 4, Loss: 0.4035]
[2025-05-20 22:27:31,885: INFO: model_training: Rank 0, Epoch 17, Batch 0, Loss: 0.3952]
[2025-05-20 22:27:32,078: INFO: model_training: Rank 0, Epoch 17, Batch 1, Loss: 0.3500]
[2025-05-20 22:27:32,266: INFO: model_training: Rank 0, Epoch 17, Batch 2, Loss: 0.3451]
[2025-05-20 22:27:32,461: INFO: model_training: Rank 0, Epoch 17, Batch 3, Loss: 0.3461]
[2025-05-20 22:27:32,658: INFO: model_training: Rank 0, Epoch 17, Batch 4, Loss: 0.3330]
[2025-05-20 22:27:32,850: INFO: model_training: Rank 0, Epoch 18, Batch 0, Loss: 0.3579]
[2025-05-20 22:27:33,046: INFO: model_training: Rank 0, Epoch 18, Batch 1, Loss: 0.4002]
[2025-05-20 22:27:33,241: INFO: model_training: Rank 0, Epoch 18, Batch 2, Loss: 0.3542]
[2025-05-20 22:27:33,439: INFO: model_training: Rank 0, Epoch 18, Batch 3, Loss: 0.3282]
[2025-05-20 22:27:33,633: INFO: model_training: Rank 0, Epoch 18, Batch 4, Loss: 0.3196]
[2025-05-20 22:27:33,829: INFO: model_training: Rank 0, Epoch 19, Batch 0, Loss: 0.3262]
[2025-05-20 22:27:34,021: INFO: model_training: Rank 0, Epoch 19, Batch 1, Loss: 0.2773]
[2025-05-20 22:27:34,213: INFO: model_training: Rank 0, Epoch 19, Batch 2, Loss: 0.3040]
[2025-05-20 22:27:34,405: INFO: model_training: Rank 0, Epoch 19, Batch 3, Loss: 0.3116]
[2025-05-20 22:27:34,599: INFO: model_training: Rank 0, Epoch 19, Batch 4, Loss: 0.2839]
[2025-05-20 22:27:34,788: INFO: model_training: Rank 0, Epoch 20, Batch 0, Loss: 0.3169]
[2025-05-20 22:27:34,982: INFO: model_training: Rank 0, Epoch 20, Batch 1, Loss: 0.3337]
[2025-05-20 22:27:35,178: INFO: model_training: Rank 0, Epoch 20, Batch 2, Loss: 0.2665]
[2025-05-20 22:27:35,374: INFO: model_training: Rank 0, Epoch 20, Batch 3, Loss: 0.2594]
[2025-05-20 22:27:35,570: INFO: model_training: Rank 0, Epoch 20, Batch 4, Loss: 0.2798]
[2025-05-20 22:27:35,767: INFO: model_training: Rank 0, Epoch 21, Batch 0, Loss: 0.2373]
[2025-05-20 22:27:36,000: INFO: model_training: Rank 0, Epoch 21, Batch 1, Loss: 0.2824]
[2025-05-20 22:27:36,228: INFO: model_training: Rank 0, Epoch 21, Batch 2, Loss: 0.2797]
[2025-05-20 22:27:36,465: INFO: model_training: Rank 0, Epoch 21, Batch 3, Loss: 0.2400]
[2025-05-20 22:27:36,682: INFO: model_training: Rank 0, Epoch 21, Batch 4, Loss: 0.2895]
[2025-05-20 22:27:36,912: INFO: model_training: Rank 0, Epoch 22, Batch 0, Loss: 0.2416]
[2025-05-20 22:27:37,125: INFO: model_training: Rank 0, Epoch 22, Batch 1, Loss: 0.2004]
[2025-05-20 22:27:37,345: INFO: model_training: Rank 0, Epoch 22, Batch 2, Loss: 0.2196]
[2025-05-20 22:27:37,577: INFO: model_training: Rank 0, Epoch 22, Batch 3, Loss: 0.2134]
[2025-05-20 22:27:37,796: INFO: model_training: Rank 0, Epoch 22, Batch 4, Loss: 0.2456]
[2025-05-20 22:27:38,016: INFO: model_training: Rank 0, Epoch 23, Batch 0, Loss: 0.1959]
[2025-05-20 22:27:38,290: INFO: model_training: Rank 0, Epoch 23, Batch 1, Loss: 0.2235]
[2025-05-20 22:27:38,522: INFO: model_training: Rank 0, Epoch 23, Batch 2, Loss: 0.1868]
[2025-05-20 22:27:38,739: INFO: model_training: Rank 0, Epoch 23, Batch 3, Loss: 0.2037]
[2025-05-20 22:27:38,963: INFO: model_training: Rank 0, Epoch 23, Batch 4, Loss: 0.1503]
[2025-05-20 22:27:39,182: INFO: model_training: Rank 0, Epoch 24, Batch 0, Loss: 0.2405]
[2025-05-20 22:27:39,389: INFO: model_training: Rank 0, Epoch 24, Batch 1, Loss: 0.2014]
[2025-05-20 22:27:39,600: INFO: model_training: Rank 0, Epoch 24, Batch 2, Loss: 0.1612]
[2025-05-20 22:27:39,808: INFO: model_training: Rank 0, Epoch 24, Batch 3, Loss: 0.1392]
[2025-05-20 22:27:40,006: INFO: model_training: Rank 0, Epoch 24, Batch 4, Loss: 0.2193]
[2025-05-20 22:27:40,211: INFO: model_training: Rank 0, Epoch 25, Batch 0, Loss: 0.2055]
[2025-05-20 22:27:40,413: INFO: model_training: Rank 0, Epoch 25, Batch 1, Loss: 0.1954]
[2025-05-20 22:27:40,619: INFO: model_training: Rank 0, Epoch 25, Batch 2, Loss: 0.2000]
[2025-05-20 22:27:40,826: INFO: model_training: Rank 0, Epoch 25, Batch 3, Loss: 0.1211]
[2025-05-20 22:27:41,035: INFO: model_training: Rank 0, Epoch 25, Batch 4, Loss: 0.1346]
[2025-05-20 22:27:41,240: INFO: model_training: Rank 0, Epoch 26, Batch 0, Loss: 0.1967]
[2025-05-20 22:27:41,453: INFO: model_training: Rank 0, Epoch 26, Batch 1, Loss: 0.1405]
[2025-05-20 22:27:41,669: INFO: model_training: Rank 0, Epoch 26, Batch 2, Loss: 0.1093]
[2025-05-20 22:27:41,878: INFO: model_training: Rank 0, Epoch 26, Batch 3, Loss: 0.1652]
[2025-05-20 22:27:42,081: INFO: model_training: Rank 0, Epoch 26, Batch 4, Loss: 0.1791]
[2025-05-20 22:27:42,292: INFO: model_training: Rank 0, Epoch 27, Batch 0, Loss: 0.1889]
[2025-05-20 22:27:42,501: INFO: model_training: Rank 0, Epoch 27, Batch 1, Loss: 0.1658]
[2025-05-20 22:27:42,709: INFO: model_training: Rank 0, Epoch 27, Batch 2, Loss: 0.1773]
[2025-05-20 22:27:42,917: INFO: model_training: Rank 0, Epoch 27, Batch 3, Loss: 0.0988]
[2025-05-20 22:27:43,130: INFO: model_training: Rank 0, Epoch 27, Batch 4, Loss: 0.1439]
[2025-05-20 22:27:43,339: INFO: model_training: Rank 0, Epoch 28, Batch 0, Loss: 0.1533]
[2025-05-20 22:27:43,551: INFO: model_training: Rank 0, Epoch 28, Batch 1, Loss: 0.1648]
[2025-05-20 22:27:43,765: INFO: model_training: Rank 0, Epoch 28, Batch 2, Loss: 0.1678]
[2025-05-20 22:27:43,969: INFO: model_training: Rank 0, Epoch 28, Batch 3, Loss: 0.1543]
[2025-05-20 22:27:44,175: INFO: model_training: Rank 0, Epoch 28, Batch 4, Loss: 0.1583]
[2025-05-20 22:27:44,384: INFO: model_training: Rank 0, Epoch 29, Batch 0, Loss: 0.1249]
[2025-05-20 22:27:44,589: INFO: model_training: Rank 0, Epoch 29, Batch 1, Loss: 0.0693]
[2025-05-20 22:27:44,800: INFO: model_training: Rank 0, Epoch 29, Batch 2, Loss: 0.1541]
[2025-05-20 22:27:45,013: INFO: model_training: Rank 0, Epoch 29, Batch 3, Loss: 0.0900]
[2025-05-20 22:27:45,342: INFO: model_training: Rank 0, Epoch 29, Batch 4, Loss: 0.1061]
[2025-05-20 22:27:45,559: INFO: model_training: Rank 0, Epoch 30, Batch 0, Loss: 0.1508]
[2025-05-20 22:27:45,765: INFO: model_training: Rank 0, Epoch 30, Batch 1, Loss: 0.1119]
[2025-05-20 22:27:45,980: INFO: model_training: Rank 0, Epoch 30, Batch 2, Loss: 0.0639]
[2025-05-20 22:27:46,209: INFO: model_training: Rank 0, Epoch 30, Batch 3, Loss: 0.0768]
[2025-05-20 22:27:46,431: INFO: model_training: Rank 0, Epoch 30, Batch 4, Loss: 0.0500]
[2025-05-20 22:27:46,658: INFO: model_training: Rank 0, Epoch 31, Batch 0, Loss: 0.1344]
[2025-05-20 22:27:46,883: INFO: model_training: Rank 0, Epoch 31, Batch 1, Loss: 0.0644]
[2025-05-20 22:27:47,087: INFO: model_training: Rank 0, Epoch 31, Batch 2, Loss: 0.0733]
[2025-05-20 22:27:47,304: INFO: model_training: Rank 0, Epoch 31, Batch 3, Loss: 0.0726]
[2025-05-20 22:27:47,518: INFO: model_training: Rank 0, Epoch 31, Batch 4, Loss: 0.0871]
[2025-05-20 22:27:47,730: INFO: model_training: Rank 0, Epoch 32, Batch 0, Loss: 0.0957]
[2025-05-20 22:27:47,945: INFO: model_training: Rank 0, Epoch 32, Batch 1, Loss: 0.1286]
[2025-05-20 22:27:48,153: INFO: model_training: Rank 0, Epoch 32, Batch 2, Loss: 0.0526]
[2025-05-20 22:27:48,347: INFO: model_training: Rank 0, Epoch 32, Batch 3, Loss: 0.1090]
[2025-05-20 22:27:48,598: INFO: model_training: Rank 0, Epoch 32, Batch 4, Loss: 0.0577]
[2025-05-20 22:27:48,805: INFO: model_training: Rank 0, Epoch 33, Batch 0, Loss: 0.0500]
[2025-05-20 22:27:49,009: INFO: model_training: Rank 0, Epoch 33, Batch 1, Loss: 0.1189]
[2025-05-20 22:27:49,216: INFO: model_training: Rank 0, Epoch 33, Batch 2, Loss: 0.0732]
[2025-05-20 22:27:49,436: INFO: model_training: Rank 0, Epoch 33, Batch 3, Loss: 0.0500]
[2025-05-20 22:27:49,650: INFO: model_training: Rank 0, Epoch 33, Batch 4, Loss: 0.1088]
[2025-05-20 22:27:49,857: INFO: model_training: Rank 0, Epoch 34, Batch 0, Loss: 0.0500]
[2025-05-20 22:27:50,063: INFO: model_training: Rank 0, Epoch 34, Batch 1, Loss: 0.1085]
[2025-05-20 22:27:50,273: INFO: model_training: Rank 0, Epoch 34, Batch 2, Loss: 0.0886]
[2025-05-20 22:27:50,496: INFO: model_training: Rank 0, Epoch 34, Batch 3, Loss: 0.0500]
[2025-05-20 22:27:50,733: INFO: model_training: Rank 0, Epoch 34, Batch 4, Loss: 0.0583]
[2025-05-20 22:27:50,949: INFO: model_training: Rank 0, Epoch 35, Batch 0, Loss: 0.1094]
[2025-05-20 22:27:51,163: INFO: model_training: Rank 0, Epoch 35, Batch 1, Loss: 0.0536]
[2025-05-20 22:27:51,368: INFO: model_training: Rank 0, Epoch 35, Batch 2, Loss: 0.0569]
[2025-05-20 22:27:51,576: INFO: model_training: Rank 0, Epoch 35, Batch 3, Loss: 0.0988]
[2025-05-20 22:27:51,793: INFO: model_training: Rank 0, Epoch 35, Batch 4, Loss: 0.0500]
[2025-05-20 22:27:52,027: INFO: model_training: Rank 0, Epoch 36, Batch 0, Loss: 0.0539]
[2025-05-20 22:27:52,261: INFO: model_training: Rank 0, Epoch 36, Batch 1, Loss: 0.0500]
[2025-05-20 22:27:52,485: INFO: model_training: Rank 0, Epoch 36, Batch 2, Loss: 0.0785]
[2025-05-20 22:27:52,750: INFO: model_training: Rank 0, Epoch 36, Batch 3, Loss: 0.0863]
[2025-05-20 22:27:52,986: INFO: model_training: Rank 0, Epoch 36, Batch 4, Loss: 0.1028]
[2025-05-20 22:27:53,233: INFO: model_training: Rank 0, Epoch 37, Batch 0, Loss: 0.0500]
[2025-05-20 22:27:53,479: INFO: model_training: Rank 0, Epoch 37, Batch 1, Loss: 0.0500]
[2025-05-20 22:27:53,708: INFO: model_training: Rank 0, Epoch 37, Batch 2, Loss: 0.0553]
[2025-05-20 22:27:53,916: INFO: model_training: Rank 0, Epoch 37, Batch 3, Loss: 0.0500]
[2025-05-20 22:27:54,117: INFO: model_training: Rank 0, Epoch 37, Batch 4, Loss: 0.0682]
[2025-05-20 22:27:54,323: INFO: model_training: Rank 0, Epoch 38, Batch 0, Loss: 0.0903]
[2025-05-20 22:27:54,537: INFO: model_training: Rank 0, Epoch 38, Batch 1, Loss: 0.0812]
[2025-05-20 22:27:54,750: INFO: model_training: Rank 0, Epoch 38, Batch 2, Loss: 0.0500]
[2025-05-20 22:27:54,979: INFO: model_training: Rank 0, Epoch 38, Batch 3, Loss: 0.0500]
[2025-05-20 22:27:55,204: INFO: model_training: Rank 0, Epoch 38, Batch 4, Loss: 0.0500]
[2025-05-20 22:27:55,449: INFO: model_training: Rank 0, Epoch 39, Batch 0, Loss: 0.0532]
[2025-05-20 22:27:55,688: INFO: model_training: Rank 0, Epoch 39, Batch 1, Loss: 0.0500]
[2025-05-20 22:27:55,923: INFO: model_training: Rank 0, Epoch 39, Batch 2, Loss: 0.0500]
[2025-05-20 22:27:56,169: INFO: model_training: Rank 0, Epoch 39, Batch 3, Loss: 0.0500]
[2025-05-20 22:27:56,406: INFO: model_training: Rank 0, Epoch 39, Batch 4, Loss: 0.0500]
[2025-05-20 22:27:56,640: INFO: model_training: Rank 0, Epoch 40, Batch 0, Loss: 0.0871]
[2025-05-20 22:27:56,867: INFO: model_training: Rank 0, Epoch 40, Batch 1, Loss: 0.0500]
[2025-05-20 22:27:57,078: INFO: model_training: Rank 0, Epoch 40, Batch 2, Loss: 0.0677]
[2025-05-20 22:27:57,310: INFO: model_training: Rank 0, Epoch 40, Batch 3, Loss: 0.0500]
[2025-05-20 22:27:57,537: INFO: model_training: Rank 0, Epoch 40, Batch 4, Loss: 0.0500]
[2025-05-20 22:27:57,760: INFO: model_training: Rank 0, Epoch 41, Batch 0, Loss: 0.0742]
[2025-05-20 22:27:57,990: INFO: model_training: Rank 0, Epoch 41, Batch 1, Loss: 0.0500]
[2025-05-20 22:27:58,198: INFO: model_training: Rank 0, Epoch 41, Batch 2, Loss: 0.0558]
[2025-05-20 22:27:58,403: INFO: model_training: Rank 0, Epoch 41, Batch 3, Loss: 0.0679]
[2025-05-20 22:27:58,623: INFO: model_training: Rank 0, Epoch 41, Batch 4, Loss: 0.0500]
[2025-05-20 22:27:58,828: INFO: model_training: Rank 0, Epoch 42, Batch 0, Loss: 0.0648]
[2025-05-20 22:27:59,028: INFO: model_training: Rank 0, Epoch 42, Batch 1, Loss: 0.0500]
[2025-05-20 22:27:59,229: INFO: model_training: Rank 0, Epoch 42, Batch 2, Loss: 0.0500]
[2025-05-20 22:27:59,429: INFO: model_training: Rank 0, Epoch 42, Batch 3, Loss: 0.0500]
[2025-05-20 22:27:59,641: INFO: model_training: Rank 0, Epoch 42, Batch 4, Loss: 0.0500]
[2025-05-20 22:27:59,852: INFO: model_training: Rank 0, Epoch 43, Batch 0, Loss: 0.0500]
[2025-05-20 22:28:00,061: INFO: model_training: Rank 0, Epoch 43, Batch 1, Loss: 0.0500]
[2025-05-20 22:28:00,269: INFO: model_training: Rank 0, Epoch 43, Batch 2, Loss: 0.0500]
[2025-05-20 22:28:00,480: INFO: model_training: Rank 0, Epoch 43, Batch 3, Loss: 0.0727]
[2025-05-20 22:28:00,692: INFO: model_training: Rank 0, Epoch 43, Batch 4, Loss: 0.0500]
[2025-05-20 22:28:00,909: INFO: model_training: Rank 0, Epoch 44, Batch 0, Loss: 0.0500]
[2025-05-20 22:28:01,124: INFO: model_training: Rank 0, Epoch 44, Batch 1, Loss: 0.0500]
[2025-05-20 22:28:01,323: INFO: model_training: Rank 0, Epoch 44, Batch 2, Loss: 0.0500]
[2025-05-20 22:28:01,533: INFO: model_training: Rank 0, Epoch 44, Batch 3, Loss: 0.0674]
[2025-05-20 22:28:01,754: INFO: model_training: Rank 0, Epoch 44, Batch 4, Loss: 0.0500]
[2025-05-20 22:28:01,973: INFO: model_training: Rank 0, Epoch 45, Batch 0, Loss: 0.0513]
[2025-05-20 22:28:02,188: INFO: model_training: Rank 0, Epoch 45, Batch 1, Loss: 0.0500]
[2025-05-20 22:28:02,402: INFO: model_training: Rank 0, Epoch 45, Batch 2, Loss: 0.0500]
[2025-05-20 22:28:02,616: INFO: model_training: Rank 0, Epoch 45, Batch 3, Loss: 0.0619]
[2025-05-20 22:28:02,824: INFO: model_training: Rank 0, Epoch 45, Batch 4, Loss: 0.0500]
[2025-05-20 22:28:03,033: INFO: model_training: Rank 0, Epoch 46, Batch 0, Loss: 0.0500]
[2025-05-20 22:28:03,245: INFO: model_training: Rank 0, Epoch 46, Batch 1, Loss: 0.0550]
[2025-05-20 22:28:03,455: INFO: model_training: Rank 0, Epoch 46, Batch 2, Loss: 0.0500]
[2025-05-20 22:28:03,654: INFO: model_training: Rank 0, Epoch 46, Batch 3, Loss: 0.0500]
[2025-05-20 22:28:03,857: INFO: model_training: Rank 0, Epoch 46, Batch 4, Loss: 0.0500]
[2025-05-20 22:28:04,060: INFO: model_training: Rank 0, Epoch 47, Batch 0, Loss: 0.0500]
[2025-05-20 22:28:04,284: INFO: model_training: Rank 0, Epoch 47, Batch 1, Loss: 0.0664]
[2025-05-20 22:28:04,490: INFO: model_training: Rank 0, Epoch 47, Batch 2, Loss: 0.0652]
[2025-05-20 22:28:04,696: INFO: model_training: Rank 0, Epoch 47, Batch 3, Loss: 0.0500]
[2025-05-20 22:28:04,902: INFO: model_training: Rank 0, Epoch 47, Batch 4, Loss: 0.0500]
[2025-05-20 22:28:05,104: INFO: model_training: Rank 0, Epoch 48, Batch 0, Loss: 0.0500]
[2025-05-20 22:29:21,888: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:29:21,890: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:29:21,891: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:29:21,891: INFO: common: created directory at: artifacts]
[2025-05-20 22:29:21,891: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:29:21,891: INFO: data_ingestion: File already exists]
[2025-05-20 22:29:21,891: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:29:21,891: INFO: main: *******************]
[2025-05-20 22:29:21,891: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:29:21,893: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:29:21,894: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:29:21,894: INFO: common: created directory at: artifacts]
[2025-05-20 22:29:21,894: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:29:28,636: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:29:28,637: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:29:28,940: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 2.0068]
[2025-05-20 22:29:29,219: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 2.0023]
[2025-05-20 22:29:29,447: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.9640]
[2025-05-20 22:29:29,659: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.9312]
[2025-05-20 22:29:29,884: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8068]
[2025-05-20 22:29:30,117: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.7934]
[2025-05-20 22:29:30,323: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.8089]
[2025-05-20 22:29:30,530: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.6959]
[2025-05-20 22:29:30,739: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.6671]
[2025-05-20 22:29:30,938: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6422]
[2025-05-20 22:29:31,144: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6601]
[2025-05-20 22:29:31,346: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.5520]
[2025-05-20 22:29:31,552: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.5240]
[2025-05-20 22:30:18,038: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:30:18,040: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:30:18,040: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:30:18,040: INFO: common: created directory at: artifacts]
[2025-05-20 22:30:18,040: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:30:18,040: INFO: data_ingestion: File already exists]
[2025-05-20 22:30:18,041: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:30:18,041: INFO: main: *******************]
[2025-05-20 22:30:18,041: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:30:18,042: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:30:18,043: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:30:18,043: INFO: common: created directory at: artifacts]
[2025-05-20 22:30:18,043: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:30:24,805: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:30:24,807: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:30:24,851: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 2.0244]
[2025-05-20 22:30:24,853: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9329]
[2025-05-20 22:30:24,854: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.9263]
[2025-05-20 22:30:24,856: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.9140]
[2025-05-20 22:30:24,857: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8449]
[2025-05-20 22:30:24,859: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.7935]
[2025-05-20 22:30:24,860: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.8110]
[2025-05-20 22:30:24,861: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.7107]
[2025-05-20 22:30:24,863: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.7278]
[2025-05-20 22:30:24,864: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6494]
[2025-05-20 22:30:24,866: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6660]
[2025-05-20 22:30:24,867: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.5624]
[2025-05-20 22:30:24,868: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.5438]
[2025-05-20 22:30:24,870: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.5565]
[2025-05-20 22:30:24,871: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.4574]
[2025-05-20 22:30:24,873: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.4755]
[2025-05-20 22:30:24,874: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4409]
[2025-05-20 22:30:24,876: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.4106]
[2025-05-20 22:30:24,877: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.4035]
[2025-05-20 22:30:24,878: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.3220]
[2025-05-20 22:30:24,879: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.2976]
[2025-05-20 22:30:24,882: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.3157]
[2025-05-20 22:30:24,883: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.2509]
[2025-05-20 22:30:24,884: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2747]
[2025-05-20 22:30:24,885: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.2126]
[2025-05-20 22:30:24,887: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.1589]
[2025-05-20 22:30:24,888: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.1656]
[2025-05-20 22:30:24,889: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.1489]
[2025-05-20 22:30:24,891: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.1825]
[2025-05-20 22:30:24,892: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.0838]
[2025-05-20 22:30:24,893: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.0774]
[2025-05-20 22:30:24,895: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.0658]
[2025-05-20 22:30:24,896: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0199]
[2025-05-20 22:30:24,897: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 1.0567]
[2025-05-20 22:30:24,898: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 1.0354]
[2025-05-20 22:30:24,899: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 0.9819]
[2025-05-20 22:30:24,901: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.9968]
[2025-05-20 22:30:24,902: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9060]
[2025-05-20 22:30:24,903: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.9036]
[2025-05-20 22:30:24,904: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.9166]
[2025-05-20 22:30:24,905: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.9278]
[2025-05-20 22:30:24,906: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.9192]
[2025-05-20 22:30:24,907: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.8633]
[2025-05-20 22:30:24,909: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8834]
[2025-05-20 22:30:24,910: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8155]
[2025-05-20 22:30:24,911: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.7732]
[2025-05-20 22:30:24,912: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.8337]
[2025-05-20 22:30:24,913: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.7557]
[2025-05-20 22:30:24,914: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.7633]
[2025-05-20 22:30:24,915: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7895]
[2025-05-20 22:30:24,916: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.7496]
[2025-05-20 22:30:24,917: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.7157]
[2025-05-20 22:30:24,918: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.7086]
[2025-05-20 22:30:24,919: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.6785]
[2025-05-20 22:30:24,920: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.6476]
[2025-05-20 22:30:24,921: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6400]
[2025-05-20 22:30:24,923: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.6154]
[2025-05-20 22:30:24,924: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.6156]
[2025-05-20 22:30:24,925: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.6312]
[2025-05-20 22:30:24,926: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.6230]
[2025-05-20 22:30:24,927: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.6117]
[2025-05-20 22:30:24,929: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.5632]
[2025-05-20 22:30:24,930: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5310]
[2025-05-20 22:30:24,931: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.5607]
[2025-05-20 22:30:24,932: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5425]
[2025-05-20 22:30:24,933: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.5534]
[2025-05-20 22:30:24,934: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.4885]
[2025-05-20 22:30:24,936: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.5218]
[2025-05-20 22:30:24,937: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.5173]
[2025-05-20 22:30:24,938: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.4725]
[2025-05-20 22:30:24,940: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.4732]
[2025-05-20 22:30:24,941: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.4618]
[2025-05-20 22:30:24,942: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4409]
[2025-05-20 22:30:24,943: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4113]
[2025-05-20 22:30:24,944: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4563]
[2025-05-20 22:30:24,945: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4250]
[2025-05-20 22:30:24,946: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.4716]
[2025-05-20 22:30:24,948: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.4102]
[2025-05-20 22:30:24,948: INFO: model_training: Rank 0, Epoch 16, Batch 3, Loss: 0.3943]
[2025-05-20 22:30:24,950: INFO: model_training: Rank 0, Epoch 16, Batch 4, Loss: 0.4531]
[2025-05-20 22:30:24,951: INFO: model_training: Rank 0, Epoch 17, Batch 0, Loss: 0.3776]
[2025-05-20 22:30:24,951: INFO: model_training: Rank 0, Epoch 17, Batch 1, Loss: 0.3805]
[2025-05-20 22:30:24,953: INFO: model_training: Rank 0, Epoch 17, Batch 2, Loss: 0.3789]
[2025-05-20 22:30:24,954: INFO: model_training: Rank 0, Epoch 17, Batch 3, Loss: 0.3252]
[2025-05-20 22:30:24,955: INFO: model_training: Rank 0, Epoch 17, Batch 4, Loss: 0.3886]
[2025-05-20 22:30:24,956: INFO: model_training: Rank 0, Epoch 18, Batch 0, Loss: 0.3445]
[2025-05-20 22:30:24,958: INFO: model_training: Rank 0, Epoch 18, Batch 1, Loss: 0.3946]
[2025-05-20 22:30:24,959: INFO: model_training: Rank 0, Epoch 18, Batch 2, Loss: 0.3452]
[2025-05-20 22:30:24,960: INFO: model_training: Rank 0, Epoch 18, Batch 3, Loss: 0.3206]
[2025-05-20 22:30:24,961: INFO: model_training: Rank 0, Epoch 18, Batch 4, Loss: 0.3252]
[2025-05-20 22:30:24,962: INFO: model_training: Rank 0, Epoch 19, Batch 0, Loss: 0.2814]
[2025-05-20 22:30:24,963: INFO: model_training: Rank 0, Epoch 19, Batch 1, Loss: 0.3264]
[2025-05-20 22:30:24,965: INFO: model_training: Rank 0, Epoch 19, Batch 2, Loss: 0.2993]
[2025-05-20 22:30:24,966: INFO: model_training: Rank 0, Epoch 19, Batch 3, Loss: 0.3176]
[2025-05-20 22:30:24,967: INFO: model_training: Rank 0, Epoch 19, Batch 4, Loss: 0.3388]
[2025-05-20 22:30:24,968: INFO: model_training: Rank 0, Epoch 20, Batch 0, Loss: 0.3310]
[2025-05-20 22:30:24,970: INFO: model_training: Rank 0, Epoch 20, Batch 1, Loss: 0.3213]
[2025-05-20 22:30:24,971: INFO: model_training: Rank 0, Epoch 20, Batch 2, Loss: 0.2356]
[2025-05-20 22:30:24,972: INFO: model_training: Rank 0, Epoch 20, Batch 3, Loss: 0.2692]
[2025-05-20 22:30:24,974: INFO: model_training: Rank 0, Epoch 20, Batch 4, Loss: 0.2957]
[2025-05-20 22:30:24,975: INFO: model_training: Rank 0, Epoch 21, Batch 0, Loss: 0.2565]
[2025-05-20 22:30:24,976: INFO: model_training: Rank 0, Epoch 21, Batch 1, Loss: 0.3091]
[2025-05-20 22:30:24,977: INFO: model_training: Rank 0, Epoch 21, Batch 2, Loss: 0.2319]
[2025-05-20 22:30:24,978: INFO: model_training: Rank 0, Epoch 21, Batch 3, Loss: 0.2971]
[2025-05-20 22:30:24,980: INFO: model_training: Rank 0, Epoch 21, Batch 4, Loss: 0.2285]
[2025-05-20 22:30:24,981: INFO: model_training: Rank 0, Epoch 22, Batch 0, Loss: 0.2356]
[2025-05-20 22:30:24,982: INFO: model_training: Rank 0, Epoch 22, Batch 1, Loss: 0.2397]
[2025-05-20 22:30:24,984: INFO: model_training: Rank 0, Epoch 22, Batch 2, Loss: 0.2737]
[2025-05-20 22:30:24,985: INFO: model_training: Rank 0, Epoch 22, Batch 3, Loss: 0.2393]
[2025-05-20 22:30:24,986: INFO: model_training: Rank 0, Epoch 22, Batch 4, Loss: 0.2473]
[2025-05-20 22:30:24,987: INFO: model_training: Rank 0, Epoch 23, Batch 0, Loss: 0.1752]
[2025-05-20 22:30:24,988: INFO: model_training: Rank 0, Epoch 23, Batch 1, Loss: 0.1808]
[2025-05-20 22:30:24,990: INFO: model_training: Rank 0, Epoch 23, Batch 2, Loss: 0.1840]
[2025-05-20 22:30:24,992: INFO: model_training: Rank 0, Epoch 23, Batch 3, Loss: 0.1584]
[2025-05-20 22:30:24,993: INFO: model_training: Rank 0, Epoch 23, Batch 4, Loss: 0.1914]
[2025-05-20 22:30:24,995: INFO: model_training: Rank 0, Epoch 24, Batch 0, Loss: 0.2224]
[2025-05-20 22:30:24,996: INFO: model_training: Rank 0, Epoch 24, Batch 1, Loss: 0.2290]
[2025-05-20 22:30:24,997: INFO: model_training: Rank 0, Epoch 24, Batch 2, Loss: 0.2131]
[2025-05-20 22:30:24,999: INFO: model_training: Rank 0, Epoch 24, Batch 3, Loss: 0.2014]
[2025-05-20 22:30:25,000: INFO: model_training: Rank 0, Epoch 24, Batch 4, Loss: 0.1882]
[2025-05-20 22:30:25,001: INFO: model_training: Rank 0, Epoch 25, Batch 0, Loss: 0.2198]
[2025-05-20 22:30:25,002: INFO: model_training: Rank 0, Epoch 25, Batch 1, Loss: 0.1509]
[2025-05-20 22:30:25,003: INFO: model_training: Rank 0, Epoch 25, Batch 2, Loss: 0.1758]
[2025-05-20 22:30:25,004: INFO: model_training: Rank 0, Epoch 25, Batch 3, Loss: 0.1870]
[2025-05-20 22:30:25,006: INFO: model_training: Rank 0, Epoch 25, Batch 4, Loss: 0.1210]
[2025-05-20 22:30:25,007: INFO: model_training: Rank 0, Epoch 26, Batch 0, Loss: 0.1995]
[2025-05-20 22:30:25,008: INFO: model_training: Rank 0, Epoch 26, Batch 1, Loss: 0.1566]
[2025-05-20 22:30:25,009: INFO: model_training: Rank 0, Epoch 26, Batch 2, Loss: 0.2008]
[2025-05-20 22:30:25,010: INFO: model_training: Rank 0, Epoch 26, Batch 3, Loss: 0.1121]
[2025-05-20 22:30:25,011: INFO: model_training: Rank 0, Epoch 26, Batch 4, Loss: 0.1652]
[2025-05-20 22:30:25,012: INFO: model_training: Rank 0, Epoch 27, Batch 0, Loss: 0.1495]
[2025-05-20 22:30:25,014: INFO: model_training: Rank 0, Epoch 27, Batch 1, Loss: 0.1914]
[2025-05-20 22:30:25,015: INFO: model_training: Rank 0, Epoch 27, Batch 2, Loss: 0.1098]
[2025-05-20 22:30:25,016: INFO: model_training: Rank 0, Epoch 27, Batch 3, Loss: 0.1135]
[2025-05-20 22:30:25,017: INFO: model_training: Rank 0, Epoch 27, Batch 4, Loss: 0.1438]
[2025-05-20 22:30:25,019: INFO: model_training: Rank 0, Epoch 28, Batch 0, Loss: 0.0861]
[2025-05-20 22:30:25,020: INFO: model_training: Rank 0, Epoch 28, Batch 1, Loss: 0.1183]
[2025-05-20 22:30:25,021: INFO: model_training: Rank 0, Epoch 28, Batch 2, Loss: 0.1097]
[2025-05-20 22:30:25,023: INFO: model_training: Rank 0, Epoch 28, Batch 3, Loss: 0.1235]
[2025-05-20 22:30:25,024: INFO: model_training: Rank 0, Epoch 28, Batch 4, Loss: 0.0734]
[2025-05-20 22:30:25,025: INFO: model_training: Rank 0, Epoch 29, Batch 0, Loss: 0.1621]
[2025-05-20 22:30:25,027: INFO: model_training: Rank 0, Epoch 29, Batch 1, Loss: 0.1647]
[2025-05-20 22:30:25,028: INFO: model_training: Rank 0, Epoch 29, Batch 2, Loss: 0.0846]
[2025-05-20 22:30:25,029: INFO: model_training: Rank 0, Epoch 29, Batch 3, Loss: 0.1085]
[2025-05-20 22:30:25,030: INFO: model_training: Rank 0, Epoch 29, Batch 4, Loss: 0.0864]
[2025-05-20 22:30:25,031: INFO: model_training: Rank 0, Epoch 30, Batch 0, Loss: 0.1156]
[2025-05-20 22:30:25,033: INFO: model_training: Rank 0, Epoch 30, Batch 1, Loss: 0.0991]
[2025-05-20 22:30:25,034: INFO: model_training: Rank 0, Epoch 30, Batch 2, Loss: 0.0724]
[2025-05-20 22:30:25,035: INFO: model_training: Rank 0, Epoch 30, Batch 3, Loss: 0.1019]
[2025-05-20 22:30:25,036: INFO: model_training: Rank 0, Epoch 30, Batch 4, Loss: 0.0570]
[2025-05-20 22:30:25,037: INFO: model_training: Rank 0, Epoch 31, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,039: INFO: model_training: Rank 0, Epoch 31, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,040: INFO: model_training: Rank 0, Epoch 31, Batch 2, Loss: 0.0829]
[2025-05-20 22:30:25,041: INFO: model_training: Rank 0, Epoch 31, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,042: INFO: model_training: Rank 0, Epoch 31, Batch 4, Loss: 0.1092]
[2025-05-20 22:30:25,044: INFO: model_training: Rank 0, Epoch 32, Batch 0, Loss: 0.0622]
[2025-05-20 22:30:25,045: INFO: model_training: Rank 0, Epoch 32, Batch 1, Loss: 0.1286]
[2025-05-20 22:30:25,046: INFO: model_training: Rank 0, Epoch 32, Batch 2, Loss: 0.0905]
[2025-05-20 22:30:25,047: INFO: model_training: Rank 0, Epoch 32, Batch 3, Loss: 0.1250]
[2025-05-20 22:30:25,049: INFO: model_training: Rank 0, Epoch 32, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,050: INFO: model_training: Rank 0, Epoch 33, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,051: INFO: model_training: Rank 0, Epoch 33, Batch 1, Loss: 0.1121]
[2025-05-20 22:30:25,052: INFO: model_training: Rank 0, Epoch 33, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,054: INFO: model_training: Rank 0, Epoch 33, Batch 3, Loss: 0.0758]
[2025-05-20 22:30:25,055: INFO: model_training: Rank 0, Epoch 33, Batch 4, Loss: 0.0935]
[2025-05-20 22:30:25,056: INFO: model_training: Rank 0, Epoch 34, Batch 0, Loss: 0.1044]
[2025-05-20 22:30:25,058: INFO: model_training: Rank 0, Epoch 34, Batch 1, Loss: 0.1130]
[2025-05-20 22:30:25,059: INFO: model_training: Rank 0, Epoch 34, Batch 2, Loss: 0.0585]
[2025-05-20 22:30:25,060: INFO: model_training: Rank 0, Epoch 34, Batch 3, Loss: 0.0649]
[2025-05-20 22:30:25,061: INFO: model_training: Rank 0, Epoch 34, Batch 4, Loss: 0.0966]
[2025-05-20 22:30:25,063: INFO: model_training: Rank 0, Epoch 35, Batch 0, Loss: 0.1001]
[2025-05-20 22:30:25,065: INFO: model_training: Rank 0, Epoch 35, Batch 1, Loss: 0.1116]
[2025-05-20 22:30:25,066: INFO: model_training: Rank 0, Epoch 35, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,067: INFO: model_training: Rank 0, Epoch 35, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,068: INFO: model_training: Rank 0, Epoch 35, Batch 4, Loss: 0.0874]
[2025-05-20 22:30:25,070: INFO: model_training: Rank 0, Epoch 36, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,071: INFO: model_training: Rank 0, Epoch 36, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,072: INFO: model_training: Rank 0, Epoch 36, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,073: INFO: model_training: Rank 0, Epoch 36, Batch 3, Loss: 0.0638]
[2025-05-20 22:30:25,074: INFO: model_training: Rank 0, Epoch 36, Batch 4, Loss: 0.0657]
[2025-05-20 22:30:25,076: INFO: model_training: Rank 0, Epoch 37, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,077: INFO: model_training: Rank 0, Epoch 37, Batch 1, Loss: 0.0894]
[2025-05-20 22:30:25,078: INFO: model_training: Rank 0, Epoch 37, Batch 2, Loss: 0.0502]
[2025-05-20 22:30:25,079: INFO: model_training: Rank 0, Epoch 37, Batch 3, Loss: 0.0545]
[2025-05-20 22:30:25,080: INFO: model_training: Rank 0, Epoch 37, Batch 4, Loss: 0.0535]
[2025-05-20 22:30:25,081: INFO: model_training: Rank 0, Epoch 38, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,083: INFO: model_training: Rank 0, Epoch 38, Batch 1, Loss: 0.0875]
[2025-05-20 22:30:25,084: INFO: model_training: Rank 0, Epoch 38, Batch 2, Loss: 0.0733]
[2025-05-20 22:30:25,085: INFO: model_training: Rank 0, Epoch 38, Batch 3, Loss: 0.0613]
[2025-05-20 22:30:25,086: INFO: model_training: Rank 0, Epoch 38, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,087: INFO: model_training: Rank 0, Epoch 39, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,088: INFO: model_training: Rank 0, Epoch 39, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,090: INFO: model_training: Rank 0, Epoch 39, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,091: INFO: model_training: Rank 0, Epoch 39, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,093: INFO: model_training: Rank 0, Epoch 39, Batch 4, Loss: 0.0747]
[2025-05-20 22:30:25,094: INFO: model_training: Rank 0, Epoch 40, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,095: INFO: model_training: Rank 0, Epoch 40, Batch 1, Loss: 0.0652]
[2025-05-20 22:30:25,096: INFO: model_training: Rank 0, Epoch 40, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,098: INFO: model_training: Rank 0, Epoch 40, Batch 3, Loss: 0.0567]
[2025-05-20 22:30:25,099: INFO: model_training: Rank 0, Epoch 40, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,100: INFO: model_training: Rank 0, Epoch 41, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,101: INFO: model_training: Rank 0, Epoch 41, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,102: INFO: model_training: Rank 0, Epoch 41, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,104: INFO: model_training: Rank 0, Epoch 41, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,105: INFO: model_training: Rank 0, Epoch 41, Batch 4, Loss: 0.0708]
[2025-05-20 22:30:25,107: INFO: model_training: Rank 0, Epoch 42, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,108: INFO: model_training: Rank 0, Epoch 42, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,109: INFO: model_training: Rank 0, Epoch 42, Batch 2, Loss: 0.0770]
[2025-05-20 22:30:25,110: INFO: model_training: Rank 0, Epoch 42, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,111: INFO: model_training: Rank 0, Epoch 42, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,112: INFO: model_training: Rank 0, Epoch 43, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,113: INFO: model_training: Rank 0, Epoch 43, Batch 1, Loss: 0.0699]
[2025-05-20 22:30:25,115: INFO: model_training: Rank 0, Epoch 43, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,116: INFO: model_training: Rank 0, Epoch 43, Batch 3, Loss: 0.0633]
[2025-05-20 22:30:25,117: INFO: model_training: Rank 0, Epoch 43, Batch 4, Loss: 0.0572]
[2025-05-20 22:30:25,119: INFO: model_training: Rank 0, Epoch 44, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,120: INFO: model_training: Rank 0, Epoch 44, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,121: INFO: model_training: Rank 0, Epoch 44, Batch 2, Loss: 0.0520]
[2025-05-20 22:30:25,122: INFO: model_training: Rank 0, Epoch 44, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,124: INFO: model_training: Rank 0, Epoch 44, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,125: INFO: model_training: Rank 0, Epoch 45, Batch 0, Loss: 0.0687]
[2025-05-20 22:30:25,126: INFO: model_training: Rank 0, Epoch 45, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,127: INFO: model_training: Rank 0, Epoch 45, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,128: INFO: model_training: Rank 0, Epoch 45, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,129: INFO: model_training: Rank 0, Epoch 45, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,131: INFO: model_training: Rank 0, Epoch 46, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,132: INFO: model_training: Rank 0, Epoch 46, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,133: INFO: model_training: Rank 0, Epoch 46, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,134: INFO: model_training: Rank 0, Epoch 46, Batch 3, Loss: 0.0545]
[2025-05-20 22:30:25,135: INFO: model_training: Rank 0, Epoch 46, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,136: INFO: model_training: Rank 0, Epoch 47, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,137: INFO: model_training: Rank 0, Epoch 47, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,139: INFO: model_training: Rank 0, Epoch 47, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,140: INFO: model_training: Rank 0, Epoch 47, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,141: INFO: model_training: Rank 0, Epoch 47, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,142: INFO: model_training: Rank 0, Epoch 48, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,144: INFO: model_training: Rank 0, Epoch 48, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,145: INFO: model_training: Rank 0, Epoch 48, Batch 2, Loss: 0.0506]
[2025-05-20 22:30:25,146: INFO: model_training: Rank 0, Epoch 48, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,148: INFO: model_training: Rank 0, Epoch 48, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,149: INFO: model_training: Rank 0, Epoch 49, Batch 0, Loss: 0.0525]
[2025-05-20 22:30:25,150: INFO: model_training: Rank 0, Epoch 49, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,151: INFO: model_training: Rank 0, Epoch 49, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,152: INFO: model_training: Rank 0, Epoch 49, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,154: INFO: model_training: Rank 0, Epoch 49, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,155: INFO: model_training: Rank 0, Epoch 50, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,157: INFO: model_training: Rank 0, Epoch 50, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,158: INFO: model_training: Rank 0, Epoch 50, Batch 2, Loss: 0.0621]
[2025-05-20 22:30:25,159: INFO: model_training: Rank 0, Epoch 50, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,160: INFO: model_training: Rank 0, Epoch 50, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,161: INFO: model_training: Rank 0, Epoch 51, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,162: INFO: model_training: Rank 0, Epoch 51, Batch 1, Loss: 0.0607]
[2025-05-20 22:30:25,163: INFO: model_training: Rank 0, Epoch 51, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,165: INFO: model_training: Rank 0, Epoch 51, Batch 3, Loss: 0.0537]
[2025-05-20 22:30:25,166: INFO: model_training: Rank 0, Epoch 51, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,167: INFO: model_training: Rank 0, Epoch 52, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,168: INFO: model_training: Rank 0, Epoch 52, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,169: INFO: model_training: Rank 0, Epoch 52, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,170: INFO: model_training: Rank 0, Epoch 52, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,172: INFO: model_training: Rank 0, Epoch 52, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,174: INFO: model_training: Rank 0, Epoch 53, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,175: INFO: model_training: Rank 0, Epoch 53, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,176: INFO: model_training: Rank 0, Epoch 53, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,176: INFO: model_training: Rank 0, Epoch 53, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,178: INFO: model_training: Rank 0, Epoch 53, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,179: INFO: model_training: Rank 0, Epoch 54, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,181: INFO: model_training: Rank 0, Epoch 54, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,182: INFO: model_training: Rank 0, Epoch 54, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,183: INFO: model_training: Rank 0, Epoch 54, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,184: INFO: model_training: Rank 0, Epoch 54, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,185: INFO: model_training: Rank 0, Epoch 55, Batch 0, Loss: 0.0553]
[2025-05-20 22:30:25,186: INFO: model_training: Rank 0, Epoch 55, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,187: INFO: model_training: Rank 0, Epoch 55, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,188: INFO: model_training: Rank 0, Epoch 55, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,190: INFO: model_training: Rank 0, Epoch 55, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,192: INFO: model_training: Rank 0, Epoch 56, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,193: INFO: model_training: Rank 0, Epoch 56, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,195: INFO: model_training: Rank 0, Epoch 56, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,197: INFO: model_training: Rank 0, Epoch 56, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,198: INFO: model_training: Rank 0, Epoch 56, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,199: INFO: model_training: Rank 0, Epoch 57, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,200: INFO: model_training: Rank 0, Epoch 57, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,201: INFO: model_training: Rank 0, Epoch 57, Batch 2, Loss: 0.0557]
[2025-05-20 22:30:25,203: INFO: model_training: Rank 0, Epoch 57, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,204: INFO: model_training: Rank 0, Epoch 57, Batch 4, Loss: 0.0524]
[2025-05-20 22:30:25,206: INFO: model_training: Rank 0, Epoch 58, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,207: INFO: model_training: Rank 0, Epoch 58, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,208: INFO: model_training: Rank 0, Epoch 58, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,209: INFO: model_training: Rank 0, Epoch 58, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,210: INFO: model_training: Rank 0, Epoch 58, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,211: INFO: model_training: Rank 0, Epoch 59, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,212: INFO: model_training: Rank 0, Epoch 59, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,214: INFO: model_training: Rank 0, Epoch 59, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,215: INFO: model_training: Rank 0, Epoch 59, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,216: INFO: model_training: Rank 0, Epoch 59, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,217: INFO: model_training: Rank 0, Epoch 60, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,218: INFO: model_training: Rank 0, Epoch 60, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,219: INFO: model_training: Rank 0, Epoch 60, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,221: INFO: model_training: Rank 0, Epoch 60, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,223: INFO: model_training: Rank 0, Epoch 60, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,224: INFO: model_training: Rank 0, Epoch 61, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,225: INFO: model_training: Rank 0, Epoch 61, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,226: INFO: model_training: Rank 0, Epoch 61, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,227: INFO: model_training: Rank 0, Epoch 61, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,228: INFO: model_training: Rank 0, Epoch 61, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,230: INFO: model_training: Rank 0, Epoch 62, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,231: INFO: model_training: Rank 0, Epoch 62, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,233: INFO: model_training: Rank 0, Epoch 62, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,234: INFO: model_training: Rank 0, Epoch 62, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,235: INFO: model_training: Rank 0, Epoch 62, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,237: INFO: model_training: Rank 0, Epoch 63, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,238: INFO: model_training: Rank 0, Epoch 63, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,239: INFO: model_training: Rank 0, Epoch 63, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,241: INFO: model_training: Rank 0, Epoch 63, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,242: INFO: model_training: Rank 0, Epoch 63, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,243: INFO: model_training: Rank 0, Epoch 64, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,244: INFO: model_training: Rank 0, Epoch 64, Batch 1, Loss: 0.0511]
[2025-05-20 22:30:25,245: INFO: model_training: Rank 0, Epoch 64, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,247: INFO: model_training: Rank 0, Epoch 64, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,248: INFO: model_training: Rank 0, Epoch 64, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,250: INFO: model_training: Rank 0, Epoch 65, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,251: INFO: model_training: Rank 0, Epoch 65, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,252: INFO: model_training: Rank 0, Epoch 65, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,254: INFO: model_training: Rank 0, Epoch 65, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,255: INFO: model_training: Rank 0, Epoch 65, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,257: INFO: model_training: Rank 0, Epoch 66, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,258: INFO: model_training: Rank 0, Epoch 66, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,259: INFO: model_training: Rank 0, Epoch 66, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,260: INFO: model_training: Rank 0, Epoch 66, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,261: INFO: model_training: Rank 0, Epoch 66, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,263: INFO: model_training: Rank 0, Epoch 67, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,265: INFO: model_training: Rank 0, Epoch 67, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,266: INFO: model_training: Rank 0, Epoch 67, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,267: INFO: model_training: Rank 0, Epoch 67, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,268: INFO: model_training: Rank 0, Epoch 67, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,269: INFO: model_training: Rank 0, Epoch 68, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,270: INFO: model_training: Rank 0, Epoch 68, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,272: INFO: model_training: Rank 0, Epoch 68, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,273: INFO: model_training: Rank 0, Epoch 68, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,274: INFO: model_training: Rank 0, Epoch 68, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,275: INFO: model_training: Rank 0, Epoch 69, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,276: INFO: model_training: Rank 0, Epoch 69, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,277: INFO: model_training: Rank 0, Epoch 69, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,279: INFO: model_training: Rank 0, Epoch 69, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,280: INFO: model_training: Rank 0, Epoch 69, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,282: INFO: model_training: Rank 0, Epoch 70, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,283: INFO: model_training: Rank 0, Epoch 70, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,284: INFO: model_training: Rank 0, Epoch 70, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,285: INFO: model_training: Rank 0, Epoch 70, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,286: INFO: model_training: Rank 0, Epoch 70, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,288: INFO: model_training: Rank 0, Epoch 71, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,289: INFO: model_training: Rank 0, Epoch 71, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,291: INFO: model_training: Rank 0, Epoch 71, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,292: INFO: model_training: Rank 0, Epoch 71, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,292: INFO: model_training: Rank 0, Epoch 71, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,293: INFO: model_training: Rank 0, Epoch 72, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,295: INFO: model_training: Rank 0, Epoch 72, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,296: INFO: model_training: Rank 0, Epoch 72, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,298: INFO: model_training: Rank 0, Epoch 72, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,299: INFO: model_training: Rank 0, Epoch 72, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,300: INFO: model_training: Rank 0, Epoch 73, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,301: INFO: model_training: Rank 0, Epoch 73, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,303: INFO: model_training: Rank 0, Epoch 73, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,304: INFO: model_training: Rank 0, Epoch 73, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,306: INFO: model_training: Rank 0, Epoch 73, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,307: INFO: model_training: Rank 0, Epoch 74, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,308: INFO: model_training: Rank 0, Epoch 74, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,309: INFO: model_training: Rank 0, Epoch 74, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,310: INFO: model_training: Rank 0, Epoch 74, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,312: INFO: model_training: Rank 0, Epoch 74, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,313: INFO: model_training: Rank 0, Epoch 75, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,315: INFO: model_training: Rank 0, Epoch 75, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,317: INFO: model_training: Rank 0, Epoch 75, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,318: INFO: model_training: Rank 0, Epoch 75, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,320: INFO: model_training: Rank 0, Epoch 75, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,321: INFO: model_training: Rank 0, Epoch 76, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,323: INFO: model_training: Rank 0, Epoch 76, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,324: INFO: model_training: Rank 0, Epoch 76, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,326: INFO: model_training: Rank 0, Epoch 76, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,327: INFO: model_training: Rank 0, Epoch 76, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,329: INFO: model_training: Rank 0, Epoch 77, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,330: INFO: model_training: Rank 0, Epoch 77, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,332: INFO: model_training: Rank 0, Epoch 77, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,334: INFO: model_training: Rank 0, Epoch 77, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,336: INFO: model_training: Rank 0, Epoch 77, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,338: INFO: model_training: Rank 0, Epoch 78, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,341: INFO: model_training: Rank 0, Epoch 78, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,343: INFO: model_training: Rank 0, Epoch 78, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,345: INFO: model_training: Rank 0, Epoch 78, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,346: INFO: model_training: Rank 0, Epoch 78, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,348: INFO: model_training: Rank 0, Epoch 79, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,350: INFO: model_training: Rank 0, Epoch 79, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,354: INFO: model_training: Rank 0, Epoch 79, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,358: INFO: model_training: Rank 0, Epoch 79, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,360: INFO: model_training: Rank 0, Epoch 79, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,363: INFO: model_training: Rank 0, Epoch 80, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,366: INFO: model_training: Rank 0, Epoch 80, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,368: INFO: model_training: Rank 0, Epoch 80, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,370: INFO: model_training: Rank 0, Epoch 80, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,371: INFO: model_training: Rank 0, Epoch 80, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,374: INFO: model_training: Rank 0, Epoch 81, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,375: INFO: model_training: Rank 0, Epoch 81, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,377: INFO: model_training: Rank 0, Epoch 81, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,378: INFO: model_training: Rank 0, Epoch 81, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,379: INFO: model_training: Rank 0, Epoch 81, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,381: INFO: model_training: Rank 0, Epoch 82, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,383: INFO: model_training: Rank 0, Epoch 82, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,385: INFO: model_training: Rank 0, Epoch 82, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,387: INFO: model_training: Rank 0, Epoch 82, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,388: INFO: model_training: Rank 0, Epoch 82, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,390: INFO: model_training: Rank 0, Epoch 83, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,391: INFO: model_training: Rank 0, Epoch 83, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,395: INFO: model_training: Rank 0, Epoch 83, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,398: INFO: model_training: Rank 0, Epoch 83, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,402: INFO: model_training: Rank 0, Epoch 83, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,405: INFO: model_training: Rank 0, Epoch 84, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,408: INFO: model_training: Rank 0, Epoch 84, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,411: INFO: model_training: Rank 0, Epoch 84, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,415: INFO: model_training: Rank 0, Epoch 84, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,419: INFO: model_training: Rank 0, Epoch 84, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,421: INFO: model_training: Rank 0, Epoch 85, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,423: INFO: model_training: Rank 0, Epoch 85, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,425: INFO: model_training: Rank 0, Epoch 85, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,426: INFO: model_training: Rank 0, Epoch 85, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,429: INFO: model_training: Rank 0, Epoch 85, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,430: INFO: model_training: Rank 0, Epoch 86, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,432: INFO: model_training: Rank 0, Epoch 86, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,434: INFO: model_training: Rank 0, Epoch 86, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,436: INFO: model_training: Rank 0, Epoch 86, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,438: INFO: model_training: Rank 0, Epoch 86, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,440: INFO: model_training: Rank 0, Epoch 87, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,443: INFO: model_training: Rank 0, Epoch 87, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,446: INFO: model_training: Rank 0, Epoch 87, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,448: INFO: model_training: Rank 0, Epoch 87, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,451: INFO: model_training: Rank 0, Epoch 87, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,454: INFO: model_training: Rank 0, Epoch 88, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,456: INFO: model_training: Rank 0, Epoch 88, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,459: INFO: model_training: Rank 0, Epoch 88, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,460: INFO: model_training: Rank 0, Epoch 88, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,462: INFO: model_training: Rank 0, Epoch 88, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,463: INFO: model_training: Rank 0, Epoch 89, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,465: INFO: model_training: Rank 0, Epoch 89, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,467: INFO: model_training: Rank 0, Epoch 89, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,470: INFO: model_training: Rank 0, Epoch 89, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,471: INFO: model_training: Rank 0, Epoch 89, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,473: INFO: model_training: Rank 0, Epoch 90, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,475: INFO: model_training: Rank 0, Epoch 90, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,476: INFO: model_training: Rank 0, Epoch 90, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,477: INFO: model_training: Rank 0, Epoch 90, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,478: INFO: model_training: Rank 0, Epoch 90, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,479: INFO: model_training: Rank 0, Epoch 91, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,481: INFO: model_training: Rank 0, Epoch 91, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,482: INFO: model_training: Rank 0, Epoch 91, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,483: INFO: model_training: Rank 0, Epoch 91, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,484: INFO: model_training: Rank 0, Epoch 91, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,485: INFO: model_training: Rank 0, Epoch 92, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,487: INFO: model_training: Rank 0, Epoch 92, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,488: INFO: model_training: Rank 0, Epoch 92, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,490: INFO: model_training: Rank 0, Epoch 92, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,491: INFO: model_training: Rank 0, Epoch 92, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,492: INFO: model_training: Rank 0, Epoch 93, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,493: INFO: model_training: Rank 0, Epoch 93, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,494: INFO: model_training: Rank 0, Epoch 93, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,495: INFO: model_training: Rank 0, Epoch 93, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,497: INFO: model_training: Rank 0, Epoch 93, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,499: INFO: model_training: Rank 0, Epoch 94, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,501: INFO: model_training: Rank 0, Epoch 94, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,509: INFO: model_training: Rank 0, Epoch 94, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,521: INFO: model_training: Rank 0, Epoch 94, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,526: INFO: model_training: Rank 0, Epoch 94, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,531: INFO: model_training: Rank 0, Epoch 95, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,532: INFO: model_training: Rank 0, Epoch 95, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,534: INFO: model_training: Rank 0, Epoch 95, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,536: INFO: model_training: Rank 0, Epoch 95, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,542: INFO: model_training: Rank 0, Epoch 95, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,545: INFO: model_training: Rank 0, Epoch 96, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,548: INFO: model_training: Rank 0, Epoch 96, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,549: INFO: model_training: Rank 0, Epoch 96, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,550: INFO: model_training: Rank 0, Epoch 96, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,551: INFO: model_training: Rank 0, Epoch 96, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,553: INFO: model_training: Rank 0, Epoch 97, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,555: INFO: model_training: Rank 0, Epoch 97, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,556: INFO: model_training: Rank 0, Epoch 97, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,558: INFO: model_training: Rank 0, Epoch 97, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,559: INFO: model_training: Rank 0, Epoch 97, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,562: INFO: model_training: Rank 0, Epoch 98, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,563: INFO: model_training: Rank 0, Epoch 98, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,565: INFO: model_training: Rank 0, Epoch 98, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,566: INFO: model_training: Rank 0, Epoch 98, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,568: INFO: model_training: Rank 0, Epoch 98, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,569: INFO: model_training: Rank 0, Epoch 99, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,571: INFO: model_training: Rank 0, Epoch 99, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,573: INFO: model_training: Rank 0, Epoch 99, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,574: INFO: model_training: Rank 0, Epoch 99, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,575: INFO: model_training: Rank 0, Epoch 99, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,577: INFO: model_training: Rank 0, Epoch 100, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,578: INFO: model_training: Rank 0, Epoch 100, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,580: INFO: model_training: Rank 0, Epoch 100, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,582: INFO: model_training: Rank 0, Epoch 100, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,583: INFO: model_training: Rank 0, Epoch 100, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,584: INFO: model_training: Rank 0, Epoch 101, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,586: INFO: model_training: Rank 0, Epoch 101, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,587: INFO: model_training: Rank 0, Epoch 101, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,588: INFO: model_training: Rank 0, Epoch 101, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,590: INFO: model_training: Rank 0, Epoch 101, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,591: INFO: model_training: Rank 0, Epoch 102, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,594: INFO: model_training: Rank 0, Epoch 102, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,595: INFO: model_training: Rank 0, Epoch 102, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,596: INFO: model_training: Rank 0, Epoch 102, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,598: INFO: model_training: Rank 0, Epoch 102, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,599: INFO: model_training: Rank 0, Epoch 103, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,600: INFO: model_training: Rank 0, Epoch 103, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,602: INFO: model_training: Rank 0, Epoch 103, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,603: INFO: model_training: Rank 0, Epoch 103, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,605: INFO: model_training: Rank 0, Epoch 103, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,606: INFO: model_training: Rank 0, Epoch 104, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,607: INFO: model_training: Rank 0, Epoch 104, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,609: INFO: model_training: Rank 0, Epoch 104, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,610: INFO: model_training: Rank 0, Epoch 104, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,612: INFO: model_training: Rank 0, Epoch 104, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,613: INFO: model_training: Rank 0, Epoch 105, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,615: INFO: model_training: Rank 0, Epoch 105, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,616: INFO: model_training: Rank 0, Epoch 105, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,617: INFO: model_training: Rank 0, Epoch 105, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,618: INFO: model_training: Rank 0, Epoch 105, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,620: INFO: model_training: Rank 0, Epoch 106, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,621: INFO: model_training: Rank 0, Epoch 106, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,623: INFO: model_training: Rank 0, Epoch 106, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,625: INFO: model_training: Rank 0, Epoch 106, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,626: INFO: model_training: Rank 0, Epoch 106, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,627: INFO: model_training: Rank 0, Epoch 107, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,629: INFO: model_training: Rank 0, Epoch 107, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,630: INFO: model_training: Rank 0, Epoch 107, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,632: INFO: model_training: Rank 0, Epoch 107, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,633: INFO: model_training: Rank 0, Epoch 107, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,634: INFO: model_training: Rank 0, Epoch 108, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,636: INFO: model_training: Rank 0, Epoch 108, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,637: INFO: model_training: Rank 0, Epoch 108, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,638: INFO: model_training: Rank 0, Epoch 108, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,640: INFO: model_training: Rank 0, Epoch 108, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,641: INFO: model_training: Rank 0, Epoch 109, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,642: INFO: model_training: Rank 0, Epoch 109, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,644: INFO: model_training: Rank 0, Epoch 109, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,645: INFO: model_training: Rank 0, Epoch 109, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,647: INFO: model_training: Rank 0, Epoch 109, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,648: INFO: model_training: Rank 0, Epoch 110, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,650: INFO: model_training: Rank 0, Epoch 110, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,651: INFO: model_training: Rank 0, Epoch 110, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,653: INFO: model_training: Rank 0, Epoch 110, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,654: INFO: model_training: Rank 0, Epoch 110, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,655: INFO: model_training: Rank 0, Epoch 111, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,657: INFO: model_training: Rank 0, Epoch 111, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,658: INFO: model_training: Rank 0, Epoch 111, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,659: INFO: model_training: Rank 0, Epoch 111, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,660: INFO: model_training: Rank 0, Epoch 111, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,661: INFO: model_training: Rank 0, Epoch 112, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,663: INFO: model_training: Rank 0, Epoch 112, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,665: INFO: model_training: Rank 0, Epoch 112, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,666: INFO: model_training: Rank 0, Epoch 112, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,667: INFO: model_training: Rank 0, Epoch 112, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,668: INFO: model_training: Rank 0, Epoch 113, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,670: INFO: model_training: Rank 0, Epoch 113, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,671: INFO: model_training: Rank 0, Epoch 113, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,673: INFO: model_training: Rank 0, Epoch 113, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,674: INFO: model_training: Rank 0, Epoch 113, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,675: INFO: model_training: Rank 0, Epoch 114, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,676: INFO: model_training: Rank 0, Epoch 114, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,677: INFO: model_training: Rank 0, Epoch 114, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,679: INFO: model_training: Rank 0, Epoch 114, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,680: INFO: model_training: Rank 0, Epoch 114, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,682: INFO: model_training: Rank 0, Epoch 115, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,683: INFO: model_training: Rank 0, Epoch 115, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,684: INFO: model_training: Rank 0, Epoch 115, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,686: INFO: model_training: Rank 0, Epoch 115, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,687: INFO: model_training: Rank 0, Epoch 115, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,688: INFO: model_training: Rank 0, Epoch 116, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,690: INFO: model_training: Rank 0, Epoch 116, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,691: INFO: model_training: Rank 0, Epoch 116, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,693: INFO: model_training: Rank 0, Epoch 116, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,694: INFO: model_training: Rank 0, Epoch 116, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,695: INFO: model_training: Rank 0, Epoch 117, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,697: INFO: model_training: Rank 0, Epoch 117, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,699: INFO: model_training: Rank 0, Epoch 117, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,700: INFO: model_training: Rank 0, Epoch 117, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,701: INFO: model_training: Rank 0, Epoch 117, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,702: INFO: model_training: Rank 0, Epoch 118, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,704: INFO: model_training: Rank 0, Epoch 118, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,706: INFO: model_training: Rank 0, Epoch 118, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,707: INFO: model_training: Rank 0, Epoch 118, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,708: INFO: model_training: Rank 0, Epoch 118, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,709: INFO: model_training: Rank 0, Epoch 119, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,710: INFO: model_training: Rank 0, Epoch 119, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,712: INFO: model_training: Rank 0, Epoch 119, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,713: INFO: model_training: Rank 0, Epoch 119, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,715: INFO: model_training: Rank 0, Epoch 119, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,716: INFO: model_training: Rank 0, Epoch 120, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,717: INFO: model_training: Rank 0, Epoch 120, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,718: INFO: model_training: Rank 0, Epoch 120, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,720: INFO: model_training: Rank 0, Epoch 120, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,721: INFO: model_training: Rank 0, Epoch 120, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,722: INFO: model_training: Rank 0, Epoch 121, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,724: INFO: model_training: Rank 0, Epoch 121, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,725: INFO: model_training: Rank 0, Epoch 121, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,726: INFO: model_training: Rank 0, Epoch 121, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,727: INFO: model_training: Rank 0, Epoch 121, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,729: INFO: model_training: Rank 0, Epoch 122, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,730: INFO: model_training: Rank 0, Epoch 122, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,731: INFO: model_training: Rank 0, Epoch 122, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,733: INFO: model_training: Rank 0, Epoch 122, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,734: INFO: model_training: Rank 0, Epoch 122, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,735: INFO: model_training: Rank 0, Epoch 123, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,737: INFO: model_training: Rank 0, Epoch 123, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,738: INFO: model_training: Rank 0, Epoch 123, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,739: INFO: model_training: Rank 0, Epoch 123, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,740: INFO: model_training: Rank 0, Epoch 123, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,741: INFO: model_training: Rank 0, Epoch 124, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,742: INFO: model_training: Rank 0, Epoch 124, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,744: INFO: model_training: Rank 0, Epoch 124, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,745: INFO: model_training: Rank 0, Epoch 124, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,746: INFO: model_training: Rank 0, Epoch 124, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,747: INFO: model_training: Rank 0, Epoch 125, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,749: INFO: model_training: Rank 0, Epoch 125, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,749: INFO: model_training: Rank 0, Epoch 125, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,751: INFO: model_training: Rank 0, Epoch 125, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,752: INFO: model_training: Rank 0, Epoch 125, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,753: INFO: model_training: Rank 0, Epoch 126, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,756: INFO: model_training: Rank 0, Epoch 126, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,758: INFO: model_training: Rank 0, Epoch 126, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,760: INFO: model_training: Rank 0, Epoch 126, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,761: INFO: model_training: Rank 0, Epoch 126, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,762: INFO: model_training: Rank 0, Epoch 127, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,763: INFO: model_training: Rank 0, Epoch 127, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,765: INFO: model_training: Rank 0, Epoch 127, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,766: INFO: model_training: Rank 0, Epoch 127, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,767: INFO: model_training: Rank 0, Epoch 127, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,768: INFO: model_training: Rank 0, Epoch 128, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,770: INFO: model_training: Rank 0, Epoch 128, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,771: INFO: model_training: Rank 0, Epoch 128, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,772: INFO: model_training: Rank 0, Epoch 128, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,773: INFO: model_training: Rank 0, Epoch 128, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,774: INFO: model_training: Rank 0, Epoch 129, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,776: INFO: model_training: Rank 0, Epoch 129, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,778: INFO: model_training: Rank 0, Epoch 129, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,779: INFO: model_training: Rank 0, Epoch 129, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,780: INFO: model_training: Rank 0, Epoch 129, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,781: INFO: model_training: Rank 0, Epoch 130, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,782: INFO: model_training: Rank 0, Epoch 130, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,783: INFO: model_training: Rank 0, Epoch 130, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,785: INFO: model_training: Rank 0, Epoch 130, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,786: INFO: model_training: Rank 0, Epoch 130, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,787: INFO: model_training: Rank 0, Epoch 131, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,788: INFO: model_training: Rank 0, Epoch 131, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,789: INFO: model_training: Rank 0, Epoch 131, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,790: INFO: model_training: Rank 0, Epoch 131, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,792: INFO: model_training: Rank 0, Epoch 131, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,793: INFO: model_training: Rank 0, Epoch 132, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,794: INFO: model_training: Rank 0, Epoch 132, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,795: INFO: model_training: Rank 0, Epoch 132, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,796: INFO: model_training: Rank 0, Epoch 132, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,797: INFO: model_training: Rank 0, Epoch 132, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,798: INFO: model_training: Rank 0, Epoch 133, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,799: INFO: model_training: Rank 0, Epoch 133, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,800: INFO: model_training: Rank 0, Epoch 133, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,801: INFO: model_training: Rank 0, Epoch 133, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,802: INFO: model_training: Rank 0, Epoch 133, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,803: INFO: model_training: Rank 0, Epoch 134, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,805: INFO: model_training: Rank 0, Epoch 134, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,807: INFO: model_training: Rank 0, Epoch 134, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,809: INFO: model_training: Rank 0, Epoch 134, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,810: INFO: model_training: Rank 0, Epoch 134, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,811: INFO: model_training: Rank 0, Epoch 135, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,813: INFO: model_training: Rank 0, Epoch 135, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,814: INFO: model_training: Rank 0, Epoch 135, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,815: INFO: model_training: Rank 0, Epoch 135, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,816: INFO: model_training: Rank 0, Epoch 135, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,817: INFO: model_training: Rank 0, Epoch 136, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,818: INFO: model_training: Rank 0, Epoch 136, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,819: INFO: model_training: Rank 0, Epoch 136, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,820: INFO: model_training: Rank 0, Epoch 136, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,822: INFO: model_training: Rank 0, Epoch 136, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,823: INFO: model_training: Rank 0, Epoch 137, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,824: INFO: model_training: Rank 0, Epoch 137, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,825: INFO: model_training: Rank 0, Epoch 137, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,826: INFO: model_training: Rank 0, Epoch 137, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,827: INFO: model_training: Rank 0, Epoch 137, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,828: INFO: model_training: Rank 0, Epoch 138, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,830: INFO: model_training: Rank 0, Epoch 138, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,831: INFO: model_training: Rank 0, Epoch 138, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,833: INFO: model_training: Rank 0, Epoch 138, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,834: INFO: model_training: Rank 0, Epoch 138, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,835: INFO: model_training: Rank 0, Epoch 139, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,836: INFO: model_training: Rank 0, Epoch 139, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,838: INFO: model_training: Rank 0, Epoch 139, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,839: INFO: model_training: Rank 0, Epoch 139, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,840: INFO: model_training: Rank 0, Epoch 139, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,841: INFO: model_training: Rank 0, Epoch 140, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,843: INFO: model_training: Rank 0, Epoch 140, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,844: INFO: model_training: Rank 0, Epoch 140, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,845: INFO: model_training: Rank 0, Epoch 140, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,847: INFO: model_training: Rank 0, Epoch 140, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,848: INFO: model_training: Rank 0, Epoch 141, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,850: INFO: model_training: Rank 0, Epoch 141, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,851: INFO: model_training: Rank 0, Epoch 141, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,852: INFO: model_training: Rank 0, Epoch 141, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,853: INFO: model_training: Rank 0, Epoch 141, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,854: INFO: model_training: Rank 0, Epoch 142, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,856: INFO: model_training: Rank 0, Epoch 142, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,857: INFO: model_training: Rank 0, Epoch 142, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,858: INFO: model_training: Rank 0, Epoch 142, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,859: INFO: model_training: Rank 0, Epoch 142, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,860: INFO: model_training: Rank 0, Epoch 143, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,861: INFO: model_training: Rank 0, Epoch 143, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,862: INFO: model_training: Rank 0, Epoch 143, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,863: INFO: model_training: Rank 0, Epoch 143, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,864: INFO: model_training: Rank 0, Epoch 143, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,866: INFO: model_training: Rank 0, Epoch 144, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,867: INFO: model_training: Rank 0, Epoch 144, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,868: INFO: model_training: Rank 0, Epoch 144, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,869: INFO: model_training: Rank 0, Epoch 144, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,870: INFO: model_training: Rank 0, Epoch 144, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,871: INFO: model_training: Rank 0, Epoch 145, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,873: INFO: model_training: Rank 0, Epoch 145, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,874: INFO: model_training: Rank 0, Epoch 145, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,875: INFO: model_training: Rank 0, Epoch 145, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,876: INFO: model_training: Rank 0, Epoch 145, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,878: INFO: model_training: Rank 0, Epoch 146, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,879: INFO: model_training: Rank 0, Epoch 146, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,880: INFO: model_training: Rank 0, Epoch 146, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,882: INFO: model_training: Rank 0, Epoch 146, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,883: INFO: model_training: Rank 0, Epoch 146, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,884: INFO: model_training: Rank 0, Epoch 147, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,886: INFO: model_training: Rank 0, Epoch 147, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,887: INFO: model_training: Rank 0, Epoch 147, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,889: INFO: model_training: Rank 0, Epoch 147, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,890: INFO: model_training: Rank 0, Epoch 147, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,891: INFO: model_training: Rank 0, Epoch 148, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,893: INFO: model_training: Rank 0, Epoch 148, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,895: INFO: model_training: Rank 0, Epoch 148, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,897: INFO: model_training: Rank 0, Epoch 148, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,898: INFO: model_training: Rank 0, Epoch 148, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,899: INFO: model_training: Rank 0, Epoch 149, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,901: INFO: model_training: Rank 0, Epoch 149, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,902: INFO: model_training: Rank 0, Epoch 149, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,904: INFO: model_training: Rank 0, Epoch 149, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,905: INFO: model_training: Rank 0, Epoch 149, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,906: INFO: model_training: Rank 0, Epoch 150, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,908: INFO: model_training: Rank 0, Epoch 150, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,909: INFO: model_training: Rank 0, Epoch 150, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,910: INFO: model_training: Rank 0, Epoch 150, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,911: INFO: model_training: Rank 0, Epoch 150, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,913: INFO: model_training: Rank 0, Epoch 151, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,913: INFO: model_training: Rank 0, Epoch 151, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,915: INFO: model_training: Rank 0, Epoch 151, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,916: INFO: model_training: Rank 0, Epoch 151, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,917: INFO: model_training: Rank 0, Epoch 151, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,918: INFO: model_training: Rank 0, Epoch 152, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,919: INFO: model_training: Rank 0, Epoch 152, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,920: INFO: model_training: Rank 0, Epoch 152, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,921: INFO: model_training: Rank 0, Epoch 152, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,923: INFO: model_training: Rank 0, Epoch 152, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,925: INFO: model_training: Rank 0, Epoch 153, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,926: INFO: model_training: Rank 0, Epoch 153, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,927: INFO: model_training: Rank 0, Epoch 153, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,928: INFO: model_training: Rank 0, Epoch 153, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,929: INFO: model_training: Rank 0, Epoch 153, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,930: INFO: model_training: Rank 0, Epoch 154, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,932: INFO: model_training: Rank 0, Epoch 154, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,933: INFO: model_training: Rank 0, Epoch 154, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,934: INFO: model_training: Rank 0, Epoch 154, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,935: INFO: model_training: Rank 0, Epoch 154, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,936: INFO: model_training: Rank 0, Epoch 155, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,937: INFO: model_training: Rank 0, Epoch 155, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,938: INFO: model_training: Rank 0, Epoch 155, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,939: INFO: model_training: Rank 0, Epoch 155, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,940: INFO: model_training: Rank 0, Epoch 155, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,942: INFO: model_training: Rank 0, Epoch 156, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,943: INFO: model_training: Rank 0, Epoch 156, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,944: INFO: model_training: Rank 0, Epoch 156, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,945: INFO: model_training: Rank 0, Epoch 156, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,946: INFO: model_training: Rank 0, Epoch 156, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,947: INFO: model_training: Rank 0, Epoch 157, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,948: INFO: model_training: Rank 0, Epoch 157, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,949: INFO: model_training: Rank 0, Epoch 157, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,950: INFO: model_training: Rank 0, Epoch 157, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,951: INFO: model_training: Rank 0, Epoch 157, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,953: INFO: model_training: Rank 0, Epoch 158, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,954: INFO: model_training: Rank 0, Epoch 158, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,955: INFO: model_training: Rank 0, Epoch 158, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,956: INFO: model_training: Rank 0, Epoch 158, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,957: INFO: model_training: Rank 0, Epoch 158, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,958: INFO: model_training: Rank 0, Epoch 159, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,959: INFO: model_training: Rank 0, Epoch 159, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,960: INFO: model_training: Rank 0, Epoch 159, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,961: INFO: model_training: Rank 0, Epoch 159, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,962: INFO: model_training: Rank 0, Epoch 159, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,963: INFO: model_training: Rank 0, Epoch 160, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,964: INFO: model_training: Rank 0, Epoch 160, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,965: INFO: model_training: Rank 0, Epoch 160, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,966: INFO: model_training: Rank 0, Epoch 160, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,967: INFO: model_training: Rank 0, Epoch 160, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,969: INFO: model_training: Rank 0, Epoch 161, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,970: INFO: model_training: Rank 0, Epoch 161, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,971: INFO: model_training: Rank 0, Epoch 161, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,972: INFO: model_training: Rank 0, Epoch 161, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,973: INFO: model_training: Rank 0, Epoch 161, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,974: INFO: model_training: Rank 0, Epoch 162, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,976: INFO: model_training: Rank 0, Epoch 162, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,977: INFO: model_training: Rank 0, Epoch 162, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,978: INFO: model_training: Rank 0, Epoch 162, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,979: INFO: model_training: Rank 0, Epoch 162, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,980: INFO: model_training: Rank 0, Epoch 163, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,981: INFO: model_training: Rank 0, Epoch 163, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,982: INFO: model_training: Rank 0, Epoch 163, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,983: INFO: model_training: Rank 0, Epoch 163, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,984: INFO: model_training: Rank 0, Epoch 163, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,985: INFO: model_training: Rank 0, Epoch 164, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,986: INFO: model_training: Rank 0, Epoch 164, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,987: INFO: model_training: Rank 0, Epoch 164, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,989: INFO: model_training: Rank 0, Epoch 164, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,990: INFO: model_training: Rank 0, Epoch 164, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,991: INFO: model_training: Rank 0, Epoch 165, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,992: INFO: model_training: Rank 0, Epoch 165, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,993: INFO: model_training: Rank 0, Epoch 165, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:25,994: INFO: model_training: Rank 0, Epoch 165, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:25,995: INFO: model_training: Rank 0, Epoch 165, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:25,997: INFO: model_training: Rank 0, Epoch 166, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:25,998: INFO: model_training: Rank 0, Epoch 166, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:25,999: INFO: model_training: Rank 0, Epoch 166, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,000: INFO: model_training: Rank 0, Epoch 166, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,001: INFO: model_training: Rank 0, Epoch 166, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,002: INFO: model_training: Rank 0, Epoch 167, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,003: INFO: model_training: Rank 0, Epoch 167, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,004: INFO: model_training: Rank 0, Epoch 167, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,005: INFO: model_training: Rank 0, Epoch 167, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,006: INFO: model_training: Rank 0, Epoch 167, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,007: INFO: model_training: Rank 0, Epoch 168, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,009: INFO: model_training: Rank 0, Epoch 168, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,009: INFO: model_training: Rank 0, Epoch 168, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,010: INFO: model_training: Rank 0, Epoch 168, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,012: INFO: model_training: Rank 0, Epoch 168, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,013: INFO: model_training: Rank 0, Epoch 169, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,014: INFO: model_training: Rank 0, Epoch 169, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,015: INFO: model_training: Rank 0, Epoch 169, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,016: INFO: model_training: Rank 0, Epoch 169, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,017: INFO: model_training: Rank 0, Epoch 169, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,018: INFO: model_training: Rank 0, Epoch 170, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,019: INFO: model_training: Rank 0, Epoch 170, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,020: INFO: model_training: Rank 0, Epoch 170, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,021: INFO: model_training: Rank 0, Epoch 170, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,022: INFO: model_training: Rank 0, Epoch 170, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,023: INFO: model_training: Rank 0, Epoch 171, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,024: INFO: model_training: Rank 0, Epoch 171, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,025: INFO: model_training: Rank 0, Epoch 171, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,026: INFO: model_training: Rank 0, Epoch 171, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,027: INFO: model_training: Rank 0, Epoch 171, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,029: INFO: model_training: Rank 0, Epoch 172, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,030: INFO: model_training: Rank 0, Epoch 172, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,031: INFO: model_training: Rank 0, Epoch 172, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,032: INFO: model_training: Rank 0, Epoch 172, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,033: INFO: model_training: Rank 0, Epoch 172, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,034: INFO: model_training: Rank 0, Epoch 173, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,035: INFO: model_training: Rank 0, Epoch 173, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,036: INFO: model_training: Rank 0, Epoch 173, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,037: INFO: model_training: Rank 0, Epoch 173, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,038: INFO: model_training: Rank 0, Epoch 173, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,039: INFO: model_training: Rank 0, Epoch 174, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,041: INFO: model_training: Rank 0, Epoch 174, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,042: INFO: model_training: Rank 0, Epoch 174, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,043: INFO: model_training: Rank 0, Epoch 174, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,044: INFO: model_training: Rank 0, Epoch 174, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,045: INFO: model_training: Rank 0, Epoch 175, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,046: INFO: model_training: Rank 0, Epoch 175, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,047: INFO: model_training: Rank 0, Epoch 175, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,048: INFO: model_training: Rank 0, Epoch 175, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,049: INFO: model_training: Rank 0, Epoch 175, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,050: INFO: model_training: Rank 0, Epoch 176, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,052: INFO: model_training: Rank 0, Epoch 176, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,053: INFO: model_training: Rank 0, Epoch 176, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,054: INFO: model_training: Rank 0, Epoch 176, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,054: INFO: model_training: Rank 0, Epoch 176, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,056: INFO: model_training: Rank 0, Epoch 177, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,056: INFO: model_training: Rank 0, Epoch 177, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,057: INFO: model_training: Rank 0, Epoch 177, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,059: INFO: model_training: Rank 0, Epoch 177, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,060: INFO: model_training: Rank 0, Epoch 177, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,061: INFO: model_training: Rank 0, Epoch 178, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,062: INFO: model_training: Rank 0, Epoch 178, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,062: INFO: model_training: Rank 0, Epoch 178, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,063: INFO: model_training: Rank 0, Epoch 178, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,064: INFO: model_training: Rank 0, Epoch 178, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,065: INFO: model_training: Rank 0, Epoch 179, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,066: INFO: model_training: Rank 0, Epoch 179, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,067: INFO: model_training: Rank 0, Epoch 179, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,068: INFO: model_training: Rank 0, Epoch 179, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,069: INFO: model_training: Rank 0, Epoch 179, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,070: INFO: model_training: Rank 0, Epoch 180, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,071: INFO: model_training: Rank 0, Epoch 180, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,072: INFO: model_training: Rank 0, Epoch 180, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,073: INFO: model_training: Rank 0, Epoch 180, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,074: INFO: model_training: Rank 0, Epoch 180, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,075: INFO: model_training: Rank 0, Epoch 181, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,076: INFO: model_training: Rank 0, Epoch 181, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,078: INFO: model_training: Rank 0, Epoch 181, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,078: INFO: model_training: Rank 0, Epoch 181, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,079: INFO: model_training: Rank 0, Epoch 181, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,080: INFO: model_training: Rank 0, Epoch 182, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,081: INFO: model_training: Rank 0, Epoch 182, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,082: INFO: model_training: Rank 0, Epoch 182, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,083: INFO: model_training: Rank 0, Epoch 182, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,084: INFO: model_training: Rank 0, Epoch 182, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,085: INFO: model_training: Rank 0, Epoch 183, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,086: INFO: model_training: Rank 0, Epoch 183, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,087: INFO: model_training: Rank 0, Epoch 183, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,088: INFO: model_training: Rank 0, Epoch 183, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,089: INFO: model_training: Rank 0, Epoch 183, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,090: INFO: model_training: Rank 0, Epoch 184, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,091: INFO: model_training: Rank 0, Epoch 184, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,092: INFO: model_training: Rank 0, Epoch 184, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,093: INFO: model_training: Rank 0, Epoch 184, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,094: INFO: model_training: Rank 0, Epoch 184, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,095: INFO: model_training: Rank 0, Epoch 185, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,097: INFO: model_training: Rank 0, Epoch 185, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,098: INFO: model_training: Rank 0, Epoch 185, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,098: INFO: model_training: Rank 0, Epoch 185, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,099: INFO: model_training: Rank 0, Epoch 185, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,100: INFO: model_training: Rank 0, Epoch 186, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,102: INFO: model_training: Rank 0, Epoch 186, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,103: INFO: model_training: Rank 0, Epoch 186, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,104: INFO: model_training: Rank 0, Epoch 186, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,105: INFO: model_training: Rank 0, Epoch 186, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,106: INFO: model_training: Rank 0, Epoch 187, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,107: INFO: model_training: Rank 0, Epoch 187, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,108: INFO: model_training: Rank 0, Epoch 187, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,109: INFO: model_training: Rank 0, Epoch 187, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,110: INFO: model_training: Rank 0, Epoch 187, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,111: INFO: model_training: Rank 0, Epoch 188, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,112: INFO: model_training: Rank 0, Epoch 188, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,113: INFO: model_training: Rank 0, Epoch 188, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,114: INFO: model_training: Rank 0, Epoch 188, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,115: INFO: model_training: Rank 0, Epoch 188, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,116: INFO: model_training: Rank 0, Epoch 189, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,117: INFO: model_training: Rank 0, Epoch 189, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,118: INFO: model_training: Rank 0, Epoch 189, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,119: INFO: model_training: Rank 0, Epoch 189, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,120: INFO: model_training: Rank 0, Epoch 189, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,121: INFO: model_training: Rank 0, Epoch 190, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,122: INFO: model_training: Rank 0, Epoch 190, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,123: INFO: model_training: Rank 0, Epoch 190, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,124: INFO: model_training: Rank 0, Epoch 190, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,125: INFO: model_training: Rank 0, Epoch 190, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,126: INFO: model_training: Rank 0, Epoch 191, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,127: INFO: model_training: Rank 0, Epoch 191, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,128: INFO: model_training: Rank 0, Epoch 191, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,129: INFO: model_training: Rank 0, Epoch 191, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,130: INFO: model_training: Rank 0, Epoch 191, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,131: INFO: model_training: Rank 0, Epoch 192, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,132: INFO: model_training: Rank 0, Epoch 192, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,133: INFO: model_training: Rank 0, Epoch 192, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,134: INFO: model_training: Rank 0, Epoch 192, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,135: INFO: model_training: Rank 0, Epoch 192, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,136: INFO: model_training: Rank 0, Epoch 193, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,137: INFO: model_training: Rank 0, Epoch 193, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,138: INFO: model_training: Rank 0, Epoch 193, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,138: INFO: model_training: Rank 0, Epoch 193, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,140: INFO: model_training: Rank 0, Epoch 193, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,140: INFO: model_training: Rank 0, Epoch 194, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,141: INFO: model_training: Rank 0, Epoch 194, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,142: INFO: model_training: Rank 0, Epoch 194, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,144: INFO: model_training: Rank 0, Epoch 194, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,145: INFO: model_training: Rank 0, Epoch 194, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,146: INFO: model_training: Rank 0, Epoch 195, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,147: INFO: model_training: Rank 0, Epoch 195, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,148: INFO: model_training: Rank 0, Epoch 195, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,148: INFO: model_training: Rank 0, Epoch 195, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,149: INFO: model_training: Rank 0, Epoch 195, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,151: INFO: model_training: Rank 0, Epoch 196, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,152: INFO: model_training: Rank 0, Epoch 196, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,153: INFO: model_training: Rank 0, Epoch 196, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,154: INFO: model_training: Rank 0, Epoch 196, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,155: INFO: model_training: Rank 0, Epoch 196, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,156: INFO: model_training: Rank 0, Epoch 197, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,157: INFO: model_training: Rank 0, Epoch 197, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,158: INFO: model_training: Rank 0, Epoch 197, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,159: INFO: model_training: Rank 0, Epoch 197, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,159: INFO: model_training: Rank 0, Epoch 197, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,161: INFO: model_training: Rank 0, Epoch 198, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,162: INFO: model_training: Rank 0, Epoch 198, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,163: INFO: model_training: Rank 0, Epoch 198, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,163: INFO: model_training: Rank 0, Epoch 198, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,165: INFO: model_training: Rank 0, Epoch 198, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,165: INFO: model_training: Rank 0, Epoch 199, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,166: INFO: model_training: Rank 0, Epoch 199, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,167: INFO: model_training: Rank 0, Epoch 199, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,168: INFO: model_training: Rank 0, Epoch 199, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,169: INFO: model_training: Rank 0, Epoch 199, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,170: INFO: model_training: Rank 0, Epoch 200, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,172: INFO: model_training: Rank 0, Epoch 200, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,173: INFO: model_training: Rank 0, Epoch 200, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,174: INFO: model_training: Rank 0, Epoch 200, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,175: INFO: model_training: Rank 0, Epoch 200, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,176: INFO: model_training: Rank 0, Epoch 201, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,177: INFO: model_training: Rank 0, Epoch 201, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,178: INFO: model_training: Rank 0, Epoch 201, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,179: INFO: model_training: Rank 0, Epoch 201, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,180: INFO: model_training: Rank 0, Epoch 201, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,181: INFO: model_training: Rank 0, Epoch 202, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,181: INFO: model_training: Rank 0, Epoch 202, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,183: INFO: model_training: Rank 0, Epoch 202, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,184: INFO: model_training: Rank 0, Epoch 202, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,185: INFO: model_training: Rank 0, Epoch 202, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,186: INFO: model_training: Rank 0, Epoch 203, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,187: INFO: model_training: Rank 0, Epoch 203, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,188: INFO: model_training: Rank 0, Epoch 203, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,189: INFO: model_training: Rank 0, Epoch 203, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,190: INFO: model_training: Rank 0, Epoch 203, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,190: INFO: model_training: Rank 0, Epoch 204, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,191: INFO: model_training: Rank 0, Epoch 204, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,193: INFO: model_training: Rank 0, Epoch 204, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,194: INFO: model_training: Rank 0, Epoch 204, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,195: INFO: model_training: Rank 0, Epoch 204, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,196: INFO: model_training: Rank 0, Epoch 205, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,197: INFO: model_training: Rank 0, Epoch 205, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,198: INFO: model_training: Rank 0, Epoch 205, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,199: INFO: model_training: Rank 0, Epoch 205, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,200: INFO: model_training: Rank 0, Epoch 205, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,200: INFO: model_training: Rank 0, Epoch 206, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,202: INFO: model_training: Rank 0, Epoch 206, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,202: INFO: model_training: Rank 0, Epoch 206, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,203: INFO: model_training: Rank 0, Epoch 206, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,204: INFO: model_training: Rank 0, Epoch 206, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,206: INFO: model_training: Rank 0, Epoch 207, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,207: INFO: model_training: Rank 0, Epoch 207, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,208: INFO: model_training: Rank 0, Epoch 207, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,209: INFO: model_training: Rank 0, Epoch 207, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,210: INFO: model_training: Rank 0, Epoch 207, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,211: INFO: model_training: Rank 0, Epoch 208, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,213: INFO: model_training: Rank 0, Epoch 208, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,213: INFO: model_training: Rank 0, Epoch 208, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,214: INFO: model_training: Rank 0, Epoch 208, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,215: INFO: model_training: Rank 0, Epoch 208, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,216: INFO: model_training: Rank 0, Epoch 209, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,217: INFO: model_training: Rank 0, Epoch 209, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,218: INFO: model_training: Rank 0, Epoch 209, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,219: INFO: model_training: Rank 0, Epoch 209, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,220: INFO: model_training: Rank 0, Epoch 209, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,221: INFO: model_training: Rank 0, Epoch 210, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,223: INFO: model_training: Rank 0, Epoch 210, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,224: INFO: model_training: Rank 0, Epoch 210, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,225: INFO: model_training: Rank 0, Epoch 210, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,226: INFO: model_training: Rank 0, Epoch 210, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,228: INFO: model_training: Rank 0, Epoch 211, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,229: INFO: model_training: Rank 0, Epoch 211, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,230: INFO: model_training: Rank 0, Epoch 211, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,231: INFO: model_training: Rank 0, Epoch 211, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,232: INFO: model_training: Rank 0, Epoch 211, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,233: INFO: model_training: Rank 0, Epoch 212, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,234: INFO: model_training: Rank 0, Epoch 212, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,235: INFO: model_training: Rank 0, Epoch 212, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,236: INFO: model_training: Rank 0, Epoch 212, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,237: INFO: model_training: Rank 0, Epoch 212, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,238: INFO: model_training: Rank 0, Epoch 213, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,239: INFO: model_training: Rank 0, Epoch 213, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,240: INFO: model_training: Rank 0, Epoch 213, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,241: INFO: model_training: Rank 0, Epoch 213, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,242: INFO: model_training: Rank 0, Epoch 213, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,243: INFO: model_training: Rank 0, Epoch 214, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,244: INFO: model_training: Rank 0, Epoch 214, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,245: INFO: model_training: Rank 0, Epoch 214, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,246: INFO: model_training: Rank 0, Epoch 214, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,247: INFO: model_training: Rank 0, Epoch 214, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,248: INFO: model_training: Rank 0, Epoch 215, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,249: INFO: model_training: Rank 0, Epoch 215, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,250: INFO: model_training: Rank 0, Epoch 215, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,251: INFO: model_training: Rank 0, Epoch 215, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,252: INFO: model_training: Rank 0, Epoch 215, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,253: INFO: model_training: Rank 0, Epoch 216, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,254: INFO: model_training: Rank 0, Epoch 216, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,256: INFO: model_training: Rank 0, Epoch 216, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,257: INFO: model_training: Rank 0, Epoch 216, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,258: INFO: model_training: Rank 0, Epoch 216, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,259: INFO: model_training: Rank 0, Epoch 217, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,260: INFO: model_training: Rank 0, Epoch 217, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,261: INFO: model_training: Rank 0, Epoch 217, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,263: INFO: model_training: Rank 0, Epoch 217, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,264: INFO: model_training: Rank 0, Epoch 217, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,265: INFO: model_training: Rank 0, Epoch 218, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,266: INFO: model_training: Rank 0, Epoch 218, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,267: INFO: model_training: Rank 0, Epoch 218, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,268: INFO: model_training: Rank 0, Epoch 218, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,269: INFO: model_training: Rank 0, Epoch 218, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,270: INFO: model_training: Rank 0, Epoch 219, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,271: INFO: model_training: Rank 0, Epoch 219, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,273: INFO: model_training: Rank 0, Epoch 219, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,276: INFO: model_training: Rank 0, Epoch 219, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,279: INFO: model_training: Rank 0, Epoch 219, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,281: INFO: model_training: Rank 0, Epoch 220, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,283: INFO: model_training: Rank 0, Epoch 220, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,285: INFO: model_training: Rank 0, Epoch 220, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,286: INFO: model_training: Rank 0, Epoch 220, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,288: INFO: model_training: Rank 0, Epoch 220, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,289: INFO: model_training: Rank 0, Epoch 221, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,290: INFO: model_training: Rank 0, Epoch 221, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,291: INFO: model_training: Rank 0, Epoch 221, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,293: INFO: model_training: Rank 0, Epoch 221, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,294: INFO: model_training: Rank 0, Epoch 221, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,295: INFO: model_training: Rank 0, Epoch 222, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,297: INFO: model_training: Rank 0, Epoch 222, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,298: INFO: model_training: Rank 0, Epoch 222, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,299: INFO: model_training: Rank 0, Epoch 222, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,301: INFO: model_training: Rank 0, Epoch 222, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,302: INFO: model_training: Rank 0, Epoch 223, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,303: INFO: model_training: Rank 0, Epoch 223, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,304: INFO: model_training: Rank 0, Epoch 223, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,306: INFO: model_training: Rank 0, Epoch 223, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,307: INFO: model_training: Rank 0, Epoch 223, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,308: INFO: model_training: Rank 0, Epoch 224, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,310: INFO: model_training: Rank 0, Epoch 224, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,311: INFO: model_training: Rank 0, Epoch 224, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,312: INFO: model_training: Rank 0, Epoch 224, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,314: INFO: model_training: Rank 0, Epoch 224, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,315: INFO: model_training: Rank 0, Epoch 225, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,316: INFO: model_training: Rank 0, Epoch 225, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,317: INFO: model_training: Rank 0, Epoch 225, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,318: INFO: model_training: Rank 0, Epoch 225, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,319: INFO: model_training: Rank 0, Epoch 225, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,320: INFO: model_training: Rank 0, Epoch 226, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,321: INFO: model_training: Rank 0, Epoch 226, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,322: INFO: model_training: Rank 0, Epoch 226, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,323: INFO: model_training: Rank 0, Epoch 226, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,324: INFO: model_training: Rank 0, Epoch 226, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,325: INFO: model_training: Rank 0, Epoch 227, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,326: INFO: model_training: Rank 0, Epoch 227, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,327: INFO: model_training: Rank 0, Epoch 227, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,328: INFO: model_training: Rank 0, Epoch 227, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,329: INFO: model_training: Rank 0, Epoch 227, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,330: INFO: model_training: Rank 0, Epoch 228, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,331: INFO: model_training: Rank 0, Epoch 228, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,332: INFO: model_training: Rank 0, Epoch 228, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,333: INFO: model_training: Rank 0, Epoch 228, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,335: INFO: model_training: Rank 0, Epoch 228, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,336: INFO: model_training: Rank 0, Epoch 229, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,337: INFO: model_training: Rank 0, Epoch 229, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,338: INFO: model_training: Rank 0, Epoch 229, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,339: INFO: model_training: Rank 0, Epoch 229, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,340: INFO: model_training: Rank 0, Epoch 229, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,341: INFO: model_training: Rank 0, Epoch 230, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,342: INFO: model_training: Rank 0, Epoch 230, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,343: INFO: model_training: Rank 0, Epoch 230, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,344: INFO: model_training: Rank 0, Epoch 230, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,345: INFO: model_training: Rank 0, Epoch 230, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,346: INFO: model_training: Rank 0, Epoch 231, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,347: INFO: model_training: Rank 0, Epoch 231, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,348: INFO: model_training: Rank 0, Epoch 231, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,349: INFO: model_training: Rank 0, Epoch 231, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,350: INFO: model_training: Rank 0, Epoch 231, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,351: INFO: model_training: Rank 0, Epoch 232, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,352: INFO: model_training: Rank 0, Epoch 232, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,353: INFO: model_training: Rank 0, Epoch 232, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,355: INFO: model_training: Rank 0, Epoch 232, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,356: INFO: model_training: Rank 0, Epoch 232, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,357: INFO: model_training: Rank 0, Epoch 233, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,358: INFO: model_training: Rank 0, Epoch 233, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,359: INFO: model_training: Rank 0, Epoch 233, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,360: INFO: model_training: Rank 0, Epoch 233, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,360: INFO: model_training: Rank 0, Epoch 233, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,362: INFO: model_training: Rank 0, Epoch 234, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,363: INFO: model_training: Rank 0, Epoch 234, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,364: INFO: model_training: Rank 0, Epoch 234, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,364: INFO: model_training: Rank 0, Epoch 234, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,365: INFO: model_training: Rank 0, Epoch 234, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,366: INFO: model_training: Rank 0, Epoch 235, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,367: INFO: model_training: Rank 0, Epoch 235, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,369: INFO: model_training: Rank 0, Epoch 235, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,370: INFO: model_training: Rank 0, Epoch 235, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,371: INFO: model_training: Rank 0, Epoch 235, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,372: INFO: model_training: Rank 0, Epoch 236, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,373: INFO: model_training: Rank 0, Epoch 236, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,374: INFO: model_training: Rank 0, Epoch 236, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,375: INFO: model_training: Rank 0, Epoch 236, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,376: INFO: model_training: Rank 0, Epoch 236, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,377: INFO: model_training: Rank 0, Epoch 237, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,379: INFO: model_training: Rank 0, Epoch 237, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,380: INFO: model_training: Rank 0, Epoch 237, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,381: INFO: model_training: Rank 0, Epoch 237, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,382: INFO: model_training: Rank 0, Epoch 237, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,383: INFO: model_training: Rank 0, Epoch 238, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,384: INFO: model_training: Rank 0, Epoch 238, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,385: INFO: model_training: Rank 0, Epoch 238, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,386: INFO: model_training: Rank 0, Epoch 238, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,387: INFO: model_training: Rank 0, Epoch 238, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,388: INFO: model_training: Rank 0, Epoch 239, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,389: INFO: model_training: Rank 0, Epoch 239, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,390: INFO: model_training: Rank 0, Epoch 239, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,391: INFO: model_training: Rank 0, Epoch 239, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,392: INFO: model_training: Rank 0, Epoch 239, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,393: INFO: model_training: Rank 0, Epoch 240, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,394: INFO: model_training: Rank 0, Epoch 240, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,395: INFO: model_training: Rank 0, Epoch 240, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,396: INFO: model_training: Rank 0, Epoch 240, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,397: INFO: model_training: Rank 0, Epoch 240, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,398: INFO: model_training: Rank 0, Epoch 241, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,399: INFO: model_training: Rank 0, Epoch 241, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,400: INFO: model_training: Rank 0, Epoch 241, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,401: INFO: model_training: Rank 0, Epoch 241, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,403: INFO: model_training: Rank 0, Epoch 241, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,404: INFO: model_training: Rank 0, Epoch 242, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,405: INFO: model_training: Rank 0, Epoch 242, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,406: INFO: model_training: Rank 0, Epoch 242, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,407: INFO: model_training: Rank 0, Epoch 242, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,408: INFO: model_training: Rank 0, Epoch 242, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,409: INFO: model_training: Rank 0, Epoch 243, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,410: INFO: model_training: Rank 0, Epoch 243, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,411: INFO: model_training: Rank 0, Epoch 243, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,412: INFO: model_training: Rank 0, Epoch 243, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,413: INFO: model_training: Rank 0, Epoch 243, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,414: INFO: model_training: Rank 0, Epoch 244, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,416: INFO: model_training: Rank 0, Epoch 244, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,417: INFO: model_training: Rank 0, Epoch 244, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,418: INFO: model_training: Rank 0, Epoch 244, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,419: INFO: model_training: Rank 0, Epoch 244, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,420: INFO: model_training: Rank 0, Epoch 245, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,421: INFO: model_training: Rank 0, Epoch 245, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,422: INFO: model_training: Rank 0, Epoch 245, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,423: INFO: model_training: Rank 0, Epoch 245, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,424: INFO: model_training: Rank 0, Epoch 245, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,426: INFO: model_training: Rank 0, Epoch 246, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,427: INFO: model_training: Rank 0, Epoch 246, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,428: INFO: model_training: Rank 0, Epoch 246, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,428: INFO: model_training: Rank 0, Epoch 246, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,429: INFO: model_training: Rank 0, Epoch 246, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,431: INFO: model_training: Rank 0, Epoch 247, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,432: INFO: model_training: Rank 0, Epoch 247, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,433: INFO: model_training: Rank 0, Epoch 247, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,434: INFO: model_training: Rank 0, Epoch 247, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,435: INFO: model_training: Rank 0, Epoch 247, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,436: INFO: model_training: Rank 0, Epoch 248, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,437: INFO: model_training: Rank 0, Epoch 248, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,438: INFO: model_training: Rank 0, Epoch 248, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,439: INFO: model_training: Rank 0, Epoch 248, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,440: INFO: model_training: Rank 0, Epoch 248, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,441: INFO: model_training: Rank 0, Epoch 249, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,442: INFO: model_training: Rank 0, Epoch 249, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,443: INFO: model_training: Rank 0, Epoch 249, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,444: INFO: model_training: Rank 0, Epoch 249, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,445: INFO: model_training: Rank 0, Epoch 249, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,446: INFO: model_training: Rank 0, Epoch 250, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,447: INFO: model_training: Rank 0, Epoch 250, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,449: INFO: model_training: Rank 0, Epoch 250, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,450: INFO: model_training: Rank 0, Epoch 250, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,451: INFO: model_training: Rank 0, Epoch 250, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,452: INFO: model_training: Rank 0, Epoch 251, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,453: INFO: model_training: Rank 0, Epoch 251, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,454: INFO: model_training: Rank 0, Epoch 251, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,455: INFO: model_training: Rank 0, Epoch 251, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,456: INFO: model_training: Rank 0, Epoch 251, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,457: INFO: model_training: Rank 0, Epoch 252, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,458: INFO: model_training: Rank 0, Epoch 252, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,459: INFO: model_training: Rank 0, Epoch 252, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,460: INFO: model_training: Rank 0, Epoch 252, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,461: INFO: model_training: Rank 0, Epoch 252, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,463: INFO: model_training: Rank 0, Epoch 253, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,464: INFO: model_training: Rank 0, Epoch 253, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,465: INFO: model_training: Rank 0, Epoch 253, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,467: INFO: model_training: Rank 0, Epoch 253, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,469: INFO: model_training: Rank 0, Epoch 253, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,470: INFO: model_training: Rank 0, Epoch 254, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,471: INFO: model_training: Rank 0, Epoch 254, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,473: INFO: model_training: Rank 0, Epoch 254, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,474: INFO: model_training: Rank 0, Epoch 254, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,475: INFO: model_training: Rank 0, Epoch 254, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,476: INFO: model_training: Rank 0, Epoch 255, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,478: INFO: model_training: Rank 0, Epoch 255, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,479: INFO: model_training: Rank 0, Epoch 255, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,480: INFO: model_training: Rank 0, Epoch 255, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,481: INFO: model_training: Rank 0, Epoch 255, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,483: INFO: model_training: Rank 0, Epoch 256, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,484: INFO: model_training: Rank 0, Epoch 256, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,485: INFO: model_training: Rank 0, Epoch 256, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,486: INFO: model_training: Rank 0, Epoch 256, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,488: INFO: model_training: Rank 0, Epoch 256, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,489: INFO: model_training: Rank 0, Epoch 257, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,490: INFO: model_training: Rank 0, Epoch 257, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,492: INFO: model_training: Rank 0, Epoch 257, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,493: INFO: model_training: Rank 0, Epoch 257, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,494: INFO: model_training: Rank 0, Epoch 257, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,496: INFO: model_training: Rank 0, Epoch 258, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,497: INFO: model_training: Rank 0, Epoch 258, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,498: INFO: model_training: Rank 0, Epoch 258, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,499: INFO: model_training: Rank 0, Epoch 258, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,500: INFO: model_training: Rank 0, Epoch 258, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,501: INFO: model_training: Rank 0, Epoch 259, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,503: INFO: model_training: Rank 0, Epoch 259, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,504: INFO: model_training: Rank 0, Epoch 259, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,505: INFO: model_training: Rank 0, Epoch 259, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,506: INFO: model_training: Rank 0, Epoch 259, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,508: INFO: model_training: Rank 0, Epoch 260, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,509: INFO: model_training: Rank 0, Epoch 260, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,510: INFO: model_training: Rank 0, Epoch 260, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,511: INFO: model_training: Rank 0, Epoch 260, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,513: INFO: model_training: Rank 0, Epoch 260, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,514: INFO: model_training: Rank 0, Epoch 261, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,515: INFO: model_training: Rank 0, Epoch 261, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,517: INFO: model_training: Rank 0, Epoch 261, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,518: INFO: model_training: Rank 0, Epoch 261, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,519: INFO: model_training: Rank 0, Epoch 261, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,520: INFO: model_training: Rank 0, Epoch 262, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,522: INFO: model_training: Rank 0, Epoch 262, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,523: INFO: model_training: Rank 0, Epoch 262, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,524: INFO: model_training: Rank 0, Epoch 262, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,525: INFO: model_training: Rank 0, Epoch 262, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,526: INFO: model_training: Rank 0, Epoch 263, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,527: INFO: model_training: Rank 0, Epoch 263, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,529: INFO: model_training: Rank 0, Epoch 263, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,530: INFO: model_training: Rank 0, Epoch 263, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,531: INFO: model_training: Rank 0, Epoch 263, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,532: INFO: model_training: Rank 0, Epoch 264, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,533: INFO: model_training: Rank 0, Epoch 264, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,534: INFO: model_training: Rank 0, Epoch 264, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,536: INFO: model_training: Rank 0, Epoch 264, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,537: INFO: model_training: Rank 0, Epoch 264, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,539: INFO: model_training: Rank 0, Epoch 265, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,540: INFO: model_training: Rank 0, Epoch 265, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,541: INFO: model_training: Rank 0, Epoch 265, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,543: INFO: model_training: Rank 0, Epoch 265, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,544: INFO: model_training: Rank 0, Epoch 265, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,545: INFO: model_training: Rank 0, Epoch 266, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,547: INFO: model_training: Rank 0, Epoch 266, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,548: INFO: model_training: Rank 0, Epoch 266, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,549: INFO: model_training: Rank 0, Epoch 266, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,550: INFO: model_training: Rank 0, Epoch 266, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,551: INFO: model_training: Rank 0, Epoch 267, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,553: INFO: model_training: Rank 0, Epoch 267, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,554: INFO: model_training: Rank 0, Epoch 267, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,555: INFO: model_training: Rank 0, Epoch 267, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,556: INFO: model_training: Rank 0, Epoch 267, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,558: INFO: model_training: Rank 0, Epoch 268, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,560: INFO: model_training: Rank 0, Epoch 268, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,562: INFO: model_training: Rank 0, Epoch 268, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,563: INFO: model_training: Rank 0, Epoch 268, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,564: INFO: model_training: Rank 0, Epoch 268, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,565: INFO: model_training: Rank 0, Epoch 269, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,566: INFO: model_training: Rank 0, Epoch 269, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,567: INFO: model_training: Rank 0, Epoch 269, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,568: INFO: model_training: Rank 0, Epoch 269, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,570: INFO: model_training: Rank 0, Epoch 269, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,571: INFO: model_training: Rank 0, Epoch 270, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,572: INFO: model_training: Rank 0, Epoch 270, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,573: INFO: model_training: Rank 0, Epoch 270, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,574: INFO: model_training: Rank 0, Epoch 270, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,576: INFO: model_training: Rank 0, Epoch 270, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,577: INFO: model_training: Rank 0, Epoch 271, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,578: INFO: model_training: Rank 0, Epoch 271, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,579: INFO: model_training: Rank 0, Epoch 271, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,580: INFO: model_training: Rank 0, Epoch 271, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,581: INFO: model_training: Rank 0, Epoch 271, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,582: INFO: model_training: Rank 0, Epoch 272, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,584: INFO: model_training: Rank 0, Epoch 272, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,585: INFO: model_training: Rank 0, Epoch 272, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,586: INFO: model_training: Rank 0, Epoch 272, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,587: INFO: model_training: Rank 0, Epoch 272, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,588: INFO: model_training: Rank 0, Epoch 273, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,589: INFO: model_training: Rank 0, Epoch 273, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,591: INFO: model_training: Rank 0, Epoch 273, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,592: INFO: model_training: Rank 0, Epoch 273, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,593: INFO: model_training: Rank 0, Epoch 273, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,594: INFO: model_training: Rank 0, Epoch 274, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,595: INFO: model_training: Rank 0, Epoch 274, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,596: INFO: model_training: Rank 0, Epoch 274, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,597: INFO: model_training: Rank 0, Epoch 274, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,599: INFO: model_training: Rank 0, Epoch 274, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,600: INFO: model_training: Rank 0, Epoch 275, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,601: INFO: model_training: Rank 0, Epoch 275, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,603: INFO: model_training: Rank 0, Epoch 275, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,604: INFO: model_training: Rank 0, Epoch 275, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,605: INFO: model_training: Rank 0, Epoch 275, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,606: INFO: model_training: Rank 0, Epoch 276, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,608: INFO: model_training: Rank 0, Epoch 276, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,609: INFO: model_training: Rank 0, Epoch 276, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,610: INFO: model_training: Rank 0, Epoch 276, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,611: INFO: model_training: Rank 0, Epoch 276, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,613: INFO: model_training: Rank 0, Epoch 277, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,614: INFO: model_training: Rank 0, Epoch 277, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,615: INFO: model_training: Rank 0, Epoch 277, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,616: INFO: model_training: Rank 0, Epoch 277, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,618: INFO: model_training: Rank 0, Epoch 277, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,619: INFO: model_training: Rank 0, Epoch 278, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,620: INFO: model_training: Rank 0, Epoch 278, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,621: INFO: model_training: Rank 0, Epoch 278, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,623: INFO: model_training: Rank 0, Epoch 278, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,624: INFO: model_training: Rank 0, Epoch 278, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,625: INFO: model_training: Rank 0, Epoch 279, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,626: INFO: model_training: Rank 0, Epoch 279, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,627: INFO: model_training: Rank 0, Epoch 279, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,628: INFO: model_training: Rank 0, Epoch 279, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,630: INFO: model_training: Rank 0, Epoch 279, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,631: INFO: model_training: Rank 0, Epoch 280, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,632: INFO: model_training: Rank 0, Epoch 280, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,633: INFO: model_training: Rank 0, Epoch 280, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,634: INFO: model_training: Rank 0, Epoch 280, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,635: INFO: model_training: Rank 0, Epoch 280, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,637: INFO: model_training: Rank 0, Epoch 281, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,638: INFO: model_training: Rank 0, Epoch 281, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,639: INFO: model_training: Rank 0, Epoch 281, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,640: INFO: model_training: Rank 0, Epoch 281, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,642: INFO: model_training: Rank 0, Epoch 281, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,643: INFO: model_training: Rank 0, Epoch 282, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,644: INFO: model_training: Rank 0, Epoch 282, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,646: INFO: model_training: Rank 0, Epoch 282, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,647: INFO: model_training: Rank 0, Epoch 282, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,648: INFO: model_training: Rank 0, Epoch 282, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,650: INFO: model_training: Rank 0, Epoch 283, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,651: INFO: model_training: Rank 0, Epoch 283, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,652: INFO: model_training: Rank 0, Epoch 283, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,653: INFO: model_training: Rank 0, Epoch 283, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,655: INFO: model_training: Rank 0, Epoch 283, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,656: INFO: model_training: Rank 0, Epoch 284, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,657: INFO: model_training: Rank 0, Epoch 284, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,659: INFO: model_training: Rank 0, Epoch 284, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,660: INFO: model_training: Rank 0, Epoch 284, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,661: INFO: model_training: Rank 0, Epoch 284, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,663: INFO: model_training: Rank 0, Epoch 285, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,664: INFO: model_training: Rank 0, Epoch 285, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,666: INFO: model_training: Rank 0, Epoch 285, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,667: INFO: model_training: Rank 0, Epoch 285, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,668: INFO: model_training: Rank 0, Epoch 285, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,670: INFO: model_training: Rank 0, Epoch 286, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,671: INFO: model_training: Rank 0, Epoch 286, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,672: INFO: model_training: Rank 0, Epoch 286, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,674: INFO: model_training: Rank 0, Epoch 286, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,675: INFO: model_training: Rank 0, Epoch 286, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,676: INFO: model_training: Rank 0, Epoch 287, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,678: INFO: model_training: Rank 0, Epoch 287, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,679: INFO: model_training: Rank 0, Epoch 287, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,680: INFO: model_training: Rank 0, Epoch 287, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,682: INFO: model_training: Rank 0, Epoch 287, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,683: INFO: model_training: Rank 0, Epoch 288, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,684: INFO: model_training: Rank 0, Epoch 288, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,685: INFO: model_training: Rank 0, Epoch 288, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,687: INFO: model_training: Rank 0, Epoch 288, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,688: INFO: model_training: Rank 0, Epoch 288, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,689: INFO: model_training: Rank 0, Epoch 289, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,690: INFO: model_training: Rank 0, Epoch 289, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,692: INFO: model_training: Rank 0, Epoch 289, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,693: INFO: model_training: Rank 0, Epoch 289, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,694: INFO: model_training: Rank 0, Epoch 289, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,695: INFO: model_training: Rank 0, Epoch 290, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,696: INFO: model_training: Rank 0, Epoch 290, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,698: INFO: model_training: Rank 0, Epoch 290, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,699: INFO: model_training: Rank 0, Epoch 290, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,700: INFO: model_training: Rank 0, Epoch 290, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,701: INFO: model_training: Rank 0, Epoch 291, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,702: INFO: model_training: Rank 0, Epoch 291, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,704: INFO: model_training: Rank 0, Epoch 291, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,705: INFO: model_training: Rank 0, Epoch 291, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,707: INFO: model_training: Rank 0, Epoch 291, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,708: INFO: model_training: Rank 0, Epoch 292, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,709: INFO: model_training: Rank 0, Epoch 292, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,710: INFO: model_training: Rank 0, Epoch 292, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,712: INFO: model_training: Rank 0, Epoch 292, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,713: INFO: model_training: Rank 0, Epoch 292, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,714: INFO: model_training: Rank 0, Epoch 293, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,715: INFO: model_training: Rank 0, Epoch 293, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,717: INFO: model_training: Rank 0, Epoch 293, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,718: INFO: model_training: Rank 0, Epoch 293, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,719: INFO: model_training: Rank 0, Epoch 293, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,720: INFO: model_training: Rank 0, Epoch 294, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,722: INFO: model_training: Rank 0, Epoch 294, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,723: INFO: model_training: Rank 0, Epoch 294, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,724: INFO: model_training: Rank 0, Epoch 294, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,725: INFO: model_training: Rank 0, Epoch 294, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,726: INFO: model_training: Rank 0, Epoch 295, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,727: INFO: model_training: Rank 0, Epoch 295, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,728: INFO: model_training: Rank 0, Epoch 295, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,729: INFO: model_training: Rank 0, Epoch 295, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,731: INFO: model_training: Rank 0, Epoch 295, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,732: INFO: model_training: Rank 0, Epoch 296, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,733: INFO: model_training: Rank 0, Epoch 296, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,734: INFO: model_training: Rank 0, Epoch 296, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,735: INFO: model_training: Rank 0, Epoch 296, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,736: INFO: model_training: Rank 0, Epoch 296, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,737: INFO: model_training: Rank 0, Epoch 297, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,738: INFO: model_training: Rank 0, Epoch 297, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,740: INFO: model_training: Rank 0, Epoch 297, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,741: INFO: model_training: Rank 0, Epoch 297, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,742: INFO: model_training: Rank 0, Epoch 297, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,744: INFO: model_training: Rank 0, Epoch 298, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,746: INFO: model_training: Rank 0, Epoch 298, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,747: INFO: model_training: Rank 0, Epoch 298, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,749: INFO: model_training: Rank 0, Epoch 298, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,750: INFO: model_training: Rank 0, Epoch 298, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,751: INFO: model_training: Rank 0, Epoch 299, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,752: INFO: model_training: Rank 0, Epoch 299, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,754: INFO: model_training: Rank 0, Epoch 299, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,755: INFO: model_training: Rank 0, Epoch 299, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,757: INFO: model_training: Rank 0, Epoch 299, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,758: INFO: model_training: Rank 0, Epoch 300, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,760: INFO: model_training: Rank 0, Epoch 300, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,762: INFO: model_training: Rank 0, Epoch 300, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,763: INFO: model_training: Rank 0, Epoch 300, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,765: INFO: model_training: Rank 0, Epoch 300, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,766: INFO: model_training: Rank 0, Epoch 301, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,768: INFO: model_training: Rank 0, Epoch 301, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,770: INFO: model_training: Rank 0, Epoch 301, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,771: INFO: model_training: Rank 0, Epoch 301, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,773: INFO: model_training: Rank 0, Epoch 301, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,775: INFO: model_training: Rank 0, Epoch 302, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,777: INFO: model_training: Rank 0, Epoch 302, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,779: INFO: model_training: Rank 0, Epoch 302, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,782: INFO: model_training: Rank 0, Epoch 302, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,784: INFO: model_training: Rank 0, Epoch 302, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,786: INFO: model_training: Rank 0, Epoch 303, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,788: INFO: model_training: Rank 0, Epoch 303, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,790: INFO: model_training: Rank 0, Epoch 303, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,791: INFO: model_training: Rank 0, Epoch 303, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,792: INFO: model_training: Rank 0, Epoch 303, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,793: INFO: model_training: Rank 0, Epoch 304, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,795: INFO: model_training: Rank 0, Epoch 304, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,796: INFO: model_training: Rank 0, Epoch 304, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,797: INFO: model_training: Rank 0, Epoch 304, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,798: INFO: model_training: Rank 0, Epoch 304, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,799: INFO: model_training: Rank 0, Epoch 305, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,800: INFO: model_training: Rank 0, Epoch 305, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,802: INFO: model_training: Rank 0, Epoch 305, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,803: INFO: model_training: Rank 0, Epoch 305, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,805: INFO: model_training: Rank 0, Epoch 305, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,806: INFO: model_training: Rank 0, Epoch 306, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,807: INFO: model_training: Rank 0, Epoch 306, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,808: INFO: model_training: Rank 0, Epoch 306, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,809: INFO: model_training: Rank 0, Epoch 306, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,810: INFO: model_training: Rank 0, Epoch 306, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,811: INFO: model_training: Rank 0, Epoch 307, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,813: INFO: model_training: Rank 0, Epoch 307, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,814: INFO: model_training: Rank 0, Epoch 307, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,815: INFO: model_training: Rank 0, Epoch 307, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,817: INFO: model_training: Rank 0, Epoch 307, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,818: INFO: model_training: Rank 0, Epoch 308, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,819: INFO: model_training: Rank 0, Epoch 308, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,820: INFO: model_training: Rank 0, Epoch 308, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,821: INFO: model_training: Rank 0, Epoch 308, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,823: INFO: model_training: Rank 0, Epoch 308, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,824: INFO: model_training: Rank 0, Epoch 309, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,825: INFO: model_training: Rank 0, Epoch 309, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,826: INFO: model_training: Rank 0, Epoch 309, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,827: INFO: model_training: Rank 0, Epoch 309, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,828: INFO: model_training: Rank 0, Epoch 309, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,829: INFO: model_training: Rank 0, Epoch 310, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,830: INFO: model_training: Rank 0, Epoch 310, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,832: INFO: model_training: Rank 0, Epoch 310, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,834: INFO: model_training: Rank 0, Epoch 310, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,835: INFO: model_training: Rank 0, Epoch 310, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,836: INFO: model_training: Rank 0, Epoch 311, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,838: INFO: model_training: Rank 0, Epoch 311, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,839: INFO: model_training: Rank 0, Epoch 311, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,840: INFO: model_training: Rank 0, Epoch 311, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,841: INFO: model_training: Rank 0, Epoch 311, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,843: INFO: model_training: Rank 0, Epoch 312, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,844: INFO: model_training: Rank 0, Epoch 312, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,845: INFO: model_training: Rank 0, Epoch 312, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,847: INFO: model_training: Rank 0, Epoch 312, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,848: INFO: model_training: Rank 0, Epoch 312, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,849: INFO: model_training: Rank 0, Epoch 313, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,850: INFO: model_training: Rank 0, Epoch 313, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,851: INFO: model_training: Rank 0, Epoch 313, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,853: INFO: model_training: Rank 0, Epoch 313, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,854: INFO: model_training: Rank 0, Epoch 313, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,855: INFO: model_training: Rank 0, Epoch 314, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,856: INFO: model_training: Rank 0, Epoch 314, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,858: INFO: model_training: Rank 0, Epoch 314, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,859: INFO: model_training: Rank 0, Epoch 314, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,860: INFO: model_training: Rank 0, Epoch 314, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,861: INFO: model_training: Rank 0, Epoch 315, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,862: INFO: model_training: Rank 0, Epoch 315, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,864: INFO: model_training: Rank 0, Epoch 315, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,865: INFO: model_training: Rank 0, Epoch 315, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,866: INFO: model_training: Rank 0, Epoch 315, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,867: INFO: model_training: Rank 0, Epoch 316, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,869: INFO: model_training: Rank 0, Epoch 316, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,870: INFO: model_training: Rank 0, Epoch 316, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,872: INFO: model_training: Rank 0, Epoch 316, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,873: INFO: model_training: Rank 0, Epoch 316, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,875: INFO: model_training: Rank 0, Epoch 317, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,876: INFO: model_training: Rank 0, Epoch 317, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,878: INFO: model_training: Rank 0, Epoch 317, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,879: INFO: model_training: Rank 0, Epoch 317, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,880: INFO: model_training: Rank 0, Epoch 317, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,881: INFO: model_training: Rank 0, Epoch 318, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,883: INFO: model_training: Rank 0, Epoch 318, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,884: INFO: model_training: Rank 0, Epoch 318, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,885: INFO: model_training: Rank 0, Epoch 318, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,886: INFO: model_training: Rank 0, Epoch 318, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,889: INFO: model_training: Rank 0, Epoch 319, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,891: INFO: model_training: Rank 0, Epoch 319, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,893: INFO: model_training: Rank 0, Epoch 319, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,896: INFO: model_training: Rank 0, Epoch 319, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,897: INFO: model_training: Rank 0, Epoch 319, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,898: INFO: model_training: Rank 0, Epoch 320, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,900: INFO: model_training: Rank 0, Epoch 320, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,901: INFO: model_training: Rank 0, Epoch 320, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,903: INFO: model_training: Rank 0, Epoch 320, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,905: INFO: model_training: Rank 0, Epoch 320, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,906: INFO: model_training: Rank 0, Epoch 321, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,909: INFO: model_training: Rank 0, Epoch 321, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,910: INFO: model_training: Rank 0, Epoch 321, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,911: INFO: model_training: Rank 0, Epoch 321, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,913: INFO: model_training: Rank 0, Epoch 321, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,914: INFO: model_training: Rank 0, Epoch 322, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,915: INFO: model_training: Rank 0, Epoch 322, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,917: INFO: model_training: Rank 0, Epoch 322, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,918: INFO: model_training: Rank 0, Epoch 322, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,919: INFO: model_training: Rank 0, Epoch 322, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,921: INFO: model_training: Rank 0, Epoch 323, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,923: INFO: model_training: Rank 0, Epoch 323, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,924: INFO: model_training: Rank 0, Epoch 323, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,926: INFO: model_training: Rank 0, Epoch 323, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,927: INFO: model_training: Rank 0, Epoch 323, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,929: INFO: model_training: Rank 0, Epoch 324, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,930: INFO: model_training: Rank 0, Epoch 324, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,931: INFO: model_training: Rank 0, Epoch 324, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,933: INFO: model_training: Rank 0, Epoch 324, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,934: INFO: model_training: Rank 0, Epoch 324, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,936: INFO: model_training: Rank 0, Epoch 325, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,937: INFO: model_training: Rank 0, Epoch 325, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,938: INFO: model_training: Rank 0, Epoch 325, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,939: INFO: model_training: Rank 0, Epoch 325, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,941: INFO: model_training: Rank 0, Epoch 325, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,943: INFO: model_training: Rank 0, Epoch 326, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,945: INFO: model_training: Rank 0, Epoch 326, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,947: INFO: model_training: Rank 0, Epoch 326, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,948: INFO: model_training: Rank 0, Epoch 326, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,950: INFO: model_training: Rank 0, Epoch 326, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,951: INFO: model_training: Rank 0, Epoch 327, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,952: INFO: model_training: Rank 0, Epoch 327, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,954: INFO: model_training: Rank 0, Epoch 327, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,955: INFO: model_training: Rank 0, Epoch 327, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,957: INFO: model_training: Rank 0, Epoch 327, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,958: INFO: model_training: Rank 0, Epoch 328, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,959: INFO: model_training: Rank 0, Epoch 328, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,960: INFO: model_training: Rank 0, Epoch 328, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,962: INFO: model_training: Rank 0, Epoch 328, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,963: INFO: model_training: Rank 0, Epoch 328, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,964: INFO: model_training: Rank 0, Epoch 329, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,966: INFO: model_training: Rank 0, Epoch 329, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,967: INFO: model_training: Rank 0, Epoch 329, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,969: INFO: model_training: Rank 0, Epoch 329, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,970: INFO: model_training: Rank 0, Epoch 329, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,971: INFO: model_training: Rank 0, Epoch 330, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,973: INFO: model_training: Rank 0, Epoch 330, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,974: INFO: model_training: Rank 0, Epoch 330, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,975: INFO: model_training: Rank 0, Epoch 330, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,977: INFO: model_training: Rank 0, Epoch 330, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,979: INFO: model_training: Rank 0, Epoch 331, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,980: INFO: model_training: Rank 0, Epoch 331, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,982: INFO: model_training: Rank 0, Epoch 331, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,983: INFO: model_training: Rank 0, Epoch 331, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,985: INFO: model_training: Rank 0, Epoch 331, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,986: INFO: model_training: Rank 0, Epoch 332, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,988: INFO: model_training: Rank 0, Epoch 332, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,989: INFO: model_training: Rank 0, Epoch 332, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,990: INFO: model_training: Rank 0, Epoch 332, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,992: INFO: model_training: Rank 0, Epoch 332, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:26,994: INFO: model_training: Rank 0, Epoch 333, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:26,995: INFO: model_training: Rank 0, Epoch 333, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:26,996: INFO: model_training: Rank 0, Epoch 333, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:26,998: INFO: model_training: Rank 0, Epoch 333, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:26,999: INFO: model_training: Rank 0, Epoch 333, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,001: INFO: model_training: Rank 0, Epoch 334, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,002: INFO: model_training: Rank 0, Epoch 334, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,005: INFO: model_training: Rank 0, Epoch 334, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,007: INFO: model_training: Rank 0, Epoch 334, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,008: INFO: model_training: Rank 0, Epoch 334, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,010: INFO: model_training: Rank 0, Epoch 335, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,011: INFO: model_training: Rank 0, Epoch 335, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,014: INFO: model_training: Rank 0, Epoch 335, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,015: INFO: model_training: Rank 0, Epoch 335, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,016: INFO: model_training: Rank 0, Epoch 335, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,017: INFO: model_training: Rank 0, Epoch 336, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,019: INFO: model_training: Rank 0, Epoch 336, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,020: INFO: model_training: Rank 0, Epoch 336, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,021: INFO: model_training: Rank 0, Epoch 336, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,023: INFO: model_training: Rank 0, Epoch 336, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,024: INFO: model_training: Rank 0, Epoch 337, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,026: INFO: model_training: Rank 0, Epoch 337, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,027: INFO: model_training: Rank 0, Epoch 337, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,028: INFO: model_training: Rank 0, Epoch 337, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,029: INFO: model_training: Rank 0, Epoch 337, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,030: INFO: model_training: Rank 0, Epoch 338, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,031: INFO: model_training: Rank 0, Epoch 338, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,032: INFO: model_training: Rank 0, Epoch 338, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,034: INFO: model_training: Rank 0, Epoch 338, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,035: INFO: model_training: Rank 0, Epoch 338, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,037: INFO: model_training: Rank 0, Epoch 339, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,038: INFO: model_training: Rank 0, Epoch 339, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,039: INFO: model_training: Rank 0, Epoch 339, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,040: INFO: model_training: Rank 0, Epoch 339, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,042: INFO: model_training: Rank 0, Epoch 339, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,043: INFO: model_training: Rank 0, Epoch 340, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,044: INFO: model_training: Rank 0, Epoch 340, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,045: INFO: model_training: Rank 0, Epoch 340, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,047: INFO: model_training: Rank 0, Epoch 340, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,048: INFO: model_training: Rank 0, Epoch 340, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,050: INFO: model_training: Rank 0, Epoch 341, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,051: INFO: model_training: Rank 0, Epoch 341, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,053: INFO: model_training: Rank 0, Epoch 341, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,055: INFO: model_training: Rank 0, Epoch 341, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,056: INFO: model_training: Rank 0, Epoch 341, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,058: INFO: model_training: Rank 0, Epoch 342, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,059: INFO: model_training: Rank 0, Epoch 342, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,062: INFO: model_training: Rank 0, Epoch 342, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,064: INFO: model_training: Rank 0, Epoch 342, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,065: INFO: model_training: Rank 0, Epoch 342, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,066: INFO: model_training: Rank 0, Epoch 343, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,067: INFO: model_training: Rank 0, Epoch 343, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,069: INFO: model_training: Rank 0, Epoch 343, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,070: INFO: model_training: Rank 0, Epoch 343, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,072: INFO: model_training: Rank 0, Epoch 343, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,074: INFO: model_training: Rank 0, Epoch 344, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,076: INFO: model_training: Rank 0, Epoch 344, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,077: INFO: model_training: Rank 0, Epoch 344, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,079: INFO: model_training: Rank 0, Epoch 344, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,080: INFO: model_training: Rank 0, Epoch 344, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,082: INFO: model_training: Rank 0, Epoch 345, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,084: INFO: model_training: Rank 0, Epoch 345, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,087: INFO: model_training: Rank 0, Epoch 345, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,090: INFO: model_training: Rank 0, Epoch 345, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,092: INFO: model_training: Rank 0, Epoch 345, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,093: INFO: model_training: Rank 0, Epoch 346, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,095: INFO: model_training: Rank 0, Epoch 346, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,096: INFO: model_training: Rank 0, Epoch 346, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,097: INFO: model_training: Rank 0, Epoch 346, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,099: INFO: model_training: Rank 0, Epoch 346, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,100: INFO: model_training: Rank 0, Epoch 347, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,102: INFO: model_training: Rank 0, Epoch 347, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,103: INFO: model_training: Rank 0, Epoch 347, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,105: INFO: model_training: Rank 0, Epoch 347, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,106: INFO: model_training: Rank 0, Epoch 347, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,107: INFO: model_training: Rank 0, Epoch 348, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,108: INFO: model_training: Rank 0, Epoch 348, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,109: INFO: model_training: Rank 0, Epoch 348, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,110: INFO: model_training: Rank 0, Epoch 348, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,111: INFO: model_training: Rank 0, Epoch 348, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,112: INFO: model_training: Rank 0, Epoch 349, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,113: INFO: model_training: Rank 0, Epoch 349, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,115: INFO: model_training: Rank 0, Epoch 349, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,116: INFO: model_training: Rank 0, Epoch 349, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,116: INFO: model_training: Rank 0, Epoch 349, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,117: INFO: model_training: Rank 0, Epoch 350, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,118: INFO: model_training: Rank 0, Epoch 350, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,119: INFO: model_training: Rank 0, Epoch 350, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,121: INFO: model_training: Rank 0, Epoch 350, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,122: INFO: model_training: Rank 0, Epoch 350, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,122: INFO: model_training: Rank 0, Epoch 351, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,124: INFO: model_training: Rank 0, Epoch 351, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,125: INFO: model_training: Rank 0, Epoch 351, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,126: INFO: model_training: Rank 0, Epoch 351, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,127: INFO: model_training: Rank 0, Epoch 351, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,128: INFO: model_training: Rank 0, Epoch 352, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,129: INFO: model_training: Rank 0, Epoch 352, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,130: INFO: model_training: Rank 0, Epoch 352, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,131: INFO: model_training: Rank 0, Epoch 352, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,132: INFO: model_training: Rank 0, Epoch 352, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,133: INFO: model_training: Rank 0, Epoch 353, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,134: INFO: model_training: Rank 0, Epoch 353, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,135: INFO: model_training: Rank 0, Epoch 353, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,136: INFO: model_training: Rank 0, Epoch 353, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,137: INFO: model_training: Rank 0, Epoch 353, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,138: INFO: model_training: Rank 0, Epoch 354, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,139: INFO: model_training: Rank 0, Epoch 354, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,140: INFO: model_training: Rank 0, Epoch 354, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,141: INFO: model_training: Rank 0, Epoch 354, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,142: INFO: model_training: Rank 0, Epoch 354, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,143: INFO: model_training: Rank 0, Epoch 355, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,144: INFO: model_training: Rank 0, Epoch 355, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,145: INFO: model_training: Rank 0, Epoch 355, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,147: INFO: model_training: Rank 0, Epoch 355, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,148: INFO: model_training: Rank 0, Epoch 355, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,149: INFO: model_training: Rank 0, Epoch 356, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,149: INFO: model_training: Rank 0, Epoch 356, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,150: INFO: model_training: Rank 0, Epoch 356, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,152: INFO: model_training: Rank 0, Epoch 356, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,153: INFO: model_training: Rank 0, Epoch 356, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,154: INFO: model_training: Rank 0, Epoch 357, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,155: INFO: model_training: Rank 0, Epoch 357, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,156: INFO: model_training: Rank 0, Epoch 357, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,157: INFO: model_training: Rank 0, Epoch 357, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,158: INFO: model_training: Rank 0, Epoch 357, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,159: INFO: model_training: Rank 0, Epoch 358, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,160: INFO: model_training: Rank 0, Epoch 358, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,161: INFO: model_training: Rank 0, Epoch 358, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,162: INFO: model_training: Rank 0, Epoch 358, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,163: INFO: model_training: Rank 0, Epoch 358, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,164: INFO: model_training: Rank 0, Epoch 359, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,165: INFO: model_training: Rank 0, Epoch 359, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,166: INFO: model_training: Rank 0, Epoch 359, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,167: INFO: model_training: Rank 0, Epoch 359, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,168: INFO: model_training: Rank 0, Epoch 359, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,169: INFO: model_training: Rank 0, Epoch 360, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,170: INFO: model_training: Rank 0, Epoch 360, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,171: INFO: model_training: Rank 0, Epoch 360, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,172: INFO: model_training: Rank 0, Epoch 360, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,174: INFO: model_training: Rank 0, Epoch 360, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,175: INFO: model_training: Rank 0, Epoch 361, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,175: INFO: model_training: Rank 0, Epoch 361, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,176: INFO: model_training: Rank 0, Epoch 361, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,178: INFO: model_training: Rank 0, Epoch 361, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,179: INFO: model_training: Rank 0, Epoch 361, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,180: INFO: model_training: Rank 0, Epoch 362, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,180: INFO: model_training: Rank 0, Epoch 362, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,181: INFO: model_training: Rank 0, Epoch 362, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,182: INFO: model_training: Rank 0, Epoch 362, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,183: INFO: model_training: Rank 0, Epoch 362, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,184: INFO: model_training: Rank 0, Epoch 363, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,185: INFO: model_training: Rank 0, Epoch 363, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,186: INFO: model_training: Rank 0, Epoch 363, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,187: INFO: model_training: Rank 0, Epoch 363, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,188: INFO: model_training: Rank 0, Epoch 363, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,190: INFO: model_training: Rank 0, Epoch 364, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,191: INFO: model_training: Rank 0, Epoch 364, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,192: INFO: model_training: Rank 0, Epoch 364, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,193: INFO: model_training: Rank 0, Epoch 364, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,194: INFO: model_training: Rank 0, Epoch 364, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,195: INFO: model_training: Rank 0, Epoch 365, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,196: INFO: model_training: Rank 0, Epoch 365, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,197: INFO: model_training: Rank 0, Epoch 365, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,198: INFO: model_training: Rank 0, Epoch 365, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,199: INFO: model_training: Rank 0, Epoch 365, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,199: INFO: model_training: Rank 0, Epoch 366, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,201: INFO: model_training: Rank 0, Epoch 366, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,202: INFO: model_training: Rank 0, Epoch 366, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,203: INFO: model_training: Rank 0, Epoch 366, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,204: INFO: model_training: Rank 0, Epoch 366, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,205: INFO: model_training: Rank 0, Epoch 367, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,206: INFO: model_training: Rank 0, Epoch 367, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,207: INFO: model_training: Rank 0, Epoch 367, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,208: INFO: model_training: Rank 0, Epoch 367, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,209: INFO: model_training: Rank 0, Epoch 367, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,211: INFO: model_training: Rank 0, Epoch 368, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,213: INFO: model_training: Rank 0, Epoch 368, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,214: INFO: model_training: Rank 0, Epoch 368, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,215: INFO: model_training: Rank 0, Epoch 368, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,216: INFO: model_training: Rank 0, Epoch 368, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,216: INFO: model_training: Rank 0, Epoch 369, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,218: INFO: model_training: Rank 0, Epoch 369, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,219: INFO: model_training: Rank 0, Epoch 369, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,220: INFO: model_training: Rank 0, Epoch 369, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,221: INFO: model_training: Rank 0, Epoch 369, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,223: INFO: model_training: Rank 0, Epoch 370, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,224: INFO: model_training: Rank 0, Epoch 370, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,226: INFO: model_training: Rank 0, Epoch 370, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,227: INFO: model_training: Rank 0, Epoch 370, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,228: INFO: model_training: Rank 0, Epoch 370, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,229: INFO: model_training: Rank 0, Epoch 371, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,230: INFO: model_training: Rank 0, Epoch 371, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,231: INFO: model_training: Rank 0, Epoch 371, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,232: INFO: model_training: Rank 0, Epoch 371, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,233: INFO: model_training: Rank 0, Epoch 371, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,234: INFO: model_training: Rank 0, Epoch 372, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,235: INFO: model_training: Rank 0, Epoch 372, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,236: INFO: model_training: Rank 0, Epoch 372, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,237: INFO: model_training: Rank 0, Epoch 372, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,238: INFO: model_training: Rank 0, Epoch 372, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,239: INFO: model_training: Rank 0, Epoch 373, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,240: INFO: model_training: Rank 0, Epoch 373, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,241: INFO: model_training: Rank 0, Epoch 373, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,242: INFO: model_training: Rank 0, Epoch 373, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,243: INFO: model_training: Rank 0, Epoch 373, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,244: INFO: model_training: Rank 0, Epoch 374, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,245: INFO: model_training: Rank 0, Epoch 374, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,246: INFO: model_training: Rank 0, Epoch 374, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,247: INFO: model_training: Rank 0, Epoch 374, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,248: INFO: model_training: Rank 0, Epoch 374, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,249: INFO: model_training: Rank 0, Epoch 375, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,250: INFO: model_training: Rank 0, Epoch 375, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,251: INFO: model_training: Rank 0, Epoch 375, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,252: INFO: model_training: Rank 0, Epoch 375, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,253: INFO: model_training: Rank 0, Epoch 375, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,254: INFO: model_training: Rank 0, Epoch 376, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,255: INFO: model_training: Rank 0, Epoch 376, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,256: INFO: model_training: Rank 0, Epoch 376, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,257: INFO: model_training: Rank 0, Epoch 376, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,258: INFO: model_training: Rank 0, Epoch 376, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,259: INFO: model_training: Rank 0, Epoch 377, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,260: INFO: model_training: Rank 0, Epoch 377, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,261: INFO: model_training: Rank 0, Epoch 377, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,262: INFO: model_training: Rank 0, Epoch 377, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,263: INFO: model_training: Rank 0, Epoch 377, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,265: INFO: model_training: Rank 0, Epoch 378, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,266: INFO: model_training: Rank 0, Epoch 378, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,267: INFO: model_training: Rank 0, Epoch 378, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,268: INFO: model_training: Rank 0, Epoch 378, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,269: INFO: model_training: Rank 0, Epoch 378, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,270: INFO: model_training: Rank 0, Epoch 379, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,271: INFO: model_training: Rank 0, Epoch 379, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,272: INFO: model_training: Rank 0, Epoch 379, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,273: INFO: model_training: Rank 0, Epoch 379, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,274: INFO: model_training: Rank 0, Epoch 379, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,275: INFO: model_training: Rank 0, Epoch 380, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,276: INFO: model_training: Rank 0, Epoch 380, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,277: INFO: model_training: Rank 0, Epoch 380, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,278: INFO: model_training: Rank 0, Epoch 380, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,279: INFO: model_training: Rank 0, Epoch 380, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,280: INFO: model_training: Rank 0, Epoch 381, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,281: INFO: model_training: Rank 0, Epoch 381, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,282: INFO: model_training: Rank 0, Epoch 381, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,283: INFO: model_training: Rank 0, Epoch 381, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,284: INFO: model_training: Rank 0, Epoch 381, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,285: INFO: model_training: Rank 0, Epoch 382, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,286: INFO: model_training: Rank 0, Epoch 382, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,287: INFO: model_training: Rank 0, Epoch 382, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,288: INFO: model_training: Rank 0, Epoch 382, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,289: INFO: model_training: Rank 0, Epoch 382, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,290: INFO: model_training: Rank 0, Epoch 383, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,291: INFO: model_training: Rank 0, Epoch 383, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,292: INFO: model_training: Rank 0, Epoch 383, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,293: INFO: model_training: Rank 0, Epoch 383, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,295: INFO: model_training: Rank 0, Epoch 383, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,296: INFO: model_training: Rank 0, Epoch 384, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,297: INFO: model_training: Rank 0, Epoch 384, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,298: INFO: model_training: Rank 0, Epoch 384, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,299: INFO: model_training: Rank 0, Epoch 384, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,300: INFO: model_training: Rank 0, Epoch 384, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,301: INFO: model_training: Rank 0, Epoch 385, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,302: INFO: model_training: Rank 0, Epoch 385, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,303: INFO: model_training: Rank 0, Epoch 385, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,304: INFO: model_training: Rank 0, Epoch 385, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,305: INFO: model_training: Rank 0, Epoch 385, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,306: INFO: model_training: Rank 0, Epoch 386, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,307: INFO: model_training: Rank 0, Epoch 386, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,308: INFO: model_training: Rank 0, Epoch 386, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,309: INFO: model_training: Rank 0, Epoch 386, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,310: INFO: model_training: Rank 0, Epoch 386, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,311: INFO: model_training: Rank 0, Epoch 387, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,312: INFO: model_training: Rank 0, Epoch 387, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,313: INFO: model_training: Rank 0, Epoch 387, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,314: INFO: model_training: Rank 0, Epoch 387, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,315: INFO: model_training: Rank 0, Epoch 387, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,316: INFO: model_training: Rank 0, Epoch 388, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,317: INFO: model_training: Rank 0, Epoch 388, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,318: INFO: model_training: Rank 0, Epoch 388, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,319: INFO: model_training: Rank 0, Epoch 388, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,321: INFO: model_training: Rank 0, Epoch 388, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,322: INFO: model_training: Rank 0, Epoch 389, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,323: INFO: model_training: Rank 0, Epoch 389, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,324: INFO: model_training: Rank 0, Epoch 389, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,326: INFO: model_training: Rank 0, Epoch 389, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,327: INFO: model_training: Rank 0, Epoch 389, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,329: INFO: model_training: Rank 0, Epoch 390, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,330: INFO: model_training: Rank 0, Epoch 390, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,331: INFO: model_training: Rank 0, Epoch 390, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,333: INFO: model_training: Rank 0, Epoch 390, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,334: INFO: model_training: Rank 0, Epoch 390, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,336: INFO: model_training: Rank 0, Epoch 391, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,338: INFO: model_training: Rank 0, Epoch 391, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,340: INFO: model_training: Rank 0, Epoch 391, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,342: INFO: model_training: Rank 0, Epoch 391, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,345: INFO: model_training: Rank 0, Epoch 391, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,347: INFO: model_training: Rank 0, Epoch 392, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,349: INFO: model_training: Rank 0, Epoch 392, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,350: INFO: model_training: Rank 0, Epoch 392, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,351: INFO: model_training: Rank 0, Epoch 392, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,352: INFO: model_training: Rank 0, Epoch 392, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,354: INFO: model_training: Rank 0, Epoch 393, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,355: INFO: model_training: Rank 0, Epoch 393, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,357: INFO: model_training: Rank 0, Epoch 393, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,358: INFO: model_training: Rank 0, Epoch 393, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,360: INFO: model_training: Rank 0, Epoch 393, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,362: INFO: model_training: Rank 0, Epoch 394, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,363: INFO: model_training: Rank 0, Epoch 394, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,364: INFO: model_training: Rank 0, Epoch 394, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,366: INFO: model_training: Rank 0, Epoch 394, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,367: INFO: model_training: Rank 0, Epoch 394, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,369: INFO: model_training: Rank 0, Epoch 395, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,370: INFO: model_training: Rank 0, Epoch 395, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,372: INFO: model_training: Rank 0, Epoch 395, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,373: INFO: model_training: Rank 0, Epoch 395, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,375: INFO: model_training: Rank 0, Epoch 395, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,376: INFO: model_training: Rank 0, Epoch 396, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,377: INFO: model_training: Rank 0, Epoch 396, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,379: INFO: model_training: Rank 0, Epoch 396, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,380: INFO: model_training: Rank 0, Epoch 396, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,381: INFO: model_training: Rank 0, Epoch 396, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,383: INFO: model_training: Rank 0, Epoch 397, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,385: INFO: model_training: Rank 0, Epoch 397, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,386: INFO: model_training: Rank 0, Epoch 397, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,388: INFO: model_training: Rank 0, Epoch 397, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,389: INFO: model_training: Rank 0, Epoch 397, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,391: INFO: model_training: Rank 0, Epoch 398, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,393: INFO: model_training: Rank 0, Epoch 398, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,394: INFO: model_training: Rank 0, Epoch 398, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,396: INFO: model_training: Rank 0, Epoch 398, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,397: INFO: model_training: Rank 0, Epoch 398, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,400: INFO: model_training: Rank 0, Epoch 399, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,404: INFO: model_training: Rank 0, Epoch 399, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,406: INFO: model_training: Rank 0, Epoch 399, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,408: INFO: model_training: Rank 0, Epoch 399, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,409: INFO: model_training: Rank 0, Epoch 399, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,411: INFO: model_training: Rank 0, Epoch 400, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,413: INFO: model_training: Rank 0, Epoch 400, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,414: INFO: model_training: Rank 0, Epoch 400, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,416: INFO: model_training: Rank 0, Epoch 400, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,418: INFO: model_training: Rank 0, Epoch 400, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,421: INFO: model_training: Rank 0, Epoch 401, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,423: INFO: model_training: Rank 0, Epoch 401, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,425: INFO: model_training: Rank 0, Epoch 401, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,426: INFO: model_training: Rank 0, Epoch 401, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,428: INFO: model_training: Rank 0, Epoch 401, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,429: INFO: model_training: Rank 0, Epoch 402, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,431: INFO: model_training: Rank 0, Epoch 402, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,432: INFO: model_training: Rank 0, Epoch 402, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,434: INFO: model_training: Rank 0, Epoch 402, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,435: INFO: model_training: Rank 0, Epoch 402, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,438: INFO: model_training: Rank 0, Epoch 403, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,440: INFO: model_training: Rank 0, Epoch 403, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,441: INFO: model_training: Rank 0, Epoch 403, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,444: INFO: model_training: Rank 0, Epoch 403, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,446: INFO: model_training: Rank 0, Epoch 403, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,448: INFO: model_training: Rank 0, Epoch 404, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,449: INFO: model_training: Rank 0, Epoch 404, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,451: INFO: model_training: Rank 0, Epoch 404, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,452: INFO: model_training: Rank 0, Epoch 404, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,454: INFO: model_training: Rank 0, Epoch 404, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,455: INFO: model_training: Rank 0, Epoch 405, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,456: INFO: model_training: Rank 0, Epoch 405, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,458: INFO: model_training: Rank 0, Epoch 405, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,460: INFO: model_training: Rank 0, Epoch 405, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,461: INFO: model_training: Rank 0, Epoch 405, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,464: INFO: model_training: Rank 0, Epoch 406, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,466: INFO: model_training: Rank 0, Epoch 406, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,468: INFO: model_training: Rank 0, Epoch 406, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,471: INFO: model_training: Rank 0, Epoch 406, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,474: INFO: model_training: Rank 0, Epoch 406, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,476: INFO: model_training: Rank 0, Epoch 407, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,478: INFO: model_training: Rank 0, Epoch 407, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,481: INFO: model_training: Rank 0, Epoch 407, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,484: INFO: model_training: Rank 0, Epoch 407, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,487: INFO: model_training: Rank 0, Epoch 407, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,491: INFO: model_training: Rank 0, Epoch 408, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,495: INFO: model_training: Rank 0, Epoch 408, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,497: INFO: model_training: Rank 0, Epoch 408, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,499: INFO: model_training: Rank 0, Epoch 408, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,502: INFO: model_training: Rank 0, Epoch 408, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,506: INFO: model_training: Rank 0, Epoch 409, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,511: INFO: model_training: Rank 0, Epoch 409, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,514: INFO: model_training: Rank 0, Epoch 409, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,516: INFO: model_training: Rank 0, Epoch 409, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,518: INFO: model_training: Rank 0, Epoch 409, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,521: INFO: model_training: Rank 0, Epoch 410, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,524: INFO: model_training: Rank 0, Epoch 410, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,526: INFO: model_training: Rank 0, Epoch 410, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,528: INFO: model_training: Rank 0, Epoch 410, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,530: INFO: model_training: Rank 0, Epoch 410, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,532: INFO: model_training: Rank 0, Epoch 411, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,535: INFO: model_training: Rank 0, Epoch 411, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,537: INFO: model_training: Rank 0, Epoch 411, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,538: INFO: model_training: Rank 0, Epoch 411, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,540: INFO: model_training: Rank 0, Epoch 411, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,541: INFO: model_training: Rank 0, Epoch 412, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,543: INFO: model_training: Rank 0, Epoch 412, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,545: INFO: model_training: Rank 0, Epoch 412, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,547: INFO: model_training: Rank 0, Epoch 412, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,549: INFO: model_training: Rank 0, Epoch 412, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,551: INFO: model_training: Rank 0, Epoch 413, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,553: INFO: model_training: Rank 0, Epoch 413, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,555: INFO: model_training: Rank 0, Epoch 413, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,557: INFO: model_training: Rank 0, Epoch 413, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,558: INFO: model_training: Rank 0, Epoch 413, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,560: INFO: model_training: Rank 0, Epoch 414, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,561: INFO: model_training: Rank 0, Epoch 414, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,564: INFO: model_training: Rank 0, Epoch 414, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,565: INFO: model_training: Rank 0, Epoch 414, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,566: INFO: model_training: Rank 0, Epoch 414, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,568: INFO: model_training: Rank 0, Epoch 415, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,569: INFO: model_training: Rank 0, Epoch 415, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,571: INFO: model_training: Rank 0, Epoch 415, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,572: INFO: model_training: Rank 0, Epoch 415, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,573: INFO: model_training: Rank 0, Epoch 415, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,574: INFO: model_training: Rank 0, Epoch 416, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,575: INFO: model_training: Rank 0, Epoch 416, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,576: INFO: model_training: Rank 0, Epoch 416, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,577: INFO: model_training: Rank 0, Epoch 416, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,578: INFO: model_training: Rank 0, Epoch 416, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,580: INFO: model_training: Rank 0, Epoch 417, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,581: INFO: model_training: Rank 0, Epoch 417, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,583: INFO: model_training: Rank 0, Epoch 417, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,585: INFO: model_training: Rank 0, Epoch 417, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,586: INFO: model_training: Rank 0, Epoch 417, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,588: INFO: model_training: Rank 0, Epoch 418, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,589: INFO: model_training: Rank 0, Epoch 418, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,590: INFO: model_training: Rank 0, Epoch 418, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,591: INFO: model_training: Rank 0, Epoch 418, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,593: INFO: model_training: Rank 0, Epoch 418, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,594: INFO: model_training: Rank 0, Epoch 419, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,596: INFO: model_training: Rank 0, Epoch 419, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,597: INFO: model_training: Rank 0, Epoch 419, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,598: INFO: model_training: Rank 0, Epoch 419, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,599: INFO: model_training: Rank 0, Epoch 419, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,600: INFO: model_training: Rank 0, Epoch 420, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,602: INFO: model_training: Rank 0, Epoch 420, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,603: INFO: model_training: Rank 0, Epoch 420, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,605: INFO: model_training: Rank 0, Epoch 420, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,607: INFO: model_training: Rank 0, Epoch 420, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,608: INFO: model_training: Rank 0, Epoch 421, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,609: INFO: model_training: Rank 0, Epoch 421, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,610: INFO: model_training: Rank 0, Epoch 421, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,612: INFO: model_training: Rank 0, Epoch 421, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,613: INFO: model_training: Rank 0, Epoch 421, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,614: INFO: model_training: Rank 0, Epoch 422, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,616: INFO: model_training: Rank 0, Epoch 422, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,617: INFO: model_training: Rank 0, Epoch 422, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,618: INFO: model_training: Rank 0, Epoch 422, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,620: INFO: model_training: Rank 0, Epoch 422, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,621: INFO: model_training: Rank 0, Epoch 423, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,622: INFO: model_training: Rank 0, Epoch 423, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,624: INFO: model_training: Rank 0, Epoch 423, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,625: INFO: model_training: Rank 0, Epoch 423, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,627: INFO: model_training: Rank 0, Epoch 423, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,628: INFO: model_training: Rank 0, Epoch 424, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,630: INFO: model_training: Rank 0, Epoch 424, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,631: INFO: model_training: Rank 0, Epoch 424, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,633: INFO: model_training: Rank 0, Epoch 424, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,634: INFO: model_training: Rank 0, Epoch 424, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,636: INFO: model_training: Rank 0, Epoch 425, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,638: INFO: model_training: Rank 0, Epoch 425, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,639: INFO: model_training: Rank 0, Epoch 425, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,641: INFO: model_training: Rank 0, Epoch 425, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,642: INFO: model_training: Rank 0, Epoch 425, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,644: INFO: model_training: Rank 0, Epoch 426, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,646: INFO: model_training: Rank 0, Epoch 426, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,648: INFO: model_training: Rank 0, Epoch 426, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,649: INFO: model_training: Rank 0, Epoch 426, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,651: INFO: model_training: Rank 0, Epoch 426, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,653: INFO: model_training: Rank 0, Epoch 427, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,655: INFO: model_training: Rank 0, Epoch 427, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,656: INFO: model_training: Rank 0, Epoch 427, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,657: INFO: model_training: Rank 0, Epoch 427, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,659: INFO: model_training: Rank 0, Epoch 427, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,661: INFO: model_training: Rank 0, Epoch 428, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,663: INFO: model_training: Rank 0, Epoch 428, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,665: INFO: model_training: Rank 0, Epoch 428, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,667: INFO: model_training: Rank 0, Epoch 428, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,668: INFO: model_training: Rank 0, Epoch 428, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,669: INFO: model_training: Rank 0, Epoch 429, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,671: INFO: model_training: Rank 0, Epoch 429, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,673: INFO: model_training: Rank 0, Epoch 429, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,675: INFO: model_training: Rank 0, Epoch 429, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,676: INFO: model_training: Rank 0, Epoch 429, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,678: INFO: model_training: Rank 0, Epoch 430, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,679: INFO: model_training: Rank 0, Epoch 430, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,680: INFO: model_training: Rank 0, Epoch 430, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,681: INFO: model_training: Rank 0, Epoch 430, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,683: INFO: model_training: Rank 0, Epoch 430, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,684: INFO: model_training: Rank 0, Epoch 431, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,686: INFO: model_training: Rank 0, Epoch 431, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,687: INFO: model_training: Rank 0, Epoch 431, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,688: INFO: model_training: Rank 0, Epoch 431, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,691: INFO: model_training: Rank 0, Epoch 431, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,693: INFO: model_training: Rank 0, Epoch 432, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,694: INFO: model_training: Rank 0, Epoch 432, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,696: INFO: model_training: Rank 0, Epoch 432, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,697: INFO: model_training: Rank 0, Epoch 432, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,699: INFO: model_training: Rank 0, Epoch 432, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,701: INFO: model_training: Rank 0, Epoch 433, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,703: INFO: model_training: Rank 0, Epoch 433, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,704: INFO: model_training: Rank 0, Epoch 433, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,707: INFO: model_training: Rank 0, Epoch 433, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,708: INFO: model_training: Rank 0, Epoch 433, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,710: INFO: model_training: Rank 0, Epoch 434, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,712: INFO: model_training: Rank 0, Epoch 434, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,713: INFO: model_training: Rank 0, Epoch 434, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,715: INFO: model_training: Rank 0, Epoch 434, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,717: INFO: model_training: Rank 0, Epoch 434, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,719: INFO: model_training: Rank 0, Epoch 435, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,721: INFO: model_training: Rank 0, Epoch 435, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,723: INFO: model_training: Rank 0, Epoch 435, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,725: INFO: model_training: Rank 0, Epoch 435, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,726: INFO: model_training: Rank 0, Epoch 435, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,727: INFO: model_training: Rank 0, Epoch 436, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,729: INFO: model_training: Rank 0, Epoch 436, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,730: INFO: model_training: Rank 0, Epoch 436, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,733: INFO: model_training: Rank 0, Epoch 436, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,734: INFO: model_training: Rank 0, Epoch 436, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,735: INFO: model_training: Rank 0, Epoch 437, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,737: INFO: model_training: Rank 0, Epoch 437, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,739: INFO: model_training: Rank 0, Epoch 437, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,740: INFO: model_training: Rank 0, Epoch 437, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,742: INFO: model_training: Rank 0, Epoch 437, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,743: INFO: model_training: Rank 0, Epoch 438, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,745: INFO: model_training: Rank 0, Epoch 438, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,746: INFO: model_training: Rank 0, Epoch 438, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,747: INFO: model_training: Rank 0, Epoch 438, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,749: INFO: model_training: Rank 0, Epoch 438, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,751: INFO: model_training: Rank 0, Epoch 439, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,752: INFO: model_training: Rank 0, Epoch 439, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,754: INFO: model_training: Rank 0, Epoch 439, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,755: INFO: model_training: Rank 0, Epoch 439, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,756: INFO: model_training: Rank 0, Epoch 439, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,758: INFO: model_training: Rank 0, Epoch 440, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,759: INFO: model_training: Rank 0, Epoch 440, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,761: INFO: model_training: Rank 0, Epoch 440, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,763: INFO: model_training: Rank 0, Epoch 440, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,764: INFO: model_training: Rank 0, Epoch 440, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,766: INFO: model_training: Rank 0, Epoch 441, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,768: INFO: model_training: Rank 0, Epoch 441, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,770: INFO: model_training: Rank 0, Epoch 441, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,771: INFO: model_training: Rank 0, Epoch 441, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,773: INFO: model_training: Rank 0, Epoch 441, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,774: INFO: model_training: Rank 0, Epoch 442, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,776: INFO: model_training: Rank 0, Epoch 442, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,777: INFO: model_training: Rank 0, Epoch 442, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,778: INFO: model_training: Rank 0, Epoch 442, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,780: INFO: model_training: Rank 0, Epoch 442, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,782: INFO: model_training: Rank 0, Epoch 443, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,783: INFO: model_training: Rank 0, Epoch 443, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,785: INFO: model_training: Rank 0, Epoch 443, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,787: INFO: model_training: Rank 0, Epoch 443, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,788: INFO: model_training: Rank 0, Epoch 443, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,790: INFO: model_training: Rank 0, Epoch 444, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,791: INFO: model_training: Rank 0, Epoch 444, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,792: INFO: model_training: Rank 0, Epoch 444, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,794: INFO: model_training: Rank 0, Epoch 444, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,795: INFO: model_training: Rank 0, Epoch 444, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,797: INFO: model_training: Rank 0, Epoch 445, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,798: INFO: model_training: Rank 0, Epoch 445, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,799: INFO: model_training: Rank 0, Epoch 445, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,801: INFO: model_training: Rank 0, Epoch 445, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,802: INFO: model_training: Rank 0, Epoch 445, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,803: INFO: model_training: Rank 0, Epoch 446, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,805: INFO: model_training: Rank 0, Epoch 446, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,806: INFO: model_training: Rank 0, Epoch 446, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,807: INFO: model_training: Rank 0, Epoch 446, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,808: INFO: model_training: Rank 0, Epoch 446, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,810: INFO: model_training: Rank 0, Epoch 447, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,811: INFO: model_training: Rank 0, Epoch 447, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,812: INFO: model_training: Rank 0, Epoch 447, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,814: INFO: model_training: Rank 0, Epoch 447, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,815: INFO: model_training: Rank 0, Epoch 447, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,817: INFO: model_training: Rank 0, Epoch 448, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,818: INFO: model_training: Rank 0, Epoch 448, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,819: INFO: model_training: Rank 0, Epoch 448, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,820: INFO: model_training: Rank 0, Epoch 448, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,821: INFO: model_training: Rank 0, Epoch 448, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,823: INFO: model_training: Rank 0, Epoch 449, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,825: INFO: model_training: Rank 0, Epoch 449, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,826: INFO: model_training: Rank 0, Epoch 449, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,827: INFO: model_training: Rank 0, Epoch 449, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,829: INFO: model_training: Rank 0, Epoch 449, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,830: INFO: model_training: Rank 0, Epoch 450, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,832: INFO: model_training: Rank 0, Epoch 450, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,833: INFO: model_training: Rank 0, Epoch 450, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,834: INFO: model_training: Rank 0, Epoch 450, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,836: INFO: model_training: Rank 0, Epoch 450, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,837: INFO: model_training: Rank 0, Epoch 451, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,839: INFO: model_training: Rank 0, Epoch 451, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,840: INFO: model_training: Rank 0, Epoch 451, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,841: INFO: model_training: Rank 0, Epoch 451, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,843: INFO: model_training: Rank 0, Epoch 451, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,844: INFO: model_training: Rank 0, Epoch 452, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,845: INFO: model_training: Rank 0, Epoch 452, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,847: INFO: model_training: Rank 0, Epoch 452, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,849: INFO: model_training: Rank 0, Epoch 452, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,850: INFO: model_training: Rank 0, Epoch 452, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,852: INFO: model_training: Rank 0, Epoch 453, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,853: INFO: model_training: Rank 0, Epoch 453, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,855: INFO: model_training: Rank 0, Epoch 453, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,857: INFO: model_training: Rank 0, Epoch 453, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,859: INFO: model_training: Rank 0, Epoch 453, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,860: INFO: model_training: Rank 0, Epoch 454, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,862: INFO: model_training: Rank 0, Epoch 454, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,864: INFO: model_training: Rank 0, Epoch 454, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,866: INFO: model_training: Rank 0, Epoch 454, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,867: INFO: model_training: Rank 0, Epoch 454, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,870: INFO: model_training: Rank 0, Epoch 455, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,872: INFO: model_training: Rank 0, Epoch 455, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,874: INFO: model_training: Rank 0, Epoch 455, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,877: INFO: model_training: Rank 0, Epoch 455, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,879: INFO: model_training: Rank 0, Epoch 455, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,880: INFO: model_training: Rank 0, Epoch 456, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,883: INFO: model_training: Rank 0, Epoch 456, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,885: INFO: model_training: Rank 0, Epoch 456, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,887: INFO: model_training: Rank 0, Epoch 456, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,888: INFO: model_training: Rank 0, Epoch 456, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,890: INFO: model_training: Rank 0, Epoch 457, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,891: INFO: model_training: Rank 0, Epoch 457, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,892: INFO: model_training: Rank 0, Epoch 457, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,894: INFO: model_training: Rank 0, Epoch 457, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,895: INFO: model_training: Rank 0, Epoch 457, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,896: INFO: model_training: Rank 0, Epoch 458, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,898: INFO: model_training: Rank 0, Epoch 458, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,899: INFO: model_training: Rank 0, Epoch 458, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,901: INFO: model_training: Rank 0, Epoch 458, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,903: INFO: model_training: Rank 0, Epoch 458, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,906: INFO: model_training: Rank 0, Epoch 459, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,908: INFO: model_training: Rank 0, Epoch 459, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,909: INFO: model_training: Rank 0, Epoch 459, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,911: INFO: model_training: Rank 0, Epoch 459, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,912: INFO: model_training: Rank 0, Epoch 459, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,914: INFO: model_training: Rank 0, Epoch 460, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,915: INFO: model_training: Rank 0, Epoch 460, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,917: INFO: model_training: Rank 0, Epoch 460, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,918: INFO: model_training: Rank 0, Epoch 460, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,920: INFO: model_training: Rank 0, Epoch 460, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,922: INFO: model_training: Rank 0, Epoch 461, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,923: INFO: model_training: Rank 0, Epoch 461, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,925: INFO: model_training: Rank 0, Epoch 461, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,928: INFO: model_training: Rank 0, Epoch 461, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,930: INFO: model_training: Rank 0, Epoch 461, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,931: INFO: model_training: Rank 0, Epoch 462, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,933: INFO: model_training: Rank 0, Epoch 462, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,935: INFO: model_training: Rank 0, Epoch 462, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,937: INFO: model_training: Rank 0, Epoch 462, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,939: INFO: model_training: Rank 0, Epoch 462, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,941: INFO: model_training: Rank 0, Epoch 463, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,944: INFO: model_training: Rank 0, Epoch 463, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,946: INFO: model_training: Rank 0, Epoch 463, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,948: INFO: model_training: Rank 0, Epoch 463, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,949: INFO: model_training: Rank 0, Epoch 463, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,950: INFO: model_training: Rank 0, Epoch 464, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,952: INFO: model_training: Rank 0, Epoch 464, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,953: INFO: model_training: Rank 0, Epoch 464, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,954: INFO: model_training: Rank 0, Epoch 464, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,955: INFO: model_training: Rank 0, Epoch 464, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,956: INFO: model_training: Rank 0, Epoch 465, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,958: INFO: model_training: Rank 0, Epoch 465, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,959: INFO: model_training: Rank 0, Epoch 465, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,960: INFO: model_training: Rank 0, Epoch 465, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,961: INFO: model_training: Rank 0, Epoch 465, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,963: INFO: model_training: Rank 0, Epoch 466, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,964: INFO: model_training: Rank 0, Epoch 466, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,966: INFO: model_training: Rank 0, Epoch 466, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,967: INFO: model_training: Rank 0, Epoch 466, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,969: INFO: model_training: Rank 0, Epoch 466, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,970: INFO: model_training: Rank 0, Epoch 467, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,972: INFO: model_training: Rank 0, Epoch 467, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,974: INFO: model_training: Rank 0, Epoch 467, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,975: INFO: model_training: Rank 0, Epoch 467, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,977: INFO: model_training: Rank 0, Epoch 467, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,978: INFO: model_training: Rank 0, Epoch 468, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,980: INFO: model_training: Rank 0, Epoch 468, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,981: INFO: model_training: Rank 0, Epoch 468, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,982: INFO: model_training: Rank 0, Epoch 468, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,984: INFO: model_training: Rank 0, Epoch 468, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,985: INFO: model_training: Rank 0, Epoch 469, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,987: INFO: model_training: Rank 0, Epoch 469, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,988: INFO: model_training: Rank 0, Epoch 469, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,989: INFO: model_training: Rank 0, Epoch 469, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,990: INFO: model_training: Rank 0, Epoch 469, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:27,992: INFO: model_training: Rank 0, Epoch 470, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:27,993: INFO: model_training: Rank 0, Epoch 470, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:27,995: INFO: model_training: Rank 0, Epoch 470, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:27,997: INFO: model_training: Rank 0, Epoch 470, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:27,998: INFO: model_training: Rank 0, Epoch 470, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,000: INFO: model_training: Rank 0, Epoch 471, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,002: INFO: model_training: Rank 0, Epoch 471, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,003: INFO: model_training: Rank 0, Epoch 471, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,004: INFO: model_training: Rank 0, Epoch 471, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,007: INFO: model_training: Rank 0, Epoch 471, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,008: INFO: model_training: Rank 0, Epoch 472, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,010: INFO: model_training: Rank 0, Epoch 472, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,012: INFO: model_training: Rank 0, Epoch 472, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,013: INFO: model_training: Rank 0, Epoch 472, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,015: INFO: model_training: Rank 0, Epoch 472, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,017: INFO: model_training: Rank 0, Epoch 473, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,019: INFO: model_training: Rank 0, Epoch 473, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,020: INFO: model_training: Rank 0, Epoch 473, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,022: INFO: model_training: Rank 0, Epoch 473, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,024: INFO: model_training: Rank 0, Epoch 473, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,025: INFO: model_training: Rank 0, Epoch 474, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,027: INFO: model_training: Rank 0, Epoch 474, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,029: INFO: model_training: Rank 0, Epoch 474, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,030: INFO: model_training: Rank 0, Epoch 474, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,031: INFO: model_training: Rank 0, Epoch 474, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,032: INFO: model_training: Rank 0, Epoch 475, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,034: INFO: model_training: Rank 0, Epoch 475, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,035: INFO: model_training: Rank 0, Epoch 475, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,037: INFO: model_training: Rank 0, Epoch 475, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,038: INFO: model_training: Rank 0, Epoch 475, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,040: INFO: model_training: Rank 0, Epoch 476, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,042: INFO: model_training: Rank 0, Epoch 476, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,043: INFO: model_training: Rank 0, Epoch 476, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,044: INFO: model_training: Rank 0, Epoch 476, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,046: INFO: model_training: Rank 0, Epoch 476, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,047: INFO: model_training: Rank 0, Epoch 477, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,049: INFO: model_training: Rank 0, Epoch 477, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,050: INFO: model_training: Rank 0, Epoch 477, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,051: INFO: model_training: Rank 0, Epoch 477, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,053: INFO: model_training: Rank 0, Epoch 477, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,054: INFO: model_training: Rank 0, Epoch 478, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,056: INFO: model_training: Rank 0, Epoch 478, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,058: INFO: model_training: Rank 0, Epoch 478, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,060: INFO: model_training: Rank 0, Epoch 478, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,062: INFO: model_training: Rank 0, Epoch 478, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,064: INFO: model_training: Rank 0, Epoch 479, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,065: INFO: model_training: Rank 0, Epoch 479, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,067: INFO: model_training: Rank 0, Epoch 479, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,069: INFO: model_training: Rank 0, Epoch 479, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,070: INFO: model_training: Rank 0, Epoch 479, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,072: INFO: model_training: Rank 0, Epoch 480, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,074: INFO: model_training: Rank 0, Epoch 480, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,075: INFO: model_training: Rank 0, Epoch 480, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,077: INFO: model_training: Rank 0, Epoch 480, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,079: INFO: model_training: Rank 0, Epoch 480, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,082: INFO: model_training: Rank 0, Epoch 481, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,084: INFO: model_training: Rank 0, Epoch 481, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,085: INFO: model_training: Rank 0, Epoch 481, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,087: INFO: model_training: Rank 0, Epoch 481, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,089: INFO: model_training: Rank 0, Epoch 481, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,090: INFO: model_training: Rank 0, Epoch 482, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,091: INFO: model_training: Rank 0, Epoch 482, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,093: INFO: model_training: Rank 0, Epoch 482, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,094: INFO: model_training: Rank 0, Epoch 482, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,096: INFO: model_training: Rank 0, Epoch 482, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,099: INFO: model_training: Rank 0, Epoch 483, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,101: INFO: model_training: Rank 0, Epoch 483, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,102: INFO: model_training: Rank 0, Epoch 483, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,104: INFO: model_training: Rank 0, Epoch 483, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,105: INFO: model_training: Rank 0, Epoch 483, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,107: INFO: model_training: Rank 0, Epoch 484, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,108: INFO: model_training: Rank 0, Epoch 484, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,111: INFO: model_training: Rank 0, Epoch 484, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,113: INFO: model_training: Rank 0, Epoch 484, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,115: INFO: model_training: Rank 0, Epoch 484, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,117: INFO: model_training: Rank 0, Epoch 485, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,118: INFO: model_training: Rank 0, Epoch 485, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,121: INFO: model_training: Rank 0, Epoch 485, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,123: INFO: model_training: Rank 0, Epoch 485, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,125: INFO: model_training: Rank 0, Epoch 485, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,126: INFO: model_training: Rank 0, Epoch 486, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,128: INFO: model_training: Rank 0, Epoch 486, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,130: INFO: model_training: Rank 0, Epoch 486, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,132: INFO: model_training: Rank 0, Epoch 486, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,133: INFO: model_training: Rank 0, Epoch 486, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,135: INFO: model_training: Rank 0, Epoch 487, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,137: INFO: model_training: Rank 0, Epoch 487, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,138: INFO: model_training: Rank 0, Epoch 487, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,140: INFO: model_training: Rank 0, Epoch 487, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,141: INFO: model_training: Rank 0, Epoch 487, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,142: INFO: model_training: Rank 0, Epoch 488, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,144: INFO: model_training: Rank 0, Epoch 488, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,146: INFO: model_training: Rank 0, Epoch 488, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,148: INFO: model_training: Rank 0, Epoch 488, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,150: INFO: model_training: Rank 0, Epoch 488, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,152: INFO: model_training: Rank 0, Epoch 489, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,153: INFO: model_training: Rank 0, Epoch 489, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,155: INFO: model_training: Rank 0, Epoch 489, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,156: INFO: model_training: Rank 0, Epoch 489, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,158: INFO: model_training: Rank 0, Epoch 489, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,159: INFO: model_training: Rank 0, Epoch 490, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,160: INFO: model_training: Rank 0, Epoch 490, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,162: INFO: model_training: Rank 0, Epoch 490, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,164: INFO: model_training: Rank 0, Epoch 490, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,166: INFO: model_training: Rank 0, Epoch 490, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,168: INFO: model_training: Rank 0, Epoch 491, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,170: INFO: model_training: Rank 0, Epoch 491, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,171: INFO: model_training: Rank 0, Epoch 491, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,172: INFO: model_training: Rank 0, Epoch 491, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,174: INFO: model_training: Rank 0, Epoch 491, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,176: INFO: model_training: Rank 0, Epoch 492, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,178: INFO: model_training: Rank 0, Epoch 492, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,180: INFO: model_training: Rank 0, Epoch 492, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,181: INFO: model_training: Rank 0, Epoch 492, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,183: INFO: model_training: Rank 0, Epoch 492, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,184: INFO: model_training: Rank 0, Epoch 493, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,186: INFO: model_training: Rank 0, Epoch 493, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,188: INFO: model_training: Rank 0, Epoch 493, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,189: INFO: model_training: Rank 0, Epoch 493, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,190: INFO: model_training: Rank 0, Epoch 493, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,192: INFO: model_training: Rank 0, Epoch 494, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,195: INFO: model_training: Rank 0, Epoch 494, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,196: INFO: model_training: Rank 0, Epoch 494, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,198: INFO: model_training: Rank 0, Epoch 494, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,199: INFO: model_training: Rank 0, Epoch 494, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,202: INFO: model_training: Rank 0, Epoch 495, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,204: INFO: model_training: Rank 0, Epoch 495, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,206: INFO: model_training: Rank 0, Epoch 495, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,207: INFO: model_training: Rank 0, Epoch 495, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,209: INFO: model_training: Rank 0, Epoch 495, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,211: INFO: model_training: Rank 0, Epoch 496, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,212: INFO: model_training: Rank 0, Epoch 496, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,214: INFO: model_training: Rank 0, Epoch 496, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,215: INFO: model_training: Rank 0, Epoch 496, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,216: INFO: model_training: Rank 0, Epoch 496, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,218: INFO: model_training: Rank 0, Epoch 497, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,219: INFO: model_training: Rank 0, Epoch 497, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,220: INFO: model_training: Rank 0, Epoch 497, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,222: INFO: model_training: Rank 0, Epoch 497, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,224: INFO: model_training: Rank 0, Epoch 497, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,226: INFO: model_training: Rank 0, Epoch 498, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,227: INFO: model_training: Rank 0, Epoch 498, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,229: INFO: model_training: Rank 0, Epoch 498, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,230: INFO: model_training: Rank 0, Epoch 498, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,232: INFO: model_training: Rank 0, Epoch 498, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,233: INFO: model_training: Rank 0, Epoch 499, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,235: INFO: model_training: Rank 0, Epoch 499, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,237: INFO: model_training: Rank 0, Epoch 499, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,239: INFO: model_training: Rank 0, Epoch 499, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,240: INFO: model_training: Rank 0, Epoch 499, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,242: INFO: model_training: Rank 0, Epoch 500, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,244: INFO: model_training: Rank 0, Epoch 500, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,246: INFO: model_training: Rank 0, Epoch 500, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,248: INFO: model_training: Rank 0, Epoch 500, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,250: INFO: model_training: Rank 0, Epoch 500, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,252: INFO: model_training: Rank 0, Epoch 501, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,254: INFO: model_training: Rank 0, Epoch 501, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,256: INFO: model_training: Rank 0, Epoch 501, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,257: INFO: model_training: Rank 0, Epoch 501, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,259: INFO: model_training: Rank 0, Epoch 501, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,261: INFO: model_training: Rank 0, Epoch 502, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,263: INFO: model_training: Rank 0, Epoch 502, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,264: INFO: model_training: Rank 0, Epoch 502, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,266: INFO: model_training: Rank 0, Epoch 502, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,268: INFO: model_training: Rank 0, Epoch 502, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,269: INFO: model_training: Rank 0, Epoch 503, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,271: INFO: model_training: Rank 0, Epoch 503, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,272: INFO: model_training: Rank 0, Epoch 503, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,274: INFO: model_training: Rank 0, Epoch 503, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,275: INFO: model_training: Rank 0, Epoch 503, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,277: INFO: model_training: Rank 0, Epoch 504, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,279: INFO: model_training: Rank 0, Epoch 504, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,281: INFO: model_training: Rank 0, Epoch 504, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,283: INFO: model_training: Rank 0, Epoch 504, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,284: INFO: model_training: Rank 0, Epoch 504, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,286: INFO: model_training: Rank 0, Epoch 505, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,288: INFO: model_training: Rank 0, Epoch 505, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,289: INFO: model_training: Rank 0, Epoch 505, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,291: INFO: model_training: Rank 0, Epoch 505, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,293: INFO: model_training: Rank 0, Epoch 505, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,295: INFO: model_training: Rank 0, Epoch 506, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,297: INFO: model_training: Rank 0, Epoch 506, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,299: INFO: model_training: Rank 0, Epoch 506, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,303: INFO: model_training: Rank 0, Epoch 506, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,305: INFO: model_training: Rank 0, Epoch 506, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,307: INFO: model_training: Rank 0, Epoch 507, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,310: INFO: model_training: Rank 0, Epoch 507, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,312: INFO: model_training: Rank 0, Epoch 507, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,313: INFO: model_training: Rank 0, Epoch 507, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,315: INFO: model_training: Rank 0, Epoch 507, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,317: INFO: model_training: Rank 0, Epoch 508, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,318: INFO: model_training: Rank 0, Epoch 508, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,319: INFO: model_training: Rank 0, Epoch 508, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,321: INFO: model_training: Rank 0, Epoch 508, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,322: INFO: model_training: Rank 0, Epoch 508, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,323: INFO: model_training: Rank 0, Epoch 509, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,325: INFO: model_training: Rank 0, Epoch 509, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,326: INFO: model_training: Rank 0, Epoch 509, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,328: INFO: model_training: Rank 0, Epoch 509, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,329: INFO: model_training: Rank 0, Epoch 509, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,331: INFO: model_training: Rank 0, Epoch 510, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,333: INFO: model_training: Rank 0, Epoch 510, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,335: INFO: model_training: Rank 0, Epoch 510, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,336: INFO: model_training: Rank 0, Epoch 510, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,338: INFO: model_training: Rank 0, Epoch 510, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,339: INFO: model_training: Rank 0, Epoch 511, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,340: INFO: model_training: Rank 0, Epoch 511, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,342: INFO: model_training: Rank 0, Epoch 511, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,343: INFO: model_training: Rank 0, Epoch 511, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,345: INFO: model_training: Rank 0, Epoch 511, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,346: INFO: model_training: Rank 0, Epoch 512, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,348: INFO: model_training: Rank 0, Epoch 512, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,350: INFO: model_training: Rank 0, Epoch 512, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,351: INFO: model_training: Rank 0, Epoch 512, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,353: INFO: model_training: Rank 0, Epoch 512, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,355: INFO: model_training: Rank 0, Epoch 513, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,356: INFO: model_training: Rank 0, Epoch 513, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,358: INFO: model_training: Rank 0, Epoch 513, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,359: INFO: model_training: Rank 0, Epoch 513, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,360: INFO: model_training: Rank 0, Epoch 513, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,362: INFO: model_training: Rank 0, Epoch 514, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,364: INFO: model_training: Rank 0, Epoch 514, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,366: INFO: model_training: Rank 0, Epoch 514, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,367: INFO: model_training: Rank 0, Epoch 514, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,368: INFO: model_training: Rank 0, Epoch 514, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,369: INFO: model_training: Rank 0, Epoch 515, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,371: INFO: model_training: Rank 0, Epoch 515, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,373: INFO: model_training: Rank 0, Epoch 515, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,375: INFO: model_training: Rank 0, Epoch 515, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,377: INFO: model_training: Rank 0, Epoch 515, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,378: INFO: model_training: Rank 0, Epoch 516, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,380: INFO: model_training: Rank 0, Epoch 516, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,382: INFO: model_training: Rank 0, Epoch 516, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,384: INFO: model_training: Rank 0, Epoch 516, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,386: INFO: model_training: Rank 0, Epoch 516, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,388: INFO: model_training: Rank 0, Epoch 517, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,389: INFO: model_training: Rank 0, Epoch 517, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,390: INFO: model_training: Rank 0, Epoch 517, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,393: INFO: model_training: Rank 0, Epoch 517, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,394: INFO: model_training: Rank 0, Epoch 517, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,395: INFO: model_training: Rank 0, Epoch 518, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,397: INFO: model_training: Rank 0, Epoch 518, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,398: INFO: model_training: Rank 0, Epoch 518, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,400: INFO: model_training: Rank 0, Epoch 518, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,401: INFO: model_training: Rank 0, Epoch 518, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,403: INFO: model_training: Rank 0, Epoch 519, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,404: INFO: model_training: Rank 0, Epoch 519, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,406: INFO: model_training: Rank 0, Epoch 519, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,408: INFO: model_training: Rank 0, Epoch 519, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,409: INFO: model_training: Rank 0, Epoch 519, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,411: INFO: model_training: Rank 0, Epoch 520, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,413: INFO: model_training: Rank 0, Epoch 520, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,414: INFO: model_training: Rank 0, Epoch 520, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,415: INFO: model_training: Rank 0, Epoch 520, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,417: INFO: model_training: Rank 0, Epoch 520, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,418: INFO: model_training: Rank 0, Epoch 521, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,419: INFO: model_training: Rank 0, Epoch 521, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,421: INFO: model_training: Rank 0, Epoch 521, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,423: INFO: model_training: Rank 0, Epoch 521, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,424: INFO: model_training: Rank 0, Epoch 521, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,426: INFO: model_training: Rank 0, Epoch 522, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,427: INFO: model_training: Rank 0, Epoch 522, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,428: INFO: model_training: Rank 0, Epoch 522, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,431: INFO: model_training: Rank 0, Epoch 522, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,433: INFO: model_training: Rank 0, Epoch 522, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,435: INFO: model_training: Rank 0, Epoch 523, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,437: INFO: model_training: Rank 0, Epoch 523, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,438: INFO: model_training: Rank 0, Epoch 523, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,440: INFO: model_training: Rank 0, Epoch 523, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,441: INFO: model_training: Rank 0, Epoch 523, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,443: INFO: model_training: Rank 0, Epoch 524, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,445: INFO: model_training: Rank 0, Epoch 524, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,448: INFO: model_training: Rank 0, Epoch 524, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,449: INFO: model_training: Rank 0, Epoch 524, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,450: INFO: model_training: Rank 0, Epoch 524, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,452: INFO: model_training: Rank 0, Epoch 525, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,453: INFO: model_training: Rank 0, Epoch 525, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,454: INFO: model_training: Rank 0, Epoch 525, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,456: INFO: model_training: Rank 0, Epoch 525, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,457: INFO: model_training: Rank 0, Epoch 525, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,458: INFO: model_training: Rank 0, Epoch 526, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,459: INFO: model_training: Rank 0, Epoch 526, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,460: INFO: model_training: Rank 0, Epoch 526, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,461: INFO: model_training: Rank 0, Epoch 526, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,463: INFO: model_training: Rank 0, Epoch 526, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,464: INFO: model_training: Rank 0, Epoch 527, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,465: INFO: model_training: Rank 0, Epoch 527, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,466: INFO: model_training: Rank 0, Epoch 527, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,467: INFO: model_training: Rank 0, Epoch 527, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,468: INFO: model_training: Rank 0, Epoch 527, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,470: INFO: model_training: Rank 0, Epoch 528, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,471: INFO: model_training: Rank 0, Epoch 528, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,472: INFO: model_training: Rank 0, Epoch 528, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,473: INFO: model_training: Rank 0, Epoch 528, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,474: INFO: model_training: Rank 0, Epoch 528, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,475: INFO: model_training: Rank 0, Epoch 529, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,476: INFO: model_training: Rank 0, Epoch 529, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,478: INFO: model_training: Rank 0, Epoch 529, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,479: INFO: model_training: Rank 0, Epoch 529, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,480: INFO: model_training: Rank 0, Epoch 529, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,482: INFO: model_training: Rank 0, Epoch 530, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,483: INFO: model_training: Rank 0, Epoch 530, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,484: INFO: model_training: Rank 0, Epoch 530, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,485: INFO: model_training: Rank 0, Epoch 530, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,487: INFO: model_training: Rank 0, Epoch 530, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,488: INFO: model_training: Rank 0, Epoch 531, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,489: INFO: model_training: Rank 0, Epoch 531, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,490: INFO: model_training: Rank 0, Epoch 531, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,492: INFO: model_training: Rank 0, Epoch 531, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,493: INFO: model_training: Rank 0, Epoch 531, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,495: INFO: model_training: Rank 0, Epoch 532, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,496: INFO: model_training: Rank 0, Epoch 532, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,497: INFO: model_training: Rank 0, Epoch 532, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,498: INFO: model_training: Rank 0, Epoch 532, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,500: INFO: model_training: Rank 0, Epoch 532, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,501: INFO: model_training: Rank 0, Epoch 533, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,502: INFO: model_training: Rank 0, Epoch 533, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,503: INFO: model_training: Rank 0, Epoch 533, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,505: INFO: model_training: Rank 0, Epoch 533, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,507: INFO: model_training: Rank 0, Epoch 533, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,508: INFO: model_training: Rank 0, Epoch 534, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,509: INFO: model_training: Rank 0, Epoch 534, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,510: INFO: model_training: Rank 0, Epoch 534, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,512: INFO: model_training: Rank 0, Epoch 534, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,514: INFO: model_training: Rank 0, Epoch 534, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,515: INFO: model_training: Rank 0, Epoch 535, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,517: INFO: model_training: Rank 0, Epoch 535, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,518: INFO: model_training: Rank 0, Epoch 535, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,520: INFO: model_training: Rank 0, Epoch 535, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,522: INFO: model_training: Rank 0, Epoch 535, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,523: INFO: model_training: Rank 0, Epoch 536, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,525: INFO: model_training: Rank 0, Epoch 536, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,526: INFO: model_training: Rank 0, Epoch 536, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,528: INFO: model_training: Rank 0, Epoch 536, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,530: INFO: model_training: Rank 0, Epoch 536, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,531: INFO: model_training: Rank 0, Epoch 537, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,534: INFO: model_training: Rank 0, Epoch 537, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,535: INFO: model_training: Rank 0, Epoch 537, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,537: INFO: model_training: Rank 0, Epoch 537, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,538: INFO: model_training: Rank 0, Epoch 537, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,540: INFO: model_training: Rank 0, Epoch 538, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,541: INFO: model_training: Rank 0, Epoch 538, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,542: INFO: model_training: Rank 0, Epoch 538, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,544: INFO: model_training: Rank 0, Epoch 538, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,546: INFO: model_training: Rank 0, Epoch 538, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,548: INFO: model_training: Rank 0, Epoch 539, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,550: INFO: model_training: Rank 0, Epoch 539, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,551: INFO: model_training: Rank 0, Epoch 539, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,553: INFO: model_training: Rank 0, Epoch 539, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,554: INFO: model_training: Rank 0, Epoch 539, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,555: INFO: model_training: Rank 0, Epoch 540, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,557: INFO: model_training: Rank 0, Epoch 540, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,558: INFO: model_training: Rank 0, Epoch 540, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,559: INFO: model_training: Rank 0, Epoch 540, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,560: INFO: model_training: Rank 0, Epoch 540, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,561: INFO: model_training: Rank 0, Epoch 541, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,561: INFO: model_training: Rank 0, Epoch 541, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,563: INFO: model_training: Rank 0, Epoch 541, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,564: INFO: model_training: Rank 0, Epoch 541, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,565: INFO: model_training: Rank 0, Epoch 541, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,566: INFO: model_training: Rank 0, Epoch 542, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,567: INFO: model_training: Rank 0, Epoch 542, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,568: INFO: model_training: Rank 0, Epoch 542, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,569: INFO: model_training: Rank 0, Epoch 542, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,570: INFO: model_training: Rank 0, Epoch 542, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,571: INFO: model_training: Rank 0, Epoch 543, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,573: INFO: model_training: Rank 0, Epoch 543, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,574: INFO: model_training: Rank 0, Epoch 543, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,575: INFO: model_training: Rank 0, Epoch 543, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,576: INFO: model_training: Rank 0, Epoch 543, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,577: INFO: model_training: Rank 0, Epoch 544, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,578: INFO: model_training: Rank 0, Epoch 544, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,579: INFO: model_training: Rank 0, Epoch 544, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,580: INFO: model_training: Rank 0, Epoch 544, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,582: INFO: model_training: Rank 0, Epoch 544, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,583: INFO: model_training: Rank 0, Epoch 545, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,584: INFO: model_training: Rank 0, Epoch 545, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,585: INFO: model_training: Rank 0, Epoch 545, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,586: INFO: model_training: Rank 0, Epoch 545, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,587: INFO: model_training: Rank 0, Epoch 545, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,588: INFO: model_training: Rank 0, Epoch 546, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,589: INFO: model_training: Rank 0, Epoch 546, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,590: INFO: model_training: Rank 0, Epoch 546, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,591: INFO: model_training: Rank 0, Epoch 546, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,592: INFO: model_training: Rank 0, Epoch 546, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,593: INFO: model_training: Rank 0, Epoch 547, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,594: INFO: model_training: Rank 0, Epoch 547, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,595: INFO: model_training: Rank 0, Epoch 547, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,596: INFO: model_training: Rank 0, Epoch 547, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,597: INFO: model_training: Rank 0, Epoch 547, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,598: INFO: model_training: Rank 0, Epoch 548, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,599: INFO: model_training: Rank 0, Epoch 548, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,601: INFO: model_training: Rank 0, Epoch 548, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,602: INFO: model_training: Rank 0, Epoch 548, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,603: INFO: model_training: Rank 0, Epoch 548, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,605: INFO: model_training: Rank 0, Epoch 549, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,606: INFO: model_training: Rank 0, Epoch 549, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,607: INFO: model_training: Rank 0, Epoch 549, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,608: INFO: model_training: Rank 0, Epoch 549, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,609: INFO: model_training: Rank 0, Epoch 549, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,610: INFO: model_training: Rank 0, Epoch 550, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,611: INFO: model_training: Rank 0, Epoch 550, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,612: INFO: model_training: Rank 0, Epoch 550, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,613: INFO: model_training: Rank 0, Epoch 550, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,614: INFO: model_training: Rank 0, Epoch 550, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,615: INFO: model_training: Rank 0, Epoch 551, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,616: INFO: model_training: Rank 0, Epoch 551, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,617: INFO: model_training: Rank 0, Epoch 551, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,618: INFO: model_training: Rank 0, Epoch 551, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,619: INFO: model_training: Rank 0, Epoch 551, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,620: INFO: model_training: Rank 0, Epoch 552, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,621: INFO: model_training: Rank 0, Epoch 552, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,622: INFO: model_training: Rank 0, Epoch 552, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,623: INFO: model_training: Rank 0, Epoch 552, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,624: INFO: model_training: Rank 0, Epoch 552, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,625: INFO: model_training: Rank 0, Epoch 553, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,626: INFO: model_training: Rank 0, Epoch 553, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,627: INFO: model_training: Rank 0, Epoch 553, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,629: INFO: model_training: Rank 0, Epoch 553, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,630: INFO: model_training: Rank 0, Epoch 553, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,631: INFO: model_training: Rank 0, Epoch 554, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,632: INFO: model_training: Rank 0, Epoch 554, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,633: INFO: model_training: Rank 0, Epoch 554, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,634: INFO: model_training: Rank 0, Epoch 554, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,635: INFO: model_training: Rank 0, Epoch 554, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,636: INFO: model_training: Rank 0, Epoch 555, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,637: INFO: model_training: Rank 0, Epoch 555, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,639: INFO: model_training: Rank 0, Epoch 555, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,640: INFO: model_training: Rank 0, Epoch 555, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,641: INFO: model_training: Rank 0, Epoch 555, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,642: INFO: model_training: Rank 0, Epoch 556, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,643: INFO: model_training: Rank 0, Epoch 556, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,644: INFO: model_training: Rank 0, Epoch 556, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,645: INFO: model_training: Rank 0, Epoch 556, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,646: INFO: model_training: Rank 0, Epoch 556, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,647: INFO: model_training: Rank 0, Epoch 557, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,648: INFO: model_training: Rank 0, Epoch 557, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,649: INFO: model_training: Rank 0, Epoch 557, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,650: INFO: model_training: Rank 0, Epoch 557, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,651: INFO: model_training: Rank 0, Epoch 557, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,652: INFO: model_training: Rank 0, Epoch 558, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,654: INFO: model_training: Rank 0, Epoch 558, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,655: INFO: model_training: Rank 0, Epoch 558, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,657: INFO: model_training: Rank 0, Epoch 558, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,658: INFO: model_training: Rank 0, Epoch 558, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,658: INFO: model_training: Rank 0, Epoch 559, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,660: INFO: model_training: Rank 0, Epoch 559, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,661: INFO: model_training: Rank 0, Epoch 559, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,662: INFO: model_training: Rank 0, Epoch 559, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,663: INFO: model_training: Rank 0, Epoch 559, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,664: INFO: model_training: Rank 0, Epoch 560, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,665: INFO: model_training: Rank 0, Epoch 560, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,666: INFO: model_training: Rank 0, Epoch 560, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,668: INFO: model_training: Rank 0, Epoch 560, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,669: INFO: model_training: Rank 0, Epoch 560, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,670: INFO: model_training: Rank 0, Epoch 561, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,671: INFO: model_training: Rank 0, Epoch 561, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,672: INFO: model_training: Rank 0, Epoch 561, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,673: INFO: model_training: Rank 0, Epoch 561, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,674: INFO: model_training: Rank 0, Epoch 561, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,675: INFO: model_training: Rank 0, Epoch 562, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,677: INFO: model_training: Rank 0, Epoch 562, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,678: INFO: model_training: Rank 0, Epoch 562, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,679: INFO: model_training: Rank 0, Epoch 562, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,680: INFO: model_training: Rank 0, Epoch 562, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,681: INFO: model_training: Rank 0, Epoch 563, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,682: INFO: model_training: Rank 0, Epoch 563, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,684: INFO: model_training: Rank 0, Epoch 563, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,685: INFO: model_training: Rank 0, Epoch 563, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,687: INFO: model_training: Rank 0, Epoch 563, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,688: INFO: model_training: Rank 0, Epoch 564, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,691: INFO: model_training: Rank 0, Epoch 564, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,692: INFO: model_training: Rank 0, Epoch 564, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,693: INFO: model_training: Rank 0, Epoch 564, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,695: INFO: model_training: Rank 0, Epoch 564, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,696: INFO: model_training: Rank 0, Epoch 565, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,697: INFO: model_training: Rank 0, Epoch 565, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,698: INFO: model_training: Rank 0, Epoch 565, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,699: INFO: model_training: Rank 0, Epoch 565, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,700: INFO: model_training: Rank 0, Epoch 565, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,701: INFO: model_training: Rank 0, Epoch 566, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,702: INFO: model_training: Rank 0, Epoch 566, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,704: INFO: model_training: Rank 0, Epoch 566, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,705: INFO: model_training: Rank 0, Epoch 566, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,706: INFO: model_training: Rank 0, Epoch 566, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,707: INFO: model_training: Rank 0, Epoch 567, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,708: INFO: model_training: Rank 0, Epoch 567, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,709: INFO: model_training: Rank 0, Epoch 567, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,710: INFO: model_training: Rank 0, Epoch 567, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,712: INFO: model_training: Rank 0, Epoch 567, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,713: INFO: model_training: Rank 0, Epoch 568, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,714: INFO: model_training: Rank 0, Epoch 568, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,715: INFO: model_training: Rank 0, Epoch 568, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,717: INFO: model_training: Rank 0, Epoch 568, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,718: INFO: model_training: Rank 0, Epoch 568, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,719: INFO: model_training: Rank 0, Epoch 569, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,720: INFO: model_training: Rank 0, Epoch 569, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,721: INFO: model_training: Rank 0, Epoch 569, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,723: INFO: model_training: Rank 0, Epoch 569, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,724: INFO: model_training: Rank 0, Epoch 569, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,725: INFO: model_training: Rank 0, Epoch 570, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,726: INFO: model_training: Rank 0, Epoch 570, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,727: INFO: model_training: Rank 0, Epoch 570, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,728: INFO: model_training: Rank 0, Epoch 570, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,729: INFO: model_training: Rank 0, Epoch 570, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,731: INFO: model_training: Rank 0, Epoch 571, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,733: INFO: model_training: Rank 0, Epoch 571, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,734: INFO: model_training: Rank 0, Epoch 571, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,736: INFO: model_training: Rank 0, Epoch 571, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,737: INFO: model_training: Rank 0, Epoch 571, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,738: INFO: model_training: Rank 0, Epoch 572, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,740: INFO: model_training: Rank 0, Epoch 572, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,741: INFO: model_training: Rank 0, Epoch 572, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,742: INFO: model_training: Rank 0, Epoch 572, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,743: INFO: model_training: Rank 0, Epoch 572, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,744: INFO: model_training: Rank 0, Epoch 573, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,745: INFO: model_training: Rank 0, Epoch 573, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,747: INFO: model_training: Rank 0, Epoch 573, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,748: INFO: model_training: Rank 0, Epoch 573, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,749: INFO: model_training: Rank 0, Epoch 573, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,750: INFO: model_training: Rank 0, Epoch 574, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,751: INFO: model_training: Rank 0, Epoch 574, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,752: INFO: model_training: Rank 0, Epoch 574, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,753: INFO: model_training: Rank 0, Epoch 574, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,754: INFO: model_training: Rank 0, Epoch 574, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,756: INFO: model_training: Rank 0, Epoch 575, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,757: INFO: model_training: Rank 0, Epoch 575, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,759: INFO: model_training: Rank 0, Epoch 575, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,760: INFO: model_training: Rank 0, Epoch 575, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,761: INFO: model_training: Rank 0, Epoch 575, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,763: INFO: model_training: Rank 0, Epoch 576, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,765: INFO: model_training: Rank 0, Epoch 576, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,766: INFO: model_training: Rank 0, Epoch 576, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,767: INFO: model_training: Rank 0, Epoch 576, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,769: INFO: model_training: Rank 0, Epoch 576, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,770: INFO: model_training: Rank 0, Epoch 577, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,771: INFO: model_training: Rank 0, Epoch 577, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,773: INFO: model_training: Rank 0, Epoch 577, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,774: INFO: model_training: Rank 0, Epoch 577, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,776: INFO: model_training: Rank 0, Epoch 577, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,777: INFO: model_training: Rank 0, Epoch 578, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,779: INFO: model_training: Rank 0, Epoch 578, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,781: INFO: model_training: Rank 0, Epoch 578, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,782: INFO: model_training: Rank 0, Epoch 578, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,784: INFO: model_training: Rank 0, Epoch 578, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,785: INFO: model_training: Rank 0, Epoch 579, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,787: INFO: model_training: Rank 0, Epoch 579, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,788: INFO: model_training: Rank 0, Epoch 579, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,789: INFO: model_training: Rank 0, Epoch 579, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,791: INFO: model_training: Rank 0, Epoch 579, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,792: INFO: model_training: Rank 0, Epoch 580, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,794: INFO: model_training: Rank 0, Epoch 580, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,795: INFO: model_training: Rank 0, Epoch 580, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,796: INFO: model_training: Rank 0, Epoch 580, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,798: INFO: model_training: Rank 0, Epoch 580, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,799: INFO: model_training: Rank 0, Epoch 581, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,800: INFO: model_training: Rank 0, Epoch 581, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,802: INFO: model_training: Rank 0, Epoch 581, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,803: INFO: model_training: Rank 0, Epoch 581, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,805: INFO: model_training: Rank 0, Epoch 581, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,806: INFO: model_training: Rank 0, Epoch 582, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,807: INFO: model_training: Rank 0, Epoch 582, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,808: INFO: model_training: Rank 0, Epoch 582, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,809: INFO: model_training: Rank 0, Epoch 582, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,810: INFO: model_training: Rank 0, Epoch 582, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,811: INFO: model_training: Rank 0, Epoch 583, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,813: INFO: model_training: Rank 0, Epoch 583, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,814: INFO: model_training: Rank 0, Epoch 583, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,815: INFO: model_training: Rank 0, Epoch 583, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,816: INFO: model_training: Rank 0, Epoch 583, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,817: INFO: model_training: Rank 0, Epoch 584, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,818: INFO: model_training: Rank 0, Epoch 584, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,819: INFO: model_training: Rank 0, Epoch 584, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,820: INFO: model_training: Rank 0, Epoch 584, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,821: INFO: model_training: Rank 0, Epoch 584, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,822: INFO: model_training: Rank 0, Epoch 585, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,824: INFO: model_training: Rank 0, Epoch 585, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,825: INFO: model_training: Rank 0, Epoch 585, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,826: INFO: model_training: Rank 0, Epoch 585, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,827: INFO: model_training: Rank 0, Epoch 585, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,828: INFO: model_training: Rank 0, Epoch 586, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,829: INFO: model_training: Rank 0, Epoch 586, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,830: INFO: model_training: Rank 0, Epoch 586, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,831: INFO: model_training: Rank 0, Epoch 586, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,832: INFO: model_training: Rank 0, Epoch 586, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,834: INFO: model_training: Rank 0, Epoch 587, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,835: INFO: model_training: Rank 0, Epoch 587, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,837: INFO: model_training: Rank 0, Epoch 587, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,838: INFO: model_training: Rank 0, Epoch 587, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,840: INFO: model_training: Rank 0, Epoch 587, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,841: INFO: model_training: Rank 0, Epoch 588, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,842: INFO: model_training: Rank 0, Epoch 588, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,844: INFO: model_training: Rank 0, Epoch 588, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,845: INFO: model_training: Rank 0, Epoch 588, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,846: INFO: model_training: Rank 0, Epoch 588, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,847: INFO: model_training: Rank 0, Epoch 589, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,848: INFO: model_training: Rank 0, Epoch 589, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,849: INFO: model_training: Rank 0, Epoch 589, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,850: INFO: model_training: Rank 0, Epoch 589, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,851: INFO: model_training: Rank 0, Epoch 589, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,853: INFO: model_training: Rank 0, Epoch 590, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,854: INFO: model_training: Rank 0, Epoch 590, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,856: INFO: model_training: Rank 0, Epoch 590, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,857: INFO: model_training: Rank 0, Epoch 590, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,858: INFO: model_training: Rank 0, Epoch 590, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,860: INFO: model_training: Rank 0, Epoch 591, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,861: INFO: model_training: Rank 0, Epoch 591, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,862: INFO: model_training: Rank 0, Epoch 591, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,863: INFO: model_training: Rank 0, Epoch 591, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,864: INFO: model_training: Rank 0, Epoch 591, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,866: INFO: model_training: Rank 0, Epoch 592, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,867: INFO: model_training: Rank 0, Epoch 592, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,868: INFO: model_training: Rank 0, Epoch 592, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,869: INFO: model_training: Rank 0, Epoch 592, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,870: INFO: model_training: Rank 0, Epoch 592, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,871: INFO: model_training: Rank 0, Epoch 593, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,873: INFO: model_training: Rank 0, Epoch 593, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,874: INFO: model_training: Rank 0, Epoch 593, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,875: INFO: model_training: Rank 0, Epoch 593, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,876: INFO: model_training: Rank 0, Epoch 593, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,877: INFO: model_training: Rank 0, Epoch 594, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,878: INFO: model_training: Rank 0, Epoch 594, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,879: INFO: model_training: Rank 0, Epoch 594, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,880: INFO: model_training: Rank 0, Epoch 594, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,881: INFO: model_training: Rank 0, Epoch 594, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,883: INFO: model_training: Rank 0, Epoch 595, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,884: INFO: model_training: Rank 0, Epoch 595, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,885: INFO: model_training: Rank 0, Epoch 595, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,887: INFO: model_training: Rank 0, Epoch 595, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,887: INFO: model_training: Rank 0, Epoch 595, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,889: INFO: model_training: Rank 0, Epoch 596, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,890: INFO: model_training: Rank 0, Epoch 596, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,891: INFO: model_training: Rank 0, Epoch 596, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,892: INFO: model_training: Rank 0, Epoch 596, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,893: INFO: model_training: Rank 0, Epoch 596, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,894: INFO: model_training: Rank 0, Epoch 597, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,895: INFO: model_training: Rank 0, Epoch 597, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,896: INFO: model_training: Rank 0, Epoch 597, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,897: INFO: model_training: Rank 0, Epoch 597, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,898: INFO: model_training: Rank 0, Epoch 597, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,900: INFO: model_training: Rank 0, Epoch 598, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,901: INFO: model_training: Rank 0, Epoch 598, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,902: INFO: model_training: Rank 0, Epoch 598, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,904: INFO: model_training: Rank 0, Epoch 598, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,905: INFO: model_training: Rank 0, Epoch 598, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,906: INFO: model_training: Rank 0, Epoch 599, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,907: INFO: model_training: Rank 0, Epoch 599, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,909: INFO: model_training: Rank 0, Epoch 599, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,910: INFO: model_training: Rank 0, Epoch 599, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,911: INFO: model_training: Rank 0, Epoch 599, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,912: INFO: model_training: Rank 0, Epoch 600, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,913: INFO: model_training: Rank 0, Epoch 600, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,915: INFO: model_training: Rank 0, Epoch 600, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,916: INFO: model_training: Rank 0, Epoch 600, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,917: INFO: model_training: Rank 0, Epoch 600, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,918: INFO: model_training: Rank 0, Epoch 601, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,920: INFO: model_training: Rank 0, Epoch 601, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,921: INFO: model_training: Rank 0, Epoch 601, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,922: INFO: model_training: Rank 0, Epoch 601, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,924: INFO: model_training: Rank 0, Epoch 601, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,925: INFO: model_training: Rank 0, Epoch 602, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,926: INFO: model_training: Rank 0, Epoch 602, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,928: INFO: model_training: Rank 0, Epoch 602, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,929: INFO: model_training: Rank 0, Epoch 602, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,930: INFO: model_training: Rank 0, Epoch 602, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,931: INFO: model_training: Rank 0, Epoch 603, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,932: INFO: model_training: Rank 0, Epoch 603, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,934: INFO: model_training: Rank 0, Epoch 603, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,936: INFO: model_training: Rank 0, Epoch 603, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,937: INFO: model_training: Rank 0, Epoch 603, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,938: INFO: model_training: Rank 0, Epoch 604, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,940: INFO: model_training: Rank 0, Epoch 604, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,941: INFO: model_training: Rank 0, Epoch 604, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,942: INFO: model_training: Rank 0, Epoch 604, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,944: INFO: model_training: Rank 0, Epoch 604, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,945: INFO: model_training: Rank 0, Epoch 605, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,946: INFO: model_training: Rank 0, Epoch 605, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,948: INFO: model_training: Rank 0, Epoch 605, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,949: INFO: model_training: Rank 0, Epoch 605, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,950: INFO: model_training: Rank 0, Epoch 605, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,951: INFO: model_training: Rank 0, Epoch 606, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,953: INFO: model_training: Rank 0, Epoch 606, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,954: INFO: model_training: Rank 0, Epoch 606, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,955: INFO: model_training: Rank 0, Epoch 606, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,956: INFO: model_training: Rank 0, Epoch 606, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,957: INFO: model_training: Rank 0, Epoch 607, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,959: INFO: model_training: Rank 0, Epoch 607, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,960: INFO: model_training: Rank 0, Epoch 607, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,961: INFO: model_training: Rank 0, Epoch 607, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,962: INFO: model_training: Rank 0, Epoch 607, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,964: INFO: model_training: Rank 0, Epoch 608, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,965: INFO: model_training: Rank 0, Epoch 608, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,966: INFO: model_training: Rank 0, Epoch 608, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,968: INFO: model_training: Rank 0, Epoch 608, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,969: INFO: model_training: Rank 0, Epoch 608, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,970: INFO: model_training: Rank 0, Epoch 609, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,972: INFO: model_training: Rank 0, Epoch 609, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,973: INFO: model_training: Rank 0, Epoch 609, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,974: INFO: model_training: Rank 0, Epoch 609, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,975: INFO: model_training: Rank 0, Epoch 609, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,977: INFO: model_training: Rank 0, Epoch 610, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,978: INFO: model_training: Rank 0, Epoch 610, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,979: INFO: model_training: Rank 0, Epoch 610, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,980: INFO: model_training: Rank 0, Epoch 610, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,981: INFO: model_training: Rank 0, Epoch 610, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,983: INFO: model_training: Rank 0, Epoch 611, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,984: INFO: model_training: Rank 0, Epoch 611, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,985: INFO: model_training: Rank 0, Epoch 611, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,986: INFO: model_training: Rank 0, Epoch 611, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,987: INFO: model_training: Rank 0, Epoch 611, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,989: INFO: model_training: Rank 0, Epoch 612, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,990: INFO: model_training: Rank 0, Epoch 612, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,992: INFO: model_training: Rank 0, Epoch 612, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:28,994: INFO: model_training: Rank 0, Epoch 612, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:28,995: INFO: model_training: Rank 0, Epoch 612, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:28,997: INFO: model_training: Rank 0, Epoch 613, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:28,998: INFO: model_training: Rank 0, Epoch 613, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:28,999: INFO: model_training: Rank 0, Epoch 613, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,000: INFO: model_training: Rank 0, Epoch 613, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,001: INFO: model_training: Rank 0, Epoch 613, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,003: INFO: model_training: Rank 0, Epoch 614, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,004: INFO: model_training: Rank 0, Epoch 614, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,005: INFO: model_training: Rank 0, Epoch 614, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,007: INFO: model_training: Rank 0, Epoch 614, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,008: INFO: model_training: Rank 0, Epoch 614, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,011: INFO: model_training: Rank 0, Epoch 615, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,013: INFO: model_training: Rank 0, Epoch 615, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,016: INFO: model_training: Rank 0, Epoch 615, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,018: INFO: model_training: Rank 0, Epoch 615, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,020: INFO: model_training: Rank 0, Epoch 615, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,022: INFO: model_training: Rank 0, Epoch 616, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,024: INFO: model_training: Rank 0, Epoch 616, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,025: INFO: model_training: Rank 0, Epoch 616, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,027: INFO: model_training: Rank 0, Epoch 616, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,028: INFO: model_training: Rank 0, Epoch 616, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,030: INFO: model_training: Rank 0, Epoch 617, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,031: INFO: model_training: Rank 0, Epoch 617, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,033: INFO: model_training: Rank 0, Epoch 617, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,034: INFO: model_training: Rank 0, Epoch 617, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,035: INFO: model_training: Rank 0, Epoch 617, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,037: INFO: model_training: Rank 0, Epoch 618, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,040: INFO: model_training: Rank 0, Epoch 618, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,042: INFO: model_training: Rank 0, Epoch 618, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,043: INFO: model_training: Rank 0, Epoch 618, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,045: INFO: model_training: Rank 0, Epoch 618, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,047: INFO: model_training: Rank 0, Epoch 619, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,048: INFO: model_training: Rank 0, Epoch 619, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,049: INFO: model_training: Rank 0, Epoch 619, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,050: INFO: model_training: Rank 0, Epoch 619, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,052: INFO: model_training: Rank 0, Epoch 619, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,053: INFO: model_training: Rank 0, Epoch 620, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,056: INFO: model_training: Rank 0, Epoch 620, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,057: INFO: model_training: Rank 0, Epoch 620, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,059: INFO: model_training: Rank 0, Epoch 620, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,064: INFO: model_training: Rank 0, Epoch 620, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,066: INFO: model_training: Rank 0, Epoch 621, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,067: INFO: model_training: Rank 0, Epoch 621, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,068: INFO: model_training: Rank 0, Epoch 621, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,071: INFO: model_training: Rank 0, Epoch 621, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,073: INFO: model_training: Rank 0, Epoch 621, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,075: INFO: model_training: Rank 0, Epoch 622, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,076: INFO: model_training: Rank 0, Epoch 622, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,078: INFO: model_training: Rank 0, Epoch 622, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,079: INFO: model_training: Rank 0, Epoch 622, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,080: INFO: model_training: Rank 0, Epoch 622, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,081: INFO: model_training: Rank 0, Epoch 623, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,083: INFO: model_training: Rank 0, Epoch 623, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,084: INFO: model_training: Rank 0, Epoch 623, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,085: INFO: model_training: Rank 0, Epoch 623, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,086: INFO: model_training: Rank 0, Epoch 623, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,087: INFO: model_training: Rank 0, Epoch 624, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,088: INFO: model_training: Rank 0, Epoch 624, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,090: INFO: model_training: Rank 0, Epoch 624, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,091: INFO: model_training: Rank 0, Epoch 624, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,092: INFO: model_training: Rank 0, Epoch 624, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,093: INFO: model_training: Rank 0, Epoch 625, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,094: INFO: model_training: Rank 0, Epoch 625, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,095: INFO: model_training: Rank 0, Epoch 625, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,096: INFO: model_training: Rank 0, Epoch 625, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,097: INFO: model_training: Rank 0, Epoch 625, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,099: INFO: model_training: Rank 0, Epoch 626, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,100: INFO: model_training: Rank 0, Epoch 626, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,101: INFO: model_training: Rank 0, Epoch 626, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,102: INFO: model_training: Rank 0, Epoch 626, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,103: INFO: model_training: Rank 0, Epoch 626, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,104: INFO: model_training: Rank 0, Epoch 627, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,105: INFO: model_training: Rank 0, Epoch 627, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,107: INFO: model_training: Rank 0, Epoch 627, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,108: INFO: model_training: Rank 0, Epoch 627, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,109: INFO: model_training: Rank 0, Epoch 627, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,110: INFO: model_training: Rank 0, Epoch 628, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,111: INFO: model_training: Rank 0, Epoch 628, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,112: INFO: model_training: Rank 0, Epoch 628, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,113: INFO: model_training: Rank 0, Epoch 628, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,114: INFO: model_training: Rank 0, Epoch 628, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,116: INFO: model_training: Rank 0, Epoch 629, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,117: INFO: model_training: Rank 0, Epoch 629, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,118: INFO: model_training: Rank 0, Epoch 629, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,119: INFO: model_training: Rank 0, Epoch 629, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,120: INFO: model_training: Rank 0, Epoch 629, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,121: INFO: model_training: Rank 0, Epoch 630, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,122: INFO: model_training: Rank 0, Epoch 630, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,123: INFO: model_training: Rank 0, Epoch 630, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,125: INFO: model_training: Rank 0, Epoch 630, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,126: INFO: model_training: Rank 0, Epoch 630, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,127: INFO: model_training: Rank 0, Epoch 631, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,128: INFO: model_training: Rank 0, Epoch 631, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,130: INFO: model_training: Rank 0, Epoch 631, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,131: INFO: model_training: Rank 0, Epoch 631, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,132: INFO: model_training: Rank 0, Epoch 631, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,133: INFO: model_training: Rank 0, Epoch 632, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,134: INFO: model_training: Rank 0, Epoch 632, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,135: INFO: model_training: Rank 0, Epoch 632, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,136: INFO: model_training: Rank 0, Epoch 632, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,137: INFO: model_training: Rank 0, Epoch 632, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,138: INFO: model_training: Rank 0, Epoch 633, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,139: INFO: model_training: Rank 0, Epoch 633, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,140: INFO: model_training: Rank 0, Epoch 633, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,141: INFO: model_training: Rank 0, Epoch 633, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,142: INFO: model_training: Rank 0, Epoch 633, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,143: INFO: model_training: Rank 0, Epoch 634, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,144: INFO: model_training: Rank 0, Epoch 634, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,146: INFO: model_training: Rank 0, Epoch 634, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,146: INFO: model_training: Rank 0, Epoch 634, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,147: INFO: model_training: Rank 0, Epoch 634, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,148: INFO: model_training: Rank 0, Epoch 635, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,150: INFO: model_training: Rank 0, Epoch 635, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,151: INFO: model_training: Rank 0, Epoch 635, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,152: INFO: model_training: Rank 0, Epoch 635, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,153: INFO: model_training: Rank 0, Epoch 635, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,154: INFO: model_training: Rank 0, Epoch 636, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,155: INFO: model_training: Rank 0, Epoch 636, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,156: INFO: model_training: Rank 0, Epoch 636, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,157: INFO: model_training: Rank 0, Epoch 636, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,158: INFO: model_training: Rank 0, Epoch 636, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,159: INFO: model_training: Rank 0, Epoch 637, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,161: INFO: model_training: Rank 0, Epoch 637, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,161: INFO: model_training: Rank 0, Epoch 637, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,163: INFO: model_training: Rank 0, Epoch 637, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,164: INFO: model_training: Rank 0, Epoch 637, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,165: INFO: model_training: Rank 0, Epoch 638, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,166: INFO: model_training: Rank 0, Epoch 638, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,167: INFO: model_training: Rank 0, Epoch 638, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,168: INFO: model_training: Rank 0, Epoch 638, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,169: INFO: model_training: Rank 0, Epoch 638, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,170: INFO: model_training: Rank 0, Epoch 639, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,171: INFO: model_training: Rank 0, Epoch 639, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,172: INFO: model_training: Rank 0, Epoch 639, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,173: INFO: model_training: Rank 0, Epoch 639, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,174: INFO: model_training: Rank 0, Epoch 639, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,175: INFO: model_training: Rank 0, Epoch 640, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,176: INFO: model_training: Rank 0, Epoch 640, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,177: INFO: model_training: Rank 0, Epoch 640, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,178: INFO: model_training: Rank 0, Epoch 640, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,179: INFO: model_training: Rank 0, Epoch 640, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,180: INFO: model_training: Rank 0, Epoch 641, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,181: INFO: model_training: Rank 0, Epoch 641, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,183: INFO: model_training: Rank 0, Epoch 641, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,184: INFO: model_training: Rank 0, Epoch 641, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,185: INFO: model_training: Rank 0, Epoch 641, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,186: INFO: model_training: Rank 0, Epoch 642, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,187: INFO: model_training: Rank 0, Epoch 642, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,188: INFO: model_training: Rank 0, Epoch 642, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,189: INFO: model_training: Rank 0, Epoch 642, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,190: INFO: model_training: Rank 0, Epoch 642, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,191: INFO: model_training: Rank 0, Epoch 643, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,192: INFO: model_training: Rank 0, Epoch 643, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,193: INFO: model_training: Rank 0, Epoch 643, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,194: INFO: model_training: Rank 0, Epoch 643, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,195: INFO: model_training: Rank 0, Epoch 643, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,196: INFO: model_training: Rank 0, Epoch 644, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,197: INFO: model_training: Rank 0, Epoch 644, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,199: INFO: model_training: Rank 0, Epoch 644, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,200: INFO: model_training: Rank 0, Epoch 644, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,201: INFO: model_training: Rank 0, Epoch 644, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,202: INFO: model_training: Rank 0, Epoch 645, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,203: INFO: model_training: Rank 0, Epoch 645, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,204: INFO: model_training: Rank 0, Epoch 645, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,205: INFO: model_training: Rank 0, Epoch 645, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,206: INFO: model_training: Rank 0, Epoch 645, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,207: INFO: model_training: Rank 0, Epoch 646, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,208: INFO: model_training: Rank 0, Epoch 646, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,210: INFO: model_training: Rank 0, Epoch 646, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,211: INFO: model_training: Rank 0, Epoch 646, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,212: INFO: model_training: Rank 0, Epoch 646, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,213: INFO: model_training: Rank 0, Epoch 647, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,214: INFO: model_training: Rank 0, Epoch 647, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,215: INFO: model_training: Rank 0, Epoch 647, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,216: INFO: model_training: Rank 0, Epoch 647, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,217: INFO: model_training: Rank 0, Epoch 647, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,218: INFO: model_training: Rank 0, Epoch 648, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,219: INFO: model_training: Rank 0, Epoch 648, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,220: INFO: model_training: Rank 0, Epoch 648, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,222: INFO: model_training: Rank 0, Epoch 648, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,223: INFO: model_training: Rank 0, Epoch 648, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,224: INFO: model_training: Rank 0, Epoch 649, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,225: INFO: model_training: Rank 0, Epoch 649, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,226: INFO: model_training: Rank 0, Epoch 649, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,227: INFO: model_training: Rank 0, Epoch 649, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,228: INFO: model_training: Rank 0, Epoch 649, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,229: INFO: model_training: Rank 0, Epoch 650, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,230: INFO: model_training: Rank 0, Epoch 650, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,231: INFO: model_training: Rank 0, Epoch 650, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,233: INFO: model_training: Rank 0, Epoch 650, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,234: INFO: model_training: Rank 0, Epoch 650, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,235: INFO: model_training: Rank 0, Epoch 651, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,236: INFO: model_training: Rank 0, Epoch 651, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,237: INFO: model_training: Rank 0, Epoch 651, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,239: INFO: model_training: Rank 0, Epoch 651, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,240: INFO: model_training: Rank 0, Epoch 651, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,241: INFO: model_training: Rank 0, Epoch 652, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,242: INFO: model_training: Rank 0, Epoch 652, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,243: INFO: model_training: Rank 0, Epoch 652, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,244: INFO: model_training: Rank 0, Epoch 652, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,245: INFO: model_training: Rank 0, Epoch 652, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,246: INFO: model_training: Rank 0, Epoch 653, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,247: INFO: model_training: Rank 0, Epoch 653, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,248: INFO: model_training: Rank 0, Epoch 653, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,249: INFO: model_training: Rank 0, Epoch 653, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,250: INFO: model_training: Rank 0, Epoch 653, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,251: INFO: model_training: Rank 0, Epoch 654, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,252: INFO: model_training: Rank 0, Epoch 654, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,253: INFO: model_training: Rank 0, Epoch 654, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,254: INFO: model_training: Rank 0, Epoch 654, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,256: INFO: model_training: Rank 0, Epoch 654, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,257: INFO: model_training: Rank 0, Epoch 655, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,258: INFO: model_training: Rank 0, Epoch 655, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,258: INFO: model_training: Rank 0, Epoch 655, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,259: INFO: model_training: Rank 0, Epoch 655, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,260: INFO: model_training: Rank 0, Epoch 655, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,261: INFO: model_training: Rank 0, Epoch 656, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,263: INFO: model_training: Rank 0, Epoch 656, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,264: INFO: model_training: Rank 0, Epoch 656, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,265: INFO: model_training: Rank 0, Epoch 656, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,267: INFO: model_training: Rank 0, Epoch 656, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,268: INFO: model_training: Rank 0, Epoch 657, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,269: INFO: model_training: Rank 0, Epoch 657, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,270: INFO: model_training: Rank 0, Epoch 657, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,271: INFO: model_training: Rank 0, Epoch 657, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,272: INFO: model_training: Rank 0, Epoch 657, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,273: INFO: model_training: Rank 0, Epoch 658, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,274: INFO: model_training: Rank 0, Epoch 658, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,276: INFO: model_training: Rank 0, Epoch 658, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,277: INFO: model_training: Rank 0, Epoch 658, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,279: INFO: model_training: Rank 0, Epoch 658, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,280: INFO: model_training: Rank 0, Epoch 659, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,281: INFO: model_training: Rank 0, Epoch 659, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,282: INFO: model_training: Rank 0, Epoch 659, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,283: INFO: model_training: Rank 0, Epoch 659, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,284: INFO: model_training: Rank 0, Epoch 659, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,285: INFO: model_training: Rank 0, Epoch 660, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,286: INFO: model_training: Rank 0, Epoch 660, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,287: INFO: model_training: Rank 0, Epoch 660, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,288: INFO: model_training: Rank 0, Epoch 660, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,290: INFO: model_training: Rank 0, Epoch 660, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,291: INFO: model_training: Rank 0, Epoch 661, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,293: INFO: model_training: Rank 0, Epoch 661, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,295: INFO: model_training: Rank 0, Epoch 661, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,297: INFO: model_training: Rank 0, Epoch 661, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,299: INFO: model_training: Rank 0, Epoch 661, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,300: INFO: model_training: Rank 0, Epoch 662, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,301: INFO: model_training: Rank 0, Epoch 662, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,303: INFO: model_training: Rank 0, Epoch 662, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,305: INFO: model_training: Rank 0, Epoch 662, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,306: INFO: model_training: Rank 0, Epoch 662, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,308: INFO: model_training: Rank 0, Epoch 663, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,310: INFO: model_training: Rank 0, Epoch 663, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,312: INFO: model_training: Rank 0, Epoch 663, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,314: INFO: model_training: Rank 0, Epoch 663, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,315: INFO: model_training: Rank 0, Epoch 663, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,317: INFO: model_training: Rank 0, Epoch 664, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,319: INFO: model_training: Rank 0, Epoch 664, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,320: INFO: model_training: Rank 0, Epoch 664, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,322: INFO: model_training: Rank 0, Epoch 664, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,323: INFO: model_training: Rank 0, Epoch 664, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,325: INFO: model_training: Rank 0, Epoch 665, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,326: INFO: model_training: Rank 0, Epoch 665, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,327: INFO: model_training: Rank 0, Epoch 665, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,328: INFO: model_training: Rank 0, Epoch 665, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,330: INFO: model_training: Rank 0, Epoch 665, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,332: INFO: model_training: Rank 0, Epoch 666, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,333: INFO: model_training: Rank 0, Epoch 666, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,334: INFO: model_training: Rank 0, Epoch 666, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,336: INFO: model_training: Rank 0, Epoch 666, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,337: INFO: model_training: Rank 0, Epoch 666, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,338: INFO: model_training: Rank 0, Epoch 667, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,340: INFO: model_training: Rank 0, Epoch 667, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,342: INFO: model_training: Rank 0, Epoch 667, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,344: INFO: model_training: Rank 0, Epoch 667, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,346: INFO: model_training: Rank 0, Epoch 667, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,348: INFO: model_training: Rank 0, Epoch 668, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,351: INFO: model_training: Rank 0, Epoch 668, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,353: INFO: model_training: Rank 0, Epoch 668, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,354: INFO: model_training: Rank 0, Epoch 668, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,356: INFO: model_training: Rank 0, Epoch 668, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,358: INFO: model_training: Rank 0, Epoch 669, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,360: INFO: model_training: Rank 0, Epoch 669, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,361: INFO: model_training: Rank 0, Epoch 669, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,363: INFO: model_training: Rank 0, Epoch 669, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,365: INFO: model_training: Rank 0, Epoch 669, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,368: INFO: model_training: Rank 0, Epoch 670, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,371: INFO: model_training: Rank 0, Epoch 670, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,373: INFO: model_training: Rank 0, Epoch 670, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,375: INFO: model_training: Rank 0, Epoch 670, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,376: INFO: model_training: Rank 0, Epoch 670, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,379: INFO: model_training: Rank 0, Epoch 671, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,381: INFO: model_training: Rank 0, Epoch 671, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,382: INFO: model_training: Rank 0, Epoch 671, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,383: INFO: model_training: Rank 0, Epoch 671, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,384: INFO: model_training: Rank 0, Epoch 671, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,385: INFO: model_training: Rank 0, Epoch 672, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,387: INFO: model_training: Rank 0, Epoch 672, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,388: INFO: model_training: Rank 0, Epoch 672, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,389: INFO: model_training: Rank 0, Epoch 672, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,390: INFO: model_training: Rank 0, Epoch 672, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,391: INFO: model_training: Rank 0, Epoch 673, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,393: INFO: model_training: Rank 0, Epoch 673, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,394: INFO: model_training: Rank 0, Epoch 673, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,395: INFO: model_training: Rank 0, Epoch 673, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,396: INFO: model_training: Rank 0, Epoch 673, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,397: INFO: model_training: Rank 0, Epoch 674, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,398: INFO: model_training: Rank 0, Epoch 674, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,399: INFO: model_training: Rank 0, Epoch 674, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,401: INFO: model_training: Rank 0, Epoch 674, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,402: INFO: model_training: Rank 0, Epoch 674, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,403: INFO: model_training: Rank 0, Epoch 675, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,405: INFO: model_training: Rank 0, Epoch 675, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,406: INFO: model_training: Rank 0, Epoch 675, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,407: INFO: model_training: Rank 0, Epoch 675, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,408: INFO: model_training: Rank 0, Epoch 675, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,409: INFO: model_training: Rank 0, Epoch 676, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,411: INFO: model_training: Rank 0, Epoch 676, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,412: INFO: model_training: Rank 0, Epoch 676, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,413: INFO: model_training: Rank 0, Epoch 676, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,414: INFO: model_training: Rank 0, Epoch 676, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,416: INFO: model_training: Rank 0, Epoch 677, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,417: INFO: model_training: Rank 0, Epoch 677, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,418: INFO: model_training: Rank 0, Epoch 677, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,420: INFO: model_training: Rank 0, Epoch 677, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,421: INFO: model_training: Rank 0, Epoch 677, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,422: INFO: model_training: Rank 0, Epoch 678, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,424: INFO: model_training: Rank 0, Epoch 678, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,425: INFO: model_training: Rank 0, Epoch 678, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,426: INFO: model_training: Rank 0, Epoch 678, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,427: INFO: model_training: Rank 0, Epoch 678, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,429: INFO: model_training: Rank 0, Epoch 679, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,431: INFO: model_training: Rank 0, Epoch 679, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,433: INFO: model_training: Rank 0, Epoch 679, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,434: INFO: model_training: Rank 0, Epoch 679, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,436: INFO: model_training: Rank 0, Epoch 679, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,437: INFO: model_training: Rank 0, Epoch 680, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,439: INFO: model_training: Rank 0, Epoch 680, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,440: INFO: model_training: Rank 0, Epoch 680, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,441: INFO: model_training: Rank 0, Epoch 680, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,443: INFO: model_training: Rank 0, Epoch 680, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,444: INFO: model_training: Rank 0, Epoch 681, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,445: INFO: model_training: Rank 0, Epoch 681, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,446: INFO: model_training: Rank 0, Epoch 681, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,447: INFO: model_training: Rank 0, Epoch 681, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,448: INFO: model_training: Rank 0, Epoch 681, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,450: INFO: model_training: Rank 0, Epoch 682, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,451: INFO: model_training: Rank 0, Epoch 682, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,452: INFO: model_training: Rank 0, Epoch 682, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,453: INFO: model_training: Rank 0, Epoch 682, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,455: INFO: model_training: Rank 0, Epoch 682, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,456: INFO: model_training: Rank 0, Epoch 683, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,457: INFO: model_training: Rank 0, Epoch 683, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,458: INFO: model_training: Rank 0, Epoch 683, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,460: INFO: model_training: Rank 0, Epoch 683, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,461: INFO: model_training: Rank 0, Epoch 683, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,463: INFO: model_training: Rank 0, Epoch 684, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,464: INFO: model_training: Rank 0, Epoch 684, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,466: INFO: model_training: Rank 0, Epoch 684, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,467: INFO: model_training: Rank 0, Epoch 684, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,468: INFO: model_training: Rank 0, Epoch 684, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,469: INFO: model_training: Rank 0, Epoch 685, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,470: INFO: model_training: Rank 0, Epoch 685, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,471: INFO: model_training: Rank 0, Epoch 685, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,473: INFO: model_training: Rank 0, Epoch 685, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,474: INFO: model_training: Rank 0, Epoch 685, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,475: INFO: model_training: Rank 0, Epoch 686, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,477: INFO: model_training: Rank 0, Epoch 686, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,478: INFO: model_training: Rank 0, Epoch 686, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,479: INFO: model_training: Rank 0, Epoch 686, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,480: INFO: model_training: Rank 0, Epoch 686, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,481: INFO: model_training: Rank 0, Epoch 687, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,482: INFO: model_training: Rank 0, Epoch 687, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,483: INFO: model_training: Rank 0, Epoch 687, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,484: INFO: model_training: Rank 0, Epoch 687, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,485: INFO: model_training: Rank 0, Epoch 687, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,486: INFO: model_training: Rank 0, Epoch 688, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,488: INFO: model_training: Rank 0, Epoch 688, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,489: INFO: model_training: Rank 0, Epoch 688, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,490: INFO: model_training: Rank 0, Epoch 688, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,491: INFO: model_training: Rank 0, Epoch 688, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,493: INFO: model_training: Rank 0, Epoch 689, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,495: INFO: model_training: Rank 0, Epoch 689, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,496: INFO: model_training: Rank 0, Epoch 689, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,497: INFO: model_training: Rank 0, Epoch 689, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,499: INFO: model_training: Rank 0, Epoch 689, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,500: INFO: model_training: Rank 0, Epoch 690, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,501: INFO: model_training: Rank 0, Epoch 690, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,503: INFO: model_training: Rank 0, Epoch 690, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,504: INFO: model_training: Rank 0, Epoch 690, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,505: INFO: model_training: Rank 0, Epoch 690, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,506: INFO: model_training: Rank 0, Epoch 691, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,508: INFO: model_training: Rank 0, Epoch 691, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,509: INFO: model_training: Rank 0, Epoch 691, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,510: INFO: model_training: Rank 0, Epoch 691, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,511: INFO: model_training: Rank 0, Epoch 691, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,513: INFO: model_training: Rank 0, Epoch 692, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,514: INFO: model_training: Rank 0, Epoch 692, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,515: INFO: model_training: Rank 0, Epoch 692, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,516: INFO: model_training: Rank 0, Epoch 692, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,517: INFO: model_training: Rank 0, Epoch 692, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,519: INFO: model_training: Rank 0, Epoch 693, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,520: INFO: model_training: Rank 0, Epoch 693, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,521: INFO: model_training: Rank 0, Epoch 693, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,523: INFO: model_training: Rank 0, Epoch 693, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,524: INFO: model_training: Rank 0, Epoch 693, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,525: INFO: model_training: Rank 0, Epoch 694, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,526: INFO: model_training: Rank 0, Epoch 694, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,528: INFO: model_training: Rank 0, Epoch 694, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,529: INFO: model_training: Rank 0, Epoch 694, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,530: INFO: model_training: Rank 0, Epoch 694, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,532: INFO: model_training: Rank 0, Epoch 695, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,533: INFO: model_training: Rank 0, Epoch 695, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,534: INFO: model_training: Rank 0, Epoch 695, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,535: INFO: model_training: Rank 0, Epoch 695, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,536: INFO: model_training: Rank 0, Epoch 695, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,537: INFO: model_training: Rank 0, Epoch 696, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,539: INFO: model_training: Rank 0, Epoch 696, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,540: INFO: model_training: Rank 0, Epoch 696, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,541: INFO: model_training: Rank 0, Epoch 696, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,543: INFO: model_training: Rank 0, Epoch 696, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,544: INFO: model_training: Rank 0, Epoch 697, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,545: INFO: model_training: Rank 0, Epoch 697, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,546: INFO: model_training: Rank 0, Epoch 697, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,547: INFO: model_training: Rank 0, Epoch 697, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,548: INFO: model_training: Rank 0, Epoch 697, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,549: INFO: model_training: Rank 0, Epoch 698, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,551: INFO: model_training: Rank 0, Epoch 698, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,553: INFO: model_training: Rank 0, Epoch 698, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,555: INFO: model_training: Rank 0, Epoch 698, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,557: INFO: model_training: Rank 0, Epoch 698, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,558: INFO: model_training: Rank 0, Epoch 699, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,560: INFO: model_training: Rank 0, Epoch 699, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,562: INFO: model_training: Rank 0, Epoch 699, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,563: INFO: model_training: Rank 0, Epoch 699, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,565: INFO: model_training: Rank 0, Epoch 699, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,566: INFO: model_training: Rank 0, Epoch 700, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,568: INFO: model_training: Rank 0, Epoch 700, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,569: INFO: model_training: Rank 0, Epoch 700, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,571: INFO: model_training: Rank 0, Epoch 700, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,572: INFO: model_training: Rank 0, Epoch 700, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,574: INFO: model_training: Rank 0, Epoch 701, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,576: INFO: model_training: Rank 0, Epoch 701, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,577: INFO: model_training: Rank 0, Epoch 701, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,579: INFO: model_training: Rank 0, Epoch 701, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,582: INFO: model_training: Rank 0, Epoch 701, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,584: INFO: model_training: Rank 0, Epoch 702, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,586: INFO: model_training: Rank 0, Epoch 702, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,587: INFO: model_training: Rank 0, Epoch 702, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,589: INFO: model_training: Rank 0, Epoch 702, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,591: INFO: model_training: Rank 0, Epoch 702, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,593: INFO: model_training: Rank 0, Epoch 703, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,595: INFO: model_training: Rank 0, Epoch 703, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,597: INFO: model_training: Rank 0, Epoch 703, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,598: INFO: model_training: Rank 0, Epoch 703, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,600: INFO: model_training: Rank 0, Epoch 703, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,601: INFO: model_training: Rank 0, Epoch 704, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,603: INFO: model_training: Rank 0, Epoch 704, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,605: INFO: model_training: Rank 0, Epoch 704, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,607: INFO: model_training: Rank 0, Epoch 704, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,609: INFO: model_training: Rank 0, Epoch 704, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,610: INFO: model_training: Rank 0, Epoch 705, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,613: INFO: model_training: Rank 0, Epoch 705, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,615: INFO: model_training: Rank 0, Epoch 705, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,617: INFO: model_training: Rank 0, Epoch 705, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,619: INFO: model_training: Rank 0, Epoch 705, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,621: INFO: model_training: Rank 0, Epoch 706, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,624: INFO: model_training: Rank 0, Epoch 706, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,626: INFO: model_training: Rank 0, Epoch 706, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,629: INFO: model_training: Rank 0, Epoch 706, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,630: INFO: model_training: Rank 0, Epoch 706, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,633: INFO: model_training: Rank 0, Epoch 707, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,635: INFO: model_training: Rank 0, Epoch 707, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,637: INFO: model_training: Rank 0, Epoch 707, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,639: INFO: model_training: Rank 0, Epoch 707, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,642: INFO: model_training: Rank 0, Epoch 707, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,644: INFO: model_training: Rank 0, Epoch 708, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,646: INFO: model_training: Rank 0, Epoch 708, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,648: INFO: model_training: Rank 0, Epoch 708, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,650: INFO: model_training: Rank 0, Epoch 708, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,652: INFO: model_training: Rank 0, Epoch 708, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,654: INFO: model_training: Rank 0, Epoch 709, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,656: INFO: model_training: Rank 0, Epoch 709, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,657: INFO: model_training: Rank 0, Epoch 709, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,659: INFO: model_training: Rank 0, Epoch 709, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,661: INFO: model_training: Rank 0, Epoch 709, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,663: INFO: model_training: Rank 0, Epoch 710, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,664: INFO: model_training: Rank 0, Epoch 710, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,665: INFO: model_training: Rank 0, Epoch 710, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,667: INFO: model_training: Rank 0, Epoch 710, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,669: INFO: model_training: Rank 0, Epoch 710, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,670: INFO: model_training: Rank 0, Epoch 711, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,671: INFO: model_training: Rank 0, Epoch 711, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,673: INFO: model_training: Rank 0, Epoch 711, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,675: INFO: model_training: Rank 0, Epoch 711, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,676: INFO: model_training: Rank 0, Epoch 711, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,678: INFO: model_training: Rank 0, Epoch 712, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,680: INFO: model_training: Rank 0, Epoch 712, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,681: INFO: model_training: Rank 0, Epoch 712, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,682: INFO: model_training: Rank 0, Epoch 712, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,684: INFO: model_training: Rank 0, Epoch 712, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,685: INFO: model_training: Rank 0, Epoch 713, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,687: INFO: model_training: Rank 0, Epoch 713, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,688: INFO: model_training: Rank 0, Epoch 713, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,689: INFO: model_training: Rank 0, Epoch 713, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,691: INFO: model_training: Rank 0, Epoch 713, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,693: INFO: model_training: Rank 0, Epoch 714, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,695: INFO: model_training: Rank 0, Epoch 714, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,697: INFO: model_training: Rank 0, Epoch 714, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,699: INFO: model_training: Rank 0, Epoch 714, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,700: INFO: model_training: Rank 0, Epoch 714, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,702: INFO: model_training: Rank 0, Epoch 715, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,704: INFO: model_training: Rank 0, Epoch 715, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,705: INFO: model_training: Rank 0, Epoch 715, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,707: INFO: model_training: Rank 0, Epoch 715, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,708: INFO: model_training: Rank 0, Epoch 715, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,710: INFO: model_training: Rank 0, Epoch 716, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,711: INFO: model_training: Rank 0, Epoch 716, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,713: INFO: model_training: Rank 0, Epoch 716, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,715: INFO: model_training: Rank 0, Epoch 716, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,716: INFO: model_training: Rank 0, Epoch 716, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,718: INFO: model_training: Rank 0, Epoch 717, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,719: INFO: model_training: Rank 0, Epoch 717, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,721: INFO: model_training: Rank 0, Epoch 717, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,722: INFO: model_training: Rank 0, Epoch 717, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,724: INFO: model_training: Rank 0, Epoch 717, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,726: INFO: model_training: Rank 0, Epoch 718, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,727: INFO: model_training: Rank 0, Epoch 718, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,729: INFO: model_training: Rank 0, Epoch 718, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,730: INFO: model_training: Rank 0, Epoch 718, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,731: INFO: model_training: Rank 0, Epoch 718, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,733: INFO: model_training: Rank 0, Epoch 719, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,734: INFO: model_training: Rank 0, Epoch 719, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,735: INFO: model_training: Rank 0, Epoch 719, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,736: INFO: model_training: Rank 0, Epoch 719, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,738: INFO: model_training: Rank 0, Epoch 719, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,740: INFO: model_training: Rank 0, Epoch 720, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,741: INFO: model_training: Rank 0, Epoch 720, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,743: INFO: model_training: Rank 0, Epoch 720, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,745: INFO: model_training: Rank 0, Epoch 720, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,746: INFO: model_training: Rank 0, Epoch 720, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,747: INFO: model_training: Rank 0, Epoch 721, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,748: INFO: model_training: Rank 0, Epoch 721, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,750: INFO: model_training: Rank 0, Epoch 721, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,751: INFO: model_training: Rank 0, Epoch 721, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,753: INFO: model_training: Rank 0, Epoch 721, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,754: INFO: model_training: Rank 0, Epoch 722, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,755: INFO: model_training: Rank 0, Epoch 722, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,757: INFO: model_training: Rank 0, Epoch 722, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,760: INFO: model_training: Rank 0, Epoch 722, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,762: INFO: model_training: Rank 0, Epoch 722, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,764: INFO: model_training: Rank 0, Epoch 723, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,766: INFO: model_training: Rank 0, Epoch 723, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,767: INFO: model_training: Rank 0, Epoch 723, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,768: INFO: model_training: Rank 0, Epoch 723, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,770: INFO: model_training: Rank 0, Epoch 723, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,771: INFO: model_training: Rank 0, Epoch 724, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,773: INFO: model_training: Rank 0, Epoch 724, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,774: INFO: model_training: Rank 0, Epoch 724, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,775: INFO: model_training: Rank 0, Epoch 724, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,777: INFO: model_training: Rank 0, Epoch 724, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,778: INFO: model_training: Rank 0, Epoch 725, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,779: INFO: model_training: Rank 0, Epoch 725, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,780: INFO: model_training: Rank 0, Epoch 725, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,782: INFO: model_training: Rank 0, Epoch 725, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,783: INFO: model_training: Rank 0, Epoch 725, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,785: INFO: model_training: Rank 0, Epoch 726, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,787: INFO: model_training: Rank 0, Epoch 726, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,788: INFO: model_training: Rank 0, Epoch 726, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,791: INFO: model_training: Rank 0, Epoch 726, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,792: INFO: model_training: Rank 0, Epoch 726, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,793: INFO: model_training: Rank 0, Epoch 727, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,795: INFO: model_training: Rank 0, Epoch 727, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,797: INFO: model_training: Rank 0, Epoch 727, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,798: INFO: model_training: Rank 0, Epoch 727, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,800: INFO: model_training: Rank 0, Epoch 727, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,801: INFO: model_training: Rank 0, Epoch 728, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,803: INFO: model_training: Rank 0, Epoch 728, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,804: INFO: model_training: Rank 0, Epoch 728, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,805: INFO: model_training: Rank 0, Epoch 728, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,806: INFO: model_training: Rank 0, Epoch 728, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,808: INFO: model_training: Rank 0, Epoch 729, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,809: INFO: model_training: Rank 0, Epoch 729, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,810: INFO: model_training: Rank 0, Epoch 729, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,812: INFO: model_training: Rank 0, Epoch 729, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,814: INFO: model_training: Rank 0, Epoch 729, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,815: INFO: model_training: Rank 0, Epoch 730, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,816: INFO: model_training: Rank 0, Epoch 730, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,818: INFO: model_training: Rank 0, Epoch 730, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,819: INFO: model_training: Rank 0, Epoch 730, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,820: INFO: model_training: Rank 0, Epoch 730, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,822: INFO: model_training: Rank 0, Epoch 731, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,823: INFO: model_training: Rank 0, Epoch 731, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,825: INFO: model_training: Rank 0, Epoch 731, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,826: INFO: model_training: Rank 0, Epoch 731, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,828: INFO: model_training: Rank 0, Epoch 731, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,830: INFO: model_training: Rank 0, Epoch 732, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,831: INFO: model_training: Rank 0, Epoch 732, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,832: INFO: model_training: Rank 0, Epoch 732, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,835: INFO: model_training: Rank 0, Epoch 732, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,836: INFO: model_training: Rank 0, Epoch 732, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,838: INFO: model_training: Rank 0, Epoch 733, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,839: INFO: model_training: Rank 0, Epoch 733, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,840: INFO: model_training: Rank 0, Epoch 733, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,841: INFO: model_training: Rank 0, Epoch 733, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,844: INFO: model_training: Rank 0, Epoch 733, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,845: INFO: model_training: Rank 0, Epoch 734, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,846: INFO: model_training: Rank 0, Epoch 734, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,848: INFO: model_training: Rank 0, Epoch 734, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,849: INFO: model_training: Rank 0, Epoch 734, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,851: INFO: model_training: Rank 0, Epoch 734, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,852: INFO: model_training: Rank 0, Epoch 735, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,853: INFO: model_training: Rank 0, Epoch 735, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,855: INFO: model_training: Rank 0, Epoch 735, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,856: INFO: model_training: Rank 0, Epoch 735, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,857: INFO: model_training: Rank 0, Epoch 735, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,859: INFO: model_training: Rank 0, Epoch 736, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,860: INFO: model_training: Rank 0, Epoch 736, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,862: INFO: model_training: Rank 0, Epoch 736, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,863: INFO: model_training: Rank 0, Epoch 736, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,865: INFO: model_training: Rank 0, Epoch 736, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,867: INFO: model_training: Rank 0, Epoch 737, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,868: INFO: model_training: Rank 0, Epoch 737, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,869: INFO: model_training: Rank 0, Epoch 737, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,871: INFO: model_training: Rank 0, Epoch 737, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,872: INFO: model_training: Rank 0, Epoch 737, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,873: INFO: model_training: Rank 0, Epoch 738, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,875: INFO: model_training: Rank 0, Epoch 738, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,876: INFO: model_training: Rank 0, Epoch 738, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,877: INFO: model_training: Rank 0, Epoch 738, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,878: INFO: model_training: Rank 0, Epoch 738, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,879: INFO: model_training: Rank 0, Epoch 739, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,880: INFO: model_training: Rank 0, Epoch 739, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,881: INFO: model_training: Rank 0, Epoch 739, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,883: INFO: model_training: Rank 0, Epoch 739, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,884: INFO: model_training: Rank 0, Epoch 739, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,885: INFO: model_training: Rank 0, Epoch 740, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,886: INFO: model_training: Rank 0, Epoch 740, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,887: INFO: model_training: Rank 0, Epoch 740, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,888: INFO: model_training: Rank 0, Epoch 740, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,889: INFO: model_training: Rank 0, Epoch 740, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,891: INFO: model_training: Rank 0, Epoch 741, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,892: INFO: model_training: Rank 0, Epoch 741, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,894: INFO: model_training: Rank 0, Epoch 741, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,895: INFO: model_training: Rank 0, Epoch 741, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,897: INFO: model_training: Rank 0, Epoch 741, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,898: INFO: model_training: Rank 0, Epoch 742, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,899: INFO: model_training: Rank 0, Epoch 742, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,900: INFO: model_training: Rank 0, Epoch 742, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,902: INFO: model_training: Rank 0, Epoch 742, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,903: INFO: model_training: Rank 0, Epoch 742, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,904: INFO: model_training: Rank 0, Epoch 743, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,905: INFO: model_training: Rank 0, Epoch 743, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,907: INFO: model_training: Rank 0, Epoch 743, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,908: INFO: model_training: Rank 0, Epoch 743, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,909: INFO: model_training: Rank 0, Epoch 743, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,910: INFO: model_training: Rank 0, Epoch 744, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,911: INFO: model_training: Rank 0, Epoch 744, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,913: INFO: model_training: Rank 0, Epoch 744, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,914: INFO: model_training: Rank 0, Epoch 744, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,915: INFO: model_training: Rank 0, Epoch 744, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,916: INFO: model_training: Rank 0, Epoch 745, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,917: INFO: model_training: Rank 0, Epoch 745, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,919: INFO: model_training: Rank 0, Epoch 745, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,920: INFO: model_training: Rank 0, Epoch 745, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,921: INFO: model_training: Rank 0, Epoch 745, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,922: INFO: model_training: Rank 0, Epoch 746, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,924: INFO: model_training: Rank 0, Epoch 746, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,925: INFO: model_training: Rank 0, Epoch 746, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,926: INFO: model_training: Rank 0, Epoch 746, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,927: INFO: model_training: Rank 0, Epoch 746, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,928: INFO: model_training: Rank 0, Epoch 747, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,929: INFO: model_training: Rank 0, Epoch 747, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,930: INFO: model_training: Rank 0, Epoch 747, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,932: INFO: model_training: Rank 0, Epoch 747, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,933: INFO: model_training: Rank 0, Epoch 747, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,935: INFO: model_training: Rank 0, Epoch 748, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,936: INFO: model_training: Rank 0, Epoch 748, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,937: INFO: model_training: Rank 0, Epoch 748, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,938: INFO: model_training: Rank 0, Epoch 748, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,939: INFO: model_training: Rank 0, Epoch 748, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,940: INFO: model_training: Rank 0, Epoch 749, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,942: INFO: model_training: Rank 0, Epoch 749, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,943: INFO: model_training: Rank 0, Epoch 749, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,944: INFO: model_training: Rank 0, Epoch 749, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,946: INFO: model_training: Rank 0, Epoch 749, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,947: INFO: model_training: Rank 0, Epoch 750, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,948: INFO: model_training: Rank 0, Epoch 750, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,949: INFO: model_training: Rank 0, Epoch 750, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,950: INFO: model_training: Rank 0, Epoch 750, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,951: INFO: model_training: Rank 0, Epoch 750, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,953: INFO: model_training: Rank 0, Epoch 751, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,954: INFO: model_training: Rank 0, Epoch 751, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,955: INFO: model_training: Rank 0, Epoch 751, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,956: INFO: model_training: Rank 0, Epoch 751, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,957: INFO: model_training: Rank 0, Epoch 751, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,958: INFO: model_training: Rank 0, Epoch 752, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,959: INFO: model_training: Rank 0, Epoch 752, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,961: INFO: model_training: Rank 0, Epoch 752, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,962: INFO: model_training: Rank 0, Epoch 752, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,963: INFO: model_training: Rank 0, Epoch 752, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,965: INFO: model_training: Rank 0, Epoch 753, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,966: INFO: model_training: Rank 0, Epoch 753, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,967: INFO: model_training: Rank 0, Epoch 753, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,969: INFO: model_training: Rank 0, Epoch 753, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,970: INFO: model_training: Rank 0, Epoch 753, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,971: INFO: model_training: Rank 0, Epoch 754, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,972: INFO: model_training: Rank 0, Epoch 754, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,974: INFO: model_training: Rank 0, Epoch 754, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,975: INFO: model_training: Rank 0, Epoch 754, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,976: INFO: model_training: Rank 0, Epoch 754, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,977: INFO: model_training: Rank 0, Epoch 755, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,978: INFO: model_training: Rank 0, Epoch 755, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,979: INFO: model_training: Rank 0, Epoch 755, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,980: INFO: model_training: Rank 0, Epoch 755, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,982: INFO: model_training: Rank 0, Epoch 755, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,983: INFO: model_training: Rank 0, Epoch 756, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,984: INFO: model_training: Rank 0, Epoch 756, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,985: INFO: model_training: Rank 0, Epoch 756, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,987: INFO: model_training: Rank 0, Epoch 756, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,989: INFO: model_training: Rank 0, Epoch 756, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,990: INFO: model_training: Rank 0, Epoch 757, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,991: INFO: model_training: Rank 0, Epoch 757, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,993: INFO: model_training: Rank 0, Epoch 757, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:29,994: INFO: model_training: Rank 0, Epoch 757, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:29,995: INFO: model_training: Rank 0, Epoch 757, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:29,997: INFO: model_training: Rank 0, Epoch 758, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:29,998: INFO: model_training: Rank 0, Epoch 758, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:29,999: INFO: model_training: Rank 0, Epoch 758, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,000: INFO: model_training: Rank 0, Epoch 758, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,001: INFO: model_training: Rank 0, Epoch 758, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,002: INFO: model_training: Rank 0, Epoch 759, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,004: INFO: model_training: Rank 0, Epoch 759, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,005: INFO: model_training: Rank 0, Epoch 759, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,006: INFO: model_training: Rank 0, Epoch 759, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,008: INFO: model_training: Rank 0, Epoch 759, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,009: INFO: model_training: Rank 0, Epoch 760, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,011: INFO: model_training: Rank 0, Epoch 760, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,012: INFO: model_training: Rank 0, Epoch 760, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,013: INFO: model_training: Rank 0, Epoch 760, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,014: INFO: model_training: Rank 0, Epoch 760, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,016: INFO: model_training: Rank 0, Epoch 761, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,017: INFO: model_training: Rank 0, Epoch 761, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,018: INFO: model_training: Rank 0, Epoch 761, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,020: INFO: model_training: Rank 0, Epoch 761, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,021: INFO: model_training: Rank 0, Epoch 761, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,022: INFO: model_training: Rank 0, Epoch 762, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,023: INFO: model_training: Rank 0, Epoch 762, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,025: INFO: model_training: Rank 0, Epoch 762, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,026: INFO: model_training: Rank 0, Epoch 762, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,027: INFO: model_training: Rank 0, Epoch 762, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,028: INFO: model_training: Rank 0, Epoch 763, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,029: INFO: model_training: Rank 0, Epoch 763, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,030: INFO: model_training: Rank 0, Epoch 763, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,032: INFO: model_training: Rank 0, Epoch 763, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,034: INFO: model_training: Rank 0, Epoch 763, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,035: INFO: model_training: Rank 0, Epoch 764, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,036: INFO: model_training: Rank 0, Epoch 764, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,037: INFO: model_training: Rank 0, Epoch 764, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,039: INFO: model_training: Rank 0, Epoch 764, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,040: INFO: model_training: Rank 0, Epoch 764, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,042: INFO: model_training: Rank 0, Epoch 765, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,043: INFO: model_training: Rank 0, Epoch 765, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,044: INFO: model_training: Rank 0, Epoch 765, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,045: INFO: model_training: Rank 0, Epoch 765, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,047: INFO: model_training: Rank 0, Epoch 765, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,048: INFO: model_training: Rank 0, Epoch 766, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,049: INFO: model_training: Rank 0, Epoch 766, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,050: INFO: model_training: Rank 0, Epoch 766, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,052: INFO: model_training: Rank 0, Epoch 766, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,053: INFO: model_training: Rank 0, Epoch 766, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,054: INFO: model_training: Rank 0, Epoch 767, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,055: INFO: model_training: Rank 0, Epoch 767, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,057: INFO: model_training: Rank 0, Epoch 767, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,058: INFO: model_training: Rank 0, Epoch 767, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,060: INFO: model_training: Rank 0, Epoch 767, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,061: INFO: model_training: Rank 0, Epoch 768, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,062: INFO: model_training: Rank 0, Epoch 768, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,063: INFO: model_training: Rank 0, Epoch 768, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,064: INFO: model_training: Rank 0, Epoch 768, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,066: INFO: model_training: Rank 0, Epoch 768, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,067: INFO: model_training: Rank 0, Epoch 769, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,068: INFO: model_training: Rank 0, Epoch 769, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,069: INFO: model_training: Rank 0, Epoch 769, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,070: INFO: model_training: Rank 0, Epoch 769, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,072: INFO: model_training: Rank 0, Epoch 769, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,073: INFO: model_training: Rank 0, Epoch 770, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,074: INFO: model_training: Rank 0, Epoch 770, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,075: INFO: model_training: Rank 0, Epoch 770, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,076: INFO: model_training: Rank 0, Epoch 770, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,077: INFO: model_training: Rank 0, Epoch 770, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,078: INFO: model_training: Rank 0, Epoch 771, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,080: INFO: model_training: Rank 0, Epoch 771, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,081: INFO: model_training: Rank 0, Epoch 771, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,082: INFO: model_training: Rank 0, Epoch 771, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,083: INFO: model_training: Rank 0, Epoch 771, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,084: INFO: model_training: Rank 0, Epoch 772, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,085: INFO: model_training: Rank 0, Epoch 772, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,087: INFO: model_training: Rank 0, Epoch 772, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,088: INFO: model_training: Rank 0, Epoch 772, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,089: INFO: model_training: Rank 0, Epoch 772, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,090: INFO: model_training: Rank 0, Epoch 773, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,091: INFO: model_training: Rank 0, Epoch 773, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,092: INFO: model_training: Rank 0, Epoch 773, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,093: INFO: model_training: Rank 0, Epoch 773, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,094: INFO: model_training: Rank 0, Epoch 773, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,095: INFO: model_training: Rank 0, Epoch 774, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,096: INFO: model_training: Rank 0, Epoch 774, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,097: INFO: model_training: Rank 0, Epoch 774, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,098: INFO: model_training: Rank 0, Epoch 774, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,099: INFO: model_training: Rank 0, Epoch 774, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,101: INFO: model_training: Rank 0, Epoch 775, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,102: INFO: model_training: Rank 0, Epoch 775, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,104: INFO: model_training: Rank 0, Epoch 775, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,105: INFO: model_training: Rank 0, Epoch 775, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,106: INFO: model_training: Rank 0, Epoch 775, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,107: INFO: model_training: Rank 0, Epoch 776, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,108: INFO: model_training: Rank 0, Epoch 776, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,109: INFO: model_training: Rank 0, Epoch 776, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,110: INFO: model_training: Rank 0, Epoch 776, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,111: INFO: model_training: Rank 0, Epoch 776, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,112: INFO: model_training: Rank 0, Epoch 777, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,114: INFO: model_training: Rank 0, Epoch 777, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,115: INFO: model_training: Rank 0, Epoch 777, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,116: INFO: model_training: Rank 0, Epoch 777, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,117: INFO: model_training: Rank 0, Epoch 777, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,118: INFO: model_training: Rank 0, Epoch 778, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,119: INFO: model_training: Rank 0, Epoch 778, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,120: INFO: model_training: Rank 0, Epoch 778, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,121: INFO: model_training: Rank 0, Epoch 778, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,123: INFO: model_training: Rank 0, Epoch 778, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,124: INFO: model_training: Rank 0, Epoch 779, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,125: INFO: model_training: Rank 0, Epoch 779, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,126: INFO: model_training: Rank 0, Epoch 779, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,127: INFO: model_training: Rank 0, Epoch 779, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,128: INFO: model_training: Rank 0, Epoch 779, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,129: INFO: model_training: Rank 0, Epoch 780, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,130: INFO: model_training: Rank 0, Epoch 780, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,131: INFO: model_training: Rank 0, Epoch 780, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,132: INFO: model_training: Rank 0, Epoch 780, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,133: INFO: model_training: Rank 0, Epoch 780, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,134: INFO: model_training: Rank 0, Epoch 781, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,135: INFO: model_training: Rank 0, Epoch 781, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,136: INFO: model_training: Rank 0, Epoch 781, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,137: INFO: model_training: Rank 0, Epoch 781, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,139: INFO: model_training: Rank 0, Epoch 781, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,140: INFO: model_training: Rank 0, Epoch 782, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,141: INFO: model_training: Rank 0, Epoch 782, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,142: INFO: model_training: Rank 0, Epoch 782, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,143: INFO: model_training: Rank 0, Epoch 782, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,144: INFO: model_training: Rank 0, Epoch 782, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,146: INFO: model_training: Rank 0, Epoch 783, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,147: INFO: model_training: Rank 0, Epoch 783, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,148: INFO: model_training: Rank 0, Epoch 783, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,149: INFO: model_training: Rank 0, Epoch 783, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,150: INFO: model_training: Rank 0, Epoch 783, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,151: INFO: model_training: Rank 0, Epoch 784, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,152: INFO: model_training: Rank 0, Epoch 784, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,153: INFO: model_training: Rank 0, Epoch 784, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,154: INFO: model_training: Rank 0, Epoch 784, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,155: INFO: model_training: Rank 0, Epoch 784, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,156: INFO: model_training: Rank 0, Epoch 785, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,157: INFO: model_training: Rank 0, Epoch 785, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,159: INFO: model_training: Rank 0, Epoch 785, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,159: INFO: model_training: Rank 0, Epoch 785, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,160: INFO: model_training: Rank 0, Epoch 785, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,162: INFO: model_training: Rank 0, Epoch 786, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,163: INFO: model_training: Rank 0, Epoch 786, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,164: INFO: model_training: Rank 0, Epoch 786, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,165: INFO: model_training: Rank 0, Epoch 786, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,166: INFO: model_training: Rank 0, Epoch 786, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,167: INFO: model_training: Rank 0, Epoch 787, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,168: INFO: model_training: Rank 0, Epoch 787, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,170: INFO: model_training: Rank 0, Epoch 787, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,171: INFO: model_training: Rank 0, Epoch 787, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,171: INFO: model_training: Rank 0, Epoch 787, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,172: INFO: model_training: Rank 0, Epoch 788, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,174: INFO: model_training: Rank 0, Epoch 788, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,175: INFO: model_training: Rank 0, Epoch 788, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,176: INFO: model_training: Rank 0, Epoch 788, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,177: INFO: model_training: Rank 0, Epoch 788, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,178: INFO: model_training: Rank 0, Epoch 789, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,180: INFO: model_training: Rank 0, Epoch 789, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,181: INFO: model_training: Rank 0, Epoch 789, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,182: INFO: model_training: Rank 0, Epoch 789, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,183: INFO: model_training: Rank 0, Epoch 789, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,185: INFO: model_training: Rank 0, Epoch 790, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,186: INFO: model_training: Rank 0, Epoch 790, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,187: INFO: model_training: Rank 0, Epoch 790, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,188: INFO: model_training: Rank 0, Epoch 790, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,189: INFO: model_training: Rank 0, Epoch 790, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,190: INFO: model_training: Rank 0, Epoch 791, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,191: INFO: model_training: Rank 0, Epoch 791, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,192: INFO: model_training: Rank 0, Epoch 791, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,193: INFO: model_training: Rank 0, Epoch 791, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,194: INFO: model_training: Rank 0, Epoch 791, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,195: INFO: model_training: Rank 0, Epoch 792, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,196: INFO: model_training: Rank 0, Epoch 792, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,197: INFO: model_training: Rank 0, Epoch 792, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,198: INFO: model_training: Rank 0, Epoch 792, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,199: INFO: model_training: Rank 0, Epoch 792, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,201: INFO: model_training: Rank 0, Epoch 793, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,202: INFO: model_training: Rank 0, Epoch 793, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,203: INFO: model_training: Rank 0, Epoch 793, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,203: INFO: model_training: Rank 0, Epoch 793, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,205: INFO: model_training: Rank 0, Epoch 793, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,206: INFO: model_training: Rank 0, Epoch 794, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,207: INFO: model_training: Rank 0, Epoch 794, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,208: INFO: model_training: Rank 0, Epoch 794, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,209: INFO: model_training: Rank 0, Epoch 794, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,210: INFO: model_training: Rank 0, Epoch 794, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,211: INFO: model_training: Rank 0, Epoch 795, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,213: INFO: model_training: Rank 0, Epoch 795, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,214: INFO: model_training: Rank 0, Epoch 795, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,215: INFO: model_training: Rank 0, Epoch 795, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,216: INFO: model_training: Rank 0, Epoch 795, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,217: INFO: model_training: Rank 0, Epoch 796, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,218: INFO: model_training: Rank 0, Epoch 796, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,219: INFO: model_training: Rank 0, Epoch 796, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,220: INFO: model_training: Rank 0, Epoch 796, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,221: INFO: model_training: Rank 0, Epoch 796, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,222: INFO: model_training: Rank 0, Epoch 797, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,224: INFO: model_training: Rank 0, Epoch 797, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,225: INFO: model_training: Rank 0, Epoch 797, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,226: INFO: model_training: Rank 0, Epoch 797, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,228: INFO: model_training: Rank 0, Epoch 797, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,229: INFO: model_training: Rank 0, Epoch 798, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,230: INFO: model_training: Rank 0, Epoch 798, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,231: INFO: model_training: Rank 0, Epoch 798, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,232: INFO: model_training: Rank 0, Epoch 798, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,233: INFO: model_training: Rank 0, Epoch 798, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,234: INFO: model_training: Rank 0, Epoch 799, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,235: INFO: model_training: Rank 0, Epoch 799, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,236: INFO: model_training: Rank 0, Epoch 799, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,237: INFO: model_training: Rank 0, Epoch 799, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,239: INFO: model_training: Rank 0, Epoch 799, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,240: INFO: model_training: Rank 0, Epoch 800, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,241: INFO: model_training: Rank 0, Epoch 800, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,242: INFO: model_training: Rank 0, Epoch 800, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,243: INFO: model_training: Rank 0, Epoch 800, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,244: INFO: model_training: Rank 0, Epoch 800, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,245: INFO: model_training: Rank 0, Epoch 801, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,247: INFO: model_training: Rank 0, Epoch 801, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,248: INFO: model_training: Rank 0, Epoch 801, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,249: INFO: model_training: Rank 0, Epoch 801, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,250: INFO: model_training: Rank 0, Epoch 801, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,251: INFO: model_training: Rank 0, Epoch 802, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,252: INFO: model_training: Rank 0, Epoch 802, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,253: INFO: model_training: Rank 0, Epoch 802, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,254: INFO: model_training: Rank 0, Epoch 802, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,255: INFO: model_training: Rank 0, Epoch 802, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,256: INFO: model_training: Rank 0, Epoch 803, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,258: INFO: model_training: Rank 0, Epoch 803, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,259: INFO: model_training: Rank 0, Epoch 803, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,260: INFO: model_training: Rank 0, Epoch 803, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,261: INFO: model_training: Rank 0, Epoch 803, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,262: INFO: model_training: Rank 0, Epoch 804, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,263: INFO: model_training: Rank 0, Epoch 804, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,264: INFO: model_training: Rank 0, Epoch 804, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,265: INFO: model_training: Rank 0, Epoch 804, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,267: INFO: model_training: Rank 0, Epoch 804, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,268: INFO: model_training: Rank 0, Epoch 805, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,269: INFO: model_training: Rank 0, Epoch 805, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,270: INFO: model_training: Rank 0, Epoch 805, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,271: INFO: model_training: Rank 0, Epoch 805, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,272: INFO: model_training: Rank 0, Epoch 805, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,273: INFO: model_training: Rank 0, Epoch 806, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,275: INFO: model_training: Rank 0, Epoch 806, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,276: INFO: model_training: Rank 0, Epoch 806, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,277: INFO: model_training: Rank 0, Epoch 806, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,278: INFO: model_training: Rank 0, Epoch 806, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,279: INFO: model_training: Rank 0, Epoch 807, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,280: INFO: model_training: Rank 0, Epoch 807, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,281: INFO: model_training: Rank 0, Epoch 807, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,282: INFO: model_training: Rank 0, Epoch 807, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,283: INFO: model_training: Rank 0, Epoch 807, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,284: INFO: model_training: Rank 0, Epoch 808, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,285: INFO: model_training: Rank 0, Epoch 808, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,287: INFO: model_training: Rank 0, Epoch 808, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,288: INFO: model_training: Rank 0, Epoch 808, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,289: INFO: model_training: Rank 0, Epoch 808, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,290: INFO: model_training: Rank 0, Epoch 809, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,291: INFO: model_training: Rank 0, Epoch 809, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,292: INFO: model_training: Rank 0, Epoch 809, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,294: INFO: model_training: Rank 0, Epoch 809, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,295: INFO: model_training: Rank 0, Epoch 809, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,296: INFO: model_training: Rank 0, Epoch 810, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,297: INFO: model_training: Rank 0, Epoch 810, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,298: INFO: model_training: Rank 0, Epoch 810, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,299: INFO: model_training: Rank 0, Epoch 810, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,300: INFO: model_training: Rank 0, Epoch 810, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,302: INFO: model_training: Rank 0, Epoch 811, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,303: INFO: model_training: Rank 0, Epoch 811, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,304: INFO: model_training: Rank 0, Epoch 811, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,305: INFO: model_training: Rank 0, Epoch 811, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,306: INFO: model_training: Rank 0, Epoch 811, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,308: INFO: model_training: Rank 0, Epoch 812, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,309: INFO: model_training: Rank 0, Epoch 812, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,310: INFO: model_training: Rank 0, Epoch 812, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,311: INFO: model_training: Rank 0, Epoch 812, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,313: INFO: model_training: Rank 0, Epoch 812, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,315: INFO: model_training: Rank 0, Epoch 813, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,316: INFO: model_training: Rank 0, Epoch 813, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,317: INFO: model_training: Rank 0, Epoch 813, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,318: INFO: model_training: Rank 0, Epoch 813, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,319: INFO: model_training: Rank 0, Epoch 813, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,320: INFO: model_training: Rank 0, Epoch 814, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,321: INFO: model_training: Rank 0, Epoch 814, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,323: INFO: model_training: Rank 0, Epoch 814, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,324: INFO: model_training: Rank 0, Epoch 814, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,325: INFO: model_training: Rank 0, Epoch 814, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,326: INFO: model_training: Rank 0, Epoch 815, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,327: INFO: model_training: Rank 0, Epoch 815, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,328: INFO: model_training: Rank 0, Epoch 815, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,330: INFO: model_training: Rank 0, Epoch 815, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,331: INFO: model_training: Rank 0, Epoch 815, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,332: INFO: model_training: Rank 0, Epoch 816, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,333: INFO: model_training: Rank 0, Epoch 816, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,334: INFO: model_training: Rank 0, Epoch 816, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,335: INFO: model_training: Rank 0, Epoch 816, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,336: INFO: model_training: Rank 0, Epoch 816, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,338: INFO: model_training: Rank 0, Epoch 817, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,339: INFO: model_training: Rank 0, Epoch 817, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,340: INFO: model_training: Rank 0, Epoch 817, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,341: INFO: model_training: Rank 0, Epoch 817, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,342: INFO: model_training: Rank 0, Epoch 817, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,343: INFO: model_training: Rank 0, Epoch 818, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,345: INFO: model_training: Rank 0, Epoch 818, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,346: INFO: model_training: Rank 0, Epoch 818, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,350: INFO: model_training: Rank 0, Epoch 818, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,351: INFO: model_training: Rank 0, Epoch 818, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,352: INFO: model_training: Rank 0, Epoch 819, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,353: INFO: model_training: Rank 0, Epoch 819, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,354: INFO: model_training: Rank 0, Epoch 819, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,356: INFO: model_training: Rank 0, Epoch 819, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,357: INFO: model_training: Rank 0, Epoch 819, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,358: INFO: model_training: Rank 0, Epoch 820, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,359: INFO: model_training: Rank 0, Epoch 820, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,360: INFO: model_training: Rank 0, Epoch 820, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,361: INFO: model_training: Rank 0, Epoch 820, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,363: INFO: model_training: Rank 0, Epoch 820, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,364: INFO: model_training: Rank 0, Epoch 821, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,365: INFO: model_training: Rank 0, Epoch 821, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,366: INFO: model_training: Rank 0, Epoch 821, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,367: INFO: model_training: Rank 0, Epoch 821, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,369: INFO: model_training: Rank 0, Epoch 821, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,370: INFO: model_training: Rank 0, Epoch 822, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,371: INFO: model_training: Rank 0, Epoch 822, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,372: INFO: model_training: Rank 0, Epoch 822, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,373: INFO: model_training: Rank 0, Epoch 822, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,374: INFO: model_training: Rank 0, Epoch 822, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,375: INFO: model_training: Rank 0, Epoch 823, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,377: INFO: model_training: Rank 0, Epoch 823, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,378: INFO: model_training: Rank 0, Epoch 823, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,380: INFO: model_training: Rank 0, Epoch 823, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,381: INFO: model_training: Rank 0, Epoch 823, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,382: INFO: model_training: Rank 0, Epoch 824, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,383: INFO: model_training: Rank 0, Epoch 824, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,384: INFO: model_training: Rank 0, Epoch 824, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,385: INFO: model_training: Rank 0, Epoch 824, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,386: INFO: model_training: Rank 0, Epoch 824, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,387: INFO: model_training: Rank 0, Epoch 825, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,389: INFO: model_training: Rank 0, Epoch 825, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,390: INFO: model_training: Rank 0, Epoch 825, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,391: INFO: model_training: Rank 0, Epoch 825, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,392: INFO: model_training: Rank 0, Epoch 825, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,394: INFO: model_training: Rank 0, Epoch 826, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,395: INFO: model_training: Rank 0, Epoch 826, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,396: INFO: model_training: Rank 0, Epoch 826, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,397: INFO: model_training: Rank 0, Epoch 826, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,398: INFO: model_training: Rank 0, Epoch 826, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,401: INFO: model_training: Rank 0, Epoch 827, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,403: INFO: model_training: Rank 0, Epoch 827, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,406: INFO: model_training: Rank 0, Epoch 827, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,408: INFO: model_training: Rank 0, Epoch 827, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,410: INFO: model_training: Rank 0, Epoch 827, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,412: INFO: model_training: Rank 0, Epoch 828, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,413: INFO: model_training: Rank 0, Epoch 828, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,414: INFO: model_training: Rank 0, Epoch 828, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,416: INFO: model_training: Rank 0, Epoch 828, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,417: INFO: model_training: Rank 0, Epoch 828, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,418: INFO: model_training: Rank 0, Epoch 829, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,420: INFO: model_training: Rank 0, Epoch 829, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,421: INFO: model_training: Rank 0, Epoch 829, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,423: INFO: model_training: Rank 0, Epoch 829, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,425: INFO: model_training: Rank 0, Epoch 829, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,426: INFO: model_training: Rank 0, Epoch 830, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,429: INFO: model_training: Rank 0, Epoch 830, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,430: INFO: model_training: Rank 0, Epoch 830, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,432: INFO: model_training: Rank 0, Epoch 830, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,433: INFO: model_training: Rank 0, Epoch 830, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,434: INFO: model_training: Rank 0, Epoch 831, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,435: INFO: model_training: Rank 0, Epoch 831, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,437: INFO: model_training: Rank 0, Epoch 831, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,439: INFO: model_training: Rank 0, Epoch 831, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,440: INFO: model_training: Rank 0, Epoch 831, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,441: INFO: model_training: Rank 0, Epoch 832, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,442: INFO: model_training: Rank 0, Epoch 832, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,444: INFO: model_training: Rank 0, Epoch 832, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,445: INFO: model_training: Rank 0, Epoch 832, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,446: INFO: model_training: Rank 0, Epoch 832, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,447: INFO: model_training: Rank 0, Epoch 833, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,448: INFO: model_training: Rank 0, Epoch 833, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,449: INFO: model_training: Rank 0, Epoch 833, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,450: INFO: model_training: Rank 0, Epoch 833, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,451: INFO: model_training: Rank 0, Epoch 833, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,453: INFO: model_training: Rank 0, Epoch 834, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,455: INFO: model_training: Rank 0, Epoch 834, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,456: INFO: model_training: Rank 0, Epoch 834, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,457: INFO: model_training: Rank 0, Epoch 834, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,458: INFO: model_training: Rank 0, Epoch 834, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,459: INFO: model_training: Rank 0, Epoch 835, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,460: INFO: model_training: Rank 0, Epoch 835, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,461: INFO: model_training: Rank 0, Epoch 835, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,463: INFO: model_training: Rank 0, Epoch 835, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,463: INFO: model_training: Rank 0, Epoch 835, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,464: INFO: model_training: Rank 0, Epoch 836, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,466: INFO: model_training: Rank 0, Epoch 836, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,466: INFO: model_training: Rank 0, Epoch 836, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,468: INFO: model_training: Rank 0, Epoch 836, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,469: INFO: model_training: Rank 0, Epoch 836, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,471: INFO: model_training: Rank 0, Epoch 837, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,472: INFO: model_training: Rank 0, Epoch 837, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,473: INFO: model_training: Rank 0, Epoch 837, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,475: INFO: model_training: Rank 0, Epoch 837, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,476: INFO: model_training: Rank 0, Epoch 837, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,477: INFO: model_training: Rank 0, Epoch 838, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,478: INFO: model_training: Rank 0, Epoch 838, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,479: INFO: model_training: Rank 0, Epoch 838, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,480: INFO: model_training: Rank 0, Epoch 838, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,481: INFO: model_training: Rank 0, Epoch 838, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,483: INFO: model_training: Rank 0, Epoch 839, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,484: INFO: model_training: Rank 0, Epoch 839, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,485: INFO: model_training: Rank 0, Epoch 839, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,486: INFO: model_training: Rank 0, Epoch 839, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,487: INFO: model_training: Rank 0, Epoch 839, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,489: INFO: model_training: Rank 0, Epoch 840, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,490: INFO: model_training: Rank 0, Epoch 840, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,491: INFO: model_training: Rank 0, Epoch 840, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,493: INFO: model_training: Rank 0, Epoch 840, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,494: INFO: model_training: Rank 0, Epoch 840, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,495: INFO: model_training: Rank 0, Epoch 841, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,497: INFO: model_training: Rank 0, Epoch 841, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,498: INFO: model_training: Rank 0, Epoch 841, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,499: INFO: model_training: Rank 0, Epoch 841, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,500: INFO: model_training: Rank 0, Epoch 841, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,501: INFO: model_training: Rank 0, Epoch 842, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,503: INFO: model_training: Rank 0, Epoch 842, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,504: INFO: model_training: Rank 0, Epoch 842, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,506: INFO: model_training: Rank 0, Epoch 842, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,507: INFO: model_training: Rank 0, Epoch 842, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,508: INFO: model_training: Rank 0, Epoch 843, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,509: INFO: model_training: Rank 0, Epoch 843, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,510: INFO: model_training: Rank 0, Epoch 843, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,511: INFO: model_training: Rank 0, Epoch 843, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,513: INFO: model_training: Rank 0, Epoch 843, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,514: INFO: model_training: Rank 0, Epoch 844, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,515: INFO: model_training: Rank 0, Epoch 844, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,517: INFO: model_training: Rank 0, Epoch 844, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,518: INFO: model_training: Rank 0, Epoch 844, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,520: INFO: model_training: Rank 0, Epoch 844, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,522: INFO: model_training: Rank 0, Epoch 845, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,523: INFO: model_training: Rank 0, Epoch 845, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,524: INFO: model_training: Rank 0, Epoch 845, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,526: INFO: model_training: Rank 0, Epoch 845, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,527: INFO: model_training: Rank 0, Epoch 845, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,528: INFO: model_training: Rank 0, Epoch 846, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,530: INFO: model_training: Rank 0, Epoch 846, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,531: INFO: model_training: Rank 0, Epoch 846, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,532: INFO: model_training: Rank 0, Epoch 846, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,533: INFO: model_training: Rank 0, Epoch 846, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,535: INFO: model_training: Rank 0, Epoch 847, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,536: INFO: model_training: Rank 0, Epoch 847, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,538: INFO: model_training: Rank 0, Epoch 847, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,539: INFO: model_training: Rank 0, Epoch 847, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,540: INFO: model_training: Rank 0, Epoch 847, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,542: INFO: model_training: Rank 0, Epoch 848, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,544: INFO: model_training: Rank 0, Epoch 848, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,545: INFO: model_training: Rank 0, Epoch 848, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,546: INFO: model_training: Rank 0, Epoch 848, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,547: INFO: model_training: Rank 0, Epoch 848, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,548: INFO: model_training: Rank 0, Epoch 849, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,550: INFO: model_training: Rank 0, Epoch 849, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,551: INFO: model_training: Rank 0, Epoch 849, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,552: INFO: model_training: Rank 0, Epoch 849, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,554: INFO: model_training: Rank 0, Epoch 849, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,555: INFO: model_training: Rank 0, Epoch 850, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,556: INFO: model_training: Rank 0, Epoch 850, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,557: INFO: model_training: Rank 0, Epoch 850, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,559: INFO: model_training: Rank 0, Epoch 850, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,560: INFO: model_training: Rank 0, Epoch 850, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,561: INFO: model_training: Rank 0, Epoch 851, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,563: INFO: model_training: Rank 0, Epoch 851, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,564: INFO: model_training: Rank 0, Epoch 851, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,565: INFO: model_training: Rank 0, Epoch 851, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,567: INFO: model_training: Rank 0, Epoch 851, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,568: INFO: model_training: Rank 0, Epoch 852, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,570: INFO: model_training: Rank 0, Epoch 852, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,571: INFO: model_training: Rank 0, Epoch 852, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,572: INFO: model_training: Rank 0, Epoch 852, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,574: INFO: model_training: Rank 0, Epoch 852, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,575: INFO: model_training: Rank 0, Epoch 853, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,576: INFO: model_training: Rank 0, Epoch 853, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,577: INFO: model_training: Rank 0, Epoch 853, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,578: INFO: model_training: Rank 0, Epoch 853, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,579: INFO: model_training: Rank 0, Epoch 853, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,581: INFO: model_training: Rank 0, Epoch 854, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,582: INFO: model_training: Rank 0, Epoch 854, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,583: INFO: model_training: Rank 0, Epoch 854, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,585: INFO: model_training: Rank 0, Epoch 854, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,587: INFO: model_training: Rank 0, Epoch 854, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,589: INFO: model_training: Rank 0, Epoch 855, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,590: INFO: model_training: Rank 0, Epoch 855, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,592: INFO: model_training: Rank 0, Epoch 855, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,594: INFO: model_training: Rank 0, Epoch 855, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,595: INFO: model_training: Rank 0, Epoch 855, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,597: INFO: model_training: Rank 0, Epoch 856, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,598: INFO: model_training: Rank 0, Epoch 856, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,600: INFO: model_training: Rank 0, Epoch 856, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,602: INFO: model_training: Rank 0, Epoch 856, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,604: INFO: model_training: Rank 0, Epoch 856, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,605: INFO: model_training: Rank 0, Epoch 857, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,607: INFO: model_training: Rank 0, Epoch 857, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,609: INFO: model_training: Rank 0, Epoch 857, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,610: INFO: model_training: Rank 0, Epoch 857, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,611: INFO: model_training: Rank 0, Epoch 857, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,613: INFO: model_training: Rank 0, Epoch 858, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,615: INFO: model_training: Rank 0, Epoch 858, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,617: INFO: model_training: Rank 0, Epoch 858, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,618: INFO: model_training: Rank 0, Epoch 858, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,620: INFO: model_training: Rank 0, Epoch 858, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,622: INFO: model_training: Rank 0, Epoch 859, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,623: INFO: model_training: Rank 0, Epoch 859, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,625: INFO: model_training: Rank 0, Epoch 859, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,626: INFO: model_training: Rank 0, Epoch 859, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,627: INFO: model_training: Rank 0, Epoch 859, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,628: INFO: model_training: Rank 0, Epoch 860, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,630: INFO: model_training: Rank 0, Epoch 860, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,631: INFO: model_training: Rank 0, Epoch 860, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,632: INFO: model_training: Rank 0, Epoch 860, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,633: INFO: model_training: Rank 0, Epoch 860, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,634: INFO: model_training: Rank 0, Epoch 861, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,635: INFO: model_training: Rank 0, Epoch 861, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,637: INFO: model_training: Rank 0, Epoch 861, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,638: INFO: model_training: Rank 0, Epoch 861, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,639: INFO: model_training: Rank 0, Epoch 861, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,640: INFO: model_training: Rank 0, Epoch 862, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,641: INFO: model_training: Rank 0, Epoch 862, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,643: INFO: model_training: Rank 0, Epoch 862, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,644: INFO: model_training: Rank 0, Epoch 862, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,645: INFO: model_training: Rank 0, Epoch 862, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,647: INFO: model_training: Rank 0, Epoch 863, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,648: INFO: model_training: Rank 0, Epoch 863, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,649: INFO: model_training: Rank 0, Epoch 863, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,650: INFO: model_training: Rank 0, Epoch 863, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,651: INFO: model_training: Rank 0, Epoch 863, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,653: INFO: model_training: Rank 0, Epoch 864, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,654: INFO: model_training: Rank 0, Epoch 864, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,655: INFO: model_training: Rank 0, Epoch 864, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,656: INFO: model_training: Rank 0, Epoch 864, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,657: INFO: model_training: Rank 0, Epoch 864, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,658: INFO: model_training: Rank 0, Epoch 865, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,660: INFO: model_training: Rank 0, Epoch 865, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,661: INFO: model_training: Rank 0, Epoch 865, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,662: INFO: model_training: Rank 0, Epoch 865, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,663: INFO: model_training: Rank 0, Epoch 865, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,665: INFO: model_training: Rank 0, Epoch 866, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,666: INFO: model_training: Rank 0, Epoch 866, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,667: INFO: model_training: Rank 0, Epoch 866, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,668: INFO: model_training: Rank 0, Epoch 866, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,669: INFO: model_training: Rank 0, Epoch 866, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,670: INFO: model_training: Rank 0, Epoch 867, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,671: INFO: model_training: Rank 0, Epoch 867, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,672: INFO: model_training: Rank 0, Epoch 867, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,673: INFO: model_training: Rank 0, Epoch 867, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,674: INFO: model_training: Rank 0, Epoch 867, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,675: INFO: model_training: Rank 0, Epoch 868, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,676: INFO: model_training: Rank 0, Epoch 868, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,678: INFO: model_training: Rank 0, Epoch 868, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,679: INFO: model_training: Rank 0, Epoch 868, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,680: INFO: model_training: Rank 0, Epoch 868, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,681: INFO: model_training: Rank 0, Epoch 869, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,682: INFO: model_training: Rank 0, Epoch 869, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,683: INFO: model_training: Rank 0, Epoch 869, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,685: INFO: model_training: Rank 0, Epoch 869, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,686: INFO: model_training: Rank 0, Epoch 869, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,687: INFO: model_training: Rank 0, Epoch 870, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,688: INFO: model_training: Rank 0, Epoch 870, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,689: INFO: model_training: Rank 0, Epoch 870, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,690: INFO: model_training: Rank 0, Epoch 870, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,691: INFO: model_training: Rank 0, Epoch 870, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,692: INFO: model_training: Rank 0, Epoch 871, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,694: INFO: model_training: Rank 0, Epoch 871, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,695: INFO: model_training: Rank 0, Epoch 871, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,696: INFO: model_training: Rank 0, Epoch 871, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,697: INFO: model_training: Rank 0, Epoch 871, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,698: INFO: model_training: Rank 0, Epoch 872, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,699: INFO: model_training: Rank 0, Epoch 872, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,700: INFO: model_training: Rank 0, Epoch 872, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,701: INFO: model_training: Rank 0, Epoch 872, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,703: INFO: model_training: Rank 0, Epoch 872, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,704: INFO: model_training: Rank 0, Epoch 873, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,705: INFO: model_training: Rank 0, Epoch 873, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,706: INFO: model_training: Rank 0, Epoch 873, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,708: INFO: model_training: Rank 0, Epoch 873, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,709: INFO: model_training: Rank 0, Epoch 873, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,710: INFO: model_training: Rank 0, Epoch 874, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,711: INFO: model_training: Rank 0, Epoch 874, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,712: INFO: model_training: Rank 0, Epoch 874, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,714: INFO: model_training: Rank 0, Epoch 874, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,715: INFO: model_training: Rank 0, Epoch 874, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,716: INFO: model_training: Rank 0, Epoch 875, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,717: INFO: model_training: Rank 0, Epoch 875, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,718: INFO: model_training: Rank 0, Epoch 875, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,719: INFO: model_training: Rank 0, Epoch 875, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,720: INFO: model_training: Rank 0, Epoch 875, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,721: INFO: model_training: Rank 0, Epoch 876, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,722: INFO: model_training: Rank 0, Epoch 876, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,723: INFO: model_training: Rank 0, Epoch 876, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,725: INFO: model_training: Rank 0, Epoch 876, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,726: INFO: model_training: Rank 0, Epoch 876, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,728: INFO: model_training: Rank 0, Epoch 877, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,729: INFO: model_training: Rank 0, Epoch 877, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,730: INFO: model_training: Rank 0, Epoch 877, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,731: INFO: model_training: Rank 0, Epoch 877, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,732: INFO: model_training: Rank 0, Epoch 877, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,734: INFO: model_training: Rank 0, Epoch 878, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,735: INFO: model_training: Rank 0, Epoch 878, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,736: INFO: model_training: Rank 0, Epoch 878, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,738: INFO: model_training: Rank 0, Epoch 878, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,739: INFO: model_training: Rank 0, Epoch 878, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,741: INFO: model_training: Rank 0, Epoch 879, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,742: INFO: model_training: Rank 0, Epoch 879, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,744: INFO: model_training: Rank 0, Epoch 879, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,745: INFO: model_training: Rank 0, Epoch 879, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,746: INFO: model_training: Rank 0, Epoch 879, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,748: INFO: model_training: Rank 0, Epoch 880, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,749: INFO: model_training: Rank 0, Epoch 880, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,751: INFO: model_training: Rank 0, Epoch 880, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,753: INFO: model_training: Rank 0, Epoch 880, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,754: INFO: model_training: Rank 0, Epoch 880, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,756: INFO: model_training: Rank 0, Epoch 881, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,757: INFO: model_training: Rank 0, Epoch 881, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,759: INFO: model_training: Rank 0, Epoch 881, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,760: INFO: model_training: Rank 0, Epoch 881, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,761: INFO: model_training: Rank 0, Epoch 881, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,763: INFO: model_training: Rank 0, Epoch 882, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,764: INFO: model_training: Rank 0, Epoch 882, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,766: INFO: model_training: Rank 0, Epoch 882, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,767: INFO: model_training: Rank 0, Epoch 882, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,768: INFO: model_training: Rank 0, Epoch 882, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,770: INFO: model_training: Rank 0, Epoch 883, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,771: INFO: model_training: Rank 0, Epoch 883, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,772: INFO: model_training: Rank 0, Epoch 883, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,774: INFO: model_training: Rank 0, Epoch 883, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,775: INFO: model_training: Rank 0, Epoch 883, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,777: INFO: model_training: Rank 0, Epoch 884, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,778: INFO: model_training: Rank 0, Epoch 884, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,780: INFO: model_training: Rank 0, Epoch 884, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,781: INFO: model_training: Rank 0, Epoch 884, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,782: INFO: model_training: Rank 0, Epoch 884, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,783: INFO: model_training: Rank 0, Epoch 885, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,785: INFO: model_training: Rank 0, Epoch 885, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,786: INFO: model_training: Rank 0, Epoch 885, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,787: INFO: model_training: Rank 0, Epoch 885, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,788: INFO: model_training: Rank 0, Epoch 885, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,790: INFO: model_training: Rank 0, Epoch 886, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,791: INFO: model_training: Rank 0, Epoch 886, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,792: INFO: model_training: Rank 0, Epoch 886, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,793: INFO: model_training: Rank 0, Epoch 886, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,795: INFO: model_training: Rank 0, Epoch 886, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,796: INFO: model_training: Rank 0, Epoch 887, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,797: INFO: model_training: Rank 0, Epoch 887, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,798: INFO: model_training: Rank 0, Epoch 887, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,799: INFO: model_training: Rank 0, Epoch 887, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,800: INFO: model_training: Rank 0, Epoch 887, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,802: INFO: model_training: Rank 0, Epoch 888, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,804: INFO: model_training: Rank 0, Epoch 888, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,805: INFO: model_training: Rank 0, Epoch 888, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,806: INFO: model_training: Rank 0, Epoch 888, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,807: INFO: model_training: Rank 0, Epoch 888, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,808: INFO: model_training: Rank 0, Epoch 889, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,809: INFO: model_training: Rank 0, Epoch 889, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,810: INFO: model_training: Rank 0, Epoch 889, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,812: INFO: model_training: Rank 0, Epoch 889, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,813: INFO: model_training: Rank 0, Epoch 889, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,814: INFO: model_training: Rank 0, Epoch 890, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,815: INFO: model_training: Rank 0, Epoch 890, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,816: INFO: model_training: Rank 0, Epoch 890, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,817: INFO: model_training: Rank 0, Epoch 890, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,818: INFO: model_training: Rank 0, Epoch 890, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,819: INFO: model_training: Rank 0, Epoch 891, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,820: INFO: model_training: Rank 0, Epoch 891, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,821: INFO: model_training: Rank 0, Epoch 891, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,823: INFO: model_training: Rank 0, Epoch 891, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,824: INFO: model_training: Rank 0, Epoch 891, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,825: INFO: model_training: Rank 0, Epoch 892, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,826: INFO: model_training: Rank 0, Epoch 892, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,827: INFO: model_training: Rank 0, Epoch 892, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,829: INFO: model_training: Rank 0, Epoch 892, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,830: INFO: model_training: Rank 0, Epoch 892, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,831: INFO: model_training: Rank 0, Epoch 893, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,832: INFO: model_training: Rank 0, Epoch 893, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,833: INFO: model_training: Rank 0, Epoch 893, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,834: INFO: model_training: Rank 0, Epoch 893, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,835: INFO: model_training: Rank 0, Epoch 893, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,836: INFO: model_training: Rank 0, Epoch 894, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,837: INFO: model_training: Rank 0, Epoch 894, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,838: INFO: model_training: Rank 0, Epoch 894, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,839: INFO: model_training: Rank 0, Epoch 894, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,840: INFO: model_training: Rank 0, Epoch 894, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,841: INFO: model_training: Rank 0, Epoch 895, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,843: INFO: model_training: Rank 0, Epoch 895, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,844: INFO: model_training: Rank 0, Epoch 895, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,846: INFO: model_training: Rank 0, Epoch 895, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,847: INFO: model_training: Rank 0, Epoch 895, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,848: INFO: model_training: Rank 0, Epoch 896, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,849: INFO: model_training: Rank 0, Epoch 896, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,850: INFO: model_training: Rank 0, Epoch 896, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,851: INFO: model_training: Rank 0, Epoch 896, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,852: INFO: model_training: Rank 0, Epoch 896, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,854: INFO: model_training: Rank 0, Epoch 897, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,855: INFO: model_training: Rank 0, Epoch 897, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,856: INFO: model_training: Rank 0, Epoch 897, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,857: INFO: model_training: Rank 0, Epoch 897, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,858: INFO: model_training: Rank 0, Epoch 897, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,859: INFO: model_training: Rank 0, Epoch 898, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,860: INFO: model_training: Rank 0, Epoch 898, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,861: INFO: model_training: Rank 0, Epoch 898, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,862: INFO: model_training: Rank 0, Epoch 898, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,863: INFO: model_training: Rank 0, Epoch 898, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,864: INFO: model_training: Rank 0, Epoch 899, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,866: INFO: model_training: Rank 0, Epoch 899, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,867: INFO: model_training: Rank 0, Epoch 899, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,869: INFO: model_training: Rank 0, Epoch 899, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,870: INFO: model_training: Rank 0, Epoch 899, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,871: INFO: model_training: Rank 0, Epoch 900, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,872: INFO: model_training: Rank 0, Epoch 900, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,873: INFO: model_training: Rank 0, Epoch 900, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,875: INFO: model_training: Rank 0, Epoch 900, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,876: INFO: model_training: Rank 0, Epoch 900, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,877: INFO: model_training: Rank 0, Epoch 901, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,878: INFO: model_training: Rank 0, Epoch 901, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,879: INFO: model_training: Rank 0, Epoch 901, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,880: INFO: model_training: Rank 0, Epoch 901, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,881: INFO: model_training: Rank 0, Epoch 901, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,883: INFO: model_training: Rank 0, Epoch 902, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,884: INFO: model_training: Rank 0, Epoch 902, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,885: INFO: model_training: Rank 0, Epoch 902, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,886: INFO: model_training: Rank 0, Epoch 902, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,887: INFO: model_training: Rank 0, Epoch 902, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,889: INFO: model_training: Rank 0, Epoch 903, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,891: INFO: model_training: Rank 0, Epoch 903, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,892: INFO: model_training: Rank 0, Epoch 903, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,893: INFO: model_training: Rank 0, Epoch 903, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,894: INFO: model_training: Rank 0, Epoch 903, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,895: INFO: model_training: Rank 0, Epoch 904, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,897: INFO: model_training: Rank 0, Epoch 904, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,898: INFO: model_training: Rank 0, Epoch 904, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,898: INFO: model_training: Rank 0, Epoch 904, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,900: INFO: model_training: Rank 0, Epoch 904, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,901: INFO: model_training: Rank 0, Epoch 905, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,902: INFO: model_training: Rank 0, Epoch 905, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,903: INFO: model_training: Rank 0, Epoch 905, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,904: INFO: model_training: Rank 0, Epoch 905, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,905: INFO: model_training: Rank 0, Epoch 905, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,906: INFO: model_training: Rank 0, Epoch 906, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,908: INFO: model_training: Rank 0, Epoch 906, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,909: INFO: model_training: Rank 0, Epoch 906, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,910: INFO: model_training: Rank 0, Epoch 906, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,911: INFO: model_training: Rank 0, Epoch 906, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,913: INFO: model_training: Rank 0, Epoch 907, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,914: INFO: model_training: Rank 0, Epoch 907, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,915: INFO: model_training: Rank 0, Epoch 907, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,916: INFO: model_training: Rank 0, Epoch 907, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,917: INFO: model_training: Rank 0, Epoch 907, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,918: INFO: model_training: Rank 0, Epoch 908, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,919: INFO: model_training: Rank 0, Epoch 908, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,920: INFO: model_training: Rank 0, Epoch 908, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,922: INFO: model_training: Rank 0, Epoch 908, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,923: INFO: model_training: Rank 0, Epoch 908, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,924: INFO: model_training: Rank 0, Epoch 909, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,925: INFO: model_training: Rank 0, Epoch 909, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,926: INFO: model_training: Rank 0, Epoch 909, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,928: INFO: model_training: Rank 0, Epoch 909, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,929: INFO: model_training: Rank 0, Epoch 909, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,930: INFO: model_training: Rank 0, Epoch 910, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,931: INFO: model_training: Rank 0, Epoch 910, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,932: INFO: model_training: Rank 0, Epoch 910, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,934: INFO: model_training: Rank 0, Epoch 910, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,935: INFO: model_training: Rank 0, Epoch 910, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,936: INFO: model_training: Rank 0, Epoch 911, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,937: INFO: model_training: Rank 0, Epoch 911, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,938: INFO: model_training: Rank 0, Epoch 911, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,939: INFO: model_training: Rank 0, Epoch 911, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,941: INFO: model_training: Rank 0, Epoch 911, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,942: INFO: model_training: Rank 0, Epoch 912, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,943: INFO: model_training: Rank 0, Epoch 912, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,944: INFO: model_training: Rank 0, Epoch 912, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,945: INFO: model_training: Rank 0, Epoch 912, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,947: INFO: model_training: Rank 0, Epoch 912, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,948: INFO: model_training: Rank 0, Epoch 913, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,949: INFO: model_training: Rank 0, Epoch 913, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,950: INFO: model_training: Rank 0, Epoch 913, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,951: INFO: model_training: Rank 0, Epoch 913, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,952: INFO: model_training: Rank 0, Epoch 913, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,954: INFO: model_training: Rank 0, Epoch 914, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,955: INFO: model_training: Rank 0, Epoch 914, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,956: INFO: model_training: Rank 0, Epoch 914, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,957: INFO: model_training: Rank 0, Epoch 914, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,958: INFO: model_training: Rank 0, Epoch 914, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,959: INFO: model_training: Rank 0, Epoch 915, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,961: INFO: model_training: Rank 0, Epoch 915, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,962: INFO: model_training: Rank 0, Epoch 915, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,963: INFO: model_training: Rank 0, Epoch 915, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,964: INFO: model_training: Rank 0, Epoch 915, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,965: INFO: model_training: Rank 0, Epoch 916, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,966: INFO: model_training: Rank 0, Epoch 916, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,967: INFO: model_training: Rank 0, Epoch 916, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,968: INFO: model_training: Rank 0, Epoch 916, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,969: INFO: model_training: Rank 0, Epoch 916, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,970: INFO: model_training: Rank 0, Epoch 917, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,972: INFO: model_training: Rank 0, Epoch 917, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,973: INFO: model_training: Rank 0, Epoch 917, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,975: INFO: model_training: Rank 0, Epoch 917, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,976: INFO: model_training: Rank 0, Epoch 917, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,977: INFO: model_training: Rank 0, Epoch 918, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,979: INFO: model_training: Rank 0, Epoch 918, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,981: INFO: model_training: Rank 0, Epoch 918, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,983: INFO: model_training: Rank 0, Epoch 918, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,984: INFO: model_training: Rank 0, Epoch 918, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,986: INFO: model_training: Rank 0, Epoch 919, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,987: INFO: model_training: Rank 0, Epoch 919, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,988: INFO: model_training: Rank 0, Epoch 919, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,990: INFO: model_training: Rank 0, Epoch 919, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,991: INFO: model_training: Rank 0, Epoch 919, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:30,993: INFO: model_training: Rank 0, Epoch 920, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:30,994: INFO: model_training: Rank 0, Epoch 920, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:30,996: INFO: model_training: Rank 0, Epoch 920, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:30,997: INFO: model_training: Rank 0, Epoch 920, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:30,999: INFO: model_training: Rank 0, Epoch 920, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,000: INFO: model_training: Rank 0, Epoch 921, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,002: INFO: model_training: Rank 0, Epoch 921, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,003: INFO: model_training: Rank 0, Epoch 921, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,004: INFO: model_training: Rank 0, Epoch 921, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,006: INFO: model_training: Rank 0, Epoch 921, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,007: INFO: model_training: Rank 0, Epoch 922, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,009: INFO: model_training: Rank 0, Epoch 922, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,010: INFO: model_training: Rank 0, Epoch 922, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,011: INFO: model_training: Rank 0, Epoch 922, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,014: INFO: model_training: Rank 0, Epoch 922, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,017: INFO: model_training: Rank 0, Epoch 923, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,018: INFO: model_training: Rank 0, Epoch 923, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,020: INFO: model_training: Rank 0, Epoch 923, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,022: INFO: model_training: Rank 0, Epoch 923, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,024: INFO: model_training: Rank 0, Epoch 923, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,026: INFO: model_training: Rank 0, Epoch 924, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,028: INFO: model_training: Rank 0, Epoch 924, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,029: INFO: model_training: Rank 0, Epoch 924, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,031: INFO: model_training: Rank 0, Epoch 924, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,033: INFO: model_training: Rank 0, Epoch 924, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,035: INFO: model_training: Rank 0, Epoch 925, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,037: INFO: model_training: Rank 0, Epoch 925, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,040: INFO: model_training: Rank 0, Epoch 925, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,043: INFO: model_training: Rank 0, Epoch 925, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,045: INFO: model_training: Rank 0, Epoch 925, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,048: INFO: model_training: Rank 0, Epoch 926, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,050: INFO: model_training: Rank 0, Epoch 926, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,053: INFO: model_training: Rank 0, Epoch 926, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,056: INFO: model_training: Rank 0, Epoch 926, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,058: INFO: model_training: Rank 0, Epoch 926, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,059: INFO: model_training: Rank 0, Epoch 927, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,060: INFO: model_training: Rank 0, Epoch 927, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,062: INFO: model_training: Rank 0, Epoch 927, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,063: INFO: model_training: Rank 0, Epoch 927, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,064: INFO: model_training: Rank 0, Epoch 927, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,065: INFO: model_training: Rank 0, Epoch 928, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,067: INFO: model_training: Rank 0, Epoch 928, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,069: INFO: model_training: Rank 0, Epoch 928, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,070: INFO: model_training: Rank 0, Epoch 928, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,072: INFO: model_training: Rank 0, Epoch 928, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,074: INFO: model_training: Rank 0, Epoch 929, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,076: INFO: model_training: Rank 0, Epoch 929, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,078: INFO: model_training: Rank 0, Epoch 929, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,079: INFO: model_training: Rank 0, Epoch 929, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,080: INFO: model_training: Rank 0, Epoch 929, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,082: INFO: model_training: Rank 0, Epoch 930, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,083: INFO: model_training: Rank 0, Epoch 930, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,085: INFO: model_training: Rank 0, Epoch 930, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,086: INFO: model_training: Rank 0, Epoch 930, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,087: INFO: model_training: Rank 0, Epoch 930, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,088: INFO: model_training: Rank 0, Epoch 931, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,090: INFO: model_training: Rank 0, Epoch 931, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,091: INFO: model_training: Rank 0, Epoch 931, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,092: INFO: model_training: Rank 0, Epoch 931, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,094: INFO: model_training: Rank 0, Epoch 931, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,095: INFO: model_training: Rank 0, Epoch 932, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,096: INFO: model_training: Rank 0, Epoch 932, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,098: INFO: model_training: Rank 0, Epoch 932, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,099: INFO: model_training: Rank 0, Epoch 932, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,100: INFO: model_training: Rank 0, Epoch 932, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,102: INFO: model_training: Rank 0, Epoch 933, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,103: INFO: model_training: Rank 0, Epoch 933, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,104: INFO: model_training: Rank 0, Epoch 933, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,105: INFO: model_training: Rank 0, Epoch 933, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,107: INFO: model_training: Rank 0, Epoch 933, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,108: INFO: model_training: Rank 0, Epoch 934, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,110: INFO: model_training: Rank 0, Epoch 934, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,111: INFO: model_training: Rank 0, Epoch 934, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,112: INFO: model_training: Rank 0, Epoch 934, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,113: INFO: model_training: Rank 0, Epoch 934, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,114: INFO: model_training: Rank 0, Epoch 935, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,116: INFO: model_training: Rank 0, Epoch 935, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,117: INFO: model_training: Rank 0, Epoch 935, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,118: INFO: model_training: Rank 0, Epoch 935, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,120: INFO: model_training: Rank 0, Epoch 935, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,121: INFO: model_training: Rank 0, Epoch 936, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,122: INFO: model_training: Rank 0, Epoch 936, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,124: INFO: model_training: Rank 0, Epoch 936, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,125: INFO: model_training: Rank 0, Epoch 936, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,127: INFO: model_training: Rank 0, Epoch 936, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,128: INFO: model_training: Rank 0, Epoch 937, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,129: INFO: model_training: Rank 0, Epoch 937, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,131: INFO: model_training: Rank 0, Epoch 937, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,132: INFO: model_training: Rank 0, Epoch 937, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,133: INFO: model_training: Rank 0, Epoch 937, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,134: INFO: model_training: Rank 0, Epoch 938, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,136: INFO: model_training: Rank 0, Epoch 938, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,137: INFO: model_training: Rank 0, Epoch 938, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,139: INFO: model_training: Rank 0, Epoch 938, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,140: INFO: model_training: Rank 0, Epoch 938, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,141: INFO: model_training: Rank 0, Epoch 939, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,143: INFO: model_training: Rank 0, Epoch 939, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,144: INFO: model_training: Rank 0, Epoch 939, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,145: INFO: model_training: Rank 0, Epoch 939, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,147: INFO: model_training: Rank 0, Epoch 939, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,148: INFO: model_training: Rank 0, Epoch 940, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,149: INFO: model_training: Rank 0, Epoch 940, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,150: INFO: model_training: Rank 0, Epoch 940, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,152: INFO: model_training: Rank 0, Epoch 940, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,153: INFO: model_training: Rank 0, Epoch 940, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,154: INFO: model_training: Rank 0, Epoch 941, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,155: INFO: model_training: Rank 0, Epoch 941, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,156: INFO: model_training: Rank 0, Epoch 941, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,157: INFO: model_training: Rank 0, Epoch 941, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,158: INFO: model_training: Rank 0, Epoch 941, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,159: INFO: model_training: Rank 0, Epoch 942, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,161: INFO: model_training: Rank 0, Epoch 942, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,162: INFO: model_training: Rank 0, Epoch 942, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,163: INFO: model_training: Rank 0, Epoch 942, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,165: INFO: model_training: Rank 0, Epoch 942, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,167: INFO: model_training: Rank 0, Epoch 943, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,168: INFO: model_training: Rank 0, Epoch 943, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,169: INFO: model_training: Rank 0, Epoch 943, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,170: INFO: model_training: Rank 0, Epoch 943, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,172: INFO: model_training: Rank 0, Epoch 943, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,173: INFO: model_training: Rank 0, Epoch 944, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,174: INFO: model_training: Rank 0, Epoch 944, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,175: INFO: model_training: Rank 0, Epoch 944, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,177: INFO: model_training: Rank 0, Epoch 944, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,178: INFO: model_training: Rank 0, Epoch 944, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,179: INFO: model_training: Rank 0, Epoch 945, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,180: INFO: model_training: Rank 0, Epoch 945, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,181: INFO: model_training: Rank 0, Epoch 945, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,183: INFO: model_training: Rank 0, Epoch 945, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,184: INFO: model_training: Rank 0, Epoch 945, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,186: INFO: model_training: Rank 0, Epoch 946, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,187: INFO: model_training: Rank 0, Epoch 946, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,188: INFO: model_training: Rank 0, Epoch 946, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,190: INFO: model_training: Rank 0, Epoch 946, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,191: INFO: model_training: Rank 0, Epoch 946, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,192: INFO: model_training: Rank 0, Epoch 947, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,194: INFO: model_training: Rank 0, Epoch 947, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,195: INFO: model_training: Rank 0, Epoch 947, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,196: INFO: model_training: Rank 0, Epoch 947, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,198: INFO: model_training: Rank 0, Epoch 947, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,199: INFO: model_training: Rank 0, Epoch 948, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,200: INFO: model_training: Rank 0, Epoch 948, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,202: INFO: model_training: Rank 0, Epoch 948, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,203: INFO: model_training: Rank 0, Epoch 948, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,205: INFO: model_training: Rank 0, Epoch 948, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,206: INFO: model_training: Rank 0, Epoch 949, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,207: INFO: model_training: Rank 0, Epoch 949, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,208: INFO: model_training: Rank 0, Epoch 949, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,210: INFO: model_training: Rank 0, Epoch 949, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,212: INFO: model_training: Rank 0, Epoch 949, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,213: INFO: model_training: Rank 0, Epoch 950, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,214: INFO: model_training: Rank 0, Epoch 950, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,215: INFO: model_training: Rank 0, Epoch 950, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,217: INFO: model_training: Rank 0, Epoch 950, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,218: INFO: model_training: Rank 0, Epoch 950, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,219: INFO: model_training: Rank 0, Epoch 951, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,220: INFO: model_training: Rank 0, Epoch 951, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,221: INFO: model_training: Rank 0, Epoch 951, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,223: INFO: model_training: Rank 0, Epoch 951, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,224: INFO: model_training: Rank 0, Epoch 951, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,225: INFO: model_training: Rank 0, Epoch 952, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,227: INFO: model_training: Rank 0, Epoch 952, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,228: INFO: model_training: Rank 0, Epoch 952, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,229: INFO: model_training: Rank 0, Epoch 952, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,231: INFO: model_training: Rank 0, Epoch 952, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,232: INFO: model_training: Rank 0, Epoch 953, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,234: INFO: model_training: Rank 0, Epoch 953, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,236: INFO: model_training: Rank 0, Epoch 953, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,237: INFO: model_training: Rank 0, Epoch 953, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,238: INFO: model_training: Rank 0, Epoch 953, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,239: INFO: model_training: Rank 0, Epoch 954, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,241: INFO: model_training: Rank 0, Epoch 954, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,242: INFO: model_training: Rank 0, Epoch 954, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,244: INFO: model_training: Rank 0, Epoch 954, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,245: INFO: model_training: Rank 0, Epoch 954, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,246: INFO: model_training: Rank 0, Epoch 955, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,247: INFO: model_training: Rank 0, Epoch 955, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,249: INFO: model_training: Rank 0, Epoch 955, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,250: INFO: model_training: Rank 0, Epoch 955, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,252: INFO: model_training: Rank 0, Epoch 955, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,253: INFO: model_training: Rank 0, Epoch 956, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,254: INFO: model_training: Rank 0, Epoch 956, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,255: INFO: model_training: Rank 0, Epoch 956, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,257: INFO: model_training: Rank 0, Epoch 956, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,258: INFO: model_training: Rank 0, Epoch 956, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,260: INFO: model_training: Rank 0, Epoch 957, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,262: INFO: model_training: Rank 0, Epoch 957, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,263: INFO: model_training: Rank 0, Epoch 957, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,264: INFO: model_training: Rank 0, Epoch 957, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,265: INFO: model_training: Rank 0, Epoch 957, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,267: INFO: model_training: Rank 0, Epoch 958, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,269: INFO: model_training: Rank 0, Epoch 958, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,270: INFO: model_training: Rank 0, Epoch 958, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,271: INFO: model_training: Rank 0, Epoch 958, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,272: INFO: model_training: Rank 0, Epoch 958, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,274: INFO: model_training: Rank 0, Epoch 959, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,275: INFO: model_training: Rank 0, Epoch 959, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,277: INFO: model_training: Rank 0, Epoch 959, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,278: INFO: model_training: Rank 0, Epoch 959, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,279: INFO: model_training: Rank 0, Epoch 959, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,280: INFO: model_training: Rank 0, Epoch 960, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,282: INFO: model_training: Rank 0, Epoch 960, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,284: INFO: model_training: Rank 0, Epoch 960, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,287: INFO: model_training: Rank 0, Epoch 960, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,288: INFO: model_training: Rank 0, Epoch 960, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,290: INFO: model_training: Rank 0, Epoch 961, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,291: INFO: model_training: Rank 0, Epoch 961, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,293: INFO: model_training: Rank 0, Epoch 961, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,295: INFO: model_training: Rank 0, Epoch 961, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,296: INFO: model_training: Rank 0, Epoch 961, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,297: INFO: model_training: Rank 0, Epoch 962, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,299: INFO: model_training: Rank 0, Epoch 962, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,300: INFO: model_training: Rank 0, Epoch 962, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,302: INFO: model_training: Rank 0, Epoch 962, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,304: INFO: model_training: Rank 0, Epoch 962, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,305: INFO: model_training: Rank 0, Epoch 963, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,307: INFO: model_training: Rank 0, Epoch 963, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,308: INFO: model_training: Rank 0, Epoch 963, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,310: INFO: model_training: Rank 0, Epoch 963, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,311: INFO: model_training: Rank 0, Epoch 963, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,314: INFO: model_training: Rank 0, Epoch 964, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,315: INFO: model_training: Rank 0, Epoch 964, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,316: INFO: model_training: Rank 0, Epoch 964, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,318: INFO: model_training: Rank 0, Epoch 964, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,319: INFO: model_training: Rank 0, Epoch 964, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,321: INFO: model_training: Rank 0, Epoch 965, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,322: INFO: model_training: Rank 0, Epoch 965, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,323: INFO: model_training: Rank 0, Epoch 965, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,324: INFO: model_training: Rank 0, Epoch 965, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,325: INFO: model_training: Rank 0, Epoch 965, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,327: INFO: model_training: Rank 0, Epoch 966, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,328: INFO: model_training: Rank 0, Epoch 966, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,329: INFO: model_training: Rank 0, Epoch 966, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,330: INFO: model_training: Rank 0, Epoch 966, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,331: INFO: model_training: Rank 0, Epoch 966, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,333: INFO: model_training: Rank 0, Epoch 967, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,334: INFO: model_training: Rank 0, Epoch 967, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,336: INFO: model_training: Rank 0, Epoch 967, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,337: INFO: model_training: Rank 0, Epoch 967, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,338: INFO: model_training: Rank 0, Epoch 967, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,340: INFO: model_training: Rank 0, Epoch 968, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,341: INFO: model_training: Rank 0, Epoch 968, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,343: INFO: model_training: Rank 0, Epoch 968, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,344: INFO: model_training: Rank 0, Epoch 968, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,345: INFO: model_training: Rank 0, Epoch 968, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,346: INFO: model_training: Rank 0, Epoch 969, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,347: INFO: model_training: Rank 0, Epoch 969, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,349: INFO: model_training: Rank 0, Epoch 969, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,350: INFO: model_training: Rank 0, Epoch 969, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,352: INFO: model_training: Rank 0, Epoch 969, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,353: INFO: model_training: Rank 0, Epoch 970, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,355: INFO: model_training: Rank 0, Epoch 970, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,356: INFO: model_training: Rank 0, Epoch 970, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,358: INFO: model_training: Rank 0, Epoch 970, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,359: INFO: model_training: Rank 0, Epoch 970, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,361: INFO: model_training: Rank 0, Epoch 971, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,363: INFO: model_training: Rank 0, Epoch 971, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,365: INFO: model_training: Rank 0, Epoch 971, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,366: INFO: model_training: Rank 0, Epoch 971, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,368: INFO: model_training: Rank 0, Epoch 971, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,369: INFO: model_training: Rank 0, Epoch 972, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,371: INFO: model_training: Rank 0, Epoch 972, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,372: INFO: model_training: Rank 0, Epoch 972, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,373: INFO: model_training: Rank 0, Epoch 972, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,374: INFO: model_training: Rank 0, Epoch 972, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,376: INFO: model_training: Rank 0, Epoch 973, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,377: INFO: model_training: Rank 0, Epoch 973, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,379: INFO: model_training: Rank 0, Epoch 973, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,380: INFO: model_training: Rank 0, Epoch 973, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,381: INFO: model_training: Rank 0, Epoch 973, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,383: INFO: model_training: Rank 0, Epoch 974, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,385: INFO: model_training: Rank 0, Epoch 974, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,386: INFO: model_training: Rank 0, Epoch 974, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,387: INFO: model_training: Rank 0, Epoch 974, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,389: INFO: model_training: Rank 0, Epoch 974, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,390: INFO: model_training: Rank 0, Epoch 975, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,392: INFO: model_training: Rank 0, Epoch 975, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,393: INFO: model_training: Rank 0, Epoch 975, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,394: INFO: model_training: Rank 0, Epoch 975, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,395: INFO: model_training: Rank 0, Epoch 975, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,397: INFO: model_training: Rank 0, Epoch 976, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,398: INFO: model_training: Rank 0, Epoch 976, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,399: INFO: model_training: Rank 0, Epoch 976, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,401: INFO: model_training: Rank 0, Epoch 976, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,402: INFO: model_training: Rank 0, Epoch 976, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,403: INFO: model_training: Rank 0, Epoch 977, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,404: INFO: model_training: Rank 0, Epoch 977, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,405: INFO: model_training: Rank 0, Epoch 977, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,407: INFO: model_training: Rank 0, Epoch 977, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,408: INFO: model_training: Rank 0, Epoch 977, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,410: INFO: model_training: Rank 0, Epoch 978, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,411: INFO: model_training: Rank 0, Epoch 978, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,413: INFO: model_training: Rank 0, Epoch 978, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,414: INFO: model_training: Rank 0, Epoch 978, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,415: INFO: model_training: Rank 0, Epoch 978, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,417: INFO: model_training: Rank 0, Epoch 979, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,418: INFO: model_training: Rank 0, Epoch 979, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,420: INFO: model_training: Rank 0, Epoch 979, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,421: INFO: model_training: Rank 0, Epoch 979, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,422: INFO: model_training: Rank 0, Epoch 979, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,423: INFO: model_training: Rank 0, Epoch 980, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,425: INFO: model_training: Rank 0, Epoch 980, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,426: INFO: model_training: Rank 0, Epoch 980, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,427: INFO: model_training: Rank 0, Epoch 980, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,428: INFO: model_training: Rank 0, Epoch 980, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,429: INFO: model_training: Rank 0, Epoch 981, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,431: INFO: model_training: Rank 0, Epoch 981, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,432: INFO: model_training: Rank 0, Epoch 981, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,433: INFO: model_training: Rank 0, Epoch 981, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,435: INFO: model_training: Rank 0, Epoch 981, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,436: INFO: model_training: Rank 0, Epoch 982, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,437: INFO: model_training: Rank 0, Epoch 982, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,439: INFO: model_training: Rank 0, Epoch 982, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,441: INFO: model_training: Rank 0, Epoch 982, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,442: INFO: model_training: Rank 0, Epoch 982, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,444: INFO: model_training: Rank 0, Epoch 983, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,445: INFO: model_training: Rank 0, Epoch 983, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,446: INFO: model_training: Rank 0, Epoch 983, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,447: INFO: model_training: Rank 0, Epoch 983, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,448: INFO: model_training: Rank 0, Epoch 983, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,450: INFO: model_training: Rank 0, Epoch 984, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,451: INFO: model_training: Rank 0, Epoch 984, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,453: INFO: model_training: Rank 0, Epoch 984, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,454: INFO: model_training: Rank 0, Epoch 984, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,455: INFO: model_training: Rank 0, Epoch 984, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,456: INFO: model_training: Rank 0, Epoch 985, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,458: INFO: model_training: Rank 0, Epoch 985, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,459: INFO: model_training: Rank 0, Epoch 985, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,460: INFO: model_training: Rank 0, Epoch 985, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,461: INFO: model_training: Rank 0, Epoch 985, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,463: INFO: model_training: Rank 0, Epoch 986, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,464: INFO: model_training: Rank 0, Epoch 986, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,466: INFO: model_training: Rank 0, Epoch 986, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,467: INFO: model_training: Rank 0, Epoch 986, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,468: INFO: model_training: Rank 0, Epoch 986, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,470: INFO: model_training: Rank 0, Epoch 987, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,471: INFO: model_training: Rank 0, Epoch 987, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,472: INFO: model_training: Rank 0, Epoch 987, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,474: INFO: model_training: Rank 0, Epoch 987, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,475: INFO: model_training: Rank 0, Epoch 987, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,476: INFO: model_training: Rank 0, Epoch 988, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,478: INFO: model_training: Rank 0, Epoch 988, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,479: INFO: model_training: Rank 0, Epoch 988, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,480: INFO: model_training: Rank 0, Epoch 988, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,481: INFO: model_training: Rank 0, Epoch 988, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,483: INFO: model_training: Rank 0, Epoch 989, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,484: INFO: model_training: Rank 0, Epoch 989, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,485: INFO: model_training: Rank 0, Epoch 989, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,487: INFO: model_training: Rank 0, Epoch 989, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,488: INFO: model_training: Rank 0, Epoch 989, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,489: INFO: model_training: Rank 0, Epoch 990, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,491: INFO: model_training: Rank 0, Epoch 990, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,492: INFO: model_training: Rank 0, Epoch 990, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,493: INFO: model_training: Rank 0, Epoch 990, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,495: INFO: model_training: Rank 0, Epoch 990, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,496: INFO: model_training: Rank 0, Epoch 991, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,497: INFO: model_training: Rank 0, Epoch 991, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,498: INFO: model_training: Rank 0, Epoch 991, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,499: INFO: model_training: Rank 0, Epoch 991, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,500: INFO: model_training: Rank 0, Epoch 991, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,501: INFO: model_training: Rank 0, Epoch 992, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,503: INFO: model_training: Rank 0, Epoch 992, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,504: INFO: model_training: Rank 0, Epoch 992, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,506: INFO: model_training: Rank 0, Epoch 992, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,507: INFO: model_training: Rank 0, Epoch 992, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,508: INFO: model_training: Rank 0, Epoch 993, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,510: INFO: model_training: Rank 0, Epoch 993, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,511: INFO: model_training: Rank 0, Epoch 993, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,513: INFO: model_training: Rank 0, Epoch 993, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,514: INFO: model_training: Rank 0, Epoch 993, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,515: INFO: model_training: Rank 0, Epoch 994, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,517: INFO: model_training: Rank 0, Epoch 994, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,518: INFO: model_training: Rank 0, Epoch 994, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,519: INFO: model_training: Rank 0, Epoch 994, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,521: INFO: model_training: Rank 0, Epoch 994, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,522: INFO: model_training: Rank 0, Epoch 995, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,523: INFO: model_training: Rank 0, Epoch 995, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,525: INFO: model_training: Rank 0, Epoch 995, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,526: INFO: model_training: Rank 0, Epoch 995, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,531: INFO: model_training: Rank 0, Epoch 995, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,546: INFO: model_training: Rank 0, Epoch 996, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,554: INFO: model_training: Rank 0, Epoch 996, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,555: INFO: model_training: Rank 0, Epoch 996, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,557: INFO: model_training: Rank 0, Epoch 996, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,559: INFO: model_training: Rank 0, Epoch 996, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,564: INFO: model_training: Rank 0, Epoch 997, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,566: INFO: model_training: Rank 0, Epoch 997, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,568: INFO: model_training: Rank 0, Epoch 997, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,569: INFO: model_training: Rank 0, Epoch 997, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,570: INFO: model_training: Rank 0, Epoch 997, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,572: INFO: model_training: Rank 0, Epoch 998, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,573: INFO: model_training: Rank 0, Epoch 998, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,574: INFO: model_training: Rank 0, Epoch 998, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,575: INFO: model_training: Rank 0, Epoch 998, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,577: INFO: model_training: Rank 0, Epoch 998, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,578: INFO: model_training: Rank 0, Epoch 999, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,579: INFO: model_training: Rank 0, Epoch 999, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,580: INFO: model_training: Rank 0, Epoch 999, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,582: INFO: model_training: Rank 0, Epoch 999, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,583: INFO: model_training: Rank 0, Epoch 999, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,584: INFO: model_training: Rank 0, Epoch 1000, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,586: INFO: model_training: Rank 0, Epoch 1000, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,587: INFO: model_training: Rank 0, Epoch 1000, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,588: INFO: model_training: Rank 0, Epoch 1000, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,590: INFO: model_training: Rank 0, Epoch 1000, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,592: INFO: model_training: Rank 0, Epoch 1001, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,593: INFO: model_training: Rank 0, Epoch 1001, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,595: INFO: model_training: Rank 0, Epoch 1001, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,596: INFO: model_training: Rank 0, Epoch 1001, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,597: INFO: model_training: Rank 0, Epoch 1001, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,599: INFO: model_training: Rank 0, Epoch 1002, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,600: INFO: model_training: Rank 0, Epoch 1002, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,601: INFO: model_training: Rank 0, Epoch 1002, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,603: INFO: model_training: Rank 0, Epoch 1002, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,604: INFO: model_training: Rank 0, Epoch 1002, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,605: INFO: model_training: Rank 0, Epoch 1003, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,607: INFO: model_training: Rank 0, Epoch 1003, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,608: INFO: model_training: Rank 0, Epoch 1003, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,609: INFO: model_training: Rank 0, Epoch 1003, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,611: INFO: model_training: Rank 0, Epoch 1003, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,613: INFO: model_training: Rank 0, Epoch 1004, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,614: INFO: model_training: Rank 0, Epoch 1004, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,615: INFO: model_training: Rank 0, Epoch 1004, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,617: INFO: model_training: Rank 0, Epoch 1004, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,619: INFO: model_training: Rank 0, Epoch 1004, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,620: INFO: model_training: Rank 0, Epoch 1005, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,621: INFO: model_training: Rank 0, Epoch 1005, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,622: INFO: model_training: Rank 0, Epoch 1005, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,624: INFO: model_training: Rank 0, Epoch 1005, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,625: INFO: model_training: Rank 0, Epoch 1005, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,626: INFO: model_training: Rank 0, Epoch 1006, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,628: INFO: model_training: Rank 0, Epoch 1006, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,629: INFO: model_training: Rank 0, Epoch 1006, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,630: INFO: model_training: Rank 0, Epoch 1006, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,631: INFO: model_training: Rank 0, Epoch 1006, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,632: INFO: model_training: Rank 0, Epoch 1007, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,634: INFO: model_training: Rank 0, Epoch 1007, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,635: INFO: model_training: Rank 0, Epoch 1007, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,637: INFO: model_training: Rank 0, Epoch 1007, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,638: INFO: model_training: Rank 0, Epoch 1007, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,639: INFO: model_training: Rank 0, Epoch 1008, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,640: INFO: model_training: Rank 0, Epoch 1008, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,642: INFO: model_training: Rank 0, Epoch 1008, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,644: INFO: model_training: Rank 0, Epoch 1008, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,645: INFO: model_training: Rank 0, Epoch 1008, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,646: INFO: model_training: Rank 0, Epoch 1009, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,647: INFO: model_training: Rank 0, Epoch 1009, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,649: INFO: model_training: Rank 0, Epoch 1009, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,650: INFO: model_training: Rank 0, Epoch 1009, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,652: INFO: model_training: Rank 0, Epoch 1009, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,654: INFO: model_training: Rank 0, Epoch 1010, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,655: INFO: model_training: Rank 0, Epoch 1010, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,656: INFO: model_training: Rank 0, Epoch 1010, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,657: INFO: model_training: Rank 0, Epoch 1010, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,659: INFO: model_training: Rank 0, Epoch 1010, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,661: INFO: model_training: Rank 0, Epoch 1011, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,663: INFO: model_training: Rank 0, Epoch 1011, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,664: INFO: model_training: Rank 0, Epoch 1011, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,665: INFO: model_training: Rank 0, Epoch 1011, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,667: INFO: model_training: Rank 0, Epoch 1011, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,668: INFO: model_training: Rank 0, Epoch 1012, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,670: INFO: model_training: Rank 0, Epoch 1012, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,672: INFO: model_training: Rank 0, Epoch 1012, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,673: INFO: model_training: Rank 0, Epoch 1012, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,675: INFO: model_training: Rank 0, Epoch 1012, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,676: INFO: model_training: Rank 0, Epoch 1013, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,678: INFO: model_training: Rank 0, Epoch 1013, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,679: INFO: model_training: Rank 0, Epoch 1013, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,680: INFO: model_training: Rank 0, Epoch 1013, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,682: INFO: model_training: Rank 0, Epoch 1013, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,683: INFO: model_training: Rank 0, Epoch 1014, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,684: INFO: model_training: Rank 0, Epoch 1014, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,686: INFO: model_training: Rank 0, Epoch 1014, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,687: INFO: model_training: Rank 0, Epoch 1014, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,688: INFO: model_training: Rank 0, Epoch 1014, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,689: INFO: model_training: Rank 0, Epoch 1015, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,690: INFO: model_training: Rank 0, Epoch 1015, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,692: INFO: model_training: Rank 0, Epoch 1015, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,693: INFO: model_training: Rank 0, Epoch 1015, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,694: INFO: model_training: Rank 0, Epoch 1015, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,695: INFO: model_training: Rank 0, Epoch 1016, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,697: INFO: model_training: Rank 0, Epoch 1016, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,699: INFO: model_training: Rank 0, Epoch 1016, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,700: INFO: model_training: Rank 0, Epoch 1016, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,701: INFO: model_training: Rank 0, Epoch 1016, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,702: INFO: model_training: Rank 0, Epoch 1017, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,704: INFO: model_training: Rank 0, Epoch 1017, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,705: INFO: model_training: Rank 0, Epoch 1017, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,706: INFO: model_training: Rank 0, Epoch 1017, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,708: INFO: model_training: Rank 0, Epoch 1017, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,709: INFO: model_training: Rank 0, Epoch 1018, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,711: INFO: model_training: Rank 0, Epoch 1018, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,712: INFO: model_training: Rank 0, Epoch 1018, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,713: INFO: model_training: Rank 0, Epoch 1018, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,715: INFO: model_training: Rank 0, Epoch 1018, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,716: INFO: model_training: Rank 0, Epoch 1019, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,717: INFO: model_training: Rank 0, Epoch 1019, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,719: INFO: model_training: Rank 0, Epoch 1019, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,720: INFO: model_training: Rank 0, Epoch 1019, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,722: INFO: model_training: Rank 0, Epoch 1019, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,723: INFO: model_training: Rank 0, Epoch 1020, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,725: INFO: model_training: Rank 0, Epoch 1020, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,726: INFO: model_training: Rank 0, Epoch 1020, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,728: INFO: model_training: Rank 0, Epoch 1020, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,729: INFO: model_training: Rank 0, Epoch 1020, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,730: INFO: model_training: Rank 0, Epoch 1021, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,732: INFO: model_training: Rank 0, Epoch 1021, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,733: INFO: model_training: Rank 0, Epoch 1021, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,734: INFO: model_training: Rank 0, Epoch 1021, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,736: INFO: model_training: Rank 0, Epoch 1021, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,737: INFO: model_training: Rank 0, Epoch 1022, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,738: INFO: model_training: Rank 0, Epoch 1022, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,739: INFO: model_training: Rank 0, Epoch 1022, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,741: INFO: model_training: Rank 0, Epoch 1022, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,742: INFO: model_training: Rank 0, Epoch 1022, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,744: INFO: model_training: Rank 0, Epoch 1023, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,745: INFO: model_training: Rank 0, Epoch 1023, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,746: INFO: model_training: Rank 0, Epoch 1023, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,748: INFO: model_training: Rank 0, Epoch 1023, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,749: INFO: model_training: Rank 0, Epoch 1023, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,751: INFO: model_training: Rank 0, Epoch 1024, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,752: INFO: model_training: Rank 0, Epoch 1024, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,753: INFO: model_training: Rank 0, Epoch 1024, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,755: INFO: model_training: Rank 0, Epoch 1024, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,756: INFO: model_training: Rank 0, Epoch 1024, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,758: INFO: model_training: Rank 0, Epoch 1025, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,759: INFO: model_training: Rank 0, Epoch 1025, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,761: INFO: model_training: Rank 0, Epoch 1025, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,762: INFO: model_training: Rank 0, Epoch 1025, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,763: INFO: model_training: Rank 0, Epoch 1025, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,764: INFO: model_training: Rank 0, Epoch 1026, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,766: INFO: model_training: Rank 0, Epoch 1026, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,767: INFO: model_training: Rank 0, Epoch 1026, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,770: INFO: model_training: Rank 0, Epoch 1026, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,771: INFO: model_training: Rank 0, Epoch 1026, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,772: INFO: model_training: Rank 0, Epoch 1027, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,774: INFO: model_training: Rank 0, Epoch 1027, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,775: INFO: model_training: Rank 0, Epoch 1027, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,776: INFO: model_training: Rank 0, Epoch 1027, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,777: INFO: model_training: Rank 0, Epoch 1027, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,779: INFO: model_training: Rank 0, Epoch 1028, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,780: INFO: model_training: Rank 0, Epoch 1028, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,782: INFO: model_training: Rank 0, Epoch 1028, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,783: INFO: model_training: Rank 0, Epoch 1028, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,784: INFO: model_training: Rank 0, Epoch 1028, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,786: INFO: model_training: Rank 0, Epoch 1029, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,787: INFO: model_training: Rank 0, Epoch 1029, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,789: INFO: model_training: Rank 0, Epoch 1029, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,790: INFO: model_training: Rank 0, Epoch 1029, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,791: INFO: model_training: Rank 0, Epoch 1029, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,793: INFO: model_training: Rank 0, Epoch 1030, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,795: INFO: model_training: Rank 0, Epoch 1030, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,796: INFO: model_training: Rank 0, Epoch 1030, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,797: INFO: model_training: Rank 0, Epoch 1030, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,799: INFO: model_training: Rank 0, Epoch 1030, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,800: INFO: model_training: Rank 0, Epoch 1031, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,802: INFO: model_training: Rank 0, Epoch 1031, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,803: INFO: model_training: Rank 0, Epoch 1031, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,805: INFO: model_training: Rank 0, Epoch 1031, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,807: INFO: model_training: Rank 0, Epoch 1031, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,808: INFO: model_training: Rank 0, Epoch 1032, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,810: INFO: model_training: Rank 0, Epoch 1032, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,811: INFO: model_training: Rank 0, Epoch 1032, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,812: INFO: model_training: Rank 0, Epoch 1032, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,814: INFO: model_training: Rank 0, Epoch 1032, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,815: INFO: model_training: Rank 0, Epoch 1033, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,816: INFO: model_training: Rank 0, Epoch 1033, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,817: INFO: model_training: Rank 0, Epoch 1033, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,819: INFO: model_training: Rank 0, Epoch 1033, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,821: INFO: model_training: Rank 0, Epoch 1033, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,822: INFO: model_training: Rank 0, Epoch 1034, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,823: INFO: model_training: Rank 0, Epoch 1034, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,825: INFO: model_training: Rank 0, Epoch 1034, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,826: INFO: model_training: Rank 0, Epoch 1034, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,828: INFO: model_training: Rank 0, Epoch 1034, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,829: INFO: model_training: Rank 0, Epoch 1035, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,830: INFO: model_training: Rank 0, Epoch 1035, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,831: INFO: model_training: Rank 0, Epoch 1035, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,833: INFO: model_training: Rank 0, Epoch 1035, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,834: INFO: model_training: Rank 0, Epoch 1035, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,836: INFO: model_training: Rank 0, Epoch 1036, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,837: INFO: model_training: Rank 0, Epoch 1036, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,838: INFO: model_training: Rank 0, Epoch 1036, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,840: INFO: model_training: Rank 0, Epoch 1036, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,841: INFO: model_training: Rank 0, Epoch 1036, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,843: INFO: model_training: Rank 0, Epoch 1037, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,844: INFO: model_training: Rank 0, Epoch 1037, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,845: INFO: model_training: Rank 0, Epoch 1037, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,846: INFO: model_training: Rank 0, Epoch 1037, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,848: INFO: model_training: Rank 0, Epoch 1037, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,849: INFO: model_training: Rank 0, Epoch 1038, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,850: INFO: model_training: Rank 0, Epoch 1038, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,852: INFO: model_training: Rank 0, Epoch 1038, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,853: INFO: model_training: Rank 0, Epoch 1038, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,854: INFO: model_training: Rank 0, Epoch 1038, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,855: INFO: model_training: Rank 0, Epoch 1039, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,857: INFO: model_training: Rank 0, Epoch 1039, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,858: INFO: model_training: Rank 0, Epoch 1039, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,859: INFO: model_training: Rank 0, Epoch 1039, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,861: INFO: model_training: Rank 0, Epoch 1039, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,863: INFO: model_training: Rank 0, Epoch 1040, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,864: INFO: model_training: Rank 0, Epoch 1040, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,865: INFO: model_training: Rank 0, Epoch 1040, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,867: INFO: model_training: Rank 0, Epoch 1040, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,868: INFO: model_training: Rank 0, Epoch 1040, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,870: INFO: model_training: Rank 0, Epoch 1041, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,871: INFO: model_training: Rank 0, Epoch 1041, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,872: INFO: model_training: Rank 0, Epoch 1041, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,874: INFO: model_training: Rank 0, Epoch 1041, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,875: INFO: model_training: Rank 0, Epoch 1041, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,877: INFO: model_training: Rank 0, Epoch 1042, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,878: INFO: model_training: Rank 0, Epoch 1042, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,879: INFO: model_training: Rank 0, Epoch 1042, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,880: INFO: model_training: Rank 0, Epoch 1042, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,882: INFO: model_training: Rank 0, Epoch 1042, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,883: INFO: model_training: Rank 0, Epoch 1043, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,885: INFO: model_training: Rank 0, Epoch 1043, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,887: INFO: model_training: Rank 0, Epoch 1043, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,887: INFO: model_training: Rank 0, Epoch 1043, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,889: INFO: model_training: Rank 0, Epoch 1043, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,890: INFO: model_training: Rank 0, Epoch 1044, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,891: INFO: model_training: Rank 0, Epoch 1044, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,892: INFO: model_training: Rank 0, Epoch 1044, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,894: INFO: model_training: Rank 0, Epoch 1044, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,895: INFO: model_training: Rank 0, Epoch 1044, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,896: INFO: model_training: Rank 0, Epoch 1045, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,897: INFO: model_training: Rank 0, Epoch 1045, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,899: INFO: model_training: Rank 0, Epoch 1045, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,900: INFO: model_training: Rank 0, Epoch 1045, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,902: INFO: model_training: Rank 0, Epoch 1045, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,903: INFO: model_training: Rank 0, Epoch 1046, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,905: INFO: model_training: Rank 0, Epoch 1046, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,906: INFO: model_training: Rank 0, Epoch 1046, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,908: INFO: model_training: Rank 0, Epoch 1046, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,909: INFO: model_training: Rank 0, Epoch 1046, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,911: INFO: model_training: Rank 0, Epoch 1047, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,913: INFO: model_training: Rank 0, Epoch 1047, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,914: INFO: model_training: Rank 0, Epoch 1047, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,916: INFO: model_training: Rank 0, Epoch 1047, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,917: INFO: model_training: Rank 0, Epoch 1047, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,919: INFO: model_training: Rank 0, Epoch 1048, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,921: INFO: model_training: Rank 0, Epoch 1048, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,923: INFO: model_training: Rank 0, Epoch 1048, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,924: INFO: model_training: Rank 0, Epoch 1048, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,925: INFO: model_training: Rank 0, Epoch 1048, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,927: INFO: model_training: Rank 0, Epoch 1049, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,928: INFO: model_training: Rank 0, Epoch 1049, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,930: INFO: model_training: Rank 0, Epoch 1049, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,931: INFO: model_training: Rank 0, Epoch 1049, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,932: INFO: model_training: Rank 0, Epoch 1049, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,934: INFO: model_training: Rank 0, Epoch 1050, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,935: INFO: model_training: Rank 0, Epoch 1050, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,937: INFO: model_training: Rank 0, Epoch 1050, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,938: INFO: model_training: Rank 0, Epoch 1050, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,940: INFO: model_training: Rank 0, Epoch 1050, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,942: INFO: model_training: Rank 0, Epoch 1051, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,944: INFO: model_training: Rank 0, Epoch 1051, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,945: INFO: model_training: Rank 0, Epoch 1051, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,946: INFO: model_training: Rank 0, Epoch 1051, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,948: INFO: model_training: Rank 0, Epoch 1051, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,949: INFO: model_training: Rank 0, Epoch 1052, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,950: INFO: model_training: Rank 0, Epoch 1052, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,952: INFO: model_training: Rank 0, Epoch 1052, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,953: INFO: model_training: Rank 0, Epoch 1052, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,955: INFO: model_training: Rank 0, Epoch 1052, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,956: INFO: model_training: Rank 0, Epoch 1053, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,957: INFO: model_training: Rank 0, Epoch 1053, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,959: INFO: model_training: Rank 0, Epoch 1053, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,961: INFO: model_training: Rank 0, Epoch 1053, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,962: INFO: model_training: Rank 0, Epoch 1053, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,963: INFO: model_training: Rank 0, Epoch 1054, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,965: INFO: model_training: Rank 0, Epoch 1054, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,966: INFO: model_training: Rank 0, Epoch 1054, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,967: INFO: model_training: Rank 0, Epoch 1054, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,969: INFO: model_training: Rank 0, Epoch 1054, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,971: INFO: model_training: Rank 0, Epoch 1055, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,972: INFO: model_training: Rank 0, Epoch 1055, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,973: INFO: model_training: Rank 0, Epoch 1055, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,974: INFO: model_training: Rank 0, Epoch 1055, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,976: INFO: model_training: Rank 0, Epoch 1055, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,978: INFO: model_training: Rank 0, Epoch 1056, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,980: INFO: model_training: Rank 0, Epoch 1056, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,982: INFO: model_training: Rank 0, Epoch 1056, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,983: INFO: model_training: Rank 0, Epoch 1056, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,985: INFO: model_training: Rank 0, Epoch 1056, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,987: INFO: model_training: Rank 0, Epoch 1057, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,989: INFO: model_training: Rank 0, Epoch 1057, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,990: INFO: model_training: Rank 0, Epoch 1057, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:31,992: INFO: model_training: Rank 0, Epoch 1057, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:31,994: INFO: model_training: Rank 0, Epoch 1057, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:31,996: INFO: model_training: Rank 0, Epoch 1058, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:31,998: INFO: model_training: Rank 0, Epoch 1058, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:31,999: INFO: model_training: Rank 0, Epoch 1058, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,001: INFO: model_training: Rank 0, Epoch 1058, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,004: INFO: model_training: Rank 0, Epoch 1058, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,006: INFO: model_training: Rank 0, Epoch 1059, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,007: INFO: model_training: Rank 0, Epoch 1059, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,009: INFO: model_training: Rank 0, Epoch 1059, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,011: INFO: model_training: Rank 0, Epoch 1059, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,013: INFO: model_training: Rank 0, Epoch 1059, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,014: INFO: model_training: Rank 0, Epoch 1060, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,015: INFO: model_training: Rank 0, Epoch 1060, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,017: INFO: model_training: Rank 0, Epoch 1060, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,019: INFO: model_training: Rank 0, Epoch 1060, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,020: INFO: model_training: Rank 0, Epoch 1060, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,022: INFO: model_training: Rank 0, Epoch 1061, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,024: INFO: model_training: Rank 0, Epoch 1061, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,027: INFO: model_training: Rank 0, Epoch 1061, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,029: INFO: model_training: Rank 0, Epoch 1061, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,032: INFO: model_training: Rank 0, Epoch 1061, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,036: INFO: model_training: Rank 0, Epoch 1062, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,040: INFO: model_training: Rank 0, Epoch 1062, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,044: INFO: model_training: Rank 0, Epoch 1062, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,048: INFO: model_training: Rank 0, Epoch 1062, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,052: INFO: model_training: Rank 0, Epoch 1062, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,054: INFO: model_training: Rank 0, Epoch 1063, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,057: INFO: model_training: Rank 0, Epoch 1063, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,060: INFO: model_training: Rank 0, Epoch 1063, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,063: INFO: model_training: Rank 0, Epoch 1063, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,065: INFO: model_training: Rank 0, Epoch 1063, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,067: INFO: model_training: Rank 0, Epoch 1064, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,068: INFO: model_training: Rank 0, Epoch 1064, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,070: INFO: model_training: Rank 0, Epoch 1064, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,072: INFO: model_training: Rank 0, Epoch 1064, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,073: INFO: model_training: Rank 0, Epoch 1064, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,075: INFO: model_training: Rank 0, Epoch 1065, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,076: INFO: model_training: Rank 0, Epoch 1065, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,077: INFO: model_training: Rank 0, Epoch 1065, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,079: INFO: model_training: Rank 0, Epoch 1065, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,081: INFO: model_training: Rank 0, Epoch 1065, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,083: INFO: model_training: Rank 0, Epoch 1066, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,084: INFO: model_training: Rank 0, Epoch 1066, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,086: INFO: model_training: Rank 0, Epoch 1066, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,088: INFO: model_training: Rank 0, Epoch 1066, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,090: INFO: model_training: Rank 0, Epoch 1066, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,091: INFO: model_training: Rank 0, Epoch 1067, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,092: INFO: model_training: Rank 0, Epoch 1067, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,094: INFO: model_training: Rank 0, Epoch 1067, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,096: INFO: model_training: Rank 0, Epoch 1067, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,098: INFO: model_training: Rank 0, Epoch 1067, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,099: INFO: model_training: Rank 0, Epoch 1068, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,101: INFO: model_training: Rank 0, Epoch 1068, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,103: INFO: model_training: Rank 0, Epoch 1068, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,105: INFO: model_training: Rank 0, Epoch 1068, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,106: INFO: model_training: Rank 0, Epoch 1068, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,107: INFO: model_training: Rank 0, Epoch 1069, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,108: INFO: model_training: Rank 0, Epoch 1069, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,110: INFO: model_training: Rank 0, Epoch 1069, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,112: INFO: model_training: Rank 0, Epoch 1069, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,113: INFO: model_training: Rank 0, Epoch 1069, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,115: INFO: model_training: Rank 0, Epoch 1070, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,116: INFO: model_training: Rank 0, Epoch 1070, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,117: INFO: model_training: Rank 0, Epoch 1070, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,119: INFO: model_training: Rank 0, Epoch 1070, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,121: INFO: model_training: Rank 0, Epoch 1070, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,122: INFO: model_training: Rank 0, Epoch 1071, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,124: INFO: model_training: Rank 0, Epoch 1071, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,125: INFO: model_training: Rank 0, Epoch 1071, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,127: INFO: model_training: Rank 0, Epoch 1071, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,129: INFO: model_training: Rank 0, Epoch 1071, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,130: INFO: model_training: Rank 0, Epoch 1072, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,131: INFO: model_training: Rank 0, Epoch 1072, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,133: INFO: model_training: Rank 0, Epoch 1072, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,134: INFO: model_training: Rank 0, Epoch 1072, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,136: INFO: model_training: Rank 0, Epoch 1072, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,137: INFO: model_training: Rank 0, Epoch 1073, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,138: INFO: model_training: Rank 0, Epoch 1073, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,140: INFO: model_training: Rank 0, Epoch 1073, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,141: INFO: model_training: Rank 0, Epoch 1073, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,144: INFO: model_training: Rank 0, Epoch 1073, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,145: INFO: model_training: Rank 0, Epoch 1074, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,146: INFO: model_training: Rank 0, Epoch 1074, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,147: INFO: model_training: Rank 0, Epoch 1074, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,149: INFO: model_training: Rank 0, Epoch 1074, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,150: INFO: model_training: Rank 0, Epoch 1074, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,152: INFO: model_training: Rank 0, Epoch 1075, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,154: INFO: model_training: Rank 0, Epoch 1075, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,155: INFO: model_training: Rank 0, Epoch 1075, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,156: INFO: model_training: Rank 0, Epoch 1075, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,157: INFO: model_training: Rank 0, Epoch 1075, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,158: INFO: model_training: Rank 0, Epoch 1076, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,159: INFO: model_training: Rank 0, Epoch 1076, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,161: INFO: model_training: Rank 0, Epoch 1076, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,163: INFO: model_training: Rank 0, Epoch 1076, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,165: INFO: model_training: Rank 0, Epoch 1076, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,166: INFO: model_training: Rank 0, Epoch 1077, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,167: INFO: model_training: Rank 0, Epoch 1077, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,169: INFO: model_training: Rank 0, Epoch 1077, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,170: INFO: model_training: Rank 0, Epoch 1077, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,172: INFO: model_training: Rank 0, Epoch 1077, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,173: INFO: model_training: Rank 0, Epoch 1078, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,174: INFO: model_training: Rank 0, Epoch 1078, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,175: INFO: model_training: Rank 0, Epoch 1078, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,176: INFO: model_training: Rank 0, Epoch 1078, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,178: INFO: model_training: Rank 0, Epoch 1078, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,179: INFO: model_training: Rank 0, Epoch 1079, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,180: INFO: model_training: Rank 0, Epoch 1079, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,181: INFO: model_training: Rank 0, Epoch 1079, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,183: INFO: model_training: Rank 0, Epoch 1079, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,184: INFO: model_training: Rank 0, Epoch 1079, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,186: INFO: model_training: Rank 0, Epoch 1080, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,188: INFO: model_training: Rank 0, Epoch 1080, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,189: INFO: model_training: Rank 0, Epoch 1080, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,190: INFO: model_training: Rank 0, Epoch 1080, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,191: INFO: model_training: Rank 0, Epoch 1080, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,193: INFO: model_training: Rank 0, Epoch 1081, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,194: INFO: model_training: Rank 0, Epoch 1081, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,195: INFO: model_training: Rank 0, Epoch 1081, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,196: INFO: model_training: Rank 0, Epoch 1081, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,197: INFO: model_training: Rank 0, Epoch 1081, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,199: INFO: model_training: Rank 0, Epoch 1082, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,200: INFO: model_training: Rank 0, Epoch 1082, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,201: INFO: model_training: Rank 0, Epoch 1082, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,203: INFO: model_training: Rank 0, Epoch 1082, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,205: INFO: model_training: Rank 0, Epoch 1082, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,206: INFO: model_training: Rank 0, Epoch 1083, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,207: INFO: model_training: Rank 0, Epoch 1083, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,208: INFO: model_training: Rank 0, Epoch 1083, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,209: INFO: model_training: Rank 0, Epoch 1083, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,211: INFO: model_training: Rank 0, Epoch 1083, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,213: INFO: model_training: Rank 0, Epoch 1084, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,214: INFO: model_training: Rank 0, Epoch 1084, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,215: INFO: model_training: Rank 0, Epoch 1084, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,216: INFO: model_training: Rank 0, Epoch 1084, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,217: INFO: model_training: Rank 0, Epoch 1084, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,219: INFO: model_training: Rank 0, Epoch 1085, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,221: INFO: model_training: Rank 0, Epoch 1085, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,222: INFO: model_training: Rank 0, Epoch 1085, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,224: INFO: model_training: Rank 0, Epoch 1085, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,225: INFO: model_training: Rank 0, Epoch 1085, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,226: INFO: model_training: Rank 0, Epoch 1086, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,228: INFO: model_training: Rank 0, Epoch 1086, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,229: INFO: model_training: Rank 0, Epoch 1086, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,230: INFO: model_training: Rank 0, Epoch 1086, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,232: INFO: model_training: Rank 0, Epoch 1086, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,233: INFO: model_training: Rank 0, Epoch 1087, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,234: INFO: model_training: Rank 0, Epoch 1087, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,236: INFO: model_training: Rank 0, Epoch 1087, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,238: INFO: model_training: Rank 0, Epoch 1087, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,239: INFO: model_training: Rank 0, Epoch 1087, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,240: INFO: model_training: Rank 0, Epoch 1088, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,241: INFO: model_training: Rank 0, Epoch 1088, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,243: INFO: model_training: Rank 0, Epoch 1088, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,245: INFO: model_training: Rank 0, Epoch 1088, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,246: INFO: model_training: Rank 0, Epoch 1088, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,248: INFO: model_training: Rank 0, Epoch 1089, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,249: INFO: model_training: Rank 0, Epoch 1089, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,250: INFO: model_training: Rank 0, Epoch 1089, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,252: INFO: model_training: Rank 0, Epoch 1089, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,253: INFO: model_training: Rank 0, Epoch 1089, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,254: INFO: model_training: Rank 0, Epoch 1090, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,255: INFO: model_training: Rank 0, Epoch 1090, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,256: INFO: model_training: Rank 0, Epoch 1090, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,258: INFO: model_training: Rank 0, Epoch 1090, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,259: INFO: model_training: Rank 0, Epoch 1090, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,261: INFO: model_training: Rank 0, Epoch 1091, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,263: INFO: model_training: Rank 0, Epoch 1091, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,264: INFO: model_training: Rank 0, Epoch 1091, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,265: INFO: model_training: Rank 0, Epoch 1091, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,266: INFO: model_training: Rank 0, Epoch 1091, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,268: INFO: model_training: Rank 0, Epoch 1092, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,270: INFO: model_training: Rank 0, Epoch 1092, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,271: INFO: model_training: Rank 0, Epoch 1092, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,272: INFO: model_training: Rank 0, Epoch 1092, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,274: INFO: model_training: Rank 0, Epoch 1092, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,275: INFO: model_training: Rank 0, Epoch 1093, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,277: INFO: model_training: Rank 0, Epoch 1093, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,278: INFO: model_training: Rank 0, Epoch 1093, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,279: INFO: model_training: Rank 0, Epoch 1093, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,280: INFO: model_training: Rank 0, Epoch 1093, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,282: INFO: model_training: Rank 0, Epoch 1094, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,284: INFO: model_training: Rank 0, Epoch 1094, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,285: INFO: model_training: Rank 0, Epoch 1094, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,286: INFO: model_training: Rank 0, Epoch 1094, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,288: INFO: model_training: Rank 0, Epoch 1094, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,289: INFO: model_training: Rank 0, Epoch 1095, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,290: INFO: model_training: Rank 0, Epoch 1095, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,291: INFO: model_training: Rank 0, Epoch 1095, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,293: INFO: model_training: Rank 0, Epoch 1095, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,294: INFO: model_training: Rank 0, Epoch 1095, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,296: INFO: model_training: Rank 0, Epoch 1096, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,297: INFO: model_training: Rank 0, Epoch 1096, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,298: INFO: model_training: Rank 0, Epoch 1096, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,299: INFO: model_training: Rank 0, Epoch 1096, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,300: INFO: model_training: Rank 0, Epoch 1096, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,303: INFO: model_training: Rank 0, Epoch 1097, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,304: INFO: model_training: Rank 0, Epoch 1097, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,305: INFO: model_training: Rank 0, Epoch 1097, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,307: INFO: model_training: Rank 0, Epoch 1097, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,308: INFO: model_training: Rank 0, Epoch 1097, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,309: INFO: model_training: Rank 0, Epoch 1098, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,311: INFO: model_training: Rank 0, Epoch 1098, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,312: INFO: model_training: Rank 0, Epoch 1098, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,313: INFO: model_training: Rank 0, Epoch 1098, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,314: INFO: model_training: Rank 0, Epoch 1098, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,316: INFO: model_training: Rank 0, Epoch 1099, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,317: INFO: model_training: Rank 0, Epoch 1099, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,319: INFO: model_training: Rank 0, Epoch 1099, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,320: INFO: model_training: Rank 0, Epoch 1099, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,321: INFO: model_training: Rank 0, Epoch 1099, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,323: INFO: model_training: Rank 0, Epoch 1100, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,324: INFO: model_training: Rank 0, Epoch 1100, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,325: INFO: model_training: Rank 0, Epoch 1100, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,327: INFO: model_training: Rank 0, Epoch 1100, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,328: INFO: model_training: Rank 0, Epoch 1100, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,330: INFO: model_training: Rank 0, Epoch 1101, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,331: INFO: model_training: Rank 0, Epoch 1101, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,332: INFO: model_training: Rank 0, Epoch 1101, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,333: INFO: model_training: Rank 0, Epoch 1101, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,335: INFO: model_training: Rank 0, Epoch 1101, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,336: INFO: model_training: Rank 0, Epoch 1102, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,337: INFO: model_training: Rank 0, Epoch 1102, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,338: INFO: model_training: Rank 0, Epoch 1102, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,339: INFO: model_training: Rank 0, Epoch 1102, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,341: INFO: model_training: Rank 0, Epoch 1102, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,342: INFO: model_training: Rank 0, Epoch 1103, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,344: INFO: model_training: Rank 0, Epoch 1103, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,345: INFO: model_training: Rank 0, Epoch 1103, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,346: INFO: model_training: Rank 0, Epoch 1103, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,348: INFO: model_training: Rank 0, Epoch 1103, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,349: INFO: model_training: Rank 0, Epoch 1104, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,350: INFO: model_training: Rank 0, Epoch 1104, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,351: INFO: model_training: Rank 0, Epoch 1104, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,353: INFO: model_training: Rank 0, Epoch 1104, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,355: INFO: model_training: Rank 0, Epoch 1104, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,356: INFO: model_training: Rank 0, Epoch 1105, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,357: INFO: model_training: Rank 0, Epoch 1105, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,358: INFO: model_training: Rank 0, Epoch 1105, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,360: INFO: model_training: Rank 0, Epoch 1105, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,361: INFO: model_training: Rank 0, Epoch 1105, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,363: INFO: model_training: Rank 0, Epoch 1106, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,365: INFO: model_training: Rank 0, Epoch 1106, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,366: INFO: model_training: Rank 0, Epoch 1106, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,367: INFO: model_training: Rank 0, Epoch 1106, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,369: INFO: model_training: Rank 0, Epoch 1106, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,370: INFO: model_training: Rank 0, Epoch 1107, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,371: INFO: model_training: Rank 0, Epoch 1107, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,373: INFO: model_training: Rank 0, Epoch 1107, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,374: INFO: model_training: Rank 0, Epoch 1107, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,375: INFO: model_training: Rank 0, Epoch 1107, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,377: INFO: model_training: Rank 0, Epoch 1108, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,378: INFO: model_training: Rank 0, Epoch 1108, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,380: INFO: model_training: Rank 0, Epoch 1108, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,381: INFO: model_training: Rank 0, Epoch 1108, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,382: INFO: model_training: Rank 0, Epoch 1108, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,384: INFO: model_training: Rank 0, Epoch 1109, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,386: INFO: model_training: Rank 0, Epoch 1109, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,387: INFO: model_training: Rank 0, Epoch 1109, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,388: INFO: model_training: Rank 0, Epoch 1109, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,389: INFO: model_training: Rank 0, Epoch 1109, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,390: INFO: model_training: Rank 0, Epoch 1110, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,391: INFO: model_training: Rank 0, Epoch 1110, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,393: INFO: model_training: Rank 0, Epoch 1110, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,394: INFO: model_training: Rank 0, Epoch 1110, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,395: INFO: model_training: Rank 0, Epoch 1110, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,396: INFO: model_training: Rank 0, Epoch 1111, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,398: INFO: model_training: Rank 0, Epoch 1111, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,399: INFO: model_training: Rank 0, Epoch 1111, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,400: INFO: model_training: Rank 0, Epoch 1111, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,402: INFO: model_training: Rank 0, Epoch 1111, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,404: INFO: model_training: Rank 0, Epoch 1112, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,405: INFO: model_training: Rank 0, Epoch 1112, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,407: INFO: model_training: Rank 0, Epoch 1112, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,408: INFO: model_training: Rank 0, Epoch 1112, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,409: INFO: model_training: Rank 0, Epoch 1112, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,410: INFO: model_training: Rank 0, Epoch 1113, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,411: INFO: model_training: Rank 0, Epoch 1113, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,412: INFO: model_training: Rank 0, Epoch 1113, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,413: INFO: model_training: Rank 0, Epoch 1113, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,414: INFO: model_training: Rank 0, Epoch 1113, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,415: INFO: model_training: Rank 0, Epoch 1114, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,417: INFO: model_training: Rank 0, Epoch 1114, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,419: INFO: model_training: Rank 0, Epoch 1114, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,420: INFO: model_training: Rank 0, Epoch 1114, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,421: INFO: model_training: Rank 0, Epoch 1114, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,423: INFO: model_training: Rank 0, Epoch 1115, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,424: INFO: model_training: Rank 0, Epoch 1115, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,425: INFO: model_training: Rank 0, Epoch 1115, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,427: INFO: model_training: Rank 0, Epoch 1115, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,428: INFO: model_training: Rank 0, Epoch 1115, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,429: INFO: model_training: Rank 0, Epoch 1116, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,430: INFO: model_training: Rank 0, Epoch 1116, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,431: INFO: model_training: Rank 0, Epoch 1116, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,432: INFO: model_training: Rank 0, Epoch 1116, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,433: INFO: model_training: Rank 0, Epoch 1116, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,434: INFO: model_training: Rank 0, Epoch 1117, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,436: INFO: model_training: Rank 0, Epoch 1117, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,437: INFO: model_training: Rank 0, Epoch 1117, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,438: INFO: model_training: Rank 0, Epoch 1117, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,439: INFO: model_training: Rank 0, Epoch 1117, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,440: INFO: model_training: Rank 0, Epoch 1118, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,442: INFO: model_training: Rank 0, Epoch 1118, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,443: INFO: model_training: Rank 0, Epoch 1118, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,445: INFO: model_training: Rank 0, Epoch 1118, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,446: INFO: model_training: Rank 0, Epoch 1118, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,447: INFO: model_training: Rank 0, Epoch 1119, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,448: INFO: model_training: Rank 0, Epoch 1119, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,449: INFO: model_training: Rank 0, Epoch 1119, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,450: INFO: model_training: Rank 0, Epoch 1119, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,452: INFO: model_training: Rank 0, Epoch 1119, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,454: INFO: model_training: Rank 0, Epoch 1120, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,455: INFO: model_training: Rank 0, Epoch 1120, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,456: INFO: model_training: Rank 0, Epoch 1120, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,457: INFO: model_training: Rank 0, Epoch 1120, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,459: INFO: model_training: Rank 0, Epoch 1120, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,460: INFO: model_training: Rank 0, Epoch 1121, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,461: INFO: model_training: Rank 0, Epoch 1121, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,463: INFO: model_training: Rank 0, Epoch 1121, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,464: INFO: model_training: Rank 0, Epoch 1121, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,465: INFO: model_training: Rank 0, Epoch 1121, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,466: INFO: model_training: Rank 0, Epoch 1122, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,467: INFO: model_training: Rank 0, Epoch 1122, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,468: INFO: model_training: Rank 0, Epoch 1122, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,470: INFO: model_training: Rank 0, Epoch 1122, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,471: INFO: model_training: Rank 0, Epoch 1122, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,472: INFO: model_training: Rank 0, Epoch 1123, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,473: INFO: model_training: Rank 0, Epoch 1123, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,475: INFO: model_training: Rank 0, Epoch 1123, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,475: INFO: model_training: Rank 0, Epoch 1123, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,477: INFO: model_training: Rank 0, Epoch 1123, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,478: INFO: model_training: Rank 0, Epoch 1124, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,479: INFO: model_training: Rank 0, Epoch 1124, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,480: INFO: model_training: Rank 0, Epoch 1124, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,481: INFO: model_training: Rank 0, Epoch 1124, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,482: INFO: model_training: Rank 0, Epoch 1124, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,484: INFO: model_training: Rank 0, Epoch 1125, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,485: INFO: model_training: Rank 0, Epoch 1125, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,486: INFO: model_training: Rank 0, Epoch 1125, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,488: INFO: model_training: Rank 0, Epoch 1125, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,490: INFO: model_training: Rank 0, Epoch 1125, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,491: INFO: model_training: Rank 0, Epoch 1126, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,492: INFO: model_training: Rank 0, Epoch 1126, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,494: INFO: model_training: Rank 0, Epoch 1126, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,495: INFO: model_training: Rank 0, Epoch 1126, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,496: INFO: model_training: Rank 0, Epoch 1126, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,497: INFO: model_training: Rank 0, Epoch 1127, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,498: INFO: model_training: Rank 0, Epoch 1127, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,499: INFO: model_training: Rank 0, Epoch 1127, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,501: INFO: model_training: Rank 0, Epoch 1127, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,502: INFO: model_training: Rank 0, Epoch 1127, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,504: INFO: model_training: Rank 0, Epoch 1128, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,505: INFO: model_training: Rank 0, Epoch 1128, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,507: INFO: model_training: Rank 0, Epoch 1128, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,508: INFO: model_training: Rank 0, Epoch 1128, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,509: INFO: model_training: Rank 0, Epoch 1128, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,510: INFO: model_training: Rank 0, Epoch 1129, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,512: INFO: model_training: Rank 0, Epoch 1129, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,513: INFO: model_training: Rank 0, Epoch 1129, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,514: INFO: model_training: Rank 0, Epoch 1129, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,515: INFO: model_training: Rank 0, Epoch 1129, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,516: INFO: model_training: Rank 0, Epoch 1130, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,517: INFO: model_training: Rank 0, Epoch 1130, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,519: INFO: model_training: Rank 0, Epoch 1130, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,520: INFO: model_training: Rank 0, Epoch 1130, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,521: INFO: model_training: Rank 0, Epoch 1130, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,523: INFO: model_training: Rank 0, Epoch 1131, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,525: INFO: model_training: Rank 0, Epoch 1131, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,526: INFO: model_training: Rank 0, Epoch 1131, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,528: INFO: model_training: Rank 0, Epoch 1131, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,529: INFO: model_training: Rank 0, Epoch 1131, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,530: INFO: model_training: Rank 0, Epoch 1132, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,531: INFO: model_training: Rank 0, Epoch 1132, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,533: INFO: model_training: Rank 0, Epoch 1132, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,534: INFO: model_training: Rank 0, Epoch 1132, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,536: INFO: model_training: Rank 0, Epoch 1132, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,537: INFO: model_training: Rank 0, Epoch 1133, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,538: INFO: model_training: Rank 0, Epoch 1133, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,539: INFO: model_training: Rank 0, Epoch 1133, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,541: INFO: model_training: Rank 0, Epoch 1133, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,542: INFO: model_training: Rank 0, Epoch 1133, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,544: INFO: model_training: Rank 0, Epoch 1134, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,545: INFO: model_training: Rank 0, Epoch 1134, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,546: INFO: model_training: Rank 0, Epoch 1134, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,547: INFO: model_training: Rank 0, Epoch 1134, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,549: INFO: model_training: Rank 0, Epoch 1134, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,550: INFO: model_training: Rank 0, Epoch 1135, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,552: INFO: model_training: Rank 0, Epoch 1135, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,553: INFO: model_training: Rank 0, Epoch 1135, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,554: INFO: model_training: Rank 0, Epoch 1135, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,556: INFO: model_training: Rank 0, Epoch 1135, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,557: INFO: model_training: Rank 0, Epoch 1136, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,558: INFO: model_training: Rank 0, Epoch 1136, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,560: INFO: model_training: Rank 0, Epoch 1136, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,561: INFO: model_training: Rank 0, Epoch 1136, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,563: INFO: model_training: Rank 0, Epoch 1136, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,564: INFO: model_training: Rank 0, Epoch 1137, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,565: INFO: model_training: Rank 0, Epoch 1137, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,566: INFO: model_training: Rank 0, Epoch 1137, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,568: INFO: model_training: Rank 0, Epoch 1137, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,569: INFO: model_training: Rank 0, Epoch 1137, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,570: INFO: model_training: Rank 0, Epoch 1138, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,571: INFO: model_training: Rank 0, Epoch 1138, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,573: INFO: model_training: Rank 0, Epoch 1138, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,574: INFO: model_training: Rank 0, Epoch 1138, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,575: INFO: model_training: Rank 0, Epoch 1138, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,577: INFO: model_training: Rank 0, Epoch 1139, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,578: INFO: model_training: Rank 0, Epoch 1139, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,580: INFO: model_training: Rank 0, Epoch 1139, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,581: INFO: model_training: Rank 0, Epoch 1139, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,583: INFO: model_training: Rank 0, Epoch 1139, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,584: INFO: model_training: Rank 0, Epoch 1140, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,586: INFO: model_training: Rank 0, Epoch 1140, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,587: INFO: model_training: Rank 0, Epoch 1140, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,588: INFO: model_training: Rank 0, Epoch 1140, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,589: INFO: model_training: Rank 0, Epoch 1140, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,591: INFO: model_training: Rank 0, Epoch 1141, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,592: INFO: model_training: Rank 0, Epoch 1141, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,594: INFO: model_training: Rank 0, Epoch 1141, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,595: INFO: model_training: Rank 0, Epoch 1141, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,596: INFO: model_training: Rank 0, Epoch 1141, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,598: INFO: model_training: Rank 0, Epoch 1142, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,599: INFO: model_training: Rank 0, Epoch 1142, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,601: INFO: model_training: Rank 0, Epoch 1142, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,603: INFO: model_training: Rank 0, Epoch 1142, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,604: INFO: model_training: Rank 0, Epoch 1142, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,605: INFO: model_training: Rank 0, Epoch 1143, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,606: INFO: model_training: Rank 0, Epoch 1143, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,607: INFO: model_training: Rank 0, Epoch 1143, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,608: INFO: model_training: Rank 0, Epoch 1143, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,609: INFO: model_training: Rank 0, Epoch 1143, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,611: INFO: model_training: Rank 0, Epoch 1144, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,612: INFO: model_training: Rank 0, Epoch 1144, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,614: INFO: model_training: Rank 0, Epoch 1144, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,615: INFO: model_training: Rank 0, Epoch 1144, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,616: INFO: model_training: Rank 0, Epoch 1144, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,617: INFO: model_training: Rank 0, Epoch 1145, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,620: INFO: model_training: Rank 0, Epoch 1145, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,621: INFO: model_training: Rank 0, Epoch 1145, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,622: INFO: model_training: Rank 0, Epoch 1145, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,623: INFO: model_training: Rank 0, Epoch 1145, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,625: INFO: model_training: Rank 0, Epoch 1146, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,627: INFO: model_training: Rank 0, Epoch 1146, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,628: INFO: model_training: Rank 0, Epoch 1146, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,629: INFO: model_training: Rank 0, Epoch 1146, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,630: INFO: model_training: Rank 0, Epoch 1146, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,632: INFO: model_training: Rank 0, Epoch 1147, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,633: INFO: model_training: Rank 0, Epoch 1147, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,635: INFO: model_training: Rank 0, Epoch 1147, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,636: INFO: model_training: Rank 0, Epoch 1147, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,637: INFO: model_training: Rank 0, Epoch 1147, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,639: INFO: model_training: Rank 0, Epoch 1148, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,640: INFO: model_training: Rank 0, Epoch 1148, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,642: INFO: model_training: Rank 0, Epoch 1148, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,644: INFO: model_training: Rank 0, Epoch 1148, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,645: INFO: model_training: Rank 0, Epoch 1148, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,646: INFO: model_training: Rank 0, Epoch 1149, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,648: INFO: model_training: Rank 0, Epoch 1149, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,649: INFO: model_training: Rank 0, Epoch 1149, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,650: INFO: model_training: Rank 0, Epoch 1149, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,652: INFO: model_training: Rank 0, Epoch 1149, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,653: INFO: model_training: Rank 0, Epoch 1150, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,654: INFO: model_training: Rank 0, Epoch 1150, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,655: INFO: model_training: Rank 0, Epoch 1150, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,656: INFO: model_training: Rank 0, Epoch 1150, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,658: INFO: model_training: Rank 0, Epoch 1150, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,659: INFO: model_training: Rank 0, Epoch 1151, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,661: INFO: model_training: Rank 0, Epoch 1151, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,662: INFO: model_training: Rank 0, Epoch 1151, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,664: INFO: model_training: Rank 0, Epoch 1151, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,665: INFO: model_training: Rank 0, Epoch 1151, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,666: INFO: model_training: Rank 0, Epoch 1152, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,668: INFO: model_training: Rank 0, Epoch 1152, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,669: INFO: model_training: Rank 0, Epoch 1152, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,670: INFO: model_training: Rank 0, Epoch 1152, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,671: INFO: model_training: Rank 0, Epoch 1152, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,673: INFO: model_training: Rank 0, Epoch 1153, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,674: INFO: model_training: Rank 0, Epoch 1153, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,675: INFO: model_training: Rank 0, Epoch 1153, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,677: INFO: model_training: Rank 0, Epoch 1153, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,678: INFO: model_training: Rank 0, Epoch 1153, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,680: INFO: model_training: Rank 0, Epoch 1154, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,682: INFO: model_training: Rank 0, Epoch 1154, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,683: INFO: model_training: Rank 0, Epoch 1154, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,684: INFO: model_training: Rank 0, Epoch 1154, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,686: INFO: model_training: Rank 0, Epoch 1154, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,687: INFO: model_training: Rank 0, Epoch 1155, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,688: INFO: model_training: Rank 0, Epoch 1155, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,690: INFO: model_training: Rank 0, Epoch 1155, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,691: INFO: model_training: Rank 0, Epoch 1155, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,693: INFO: model_training: Rank 0, Epoch 1155, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,694: INFO: model_training: Rank 0, Epoch 1156, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,695: INFO: model_training: Rank 0, Epoch 1156, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,696: INFO: model_training: Rank 0, Epoch 1156, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,697: INFO: model_training: Rank 0, Epoch 1156, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,698: INFO: model_training: Rank 0, Epoch 1156, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,700: INFO: model_training: Rank 0, Epoch 1157, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,701: INFO: model_training: Rank 0, Epoch 1157, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,703: INFO: model_training: Rank 0, Epoch 1157, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,704: INFO: model_training: Rank 0, Epoch 1157, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,706: INFO: model_training: Rank 0, Epoch 1157, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,707: INFO: model_training: Rank 0, Epoch 1158, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,708: INFO: model_training: Rank 0, Epoch 1158, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,709: INFO: model_training: Rank 0, Epoch 1158, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,710: INFO: model_training: Rank 0, Epoch 1158, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,712: INFO: model_training: Rank 0, Epoch 1158, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,713: INFO: model_training: Rank 0, Epoch 1159, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,714: INFO: model_training: Rank 0, Epoch 1159, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,715: INFO: model_training: Rank 0, Epoch 1159, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,717: INFO: model_training: Rank 0, Epoch 1159, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,718: INFO: model_training: Rank 0, Epoch 1159, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,720: INFO: model_training: Rank 0, Epoch 1160, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,722: INFO: model_training: Rank 0, Epoch 1160, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,723: INFO: model_training: Rank 0, Epoch 1160, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,724: INFO: model_training: Rank 0, Epoch 1160, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,725: INFO: model_training: Rank 0, Epoch 1160, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,727: INFO: model_training: Rank 0, Epoch 1161, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,728: INFO: model_training: Rank 0, Epoch 1161, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,729: INFO: model_training: Rank 0, Epoch 1161, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,731: INFO: model_training: Rank 0, Epoch 1161, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,732: INFO: model_training: Rank 0, Epoch 1161, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,733: INFO: model_training: Rank 0, Epoch 1162, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,735: INFO: model_training: Rank 0, Epoch 1162, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,736: INFO: model_training: Rank 0, Epoch 1162, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,737: INFO: model_training: Rank 0, Epoch 1162, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,738: INFO: model_training: Rank 0, Epoch 1162, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,740: INFO: model_training: Rank 0, Epoch 1163, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,742: INFO: model_training: Rank 0, Epoch 1163, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,743: INFO: model_training: Rank 0, Epoch 1163, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,745: INFO: model_training: Rank 0, Epoch 1163, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,746: INFO: model_training: Rank 0, Epoch 1163, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,747: INFO: model_training: Rank 0, Epoch 1164, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,748: INFO: model_training: Rank 0, Epoch 1164, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,749: INFO: model_training: Rank 0, Epoch 1164, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,750: INFO: model_training: Rank 0, Epoch 1164, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,752: INFO: model_training: Rank 0, Epoch 1164, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,753: INFO: model_training: Rank 0, Epoch 1165, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,755: INFO: model_training: Rank 0, Epoch 1165, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,756: INFO: model_training: Rank 0, Epoch 1165, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,757: INFO: model_training: Rank 0, Epoch 1165, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,758: INFO: model_training: Rank 0, Epoch 1165, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,760: INFO: model_training: Rank 0, Epoch 1166, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,762: INFO: model_training: Rank 0, Epoch 1166, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,763: INFO: model_training: Rank 0, Epoch 1166, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,764: INFO: model_training: Rank 0, Epoch 1166, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,766: INFO: model_training: Rank 0, Epoch 1166, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,767: INFO: model_training: Rank 0, Epoch 1167, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,768: INFO: model_training: Rank 0, Epoch 1167, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,770: INFO: model_training: Rank 0, Epoch 1167, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,771: INFO: model_training: Rank 0, Epoch 1167, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,772: INFO: model_training: Rank 0, Epoch 1167, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,774: INFO: model_training: Rank 0, Epoch 1168, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,775: INFO: model_training: Rank 0, Epoch 1168, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,776: INFO: model_training: Rank 0, Epoch 1168, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,778: INFO: model_training: Rank 0, Epoch 1168, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,779: INFO: model_training: Rank 0, Epoch 1168, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,781: INFO: model_training: Rank 0, Epoch 1169, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,782: INFO: model_training: Rank 0, Epoch 1169, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,783: INFO: model_training: Rank 0, Epoch 1169, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,784: INFO: model_training: Rank 0, Epoch 1169, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,786: INFO: model_training: Rank 0, Epoch 1169, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,787: INFO: model_training: Rank 0, Epoch 1170, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,788: INFO: model_training: Rank 0, Epoch 1170, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,789: INFO: model_training: Rank 0, Epoch 1170, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,790: INFO: model_training: Rank 0, Epoch 1170, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,791: INFO: model_training: Rank 0, Epoch 1170, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,793: INFO: model_training: Rank 0, Epoch 1171, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,794: INFO: model_training: Rank 0, Epoch 1171, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,796: INFO: model_training: Rank 0, Epoch 1171, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,797: INFO: model_training: Rank 0, Epoch 1171, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,798: INFO: model_training: Rank 0, Epoch 1171, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,800: INFO: model_training: Rank 0, Epoch 1172, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,802: INFO: model_training: Rank 0, Epoch 1172, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,803: INFO: model_training: Rank 0, Epoch 1172, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,804: INFO: model_training: Rank 0, Epoch 1172, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,805: INFO: model_training: Rank 0, Epoch 1172, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,806: INFO: model_training: Rank 0, Epoch 1173, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,807: INFO: model_training: Rank 0, Epoch 1173, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,808: INFO: model_training: Rank 0, Epoch 1173, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,810: INFO: model_training: Rank 0, Epoch 1173, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,811: INFO: model_training: Rank 0, Epoch 1173, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,813: INFO: model_training: Rank 0, Epoch 1174, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,814: INFO: model_training: Rank 0, Epoch 1174, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,816: INFO: model_training: Rank 0, Epoch 1174, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,817: INFO: model_training: Rank 0, Epoch 1174, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,818: INFO: model_training: Rank 0, Epoch 1174, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,821: INFO: model_training: Rank 0, Epoch 1175, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,822: INFO: model_training: Rank 0, Epoch 1175, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,823: INFO: model_training: Rank 0, Epoch 1175, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,824: INFO: model_training: Rank 0, Epoch 1175, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,825: INFO: model_training: Rank 0, Epoch 1175, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,827: INFO: model_training: Rank 0, Epoch 1176, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,829: INFO: model_training: Rank 0, Epoch 1176, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,830: INFO: model_training: Rank 0, Epoch 1176, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,831: INFO: model_training: Rank 0, Epoch 1176, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,833: INFO: model_training: Rank 0, Epoch 1176, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,834: INFO: model_training: Rank 0, Epoch 1177, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,836: INFO: model_training: Rank 0, Epoch 1177, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,837: INFO: model_training: Rank 0, Epoch 1177, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,838: INFO: model_training: Rank 0, Epoch 1177, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,839: INFO: model_training: Rank 0, Epoch 1177, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,841: INFO: model_training: Rank 0, Epoch 1178, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,843: INFO: model_training: Rank 0, Epoch 1178, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,845: INFO: model_training: Rank 0, Epoch 1178, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,846: INFO: model_training: Rank 0, Epoch 1178, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,847: INFO: model_training: Rank 0, Epoch 1178, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,848: INFO: model_training: Rank 0, Epoch 1179, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,850: INFO: model_training: Rank 0, Epoch 1179, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,851: INFO: model_training: Rank 0, Epoch 1179, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,853: INFO: model_training: Rank 0, Epoch 1179, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,854: INFO: model_training: Rank 0, Epoch 1179, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,856: INFO: model_training: Rank 0, Epoch 1180, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,857: INFO: model_training: Rank 0, Epoch 1180, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,858: INFO: model_training: Rank 0, Epoch 1180, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,859: INFO: model_training: Rank 0, Epoch 1180, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,861: INFO: model_training: Rank 0, Epoch 1180, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,862: INFO: model_training: Rank 0, Epoch 1181, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,864: INFO: model_training: Rank 0, Epoch 1181, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,865: INFO: model_training: Rank 0, Epoch 1181, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,866: INFO: model_training: Rank 0, Epoch 1181, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,867: INFO: model_training: Rank 0, Epoch 1181, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,869: INFO: model_training: Rank 0, Epoch 1182, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,870: INFO: model_training: Rank 0, Epoch 1182, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,871: INFO: model_training: Rank 0, Epoch 1182, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,873: INFO: model_training: Rank 0, Epoch 1182, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,874: INFO: model_training: Rank 0, Epoch 1182, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,875: INFO: model_training: Rank 0, Epoch 1183, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,877: INFO: model_training: Rank 0, Epoch 1183, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,878: INFO: model_training: Rank 0, Epoch 1183, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,880: INFO: model_training: Rank 0, Epoch 1183, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,881: INFO: model_training: Rank 0, Epoch 1183, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,883: INFO: model_training: Rank 0, Epoch 1184, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,884: INFO: model_training: Rank 0, Epoch 1184, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,886: INFO: model_training: Rank 0, Epoch 1184, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,887: INFO: model_training: Rank 0, Epoch 1184, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,888: INFO: model_training: Rank 0, Epoch 1184, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,890: INFO: model_training: Rank 0, Epoch 1185, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,891: INFO: model_training: Rank 0, Epoch 1185, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,892: INFO: model_training: Rank 0, Epoch 1185, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,893: INFO: model_training: Rank 0, Epoch 1185, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,895: INFO: model_training: Rank 0, Epoch 1185, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,896: INFO: model_training: Rank 0, Epoch 1186, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,897: INFO: model_training: Rank 0, Epoch 1186, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,899: INFO: model_training: Rank 0, Epoch 1186, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,900: INFO: model_training: Rank 0, Epoch 1186, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,902: INFO: model_training: Rank 0, Epoch 1186, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,910: INFO: model_training: Rank 0, Epoch 1187, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,923: INFO: model_training: Rank 0, Epoch 1187, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,927: INFO: model_training: Rank 0, Epoch 1187, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,930: INFO: model_training: Rank 0, Epoch 1187, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,931: INFO: model_training: Rank 0, Epoch 1187, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,936: INFO: model_training: Rank 0, Epoch 1188, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,939: INFO: model_training: Rank 0, Epoch 1188, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,941: INFO: model_training: Rank 0, Epoch 1188, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,942: INFO: model_training: Rank 0, Epoch 1188, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,944: INFO: model_training: Rank 0, Epoch 1188, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,946: INFO: model_training: Rank 0, Epoch 1189, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,947: INFO: model_training: Rank 0, Epoch 1189, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,948: INFO: model_training: Rank 0, Epoch 1189, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,949: INFO: model_training: Rank 0, Epoch 1189, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,951: INFO: model_training: Rank 0, Epoch 1189, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,953: INFO: model_training: Rank 0, Epoch 1190, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,954: INFO: model_training: Rank 0, Epoch 1190, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,955: INFO: model_training: Rank 0, Epoch 1190, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,957: INFO: model_training: Rank 0, Epoch 1190, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,958: INFO: model_training: Rank 0, Epoch 1190, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,959: INFO: model_training: Rank 0, Epoch 1191, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,961: INFO: model_training: Rank 0, Epoch 1191, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,962: INFO: model_training: Rank 0, Epoch 1191, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,964: INFO: model_training: Rank 0, Epoch 1191, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,965: INFO: model_training: Rank 0, Epoch 1191, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,966: INFO: model_training: Rank 0, Epoch 1192, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,968: INFO: model_training: Rank 0, Epoch 1192, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,969: INFO: model_training: Rank 0, Epoch 1192, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,971: INFO: model_training: Rank 0, Epoch 1192, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,973: INFO: model_training: Rank 0, Epoch 1192, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,975: INFO: model_training: Rank 0, Epoch 1193, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,976: INFO: model_training: Rank 0, Epoch 1193, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,978: INFO: model_training: Rank 0, Epoch 1193, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,980: INFO: model_training: Rank 0, Epoch 1193, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,981: INFO: model_training: Rank 0, Epoch 1193, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,982: INFO: model_training: Rank 0, Epoch 1194, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,984: INFO: model_training: Rank 0, Epoch 1194, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,986: INFO: model_training: Rank 0, Epoch 1194, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,987: INFO: model_training: Rank 0, Epoch 1194, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,988: INFO: model_training: Rank 0, Epoch 1194, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,990: INFO: model_training: Rank 0, Epoch 1195, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,991: INFO: model_training: Rank 0, Epoch 1195, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:32,993: INFO: model_training: Rank 0, Epoch 1195, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:32,995: INFO: model_training: Rank 0, Epoch 1195, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:32,996: INFO: model_training: Rank 0, Epoch 1195, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:32,998: INFO: model_training: Rank 0, Epoch 1196, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:32,999: INFO: model_training: Rank 0, Epoch 1196, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,000: INFO: model_training: Rank 0, Epoch 1196, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,002: INFO: model_training: Rank 0, Epoch 1196, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,003: INFO: model_training: Rank 0, Epoch 1196, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,005: INFO: model_training: Rank 0, Epoch 1197, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,006: INFO: model_training: Rank 0, Epoch 1197, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,007: INFO: model_training: Rank 0, Epoch 1197, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,008: INFO: model_training: Rank 0, Epoch 1197, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,010: INFO: model_training: Rank 0, Epoch 1197, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,012: INFO: model_training: Rank 0, Epoch 1198, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,013: INFO: model_training: Rank 0, Epoch 1198, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,015: INFO: model_training: Rank 0, Epoch 1198, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,016: INFO: model_training: Rank 0, Epoch 1198, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,017: INFO: model_training: Rank 0, Epoch 1198, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,019: INFO: model_training: Rank 0, Epoch 1199, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,020: INFO: model_training: Rank 0, Epoch 1199, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,022: INFO: model_training: Rank 0, Epoch 1199, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,023: INFO: model_training: Rank 0, Epoch 1199, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,024: INFO: model_training: Rank 0, Epoch 1199, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,026: INFO: model_training: Rank 0, Epoch 1200, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,028: INFO: model_training: Rank 0, Epoch 1200, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,030: INFO: model_training: Rank 0, Epoch 1200, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,031: INFO: model_training: Rank 0, Epoch 1200, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,033: INFO: model_training: Rank 0, Epoch 1200, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,034: INFO: model_training: Rank 0, Epoch 1201, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,036: INFO: model_training: Rank 0, Epoch 1201, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,038: INFO: model_training: Rank 0, Epoch 1201, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,040: INFO: model_training: Rank 0, Epoch 1201, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,041: INFO: model_training: Rank 0, Epoch 1201, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,042: INFO: model_training: Rank 0, Epoch 1202, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,044: INFO: model_training: Rank 0, Epoch 1202, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,046: INFO: model_training: Rank 0, Epoch 1202, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,047: INFO: model_training: Rank 0, Epoch 1202, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,048: INFO: model_training: Rank 0, Epoch 1202, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,050: INFO: model_training: Rank 0, Epoch 1203, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,052: INFO: model_training: Rank 0, Epoch 1203, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,053: INFO: model_training: Rank 0, Epoch 1203, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,054: INFO: model_training: Rank 0, Epoch 1203, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,055: INFO: model_training: Rank 0, Epoch 1203, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,057: INFO: model_training: Rank 0, Epoch 1204, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,058: INFO: model_training: Rank 0, Epoch 1204, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,059: INFO: model_training: Rank 0, Epoch 1204, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,061: INFO: model_training: Rank 0, Epoch 1204, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,062: INFO: model_training: Rank 0, Epoch 1204, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,063: INFO: model_training: Rank 0, Epoch 1205, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,065: INFO: model_training: Rank 0, Epoch 1205, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,066: INFO: model_training: Rank 0, Epoch 1205, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,067: INFO: model_training: Rank 0, Epoch 1205, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,068: INFO: model_training: Rank 0, Epoch 1205, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,070: INFO: model_training: Rank 0, Epoch 1206, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,071: INFO: model_training: Rank 0, Epoch 1206, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,072: INFO: model_training: Rank 0, Epoch 1206, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,074: INFO: model_training: Rank 0, Epoch 1206, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,075: INFO: model_training: Rank 0, Epoch 1206, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,077: INFO: model_training: Rank 0, Epoch 1207, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,079: INFO: model_training: Rank 0, Epoch 1207, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,080: INFO: model_training: Rank 0, Epoch 1207, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,081: INFO: model_training: Rank 0, Epoch 1207, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,083: INFO: model_training: Rank 0, Epoch 1207, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,084: INFO: model_training: Rank 0, Epoch 1208, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,086: INFO: model_training: Rank 0, Epoch 1208, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,087: INFO: model_training: Rank 0, Epoch 1208, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,088: INFO: model_training: Rank 0, Epoch 1208, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,089: INFO: model_training: Rank 0, Epoch 1208, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,091: INFO: model_training: Rank 0, Epoch 1209, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,092: INFO: model_training: Rank 0, Epoch 1209, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,093: INFO: model_training: Rank 0, Epoch 1209, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,095: INFO: model_training: Rank 0, Epoch 1209, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,096: INFO: model_training: Rank 0, Epoch 1209, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,098: INFO: model_training: Rank 0, Epoch 1210, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,099: INFO: model_training: Rank 0, Epoch 1210, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,100: INFO: model_training: Rank 0, Epoch 1210, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,102: INFO: model_training: Rank 0, Epoch 1210, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,103: INFO: model_training: Rank 0, Epoch 1210, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,104: INFO: model_training: Rank 0, Epoch 1211, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,106: INFO: model_training: Rank 0, Epoch 1211, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,107: INFO: model_training: Rank 0, Epoch 1211, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,108: INFO: model_training: Rank 0, Epoch 1211, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,109: INFO: model_training: Rank 0, Epoch 1211, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,111: INFO: model_training: Rank 0, Epoch 1212, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,112: INFO: model_training: Rank 0, Epoch 1212, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,114: INFO: model_training: Rank 0, Epoch 1212, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,115: INFO: model_training: Rank 0, Epoch 1212, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,116: INFO: model_training: Rank 0, Epoch 1212, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,118: INFO: model_training: Rank 0, Epoch 1213, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,119: INFO: model_training: Rank 0, Epoch 1213, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,120: INFO: model_training: Rank 0, Epoch 1213, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,122: INFO: model_training: Rank 0, Epoch 1213, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,123: INFO: model_training: Rank 0, Epoch 1213, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,124: INFO: model_training: Rank 0, Epoch 1214, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,125: INFO: model_training: Rank 0, Epoch 1214, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,127: INFO: model_training: Rank 0, Epoch 1214, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,128: INFO: model_training: Rank 0, Epoch 1214, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,129: INFO: model_training: Rank 0, Epoch 1214, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,130: INFO: model_training: Rank 0, Epoch 1215, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,132: INFO: model_training: Rank 0, Epoch 1215, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,133: INFO: model_training: Rank 0, Epoch 1215, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,134: INFO: model_training: Rank 0, Epoch 1215, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,136: INFO: model_training: Rank 0, Epoch 1215, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,137: INFO: model_training: Rank 0, Epoch 1216, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,138: INFO: model_training: Rank 0, Epoch 1216, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,140: INFO: model_training: Rank 0, Epoch 1216, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,141: INFO: model_training: Rank 0, Epoch 1216, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,142: INFO: model_training: Rank 0, Epoch 1216, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,143: INFO: model_training: Rank 0, Epoch 1217, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,145: INFO: model_training: Rank 0, Epoch 1217, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,146: INFO: model_training: Rank 0, Epoch 1217, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,147: INFO: model_training: Rank 0, Epoch 1217, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,148: INFO: model_training: Rank 0, Epoch 1217, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,150: INFO: model_training: Rank 0, Epoch 1218, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,152: INFO: model_training: Rank 0, Epoch 1218, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,153: INFO: model_training: Rank 0, Epoch 1218, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,154: INFO: model_training: Rank 0, Epoch 1218, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,155: INFO: model_training: Rank 0, Epoch 1218, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,157: INFO: model_training: Rank 0, Epoch 1219, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,158: INFO: model_training: Rank 0, Epoch 1219, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,159: INFO: model_training: Rank 0, Epoch 1219, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,160: INFO: model_training: Rank 0, Epoch 1219, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,161: INFO: model_training: Rank 0, Epoch 1219, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,163: INFO: model_training: Rank 0, Epoch 1220, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,164: INFO: model_training: Rank 0, Epoch 1220, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,165: INFO: model_training: Rank 0, Epoch 1220, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,167: INFO: model_training: Rank 0, Epoch 1220, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,169: INFO: model_training: Rank 0, Epoch 1220, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,170: INFO: model_training: Rank 0, Epoch 1221, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,172: INFO: model_training: Rank 0, Epoch 1221, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,173: INFO: model_training: Rank 0, Epoch 1221, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,174: INFO: model_training: Rank 0, Epoch 1221, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,175: INFO: model_training: Rank 0, Epoch 1221, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,176: INFO: model_training: Rank 0, Epoch 1222, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,178: INFO: model_training: Rank 0, Epoch 1222, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,179: INFO: model_training: Rank 0, Epoch 1222, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,180: INFO: model_training: Rank 0, Epoch 1222, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,181: INFO: model_training: Rank 0, Epoch 1222, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,182: INFO: model_training: Rank 0, Epoch 1223, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,184: INFO: model_training: Rank 0, Epoch 1223, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,185: INFO: model_training: Rank 0, Epoch 1223, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,187: INFO: model_training: Rank 0, Epoch 1223, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,187: INFO: model_training: Rank 0, Epoch 1223, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,189: INFO: model_training: Rank 0, Epoch 1224, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,190: INFO: model_training: Rank 0, Epoch 1224, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,191: INFO: model_training: Rank 0, Epoch 1224, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,192: INFO: model_training: Rank 0, Epoch 1224, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,193: INFO: model_training: Rank 0, Epoch 1224, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,195: INFO: model_training: Rank 0, Epoch 1225, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,196: INFO: model_training: Rank 0, Epoch 1225, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,197: INFO: model_training: Rank 0, Epoch 1225, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,198: INFO: model_training: Rank 0, Epoch 1225, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,199: INFO: model_training: Rank 0, Epoch 1225, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,200: INFO: model_training: Rank 0, Epoch 1226, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,202: INFO: model_training: Rank 0, Epoch 1226, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,204: INFO: model_training: Rank 0, Epoch 1226, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,205: INFO: model_training: Rank 0, Epoch 1226, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,206: INFO: model_training: Rank 0, Epoch 1226, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,207: INFO: model_training: Rank 0, Epoch 1227, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,208: INFO: model_training: Rank 0, Epoch 1227, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,209: INFO: model_training: Rank 0, Epoch 1227, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,211: INFO: model_training: Rank 0, Epoch 1227, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,212: INFO: model_training: Rank 0, Epoch 1227, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,213: INFO: model_training: Rank 0, Epoch 1228, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,214: INFO: model_training: Rank 0, Epoch 1228, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,216: INFO: model_training: Rank 0, Epoch 1228, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,217: INFO: model_training: Rank 0, Epoch 1228, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,218: INFO: model_training: Rank 0, Epoch 1228, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,219: INFO: model_training: Rank 0, Epoch 1229, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,220: INFO: model_training: Rank 0, Epoch 1229, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,222: INFO: model_training: Rank 0, Epoch 1229, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,223: INFO: model_training: Rank 0, Epoch 1229, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,225: INFO: model_training: Rank 0, Epoch 1229, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,226: INFO: model_training: Rank 0, Epoch 1230, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,228: INFO: model_training: Rank 0, Epoch 1230, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,229: INFO: model_training: Rank 0, Epoch 1230, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,230: INFO: model_training: Rank 0, Epoch 1230, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,232: INFO: model_training: Rank 0, Epoch 1230, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,233: INFO: model_training: Rank 0, Epoch 1231, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,234: INFO: model_training: Rank 0, Epoch 1231, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,236: INFO: model_training: Rank 0, Epoch 1231, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,237: INFO: model_training: Rank 0, Epoch 1231, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,238: INFO: model_training: Rank 0, Epoch 1231, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,240: INFO: model_training: Rank 0, Epoch 1232, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,241: INFO: model_training: Rank 0, Epoch 1232, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,243: INFO: model_training: Rank 0, Epoch 1232, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,244: INFO: model_training: Rank 0, Epoch 1232, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,246: INFO: model_training: Rank 0, Epoch 1232, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,247: INFO: model_training: Rank 0, Epoch 1233, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,249: INFO: model_training: Rank 0, Epoch 1233, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,250: INFO: model_training: Rank 0, Epoch 1233, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,252: INFO: model_training: Rank 0, Epoch 1233, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,253: INFO: model_training: Rank 0, Epoch 1233, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,254: INFO: model_training: Rank 0, Epoch 1234, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,255: INFO: model_training: Rank 0, Epoch 1234, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,256: INFO: model_training: Rank 0, Epoch 1234, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,257: INFO: model_training: Rank 0, Epoch 1234, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,259: INFO: model_training: Rank 0, Epoch 1234, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,261: INFO: model_training: Rank 0, Epoch 1235, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,262: INFO: model_training: Rank 0, Epoch 1235, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,263: INFO: model_training: Rank 0, Epoch 1235, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,265: INFO: model_training: Rank 0, Epoch 1235, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,266: INFO: model_training: Rank 0, Epoch 1235, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,267: INFO: model_training: Rank 0, Epoch 1236, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,268: INFO: model_training: Rank 0, Epoch 1236, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,269: INFO: model_training: Rank 0, Epoch 1236, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,270: INFO: model_training: Rank 0, Epoch 1236, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,272: INFO: model_training: Rank 0, Epoch 1236, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,273: INFO: model_training: Rank 0, Epoch 1237, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,274: INFO: model_training: Rank 0, Epoch 1237, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,275: INFO: model_training: Rank 0, Epoch 1237, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,276: INFO: model_training: Rank 0, Epoch 1237, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,278: INFO: model_training: Rank 0, Epoch 1237, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,279: INFO: model_training: Rank 0, Epoch 1238, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,281: INFO: model_training: Rank 0, Epoch 1238, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,282: INFO: model_training: Rank 0, Epoch 1238, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,283: INFO: model_training: Rank 0, Epoch 1238, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,284: INFO: model_training: Rank 0, Epoch 1238, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,286: INFO: model_training: Rank 0, Epoch 1239, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,287: INFO: model_training: Rank 0, Epoch 1239, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,288: INFO: model_training: Rank 0, Epoch 1239, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,289: INFO: model_training: Rank 0, Epoch 1239, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,290: INFO: model_training: Rank 0, Epoch 1239, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,291: INFO: model_training: Rank 0, Epoch 1240, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,292: INFO: model_training: Rank 0, Epoch 1240, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,294: INFO: model_training: Rank 0, Epoch 1240, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,295: INFO: model_training: Rank 0, Epoch 1240, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,296: INFO: model_training: Rank 0, Epoch 1240, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,298: INFO: model_training: Rank 0, Epoch 1241, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,299: INFO: model_training: Rank 0, Epoch 1241, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,300: INFO: model_training: Rank 0, Epoch 1241, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,301: INFO: model_training: Rank 0, Epoch 1241, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,303: INFO: model_training: Rank 0, Epoch 1241, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,304: INFO: model_training: Rank 0, Epoch 1242, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,305: INFO: model_training: Rank 0, Epoch 1242, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,307: INFO: model_training: Rank 0, Epoch 1242, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,308: INFO: model_training: Rank 0, Epoch 1242, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,309: INFO: model_training: Rank 0, Epoch 1242, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,310: INFO: model_training: Rank 0, Epoch 1243, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,312: INFO: model_training: Rank 0, Epoch 1243, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,313: INFO: model_training: Rank 0, Epoch 1243, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,314: INFO: model_training: Rank 0, Epoch 1243, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,315: INFO: model_training: Rank 0, Epoch 1243, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,317: INFO: model_training: Rank 0, Epoch 1244, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,319: INFO: model_training: Rank 0, Epoch 1244, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,320: INFO: model_training: Rank 0, Epoch 1244, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,321: INFO: model_training: Rank 0, Epoch 1244, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,323: INFO: model_training: Rank 0, Epoch 1244, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,324: INFO: model_training: Rank 0, Epoch 1245, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,325: INFO: model_training: Rank 0, Epoch 1245, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,327: INFO: model_training: Rank 0, Epoch 1245, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,328: INFO: model_training: Rank 0, Epoch 1245, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,329: INFO: model_training: Rank 0, Epoch 1245, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,330: INFO: model_training: Rank 0, Epoch 1246, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,331: INFO: model_training: Rank 0, Epoch 1246, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,332: INFO: model_training: Rank 0, Epoch 1246, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,334: INFO: model_training: Rank 0, Epoch 1246, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,335: INFO: model_training: Rank 0, Epoch 1246, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,337: INFO: model_training: Rank 0, Epoch 1247, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,339: INFO: model_training: Rank 0, Epoch 1247, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,340: INFO: model_training: Rank 0, Epoch 1247, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,341: INFO: model_training: Rank 0, Epoch 1247, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,342: INFO: model_training: Rank 0, Epoch 1247, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,344: INFO: model_training: Rank 0, Epoch 1248, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,345: INFO: model_training: Rank 0, Epoch 1248, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,346: INFO: model_training: Rank 0, Epoch 1248, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,347: INFO: model_training: Rank 0, Epoch 1248, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,348: INFO: model_training: Rank 0, Epoch 1248, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,349: INFO: model_training: Rank 0, Epoch 1249, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,350: INFO: model_training: Rank 0, Epoch 1249, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,352: INFO: model_training: Rank 0, Epoch 1249, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,353: INFO: model_training: Rank 0, Epoch 1249, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,354: INFO: model_training: Rank 0, Epoch 1249, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,356: INFO: model_training: Rank 0, Epoch 1250, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,357: INFO: model_training: Rank 0, Epoch 1250, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,358: INFO: model_training: Rank 0, Epoch 1250, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,360: INFO: model_training: Rank 0, Epoch 1250, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,361: INFO: model_training: Rank 0, Epoch 1250, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,363: INFO: model_training: Rank 0, Epoch 1251, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,367: INFO: model_training: Rank 0, Epoch 1251, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,387: INFO: model_training: Rank 0, Epoch 1251, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,392: INFO: model_training: Rank 0, Epoch 1251, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,401: INFO: model_training: Rank 0, Epoch 1251, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,403: INFO: model_training: Rank 0, Epoch 1252, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,406: INFO: model_training: Rank 0, Epoch 1252, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,409: INFO: model_training: Rank 0, Epoch 1252, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,411: INFO: model_training: Rank 0, Epoch 1252, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,412: INFO: model_training: Rank 0, Epoch 1252, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,414: INFO: model_training: Rank 0, Epoch 1253, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,416: INFO: model_training: Rank 0, Epoch 1253, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,418: INFO: model_training: Rank 0, Epoch 1253, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,420: INFO: model_training: Rank 0, Epoch 1253, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,422: INFO: model_training: Rank 0, Epoch 1253, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,425: INFO: model_training: Rank 0, Epoch 1254, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,426: INFO: model_training: Rank 0, Epoch 1254, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,431: INFO: model_training: Rank 0, Epoch 1254, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,434: INFO: model_training: Rank 0, Epoch 1254, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,436: INFO: model_training: Rank 0, Epoch 1254, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,439: INFO: model_training: Rank 0, Epoch 1255, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,441: INFO: model_training: Rank 0, Epoch 1255, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,443: INFO: model_training: Rank 0, Epoch 1255, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,445: INFO: model_training: Rank 0, Epoch 1255, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,447: INFO: model_training: Rank 0, Epoch 1255, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,455: INFO: model_training: Rank 0, Epoch 1256, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,468: INFO: model_training: Rank 0, Epoch 1256, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,477: INFO: model_training: Rank 0, Epoch 1256, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,483: INFO: model_training: Rank 0, Epoch 1256, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,486: INFO: model_training: Rank 0, Epoch 1256, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,488: INFO: model_training: Rank 0, Epoch 1257, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,491: INFO: model_training: Rank 0, Epoch 1257, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,493: INFO: model_training: Rank 0, Epoch 1257, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,499: INFO: model_training: Rank 0, Epoch 1257, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,501: INFO: model_training: Rank 0, Epoch 1257, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,503: INFO: model_training: Rank 0, Epoch 1258, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,505: INFO: model_training: Rank 0, Epoch 1258, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,506: INFO: model_training: Rank 0, Epoch 1258, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,508: INFO: model_training: Rank 0, Epoch 1258, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,511: INFO: model_training: Rank 0, Epoch 1258, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,512: INFO: model_training: Rank 0, Epoch 1259, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,514: INFO: model_training: Rank 0, Epoch 1259, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,516: INFO: model_training: Rank 0, Epoch 1259, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,517: INFO: model_training: Rank 0, Epoch 1259, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,530: INFO: model_training: Rank 0, Epoch 1259, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,543: INFO: model_training: Rank 0, Epoch 1260, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,553: INFO: model_training: Rank 0, Epoch 1260, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,555: INFO: model_training: Rank 0, Epoch 1260, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,557: INFO: model_training: Rank 0, Epoch 1260, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,561: INFO: model_training: Rank 0, Epoch 1260, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,565: INFO: model_training: Rank 0, Epoch 1261, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,567: INFO: model_training: Rank 0, Epoch 1261, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,569: INFO: model_training: Rank 0, Epoch 1261, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,571: INFO: model_training: Rank 0, Epoch 1261, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,573: INFO: model_training: Rank 0, Epoch 1261, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,575: INFO: model_training: Rank 0, Epoch 1262, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,577: INFO: model_training: Rank 0, Epoch 1262, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,578: INFO: model_training: Rank 0, Epoch 1262, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,580: INFO: model_training: Rank 0, Epoch 1262, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,581: INFO: model_training: Rank 0, Epoch 1262, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,582: INFO: model_training: Rank 0, Epoch 1263, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,583: INFO: model_training: Rank 0, Epoch 1263, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,585: INFO: model_training: Rank 0, Epoch 1263, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,587: INFO: model_training: Rank 0, Epoch 1263, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,588: INFO: model_training: Rank 0, Epoch 1263, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,589: INFO: model_training: Rank 0, Epoch 1264, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,590: INFO: model_training: Rank 0, Epoch 1264, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,592: INFO: model_training: Rank 0, Epoch 1264, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,594: INFO: model_training: Rank 0, Epoch 1264, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,596: INFO: model_training: Rank 0, Epoch 1264, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,597: INFO: model_training: Rank 0, Epoch 1265, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,598: INFO: model_training: Rank 0, Epoch 1265, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,600: INFO: model_training: Rank 0, Epoch 1265, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,602: INFO: model_training: Rank 0, Epoch 1265, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,603: INFO: model_training: Rank 0, Epoch 1265, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,604: INFO: model_training: Rank 0, Epoch 1266, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,605: INFO: model_training: Rank 0, Epoch 1266, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,606: INFO: model_training: Rank 0, Epoch 1266, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,607: INFO: model_training: Rank 0, Epoch 1266, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,609: INFO: model_training: Rank 0, Epoch 1266, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,611: INFO: model_training: Rank 0, Epoch 1267, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,612: INFO: model_training: Rank 0, Epoch 1267, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,614: INFO: model_training: Rank 0, Epoch 1267, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,615: INFO: model_training: Rank 0, Epoch 1267, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,617: INFO: model_training: Rank 0, Epoch 1267, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,619: INFO: model_training: Rank 0, Epoch 1268, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,628: INFO: model_training: Rank 0, Epoch 1268, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,639: INFO: model_training: Rank 0, Epoch 1268, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,647: INFO: model_training: Rank 0, Epoch 1268, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,648: INFO: model_training: Rank 0, Epoch 1268, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,650: INFO: model_training: Rank 0, Epoch 1269, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,653: INFO: model_training: Rank 0, Epoch 1269, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,657: INFO: model_training: Rank 0, Epoch 1269, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,659: INFO: model_training: Rank 0, Epoch 1269, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,661: INFO: model_training: Rank 0, Epoch 1269, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,662: INFO: model_training: Rank 0, Epoch 1270, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,664: INFO: model_training: Rank 0, Epoch 1270, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,666: INFO: model_training: Rank 0, Epoch 1270, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,667: INFO: model_training: Rank 0, Epoch 1270, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,669: INFO: model_training: Rank 0, Epoch 1270, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,670: INFO: model_training: Rank 0, Epoch 1271, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,671: INFO: model_training: Rank 0, Epoch 1271, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,673: INFO: model_training: Rank 0, Epoch 1271, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,674: INFO: model_training: Rank 0, Epoch 1271, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,675: INFO: model_training: Rank 0, Epoch 1271, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,677: INFO: model_training: Rank 0, Epoch 1272, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,678: INFO: model_training: Rank 0, Epoch 1272, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,680: INFO: model_training: Rank 0, Epoch 1272, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,681: INFO: model_training: Rank 0, Epoch 1272, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,683: INFO: model_training: Rank 0, Epoch 1272, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,684: INFO: model_training: Rank 0, Epoch 1273, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,687: INFO: model_training: Rank 0, Epoch 1273, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,689: INFO: model_training: Rank 0, Epoch 1273, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,690: INFO: model_training: Rank 0, Epoch 1273, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,691: INFO: model_training: Rank 0, Epoch 1273, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,693: INFO: model_training: Rank 0, Epoch 1274, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,694: INFO: model_training: Rank 0, Epoch 1274, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,696: INFO: model_training: Rank 0, Epoch 1274, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,697: INFO: model_training: Rank 0, Epoch 1274, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,698: INFO: model_training: Rank 0, Epoch 1274, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,700: INFO: model_training: Rank 0, Epoch 1275, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,701: INFO: model_training: Rank 0, Epoch 1275, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,703: INFO: model_training: Rank 0, Epoch 1275, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,704: INFO: model_training: Rank 0, Epoch 1275, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,705: INFO: model_training: Rank 0, Epoch 1275, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,706: INFO: model_training: Rank 0, Epoch 1276, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,708: INFO: model_training: Rank 0, Epoch 1276, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,710: INFO: model_training: Rank 0, Epoch 1276, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,712: INFO: model_training: Rank 0, Epoch 1276, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,713: INFO: model_training: Rank 0, Epoch 1276, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,714: INFO: model_training: Rank 0, Epoch 1277, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,716: INFO: model_training: Rank 0, Epoch 1277, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,717: INFO: model_training: Rank 0, Epoch 1277, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,718: INFO: model_training: Rank 0, Epoch 1277, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,720: INFO: model_training: Rank 0, Epoch 1277, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,721: INFO: model_training: Rank 0, Epoch 1278, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,722: INFO: model_training: Rank 0, Epoch 1278, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,723: INFO: model_training: Rank 0, Epoch 1278, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,724: INFO: model_training: Rank 0, Epoch 1278, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,726: INFO: model_training: Rank 0, Epoch 1278, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,727: INFO: model_training: Rank 0, Epoch 1279, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,729: INFO: model_training: Rank 0, Epoch 1279, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,730: INFO: model_training: Rank 0, Epoch 1279, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,732: INFO: model_training: Rank 0, Epoch 1279, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,733: INFO: model_training: Rank 0, Epoch 1279, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,735: INFO: model_training: Rank 0, Epoch 1280, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,736: INFO: model_training: Rank 0, Epoch 1280, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,738: INFO: model_training: Rank 0, Epoch 1280, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,739: INFO: model_training: Rank 0, Epoch 1280, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,740: INFO: model_training: Rank 0, Epoch 1280, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,742: INFO: model_training: Rank 0, Epoch 1281, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,743: INFO: model_training: Rank 0, Epoch 1281, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,744: INFO: model_training: Rank 0, Epoch 1281, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,745: INFO: model_training: Rank 0, Epoch 1281, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,746: INFO: model_training: Rank 0, Epoch 1281, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,748: INFO: model_training: Rank 0, Epoch 1282, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,749: INFO: model_training: Rank 0, Epoch 1282, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,750: INFO: model_training: Rank 0, Epoch 1282, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,752: INFO: model_training: Rank 0, Epoch 1282, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,754: INFO: model_training: Rank 0, Epoch 1282, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,755: INFO: model_training: Rank 0, Epoch 1283, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,756: INFO: model_training: Rank 0, Epoch 1283, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,757: INFO: model_training: Rank 0, Epoch 1283, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,758: INFO: model_training: Rank 0, Epoch 1283, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,760: INFO: model_training: Rank 0, Epoch 1283, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,761: INFO: model_training: Rank 0, Epoch 1284, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,762: INFO: model_training: Rank 0, Epoch 1284, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,763: INFO: model_training: Rank 0, Epoch 1284, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,764: INFO: model_training: Rank 0, Epoch 1284, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,765: INFO: model_training: Rank 0, Epoch 1284, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,767: INFO: model_training: Rank 0, Epoch 1285, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,769: INFO: model_training: Rank 0, Epoch 1285, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,770: INFO: model_training: Rank 0, Epoch 1285, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,771: INFO: model_training: Rank 0, Epoch 1285, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,772: INFO: model_training: Rank 0, Epoch 1285, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,774: INFO: model_training: Rank 0, Epoch 1286, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,775: INFO: model_training: Rank 0, Epoch 1286, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,776: INFO: model_training: Rank 0, Epoch 1286, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,777: INFO: model_training: Rank 0, Epoch 1286, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,779: INFO: model_training: Rank 0, Epoch 1286, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,780: INFO: model_training: Rank 0, Epoch 1287, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,781: INFO: model_training: Rank 0, Epoch 1287, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,783: INFO: model_training: Rank 0, Epoch 1287, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,784: INFO: model_training: Rank 0, Epoch 1287, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,786: INFO: model_training: Rank 0, Epoch 1287, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,788: INFO: model_training: Rank 0, Epoch 1288, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,790: INFO: model_training: Rank 0, Epoch 1288, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,791: INFO: model_training: Rank 0, Epoch 1288, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,793: INFO: model_training: Rank 0, Epoch 1288, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,794: INFO: model_training: Rank 0, Epoch 1288, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,795: INFO: model_training: Rank 0, Epoch 1289, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,796: INFO: model_training: Rank 0, Epoch 1289, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,798: INFO: model_training: Rank 0, Epoch 1289, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,799: INFO: model_training: Rank 0, Epoch 1289, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,800: INFO: model_training: Rank 0, Epoch 1289, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,802: INFO: model_training: Rank 0, Epoch 1290, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,803: INFO: model_training: Rank 0, Epoch 1290, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,805: INFO: model_training: Rank 0, Epoch 1290, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,806: INFO: model_training: Rank 0, Epoch 1290, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,807: INFO: model_training: Rank 0, Epoch 1290, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,809: INFO: model_training: Rank 0, Epoch 1291, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,811: INFO: model_training: Rank 0, Epoch 1291, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,813: INFO: model_training: Rank 0, Epoch 1291, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,815: INFO: model_training: Rank 0, Epoch 1291, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,816: INFO: model_training: Rank 0, Epoch 1291, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,818: INFO: model_training: Rank 0, Epoch 1292, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,819: INFO: model_training: Rank 0, Epoch 1292, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,820: INFO: model_training: Rank 0, Epoch 1292, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,822: INFO: model_training: Rank 0, Epoch 1292, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,823: INFO: model_training: Rank 0, Epoch 1292, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,825: INFO: model_training: Rank 0, Epoch 1293, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,826: INFO: model_training: Rank 0, Epoch 1293, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,827: INFO: model_training: Rank 0, Epoch 1293, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,828: INFO: model_training: Rank 0, Epoch 1293, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,831: INFO: model_training: Rank 0, Epoch 1293, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,833: INFO: model_training: Rank 0, Epoch 1294, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,834: INFO: model_training: Rank 0, Epoch 1294, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,836: INFO: model_training: Rank 0, Epoch 1294, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,838: INFO: model_training: Rank 0, Epoch 1294, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,839: INFO: model_training: Rank 0, Epoch 1294, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,841: INFO: model_training: Rank 0, Epoch 1295, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,842: INFO: model_training: Rank 0, Epoch 1295, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,844: INFO: model_training: Rank 0, Epoch 1295, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,845: INFO: model_training: Rank 0, Epoch 1295, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,847: INFO: model_training: Rank 0, Epoch 1295, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,848: INFO: model_training: Rank 0, Epoch 1296, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,849: INFO: model_training: Rank 0, Epoch 1296, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,850: INFO: model_training: Rank 0, Epoch 1296, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,852: INFO: model_training: Rank 0, Epoch 1296, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,853: INFO: model_training: Rank 0, Epoch 1296, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,855: INFO: model_training: Rank 0, Epoch 1297, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,856: INFO: model_training: Rank 0, Epoch 1297, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,857: INFO: model_training: Rank 0, Epoch 1297, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,859: INFO: model_training: Rank 0, Epoch 1297, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,860: INFO: model_training: Rank 0, Epoch 1297, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,861: INFO: model_training: Rank 0, Epoch 1298, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,863: INFO: model_training: Rank 0, Epoch 1298, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,864: INFO: model_training: Rank 0, Epoch 1298, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,865: INFO: model_training: Rank 0, Epoch 1298, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,866: INFO: model_training: Rank 0, Epoch 1298, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,867: INFO: model_training: Rank 0, Epoch 1299, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,869: INFO: model_training: Rank 0, Epoch 1299, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,871: INFO: model_training: Rank 0, Epoch 1299, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,873: INFO: model_training: Rank 0, Epoch 1299, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,874: INFO: model_training: Rank 0, Epoch 1299, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,875: INFO: model_training: Rank 0, Epoch 1300, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,877: INFO: model_training: Rank 0, Epoch 1300, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,878: INFO: model_training: Rank 0, Epoch 1300, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,879: INFO: model_training: Rank 0, Epoch 1300, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,881: INFO: model_training: Rank 0, Epoch 1300, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,882: INFO: model_training: Rank 0, Epoch 1301, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,883: INFO: model_training: Rank 0, Epoch 1301, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,885: INFO: model_training: Rank 0, Epoch 1301, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,886: INFO: model_training: Rank 0, Epoch 1301, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,887: INFO: model_training: Rank 0, Epoch 1301, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,889: INFO: model_training: Rank 0, Epoch 1302, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,890: INFO: model_training: Rank 0, Epoch 1302, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,892: INFO: model_training: Rank 0, Epoch 1302, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,894: INFO: model_training: Rank 0, Epoch 1302, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,895: INFO: model_training: Rank 0, Epoch 1302, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,896: INFO: model_training: Rank 0, Epoch 1303, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,897: INFO: model_training: Rank 0, Epoch 1303, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,898: INFO: model_training: Rank 0, Epoch 1303, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,899: INFO: model_training: Rank 0, Epoch 1303, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,900: INFO: model_training: Rank 0, Epoch 1303, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,902: INFO: model_training: Rank 0, Epoch 1304, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,904: INFO: model_training: Rank 0, Epoch 1304, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,905: INFO: model_training: Rank 0, Epoch 1304, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,906: INFO: model_training: Rank 0, Epoch 1304, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,907: INFO: model_training: Rank 0, Epoch 1304, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,908: INFO: model_training: Rank 0, Epoch 1305, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,911: INFO: model_training: Rank 0, Epoch 1305, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,912: INFO: model_training: Rank 0, Epoch 1305, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,913: INFO: model_training: Rank 0, Epoch 1305, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,914: INFO: model_training: Rank 0, Epoch 1305, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,915: INFO: model_training: Rank 0, Epoch 1306, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,916: INFO: model_training: Rank 0, Epoch 1306, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,918: INFO: model_training: Rank 0, Epoch 1306, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,919: INFO: model_training: Rank 0, Epoch 1306, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,920: INFO: model_training: Rank 0, Epoch 1306, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,922: INFO: model_training: Rank 0, Epoch 1307, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,923: INFO: model_training: Rank 0, Epoch 1307, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,924: INFO: model_training: Rank 0, Epoch 1307, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,925: INFO: model_training: Rank 0, Epoch 1307, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,927: INFO: model_training: Rank 0, Epoch 1307, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,929: INFO: model_training: Rank 0, Epoch 1308, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,930: INFO: model_training: Rank 0, Epoch 1308, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,931: INFO: model_training: Rank 0, Epoch 1308, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,933: INFO: model_training: Rank 0, Epoch 1308, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,934: INFO: model_training: Rank 0, Epoch 1308, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,936: INFO: model_training: Rank 0, Epoch 1309, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,937: INFO: model_training: Rank 0, Epoch 1309, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,938: INFO: model_training: Rank 0, Epoch 1309, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,939: INFO: model_training: Rank 0, Epoch 1309, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,940: INFO: model_training: Rank 0, Epoch 1309, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,941: INFO: model_training: Rank 0, Epoch 1310, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,943: INFO: model_training: Rank 0, Epoch 1310, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,944: INFO: model_training: Rank 0, Epoch 1310, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,946: INFO: model_training: Rank 0, Epoch 1310, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,948: INFO: model_training: Rank 0, Epoch 1310, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,949: INFO: model_training: Rank 0, Epoch 1311, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,950: INFO: model_training: Rank 0, Epoch 1311, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,952: INFO: model_training: Rank 0, Epoch 1311, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,953: INFO: model_training: Rank 0, Epoch 1311, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,954: INFO: model_training: Rank 0, Epoch 1311, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,956: INFO: model_training: Rank 0, Epoch 1312, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,957: INFO: model_training: Rank 0, Epoch 1312, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,958: INFO: model_training: Rank 0, Epoch 1312, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,959: INFO: model_training: Rank 0, Epoch 1312, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,961: INFO: model_training: Rank 0, Epoch 1312, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,962: INFO: model_training: Rank 0, Epoch 1313, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,964: INFO: model_training: Rank 0, Epoch 1313, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,965: INFO: model_training: Rank 0, Epoch 1313, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,966: INFO: model_training: Rank 0, Epoch 1313, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,968: INFO: model_training: Rank 0, Epoch 1313, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,970: INFO: model_training: Rank 0, Epoch 1314, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,971: INFO: model_training: Rank 0, Epoch 1314, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,972: INFO: model_training: Rank 0, Epoch 1314, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,973: INFO: model_training: Rank 0, Epoch 1314, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,975: INFO: model_training: Rank 0, Epoch 1314, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,976: INFO: model_training: Rank 0, Epoch 1315, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,978: INFO: model_training: Rank 0, Epoch 1315, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,979: INFO: model_training: Rank 0, Epoch 1315, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,980: INFO: model_training: Rank 0, Epoch 1315, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,981: INFO: model_training: Rank 0, Epoch 1315, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,983: INFO: model_training: Rank 0, Epoch 1316, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,984: INFO: model_training: Rank 0, Epoch 1316, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,986: INFO: model_training: Rank 0, Epoch 1316, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,988: INFO: model_training: Rank 0, Epoch 1316, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,989: INFO: model_training: Rank 0, Epoch 1316, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,990: INFO: model_training: Rank 0, Epoch 1317, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,991: INFO: model_training: Rank 0, Epoch 1317, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,993: INFO: model_training: Rank 0, Epoch 1317, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:33,994: INFO: model_training: Rank 0, Epoch 1317, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:33,995: INFO: model_training: Rank 0, Epoch 1317, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:33,997: INFO: model_training: Rank 0, Epoch 1318, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:33,998: INFO: model_training: Rank 0, Epoch 1318, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:33,999: INFO: model_training: Rank 0, Epoch 1318, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,000: INFO: model_training: Rank 0, Epoch 1318, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,002: INFO: model_training: Rank 0, Epoch 1318, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,003: INFO: model_training: Rank 0, Epoch 1319, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,004: INFO: model_training: Rank 0, Epoch 1319, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,006: INFO: model_training: Rank 0, Epoch 1319, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,007: INFO: model_training: Rank 0, Epoch 1319, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,009: INFO: model_training: Rank 0, Epoch 1319, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,010: INFO: model_training: Rank 0, Epoch 1320, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,012: INFO: model_training: Rank 0, Epoch 1320, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,013: INFO: model_training: Rank 0, Epoch 1320, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,014: INFO: model_training: Rank 0, Epoch 1320, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,016: INFO: model_training: Rank 0, Epoch 1320, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,017: INFO: model_training: Rank 0, Epoch 1321, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,019: INFO: model_training: Rank 0, Epoch 1321, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,020: INFO: model_training: Rank 0, Epoch 1321, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,022: INFO: model_training: Rank 0, Epoch 1321, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,023: INFO: model_training: Rank 0, Epoch 1321, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,024: INFO: model_training: Rank 0, Epoch 1322, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,025: INFO: model_training: Rank 0, Epoch 1322, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,027: INFO: model_training: Rank 0, Epoch 1322, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,029: INFO: model_training: Rank 0, Epoch 1322, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,030: INFO: model_training: Rank 0, Epoch 1322, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,031: INFO: model_training: Rank 0, Epoch 1323, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,033: INFO: model_training: Rank 0, Epoch 1323, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,034: INFO: model_training: Rank 0, Epoch 1323, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,036: INFO: model_training: Rank 0, Epoch 1323, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,038: INFO: model_training: Rank 0, Epoch 1323, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,039: INFO: model_training: Rank 0, Epoch 1324, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,040: INFO: model_training: Rank 0, Epoch 1324, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,041: INFO: model_training: Rank 0, Epoch 1324, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,043: INFO: model_training: Rank 0, Epoch 1324, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,044: INFO: model_training: Rank 0, Epoch 1324, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,046: INFO: model_training: Rank 0, Epoch 1325, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,047: INFO: model_training: Rank 0, Epoch 1325, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,050: INFO: model_training: Rank 0, Epoch 1325, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,051: INFO: model_training: Rank 0, Epoch 1325, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,053: INFO: model_training: Rank 0, Epoch 1325, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,054: INFO: model_training: Rank 0, Epoch 1326, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,056: INFO: model_training: Rank 0, Epoch 1326, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,057: INFO: model_training: Rank 0, Epoch 1326, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,058: INFO: model_training: Rank 0, Epoch 1326, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,059: INFO: model_training: Rank 0, Epoch 1326, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,061: INFO: model_training: Rank 0, Epoch 1327, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,063: INFO: model_training: Rank 0, Epoch 1327, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,064: INFO: model_training: Rank 0, Epoch 1327, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,065: INFO: model_training: Rank 0, Epoch 1327, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,066: INFO: model_training: Rank 0, Epoch 1327, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,067: INFO: model_training: Rank 0, Epoch 1328, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,070: INFO: model_training: Rank 0, Epoch 1328, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,071: INFO: model_training: Rank 0, Epoch 1328, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,073: INFO: model_training: Rank 0, Epoch 1328, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,074: INFO: model_training: Rank 0, Epoch 1328, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,075: INFO: model_training: Rank 0, Epoch 1329, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,077: INFO: model_training: Rank 0, Epoch 1329, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,079: INFO: model_training: Rank 0, Epoch 1329, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,080: INFO: model_training: Rank 0, Epoch 1329, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,081: INFO: model_training: Rank 0, Epoch 1329, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,082: INFO: model_training: Rank 0, Epoch 1330, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,084: INFO: model_training: Rank 0, Epoch 1330, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,086: INFO: model_training: Rank 0, Epoch 1330, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,099: INFO: model_training: Rank 0, Epoch 1330, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,111: INFO: model_training: Rank 0, Epoch 1330, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,114: INFO: model_training: Rank 0, Epoch 1331, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,116: INFO: model_training: Rank 0, Epoch 1331, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,117: INFO: model_training: Rank 0, Epoch 1331, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,124: INFO: model_training: Rank 0, Epoch 1331, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,126: INFO: model_training: Rank 0, Epoch 1331, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,128: INFO: model_training: Rank 0, Epoch 1332, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,129: INFO: model_training: Rank 0, Epoch 1332, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,130: INFO: model_training: Rank 0, Epoch 1332, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,132: INFO: model_training: Rank 0, Epoch 1332, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,133: INFO: model_training: Rank 0, Epoch 1332, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,135: INFO: model_training: Rank 0, Epoch 1333, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,137: INFO: model_training: Rank 0, Epoch 1333, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,138: INFO: model_training: Rank 0, Epoch 1333, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,140: INFO: model_training: Rank 0, Epoch 1333, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,141: INFO: model_training: Rank 0, Epoch 1333, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,142: INFO: model_training: Rank 0, Epoch 1334, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,144: INFO: model_training: Rank 0, Epoch 1334, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,146: INFO: model_training: Rank 0, Epoch 1334, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,147: INFO: model_training: Rank 0, Epoch 1334, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,148: INFO: model_training: Rank 0, Epoch 1334, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,149: INFO: model_training: Rank 0, Epoch 1335, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,151: INFO: model_training: Rank 0, Epoch 1335, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,153: INFO: model_training: Rank 0, Epoch 1335, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,154: INFO: model_training: Rank 0, Epoch 1335, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,156: INFO: model_training: Rank 0, Epoch 1335, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,157: INFO: model_training: Rank 0, Epoch 1336, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,158: INFO: model_training: Rank 0, Epoch 1336, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,159: INFO: model_training: Rank 0, Epoch 1336, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,161: INFO: model_training: Rank 0, Epoch 1336, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,162: INFO: model_training: Rank 0, Epoch 1336, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,164: INFO: model_training: Rank 0, Epoch 1337, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,165: INFO: model_training: Rank 0, Epoch 1337, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,166: INFO: model_training: Rank 0, Epoch 1337, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,167: INFO: model_training: Rank 0, Epoch 1337, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,169: INFO: model_training: Rank 0, Epoch 1337, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,171: INFO: model_training: Rank 0, Epoch 1338, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,172: INFO: model_training: Rank 0, Epoch 1338, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,174: INFO: model_training: Rank 0, Epoch 1338, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,174: INFO: model_training: Rank 0, Epoch 1338, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,176: INFO: model_training: Rank 0, Epoch 1338, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,177: INFO: model_training: Rank 0, Epoch 1339, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,179: INFO: model_training: Rank 0, Epoch 1339, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,180: INFO: model_training: Rank 0, Epoch 1339, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,182: INFO: model_training: Rank 0, Epoch 1339, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,183: INFO: model_training: Rank 0, Epoch 1339, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,185: INFO: model_training: Rank 0, Epoch 1340, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,187: INFO: model_training: Rank 0, Epoch 1340, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,188: INFO: model_training: Rank 0, Epoch 1340, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,190: INFO: model_training: Rank 0, Epoch 1340, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,191: INFO: model_training: Rank 0, Epoch 1340, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,192: INFO: model_training: Rank 0, Epoch 1341, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,194: INFO: model_training: Rank 0, Epoch 1341, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,195: INFO: model_training: Rank 0, Epoch 1341, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,197: INFO: model_training: Rank 0, Epoch 1341, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,198: INFO: model_training: Rank 0, Epoch 1341, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,199: INFO: model_training: Rank 0, Epoch 1342, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,201: INFO: model_training: Rank 0, Epoch 1342, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,203: INFO: model_training: Rank 0, Epoch 1342, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,204: INFO: model_training: Rank 0, Epoch 1342, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,205: INFO: model_training: Rank 0, Epoch 1342, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,207: INFO: model_training: Rank 0, Epoch 1343, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,208: INFO: model_training: Rank 0, Epoch 1343, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,209: INFO: model_training: Rank 0, Epoch 1343, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,211: INFO: model_training: Rank 0, Epoch 1343, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,212: INFO: model_training: Rank 0, Epoch 1343, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,213: INFO: model_training: Rank 0, Epoch 1344, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,215: INFO: model_training: Rank 0, Epoch 1344, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,217: INFO: model_training: Rank 0, Epoch 1344, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,218: INFO: model_training: Rank 0, Epoch 1344, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,220: INFO: model_training: Rank 0, Epoch 1344, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,221: INFO: model_training: Rank 0, Epoch 1345, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,222: INFO: model_training: Rank 0, Epoch 1345, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,223: INFO: model_training: Rank 0, Epoch 1345, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,225: INFO: model_training: Rank 0, Epoch 1345, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,226: INFO: model_training: Rank 0, Epoch 1345, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,228: INFO: model_training: Rank 0, Epoch 1346, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,229: INFO: model_training: Rank 0, Epoch 1346, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,231: INFO: model_training: Rank 0, Epoch 1346, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,232: INFO: model_training: Rank 0, Epoch 1346, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,233: INFO: model_training: Rank 0, Epoch 1346, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,234: INFO: model_training: Rank 0, Epoch 1347, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,236: INFO: model_training: Rank 0, Epoch 1347, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,238: INFO: model_training: Rank 0, Epoch 1347, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,239: INFO: model_training: Rank 0, Epoch 1347, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,241: INFO: model_training: Rank 0, Epoch 1347, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,242: INFO: model_training: Rank 0, Epoch 1348, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,243: INFO: model_training: Rank 0, Epoch 1348, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,245: INFO: model_training: Rank 0, Epoch 1348, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,246: INFO: model_training: Rank 0, Epoch 1348, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,248: INFO: model_training: Rank 0, Epoch 1348, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,249: INFO: model_training: Rank 0, Epoch 1349, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,250: INFO: model_training: Rank 0, Epoch 1349, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,252: INFO: model_training: Rank 0, Epoch 1349, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,253: INFO: model_training: Rank 0, Epoch 1349, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,254: INFO: model_training: Rank 0, Epoch 1349, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,256: INFO: model_training: Rank 0, Epoch 1350, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,257: INFO: model_training: Rank 0, Epoch 1350, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,258: INFO: model_training: Rank 0, Epoch 1350, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,260: INFO: model_training: Rank 0, Epoch 1350, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,262: INFO: model_training: Rank 0, Epoch 1350, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,263: INFO: model_training: Rank 0, Epoch 1351, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,264: INFO: model_training: Rank 0, Epoch 1351, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,265: INFO: model_training: Rank 0, Epoch 1351, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,266: INFO: model_training: Rank 0, Epoch 1351, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,269: INFO: model_training: Rank 0, Epoch 1351, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,270: INFO: model_training: Rank 0, Epoch 1352, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,271: INFO: model_training: Rank 0, Epoch 1352, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,273: INFO: model_training: Rank 0, Epoch 1352, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,274: INFO: model_training: Rank 0, Epoch 1352, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,276: INFO: model_training: Rank 0, Epoch 1352, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,278: INFO: model_training: Rank 0, Epoch 1353, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,279: INFO: model_training: Rank 0, Epoch 1353, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,280: INFO: model_training: Rank 0, Epoch 1353, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,281: INFO: model_training: Rank 0, Epoch 1353, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,282: INFO: model_training: Rank 0, Epoch 1353, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,284: INFO: model_training: Rank 0, Epoch 1354, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,285: INFO: model_training: Rank 0, Epoch 1354, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,287: INFO: model_training: Rank 0, Epoch 1354, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,288: INFO: model_training: Rank 0, Epoch 1354, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,290: INFO: model_training: Rank 0, Epoch 1354, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,291: INFO: model_training: Rank 0, Epoch 1355, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,292: INFO: model_training: Rank 0, Epoch 1355, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,294: INFO: model_training: Rank 0, Epoch 1355, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,296: INFO: model_training: Rank 0, Epoch 1355, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,297: INFO: model_training: Rank 0, Epoch 1355, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,298: INFO: model_training: Rank 0, Epoch 1356, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,299: INFO: model_training: Rank 0, Epoch 1356, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,300: INFO: model_training: Rank 0, Epoch 1356, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,303: INFO: model_training: Rank 0, Epoch 1356, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,304: INFO: model_training: Rank 0, Epoch 1356, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,305: INFO: model_training: Rank 0, Epoch 1357, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,306: INFO: model_training: Rank 0, Epoch 1357, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,308: INFO: model_training: Rank 0, Epoch 1357, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,309: INFO: model_training: Rank 0, Epoch 1357, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,311: INFO: model_training: Rank 0, Epoch 1357, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,312: INFO: model_training: Rank 0, Epoch 1358, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,314: INFO: model_training: Rank 0, Epoch 1358, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,315: INFO: model_training: Rank 0, Epoch 1358, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,316: INFO: model_training: Rank 0, Epoch 1358, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,318: INFO: model_training: Rank 0, Epoch 1358, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,320: INFO: model_training: Rank 0, Epoch 1359, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,321: INFO: model_training: Rank 0, Epoch 1359, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,323: INFO: model_training: Rank 0, Epoch 1359, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,324: INFO: model_training: Rank 0, Epoch 1359, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,325: INFO: model_training: Rank 0, Epoch 1359, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,327: INFO: model_training: Rank 0, Epoch 1360, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,328: INFO: model_training: Rank 0, Epoch 1360, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,330: INFO: model_training: Rank 0, Epoch 1360, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,331: INFO: model_training: Rank 0, Epoch 1360, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,332: INFO: model_training: Rank 0, Epoch 1360, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,334: INFO: model_training: Rank 0, Epoch 1361, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,335: INFO: model_training: Rank 0, Epoch 1361, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,337: INFO: model_training: Rank 0, Epoch 1361, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,338: INFO: model_training: Rank 0, Epoch 1361, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,339: INFO: model_training: Rank 0, Epoch 1361, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,340: INFO: model_training: Rank 0, Epoch 1362, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,341: INFO: model_training: Rank 0, Epoch 1362, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,342: INFO: model_training: Rank 0, Epoch 1362, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,344: INFO: model_training: Rank 0, Epoch 1362, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,345: INFO: model_training: Rank 0, Epoch 1362, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,346: INFO: model_training: Rank 0, Epoch 1363, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,348: INFO: model_training: Rank 0, Epoch 1363, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,349: INFO: model_training: Rank 0, Epoch 1363, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,350: INFO: model_training: Rank 0, Epoch 1363, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,352: INFO: model_training: Rank 0, Epoch 1363, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,354: INFO: model_training: Rank 0, Epoch 1364, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,355: INFO: model_training: Rank 0, Epoch 1364, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,356: INFO: model_training: Rank 0, Epoch 1364, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,357: INFO: model_training: Rank 0, Epoch 1364, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,358: INFO: model_training: Rank 0, Epoch 1364, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,359: INFO: model_training: Rank 0, Epoch 1365, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,361: INFO: model_training: Rank 0, Epoch 1365, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,363: INFO: model_training: Rank 0, Epoch 1365, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,365: INFO: model_training: Rank 0, Epoch 1365, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,366: INFO: model_training: Rank 0, Epoch 1365, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,367: INFO: model_training: Rank 0, Epoch 1366, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,369: INFO: model_training: Rank 0, Epoch 1366, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,370: INFO: model_training: Rank 0, Epoch 1366, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,371: INFO: model_training: Rank 0, Epoch 1366, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,372: INFO: model_training: Rank 0, Epoch 1366, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,374: INFO: model_training: Rank 0, Epoch 1367, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,375: INFO: model_training: Rank 0, Epoch 1367, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,376: INFO: model_training: Rank 0, Epoch 1367, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,378: INFO: model_training: Rank 0, Epoch 1367, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,379: INFO: model_training: Rank 0, Epoch 1367, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,381: INFO: model_training: Rank 0, Epoch 1368, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,382: INFO: model_training: Rank 0, Epoch 1368, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,383: INFO: model_training: Rank 0, Epoch 1368, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,385: INFO: model_training: Rank 0, Epoch 1368, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,386: INFO: model_training: Rank 0, Epoch 1368, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,388: INFO: model_training: Rank 0, Epoch 1369, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,389: INFO: model_training: Rank 0, Epoch 1369, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,390: INFO: model_training: Rank 0, Epoch 1369, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,391: INFO: model_training: Rank 0, Epoch 1369, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,393: INFO: model_training: Rank 0, Epoch 1369, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,395: INFO: model_training: Rank 0, Epoch 1370, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,396: INFO: model_training: Rank 0, Epoch 1370, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,398: INFO: model_training: Rank 0, Epoch 1370, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,399: INFO: model_training: Rank 0, Epoch 1370, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,400: INFO: model_training: Rank 0, Epoch 1370, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,402: INFO: model_training: Rank 0, Epoch 1371, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,404: INFO: model_training: Rank 0, Epoch 1371, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,405: INFO: model_training: Rank 0, Epoch 1371, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,406: INFO: model_training: Rank 0, Epoch 1371, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,407: INFO: model_training: Rank 0, Epoch 1371, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,409: INFO: model_training: Rank 0, Epoch 1372, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,411: INFO: model_training: Rank 0, Epoch 1372, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,413: INFO: model_training: Rank 0, Epoch 1372, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,414: INFO: model_training: Rank 0, Epoch 1372, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,415: INFO: model_training: Rank 0, Epoch 1372, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,416: INFO: model_training: Rank 0, Epoch 1373, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,417: INFO: model_training: Rank 0, Epoch 1373, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,419: INFO: model_training: Rank 0, Epoch 1373, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,420: INFO: model_training: Rank 0, Epoch 1373, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,422: INFO: model_training: Rank 0, Epoch 1373, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,423: INFO: model_training: Rank 0, Epoch 1374, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,424: INFO: model_training: Rank 0, Epoch 1374, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,425: INFO: model_training: Rank 0, Epoch 1374, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,428: INFO: model_training: Rank 0, Epoch 1374, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,429: INFO: model_training: Rank 0, Epoch 1374, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,430: INFO: model_training: Rank 0, Epoch 1375, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,432: INFO: model_training: Rank 0, Epoch 1375, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,433: INFO: model_training: Rank 0, Epoch 1375, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,434: INFO: model_training: Rank 0, Epoch 1375, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,435: INFO: model_training: Rank 0, Epoch 1375, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,437: INFO: model_training: Rank 0, Epoch 1376, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,438: INFO: model_training: Rank 0, Epoch 1376, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,439: INFO: model_training: Rank 0, Epoch 1376, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,440: INFO: model_training: Rank 0, Epoch 1376, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,442: INFO: model_training: Rank 0, Epoch 1376, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,444: INFO: model_training: Rank 0, Epoch 1377, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,445: INFO: model_training: Rank 0, Epoch 1377, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,446: INFO: model_training: Rank 0, Epoch 1377, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,447: INFO: model_training: Rank 0, Epoch 1377, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,449: INFO: model_training: Rank 0, Epoch 1377, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,450: INFO: model_training: Rank 0, Epoch 1378, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,452: INFO: model_training: Rank 0, Epoch 1378, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,453: INFO: model_training: Rank 0, Epoch 1378, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,454: INFO: model_training: Rank 0, Epoch 1378, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,455: INFO: model_training: Rank 0, Epoch 1378, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,457: INFO: model_training: Rank 0, Epoch 1379, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,458: INFO: model_training: Rank 0, Epoch 1379, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,460: INFO: model_training: Rank 0, Epoch 1379, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,462: INFO: model_training: Rank 0, Epoch 1379, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,463: INFO: model_training: Rank 0, Epoch 1379, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,465: INFO: model_training: Rank 0, Epoch 1380, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,466: INFO: model_training: Rank 0, Epoch 1380, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,466: INFO: model_training: Rank 0, Epoch 1380, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,467: INFO: model_training: Rank 0, Epoch 1380, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,469: INFO: model_training: Rank 0, Epoch 1380, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,471: INFO: model_training: Rank 0, Epoch 1381, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,472: INFO: model_training: Rank 0, Epoch 1381, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,474: INFO: model_training: Rank 0, Epoch 1381, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,475: INFO: model_training: Rank 0, Epoch 1381, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,477: INFO: model_training: Rank 0, Epoch 1381, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,479: INFO: model_training: Rank 0, Epoch 1382, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,480: INFO: model_training: Rank 0, Epoch 1382, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,482: INFO: model_training: Rank 0, Epoch 1382, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,483: INFO: model_training: Rank 0, Epoch 1382, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,484: INFO: model_training: Rank 0, Epoch 1382, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,486: INFO: model_training: Rank 0, Epoch 1383, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,487: INFO: model_training: Rank 0, Epoch 1383, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,489: INFO: model_training: Rank 0, Epoch 1383, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,491: INFO: model_training: Rank 0, Epoch 1383, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,493: INFO: model_training: Rank 0, Epoch 1383, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,495: INFO: model_training: Rank 0, Epoch 1384, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,496: INFO: model_training: Rank 0, Epoch 1384, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,497: INFO: model_training: Rank 0, Epoch 1384, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,498: INFO: model_training: Rank 0, Epoch 1384, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,500: INFO: model_training: Rank 0, Epoch 1384, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,501: INFO: model_training: Rank 0, Epoch 1385, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,503: INFO: model_training: Rank 0, Epoch 1385, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,504: INFO: model_training: Rank 0, Epoch 1385, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,506: INFO: model_training: Rank 0, Epoch 1385, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,507: INFO: model_training: Rank 0, Epoch 1385, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,509: INFO: model_training: Rank 0, Epoch 1386, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,511: INFO: model_training: Rank 0, Epoch 1386, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,512: INFO: model_training: Rank 0, Epoch 1386, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,514: INFO: model_training: Rank 0, Epoch 1386, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,515: INFO: model_training: Rank 0, Epoch 1386, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,516: INFO: model_training: Rank 0, Epoch 1387, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,518: INFO: model_training: Rank 0, Epoch 1387, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,519: INFO: model_training: Rank 0, Epoch 1387, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,520: INFO: model_training: Rank 0, Epoch 1387, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,521: INFO: model_training: Rank 0, Epoch 1387, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,523: INFO: model_training: Rank 0, Epoch 1388, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,525: INFO: model_training: Rank 0, Epoch 1388, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,526: INFO: model_training: Rank 0, Epoch 1388, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,527: INFO: model_training: Rank 0, Epoch 1388, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,529: INFO: model_training: Rank 0, Epoch 1388, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,530: INFO: model_training: Rank 0, Epoch 1389, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,532: INFO: model_training: Rank 0, Epoch 1389, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,533: INFO: model_training: Rank 0, Epoch 1389, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,535: INFO: model_training: Rank 0, Epoch 1389, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,536: INFO: model_training: Rank 0, Epoch 1389, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,538: INFO: model_training: Rank 0, Epoch 1390, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,539: INFO: model_training: Rank 0, Epoch 1390, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,541: INFO: model_training: Rank 0, Epoch 1390, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,543: INFO: model_training: Rank 0, Epoch 1390, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,544: INFO: model_training: Rank 0, Epoch 1390, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,546: INFO: model_training: Rank 0, Epoch 1391, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,547: INFO: model_training: Rank 0, Epoch 1391, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,548: INFO: model_training: Rank 0, Epoch 1391, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,550: INFO: model_training: Rank 0, Epoch 1391, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,552: INFO: model_training: Rank 0, Epoch 1391, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,553: INFO: model_training: Rank 0, Epoch 1392, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,554: INFO: model_training: Rank 0, Epoch 1392, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,556: INFO: model_training: Rank 0, Epoch 1392, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,558: INFO: model_training: Rank 0, Epoch 1392, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,559: INFO: model_training: Rank 0, Epoch 1392, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,561: INFO: model_training: Rank 0, Epoch 1393, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,563: INFO: model_training: Rank 0, Epoch 1393, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,565: INFO: model_training: Rank 0, Epoch 1393, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,566: INFO: model_training: Rank 0, Epoch 1393, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,567: INFO: model_training: Rank 0, Epoch 1393, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,569: INFO: model_training: Rank 0, Epoch 1394, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,571: INFO: model_training: Rank 0, Epoch 1394, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,572: INFO: model_training: Rank 0, Epoch 1394, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,574: INFO: model_training: Rank 0, Epoch 1394, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,576: INFO: model_training: Rank 0, Epoch 1394, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,577: INFO: model_training: Rank 0, Epoch 1395, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,579: INFO: model_training: Rank 0, Epoch 1395, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,580: INFO: model_training: Rank 0, Epoch 1395, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,581: INFO: model_training: Rank 0, Epoch 1395, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,583: INFO: model_training: Rank 0, Epoch 1395, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,584: INFO: model_training: Rank 0, Epoch 1396, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,586: INFO: model_training: Rank 0, Epoch 1396, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,587: INFO: model_training: Rank 0, Epoch 1396, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,588: INFO: model_training: Rank 0, Epoch 1396, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,590: INFO: model_training: Rank 0, Epoch 1396, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,591: INFO: model_training: Rank 0, Epoch 1397, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,593: INFO: model_training: Rank 0, Epoch 1397, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,595: INFO: model_training: Rank 0, Epoch 1397, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,596: INFO: model_training: Rank 0, Epoch 1397, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,598: INFO: model_training: Rank 0, Epoch 1397, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,600: INFO: model_training: Rank 0, Epoch 1398, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,601: INFO: model_training: Rank 0, Epoch 1398, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,603: INFO: model_training: Rank 0, Epoch 1398, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,604: INFO: model_training: Rank 0, Epoch 1398, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,605: INFO: model_training: Rank 0, Epoch 1398, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,607: INFO: model_training: Rank 0, Epoch 1399, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,608: INFO: model_training: Rank 0, Epoch 1399, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,610: INFO: model_training: Rank 0, Epoch 1399, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,612: INFO: model_training: Rank 0, Epoch 1399, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,613: INFO: model_training: Rank 0, Epoch 1399, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,615: INFO: model_training: Rank 0, Epoch 1400, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,616: INFO: model_training: Rank 0, Epoch 1400, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,618: INFO: model_training: Rank 0, Epoch 1400, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,619: INFO: model_training: Rank 0, Epoch 1400, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,620: INFO: model_training: Rank 0, Epoch 1400, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,622: INFO: model_training: Rank 0, Epoch 1401, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,624: INFO: model_training: Rank 0, Epoch 1401, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,625: INFO: model_training: Rank 0, Epoch 1401, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,627: INFO: model_training: Rank 0, Epoch 1401, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,628: INFO: model_training: Rank 0, Epoch 1401, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,630: INFO: model_training: Rank 0, Epoch 1402, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,632: INFO: model_training: Rank 0, Epoch 1402, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,633: INFO: model_training: Rank 0, Epoch 1402, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,634: INFO: model_training: Rank 0, Epoch 1402, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,636: INFO: model_training: Rank 0, Epoch 1402, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,638: INFO: model_training: Rank 0, Epoch 1403, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,639: INFO: model_training: Rank 0, Epoch 1403, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,641: INFO: model_training: Rank 0, Epoch 1403, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,642: INFO: model_training: Rank 0, Epoch 1403, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,644: INFO: model_training: Rank 0, Epoch 1403, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,646: INFO: model_training: Rank 0, Epoch 1404, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,647: INFO: model_training: Rank 0, Epoch 1404, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,649: INFO: model_training: Rank 0, Epoch 1404, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,650: INFO: model_training: Rank 0, Epoch 1404, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,653: INFO: model_training: Rank 0, Epoch 1404, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,655: INFO: model_training: Rank 0, Epoch 1405, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,658: INFO: model_training: Rank 0, Epoch 1405, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,660: INFO: model_training: Rank 0, Epoch 1405, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,661: INFO: model_training: Rank 0, Epoch 1405, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,663: INFO: model_training: Rank 0, Epoch 1405, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,665: INFO: model_training: Rank 0, Epoch 1406, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,667: INFO: model_training: Rank 0, Epoch 1406, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,669: INFO: model_training: Rank 0, Epoch 1406, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,670: INFO: model_training: Rank 0, Epoch 1406, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,672: INFO: model_training: Rank 0, Epoch 1406, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,674: INFO: model_training: Rank 0, Epoch 1407, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,675: INFO: model_training: Rank 0, Epoch 1407, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,676: INFO: model_training: Rank 0, Epoch 1407, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,678: INFO: model_training: Rank 0, Epoch 1407, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,679: INFO: model_training: Rank 0, Epoch 1407, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,681: INFO: model_training: Rank 0, Epoch 1408, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,682: INFO: model_training: Rank 0, Epoch 1408, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,684: INFO: model_training: Rank 0, Epoch 1408, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,686: INFO: model_training: Rank 0, Epoch 1408, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,687: INFO: model_training: Rank 0, Epoch 1408, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,689: INFO: model_training: Rank 0, Epoch 1409, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,691: INFO: model_training: Rank 0, Epoch 1409, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,692: INFO: model_training: Rank 0, Epoch 1409, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,693: INFO: model_training: Rank 0, Epoch 1409, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,695: INFO: model_training: Rank 0, Epoch 1409, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,696: INFO: model_training: Rank 0, Epoch 1410, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,698: INFO: model_training: Rank 0, Epoch 1410, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,700: INFO: model_training: Rank 0, Epoch 1410, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,701: INFO: model_training: Rank 0, Epoch 1410, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,703: INFO: model_training: Rank 0, Epoch 1410, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,705: INFO: model_training: Rank 0, Epoch 1411, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,706: INFO: model_training: Rank 0, Epoch 1411, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,708: INFO: model_training: Rank 0, Epoch 1411, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,710: INFO: model_training: Rank 0, Epoch 1411, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,711: INFO: model_training: Rank 0, Epoch 1411, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,713: INFO: model_training: Rank 0, Epoch 1412, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,715: INFO: model_training: Rank 0, Epoch 1412, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,717: INFO: model_training: Rank 0, Epoch 1412, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,720: INFO: model_training: Rank 0, Epoch 1412, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,721: INFO: model_training: Rank 0, Epoch 1412, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,723: INFO: model_training: Rank 0, Epoch 1413, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,724: INFO: model_training: Rank 0, Epoch 1413, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,726: INFO: model_training: Rank 0, Epoch 1413, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,727: INFO: model_training: Rank 0, Epoch 1413, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,729: INFO: model_training: Rank 0, Epoch 1413, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,730: INFO: model_training: Rank 0, Epoch 1414, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,732: INFO: model_training: Rank 0, Epoch 1414, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,733: INFO: model_training: Rank 0, Epoch 1414, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,736: INFO: model_training: Rank 0, Epoch 1414, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,737: INFO: model_training: Rank 0, Epoch 1414, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,738: INFO: model_training: Rank 0, Epoch 1415, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,740: INFO: model_training: Rank 0, Epoch 1415, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,741: INFO: model_training: Rank 0, Epoch 1415, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,743: INFO: model_training: Rank 0, Epoch 1415, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,744: INFO: model_training: Rank 0, Epoch 1415, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,745: INFO: model_training: Rank 0, Epoch 1416, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,747: INFO: model_training: Rank 0, Epoch 1416, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,749: INFO: model_training: Rank 0, Epoch 1416, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,750: INFO: model_training: Rank 0, Epoch 1416, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,751: INFO: model_training: Rank 0, Epoch 1416, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,754: INFO: model_training: Rank 0, Epoch 1417, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,755: INFO: model_training: Rank 0, Epoch 1417, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,757: INFO: model_training: Rank 0, Epoch 1417, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,758: INFO: model_training: Rank 0, Epoch 1417, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,760: INFO: model_training: Rank 0, Epoch 1417, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,761: INFO: model_training: Rank 0, Epoch 1418, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,763: INFO: model_training: Rank 0, Epoch 1418, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,764: INFO: model_training: Rank 0, Epoch 1418, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,766: INFO: model_training: Rank 0, Epoch 1418, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,767: INFO: model_training: Rank 0, Epoch 1418, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,769: INFO: model_training: Rank 0, Epoch 1419, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,771: INFO: model_training: Rank 0, Epoch 1419, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,772: INFO: model_training: Rank 0, Epoch 1419, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,774: INFO: model_training: Rank 0, Epoch 1419, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,775: INFO: model_training: Rank 0, Epoch 1419, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,777: INFO: model_training: Rank 0, Epoch 1420, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,779: INFO: model_training: Rank 0, Epoch 1420, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,780: INFO: model_training: Rank 0, Epoch 1420, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,782: INFO: model_training: Rank 0, Epoch 1420, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,784: INFO: model_training: Rank 0, Epoch 1420, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,786: INFO: model_training: Rank 0, Epoch 1421, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,787: INFO: model_training: Rank 0, Epoch 1421, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,789: INFO: model_training: Rank 0, Epoch 1421, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,791: INFO: model_training: Rank 0, Epoch 1421, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,792: INFO: model_training: Rank 0, Epoch 1421, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,794: INFO: model_training: Rank 0, Epoch 1422, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,795: INFO: model_training: Rank 0, Epoch 1422, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,797: INFO: model_training: Rank 0, Epoch 1422, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,798: INFO: model_training: Rank 0, Epoch 1422, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,800: INFO: model_training: Rank 0, Epoch 1422, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,801: INFO: model_training: Rank 0, Epoch 1423, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,803: INFO: model_training: Rank 0, Epoch 1423, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,804: INFO: model_training: Rank 0, Epoch 1423, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,806: INFO: model_training: Rank 0, Epoch 1423, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,807: INFO: model_training: Rank 0, Epoch 1423, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,808: INFO: model_training: Rank 0, Epoch 1424, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,810: INFO: model_training: Rank 0, Epoch 1424, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,812: INFO: model_training: Rank 0, Epoch 1424, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,813: INFO: model_training: Rank 0, Epoch 1424, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,814: INFO: model_training: Rank 0, Epoch 1424, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,816: INFO: model_training: Rank 0, Epoch 1425, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,818: INFO: model_training: Rank 0, Epoch 1425, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,819: INFO: model_training: Rank 0, Epoch 1425, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,821: INFO: model_training: Rank 0, Epoch 1425, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,821: INFO: model_training: Rank 0, Epoch 1425, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,823: INFO: model_training: Rank 0, Epoch 1426, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,825: INFO: model_training: Rank 0, Epoch 1426, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,826: INFO: model_training: Rank 0, Epoch 1426, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,828: INFO: model_training: Rank 0, Epoch 1426, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,830: INFO: model_training: Rank 0, Epoch 1426, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,831: INFO: model_training: Rank 0, Epoch 1427, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,833: INFO: model_training: Rank 0, Epoch 1427, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,834: INFO: model_training: Rank 0, Epoch 1427, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,836: INFO: model_training: Rank 0, Epoch 1427, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,837: INFO: model_training: Rank 0, Epoch 1427, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,839: INFO: model_training: Rank 0, Epoch 1428, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,841: INFO: model_training: Rank 0, Epoch 1428, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,842: INFO: model_training: Rank 0, Epoch 1428, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,844: INFO: model_training: Rank 0, Epoch 1428, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,845: INFO: model_training: Rank 0, Epoch 1428, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,847: INFO: model_training: Rank 0, Epoch 1429, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,849: INFO: model_training: Rank 0, Epoch 1429, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,850: INFO: model_training: Rank 0, Epoch 1429, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,851: INFO: model_training: Rank 0, Epoch 1429, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,853: INFO: model_training: Rank 0, Epoch 1429, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,855: INFO: model_training: Rank 0, Epoch 1430, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,856: INFO: model_training: Rank 0, Epoch 1430, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,858: INFO: model_training: Rank 0, Epoch 1430, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,859: INFO: model_training: Rank 0, Epoch 1430, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,861: INFO: model_training: Rank 0, Epoch 1430, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,862: INFO: model_training: Rank 0, Epoch 1431, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,864: INFO: model_training: Rank 0, Epoch 1431, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,865: INFO: model_training: Rank 0, Epoch 1431, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,867: INFO: model_training: Rank 0, Epoch 1431, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,868: INFO: model_training: Rank 0, Epoch 1431, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,870: INFO: model_training: Rank 0, Epoch 1432, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,871: INFO: model_training: Rank 0, Epoch 1432, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,873: INFO: model_training: Rank 0, Epoch 1432, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,875: INFO: model_training: Rank 0, Epoch 1432, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,877: INFO: model_training: Rank 0, Epoch 1432, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,878: INFO: model_training: Rank 0, Epoch 1433, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,879: INFO: model_training: Rank 0, Epoch 1433, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,880: INFO: model_training: Rank 0, Epoch 1433, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,882: INFO: model_training: Rank 0, Epoch 1433, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,884: INFO: model_training: Rank 0, Epoch 1433, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,885: INFO: model_training: Rank 0, Epoch 1434, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,887: INFO: model_training: Rank 0, Epoch 1434, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,888: INFO: model_training: Rank 0, Epoch 1434, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,890: INFO: model_training: Rank 0, Epoch 1434, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,892: INFO: model_training: Rank 0, Epoch 1434, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,893: INFO: model_training: Rank 0, Epoch 1435, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,895: INFO: model_training: Rank 0, Epoch 1435, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,896: INFO: model_training: Rank 0, Epoch 1435, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,897: INFO: model_training: Rank 0, Epoch 1435, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,899: INFO: model_training: Rank 0, Epoch 1435, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,900: INFO: model_training: Rank 0, Epoch 1436, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,902: INFO: model_training: Rank 0, Epoch 1436, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,904: INFO: model_training: Rank 0, Epoch 1436, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,905: INFO: model_training: Rank 0, Epoch 1436, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,906: INFO: model_training: Rank 0, Epoch 1436, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,908: INFO: model_training: Rank 0, Epoch 1437, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,910: INFO: model_training: Rank 0, Epoch 1437, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,912: INFO: model_training: Rank 0, Epoch 1437, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,913: INFO: model_training: Rank 0, Epoch 1437, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,915: INFO: model_training: Rank 0, Epoch 1437, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,917: INFO: model_training: Rank 0, Epoch 1438, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,918: INFO: model_training: Rank 0, Epoch 1438, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,920: INFO: model_training: Rank 0, Epoch 1438, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,921: INFO: model_training: Rank 0, Epoch 1438, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,923: INFO: model_training: Rank 0, Epoch 1438, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,925: INFO: model_training: Rank 0, Epoch 1439, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,926: INFO: model_training: Rank 0, Epoch 1439, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,928: INFO: model_training: Rank 0, Epoch 1439, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,929: INFO: model_training: Rank 0, Epoch 1439, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,930: INFO: model_training: Rank 0, Epoch 1439, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,932: INFO: model_training: Rank 0, Epoch 1440, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,934: INFO: model_training: Rank 0, Epoch 1440, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,935: INFO: model_training: Rank 0, Epoch 1440, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,937: INFO: model_training: Rank 0, Epoch 1440, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,938: INFO: model_training: Rank 0, Epoch 1440, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,940: INFO: model_training: Rank 0, Epoch 1441, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,941: INFO: model_training: Rank 0, Epoch 1441, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,942: INFO: model_training: Rank 0, Epoch 1441, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,944: INFO: model_training: Rank 0, Epoch 1441, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,946: INFO: model_training: Rank 0, Epoch 1441, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,948: INFO: model_training: Rank 0, Epoch 1442, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,949: INFO: model_training: Rank 0, Epoch 1442, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,951: INFO: model_training: Rank 0, Epoch 1442, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,953: INFO: model_training: Rank 0, Epoch 1442, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,954: INFO: model_training: Rank 0, Epoch 1442, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,955: INFO: model_training: Rank 0, Epoch 1443, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,958: INFO: model_training: Rank 0, Epoch 1443, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,959: INFO: model_training: Rank 0, Epoch 1443, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,961: INFO: model_training: Rank 0, Epoch 1443, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,962: INFO: model_training: Rank 0, Epoch 1443, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,964: INFO: model_training: Rank 0, Epoch 1444, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,965: INFO: model_training: Rank 0, Epoch 1444, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,967: INFO: model_training: Rank 0, Epoch 1444, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,968: INFO: model_training: Rank 0, Epoch 1444, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,969: INFO: model_training: Rank 0, Epoch 1444, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,970: INFO: model_training: Rank 0, Epoch 1445, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,971: INFO: model_training: Rank 0, Epoch 1445, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,973: INFO: model_training: Rank 0, Epoch 1445, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,974: INFO: model_training: Rank 0, Epoch 1445, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,975: INFO: model_training: Rank 0, Epoch 1445, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,977: INFO: model_training: Rank 0, Epoch 1446, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,978: INFO: model_training: Rank 0, Epoch 1446, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,979: INFO: model_training: Rank 0, Epoch 1446, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,980: INFO: model_training: Rank 0, Epoch 1446, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,982: INFO: model_training: Rank 0, Epoch 1446, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,983: INFO: model_training: Rank 0, Epoch 1447, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,985: INFO: model_training: Rank 0, Epoch 1447, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,986: INFO: model_training: Rank 0, Epoch 1447, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,987: INFO: model_training: Rank 0, Epoch 1447, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,988: INFO: model_training: Rank 0, Epoch 1447, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,990: INFO: model_training: Rank 0, Epoch 1448, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,991: INFO: model_training: Rank 0, Epoch 1448, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,993: INFO: model_training: Rank 0, Epoch 1448, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:34,994: INFO: model_training: Rank 0, Epoch 1448, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:34,995: INFO: model_training: Rank 0, Epoch 1448, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:34,996: INFO: model_training: Rank 0, Epoch 1449, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:34,998: INFO: model_training: Rank 0, Epoch 1449, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:34,999: INFO: model_training: Rank 0, Epoch 1449, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,000: INFO: model_training: Rank 0, Epoch 1449, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,002: INFO: model_training: Rank 0, Epoch 1449, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,003: INFO: model_training: Rank 0, Epoch 1450, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,005: INFO: model_training: Rank 0, Epoch 1450, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,006: INFO: model_training: Rank 0, Epoch 1450, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,007: INFO: model_training: Rank 0, Epoch 1450, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,008: INFO: model_training: Rank 0, Epoch 1450, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,010: INFO: model_training: Rank 0, Epoch 1451, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,011: INFO: model_training: Rank 0, Epoch 1451, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,012: INFO: model_training: Rank 0, Epoch 1451, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,013: INFO: model_training: Rank 0, Epoch 1451, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,015: INFO: model_training: Rank 0, Epoch 1451, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,016: INFO: model_training: Rank 0, Epoch 1452, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,018: INFO: model_training: Rank 0, Epoch 1452, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,019: INFO: model_training: Rank 0, Epoch 1452, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,020: INFO: model_training: Rank 0, Epoch 1452, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,021: INFO: model_training: Rank 0, Epoch 1452, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,023: INFO: model_training: Rank 0, Epoch 1453, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,024: INFO: model_training: Rank 0, Epoch 1453, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,025: INFO: model_training: Rank 0, Epoch 1453, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,027: INFO: model_training: Rank 0, Epoch 1453, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,028: INFO: model_training: Rank 0, Epoch 1453, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,029: INFO: model_training: Rank 0, Epoch 1454, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,031: INFO: model_training: Rank 0, Epoch 1454, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,032: INFO: model_training: Rank 0, Epoch 1454, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,033: INFO: model_training: Rank 0, Epoch 1454, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,034: INFO: model_training: Rank 0, Epoch 1454, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,035: INFO: model_training: Rank 0, Epoch 1455, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,037: INFO: model_training: Rank 0, Epoch 1455, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,038: INFO: model_training: Rank 0, Epoch 1455, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,040: INFO: model_training: Rank 0, Epoch 1455, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,041: INFO: model_training: Rank 0, Epoch 1455, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,043: INFO: model_training: Rank 0, Epoch 1456, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,044: INFO: model_training: Rank 0, Epoch 1456, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,045: INFO: model_training: Rank 0, Epoch 1456, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,046: INFO: model_training: Rank 0, Epoch 1456, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,047: INFO: model_training: Rank 0, Epoch 1456, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,049: INFO: model_training: Rank 0, Epoch 1457, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,050: INFO: model_training: Rank 0, Epoch 1457, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,051: INFO: model_training: Rank 0, Epoch 1457, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,053: INFO: model_training: Rank 0, Epoch 1457, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,054: INFO: model_training: Rank 0, Epoch 1457, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,055: INFO: model_training: Rank 0, Epoch 1458, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,056: INFO: model_training: Rank 0, Epoch 1458, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,057: INFO: model_training: Rank 0, Epoch 1458, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,058: INFO: model_training: Rank 0, Epoch 1458, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,060: INFO: model_training: Rank 0, Epoch 1458, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,061: INFO: model_training: Rank 0, Epoch 1459, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,062: INFO: model_training: Rank 0, Epoch 1459, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,063: INFO: model_training: Rank 0, Epoch 1459, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,065: INFO: model_training: Rank 0, Epoch 1459, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,066: INFO: model_training: Rank 0, Epoch 1459, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,068: INFO: model_training: Rank 0, Epoch 1460, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,070: INFO: model_training: Rank 0, Epoch 1460, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,071: INFO: model_training: Rank 0, Epoch 1460, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,072: INFO: model_training: Rank 0, Epoch 1460, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,074: INFO: model_training: Rank 0, Epoch 1460, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,075: INFO: model_training: Rank 0, Epoch 1461, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,076: INFO: model_training: Rank 0, Epoch 1461, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,077: INFO: model_training: Rank 0, Epoch 1461, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,079: INFO: model_training: Rank 0, Epoch 1461, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,080: INFO: model_training: Rank 0, Epoch 1461, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,082: INFO: model_training: Rank 0, Epoch 1462, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,083: INFO: model_training: Rank 0, Epoch 1462, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,084: INFO: model_training: Rank 0, Epoch 1462, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,085: INFO: model_training: Rank 0, Epoch 1462, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,086: INFO: model_training: Rank 0, Epoch 1462, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,087: INFO: model_training: Rank 0, Epoch 1463, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,089: INFO: model_training: Rank 0, Epoch 1463, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,090: INFO: model_training: Rank 0, Epoch 1463, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,092: INFO: model_training: Rank 0, Epoch 1463, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,093: INFO: model_training: Rank 0, Epoch 1463, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,094: INFO: model_training: Rank 0, Epoch 1464, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,096: INFO: model_training: Rank 0, Epoch 1464, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,097: INFO: model_training: Rank 0, Epoch 1464, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,099: INFO: model_training: Rank 0, Epoch 1464, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,100: INFO: model_training: Rank 0, Epoch 1464, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,101: INFO: model_training: Rank 0, Epoch 1465, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,103: INFO: model_training: Rank 0, Epoch 1465, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,104: INFO: model_training: Rank 0, Epoch 1465, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,105: INFO: model_training: Rank 0, Epoch 1465, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,107: INFO: model_training: Rank 0, Epoch 1465, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,108: INFO: model_training: Rank 0, Epoch 1466, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,109: INFO: model_training: Rank 0, Epoch 1466, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,110: INFO: model_training: Rank 0, Epoch 1466, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,111: INFO: model_training: Rank 0, Epoch 1466, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,112: INFO: model_training: Rank 0, Epoch 1466, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,114: INFO: model_training: Rank 0, Epoch 1467, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,115: INFO: model_training: Rank 0, Epoch 1467, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,117: INFO: model_training: Rank 0, Epoch 1467, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,119: INFO: model_training: Rank 0, Epoch 1467, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,120: INFO: model_training: Rank 0, Epoch 1467, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,122: INFO: model_training: Rank 0, Epoch 1468, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,123: INFO: model_training: Rank 0, Epoch 1468, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,125: INFO: model_training: Rank 0, Epoch 1468, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,127: INFO: model_training: Rank 0, Epoch 1468, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,129: INFO: model_training: Rank 0, Epoch 1468, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,131: INFO: model_training: Rank 0, Epoch 1469, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,133: INFO: model_training: Rank 0, Epoch 1469, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,134: INFO: model_training: Rank 0, Epoch 1469, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,136: INFO: model_training: Rank 0, Epoch 1469, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,137: INFO: model_training: Rank 0, Epoch 1469, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,138: INFO: model_training: Rank 0, Epoch 1470, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,140: INFO: model_training: Rank 0, Epoch 1470, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,142: INFO: model_training: Rank 0, Epoch 1470, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,143: INFO: model_training: Rank 0, Epoch 1470, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,145: INFO: model_training: Rank 0, Epoch 1470, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,146: INFO: model_training: Rank 0, Epoch 1471, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,149: INFO: model_training: Rank 0, Epoch 1471, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,150: INFO: model_training: Rank 0, Epoch 1471, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,152: INFO: model_training: Rank 0, Epoch 1471, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,153: INFO: model_training: Rank 0, Epoch 1471, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,155: INFO: model_training: Rank 0, Epoch 1472, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,157: INFO: model_training: Rank 0, Epoch 1472, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,158: INFO: model_training: Rank 0, Epoch 1472, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,160: INFO: model_training: Rank 0, Epoch 1472, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,162: INFO: model_training: Rank 0, Epoch 1472, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,163: INFO: model_training: Rank 0, Epoch 1473, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,164: INFO: model_training: Rank 0, Epoch 1473, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,166: INFO: model_training: Rank 0, Epoch 1473, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,167: INFO: model_training: Rank 0, Epoch 1473, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,169: INFO: model_training: Rank 0, Epoch 1473, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,171: INFO: model_training: Rank 0, Epoch 1474, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,172: INFO: model_training: Rank 0, Epoch 1474, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,174: INFO: model_training: Rank 0, Epoch 1474, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,175: INFO: model_training: Rank 0, Epoch 1474, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,177: INFO: model_training: Rank 0, Epoch 1474, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,178: INFO: model_training: Rank 0, Epoch 1475, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,180: INFO: model_training: Rank 0, Epoch 1475, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,181: INFO: model_training: Rank 0, Epoch 1475, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,183: INFO: model_training: Rank 0, Epoch 1475, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,184: INFO: model_training: Rank 0, Epoch 1475, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,185: INFO: model_training: Rank 0, Epoch 1476, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,187: INFO: model_training: Rank 0, Epoch 1476, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,188: INFO: model_training: Rank 0, Epoch 1476, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,190: INFO: model_training: Rank 0, Epoch 1476, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,191: INFO: model_training: Rank 0, Epoch 1476, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,193: INFO: model_training: Rank 0, Epoch 1477, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,194: INFO: model_training: Rank 0, Epoch 1477, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,195: INFO: model_training: Rank 0, Epoch 1477, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,197: INFO: model_training: Rank 0, Epoch 1477, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,198: INFO: model_training: Rank 0, Epoch 1477, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,200: INFO: model_training: Rank 0, Epoch 1478, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,201: INFO: model_training: Rank 0, Epoch 1478, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,203: INFO: model_training: Rank 0, Epoch 1478, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,204: INFO: model_training: Rank 0, Epoch 1478, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,205: INFO: model_training: Rank 0, Epoch 1478, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,207: INFO: model_training: Rank 0, Epoch 1479, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,208: INFO: model_training: Rank 0, Epoch 1479, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,210: INFO: model_training: Rank 0, Epoch 1479, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,211: INFO: model_training: Rank 0, Epoch 1479, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,213: INFO: model_training: Rank 0, Epoch 1479, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,214: INFO: model_training: Rank 0, Epoch 1480, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,215: INFO: model_training: Rank 0, Epoch 1480, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,217: INFO: model_training: Rank 0, Epoch 1480, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,218: INFO: model_training: Rank 0, Epoch 1480, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,220: INFO: model_training: Rank 0, Epoch 1480, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,221: INFO: model_training: Rank 0, Epoch 1481, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,222: INFO: model_training: Rank 0, Epoch 1481, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,224: INFO: model_training: Rank 0, Epoch 1481, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,226: INFO: model_training: Rank 0, Epoch 1481, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,227: INFO: model_training: Rank 0, Epoch 1481, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,229: INFO: model_training: Rank 0, Epoch 1482, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,230: INFO: model_training: Rank 0, Epoch 1482, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,232: INFO: model_training: Rank 0, Epoch 1482, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,233: INFO: model_training: Rank 0, Epoch 1482, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,235: INFO: model_training: Rank 0, Epoch 1482, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,236: INFO: model_training: Rank 0, Epoch 1483, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,238: INFO: model_training: Rank 0, Epoch 1483, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,239: INFO: model_training: Rank 0, Epoch 1483, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,241: INFO: model_training: Rank 0, Epoch 1483, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,242: INFO: model_training: Rank 0, Epoch 1483, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,244: INFO: model_training: Rank 0, Epoch 1484, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,245: INFO: model_training: Rank 0, Epoch 1484, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,246: INFO: model_training: Rank 0, Epoch 1484, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,247: INFO: model_training: Rank 0, Epoch 1484, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,249: INFO: model_training: Rank 0, Epoch 1484, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,251: INFO: model_training: Rank 0, Epoch 1485, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,252: INFO: model_training: Rank 0, Epoch 1485, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,253: INFO: model_training: Rank 0, Epoch 1485, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,254: INFO: model_training: Rank 0, Epoch 1485, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,257: INFO: model_training: Rank 0, Epoch 1485, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,258: INFO: model_training: Rank 0, Epoch 1486, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,259: INFO: model_training: Rank 0, Epoch 1486, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,260: INFO: model_training: Rank 0, Epoch 1486, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,262: INFO: model_training: Rank 0, Epoch 1486, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,263: INFO: model_training: Rank 0, Epoch 1486, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,265: INFO: model_training: Rank 0, Epoch 1487, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,266: INFO: model_training: Rank 0, Epoch 1487, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,268: INFO: model_training: Rank 0, Epoch 1487, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,270: INFO: model_training: Rank 0, Epoch 1487, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,271: INFO: model_training: Rank 0, Epoch 1487, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,273: INFO: model_training: Rank 0, Epoch 1488, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,274: INFO: model_training: Rank 0, Epoch 1488, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,276: INFO: model_training: Rank 0, Epoch 1488, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,277: INFO: model_training: Rank 0, Epoch 1488, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,279: INFO: model_training: Rank 0, Epoch 1488, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,280: INFO: model_training: Rank 0, Epoch 1489, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,282: INFO: model_training: Rank 0, Epoch 1489, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,283: INFO: model_training: Rank 0, Epoch 1489, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,285: INFO: model_training: Rank 0, Epoch 1489, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,286: INFO: model_training: Rank 0, Epoch 1489, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,287: INFO: model_training: Rank 0, Epoch 1490, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,288: INFO: model_training: Rank 0, Epoch 1490, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,291: INFO: model_training: Rank 0, Epoch 1490, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,292: INFO: model_training: Rank 0, Epoch 1490, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,293: INFO: model_training: Rank 0, Epoch 1490, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,294: INFO: model_training: Rank 0, Epoch 1491, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,295: INFO: model_training: Rank 0, Epoch 1491, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,296: INFO: model_training: Rank 0, Epoch 1491, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,298: INFO: model_training: Rank 0, Epoch 1491, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,299: INFO: model_training: Rank 0, Epoch 1491, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,300: INFO: model_training: Rank 0, Epoch 1492, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,302: INFO: model_training: Rank 0, Epoch 1492, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,304: INFO: model_training: Rank 0, Epoch 1492, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,305: INFO: model_training: Rank 0, Epoch 1492, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,306: INFO: model_training: Rank 0, Epoch 1492, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,308: INFO: model_training: Rank 0, Epoch 1493, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,309: INFO: model_training: Rank 0, Epoch 1493, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,310: INFO: model_training: Rank 0, Epoch 1493, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,311: INFO: model_training: Rank 0, Epoch 1493, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,312: INFO: model_training: Rank 0, Epoch 1493, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,314: INFO: model_training: Rank 0, Epoch 1494, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,316: INFO: model_training: Rank 0, Epoch 1494, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,317: INFO: model_training: Rank 0, Epoch 1494, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,319: INFO: model_training: Rank 0, Epoch 1494, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,320: INFO: model_training: Rank 0, Epoch 1494, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,321: INFO: model_training: Rank 0, Epoch 1495, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,323: INFO: model_training: Rank 0, Epoch 1495, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,324: INFO: model_training: Rank 0, Epoch 1495, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,325: INFO: model_training: Rank 0, Epoch 1495, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,327: INFO: model_training: Rank 0, Epoch 1495, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,328: INFO: model_training: Rank 0, Epoch 1496, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,329: INFO: model_training: Rank 0, Epoch 1496, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,330: INFO: model_training: Rank 0, Epoch 1496, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,332: INFO: model_training: Rank 0, Epoch 1496, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,334: INFO: model_training: Rank 0, Epoch 1496, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,335: INFO: model_training: Rank 0, Epoch 1497, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,336: INFO: model_training: Rank 0, Epoch 1497, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,338: INFO: model_training: Rank 0, Epoch 1497, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,339: INFO: model_training: Rank 0, Epoch 1497, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,341: INFO: model_training: Rank 0, Epoch 1497, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,342: INFO: model_training: Rank 0, Epoch 1498, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,343: INFO: model_training: Rank 0, Epoch 1498, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,344: INFO: model_training: Rank 0, Epoch 1498, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,346: INFO: model_training: Rank 0, Epoch 1498, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,347: INFO: model_training: Rank 0, Epoch 1498, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,349: INFO: model_training: Rank 0, Epoch 1499, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,350: INFO: model_training: Rank 0, Epoch 1499, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,351: INFO: model_training: Rank 0, Epoch 1499, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,352: INFO: model_training: Rank 0, Epoch 1499, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,354: INFO: model_training: Rank 0, Epoch 1499, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,355: INFO: model_training: Rank 0, Epoch 1500, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,356: INFO: model_training: Rank 0, Epoch 1500, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,358: INFO: model_training: Rank 0, Epoch 1500, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,359: INFO: model_training: Rank 0, Epoch 1500, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,360: INFO: model_training: Rank 0, Epoch 1500, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,361: INFO: model_training: Rank 0, Epoch 1501, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,363: INFO: model_training: Rank 0, Epoch 1501, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,364: INFO: model_training: Rank 0, Epoch 1501, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,366: INFO: model_training: Rank 0, Epoch 1501, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,367: INFO: model_training: Rank 0, Epoch 1501, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,368: INFO: model_training: Rank 0, Epoch 1502, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,370: INFO: model_training: Rank 0, Epoch 1502, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,371: INFO: model_training: Rank 0, Epoch 1502, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,373: INFO: model_training: Rank 0, Epoch 1502, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,374: INFO: model_training: Rank 0, Epoch 1502, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,375: INFO: model_training: Rank 0, Epoch 1503, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,377: INFO: model_training: Rank 0, Epoch 1503, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,378: INFO: model_training: Rank 0, Epoch 1503, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,379: INFO: model_training: Rank 0, Epoch 1503, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,381: INFO: model_training: Rank 0, Epoch 1503, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,382: INFO: model_training: Rank 0, Epoch 1504, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,383: INFO: model_training: Rank 0, Epoch 1504, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,385: INFO: model_training: Rank 0, Epoch 1504, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,386: INFO: model_training: Rank 0, Epoch 1504, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,387: INFO: model_training: Rank 0, Epoch 1504, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,389: INFO: model_training: Rank 0, Epoch 1505, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,390: INFO: model_training: Rank 0, Epoch 1505, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,391: INFO: model_training: Rank 0, Epoch 1505, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,392: INFO: model_training: Rank 0, Epoch 1505, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,393: INFO: model_training: Rank 0, Epoch 1505, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,395: INFO: model_training: Rank 0, Epoch 1506, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,396: INFO: model_training: Rank 0, Epoch 1506, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,398: INFO: model_training: Rank 0, Epoch 1506, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,399: INFO: model_training: Rank 0, Epoch 1506, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,400: INFO: model_training: Rank 0, Epoch 1506, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,402: INFO: model_training: Rank 0, Epoch 1507, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,403: INFO: model_training: Rank 0, Epoch 1507, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,404: INFO: model_training: Rank 0, Epoch 1507, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,405: INFO: model_training: Rank 0, Epoch 1507, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,406: INFO: model_training: Rank 0, Epoch 1507, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,407: INFO: model_training: Rank 0, Epoch 1508, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,409: INFO: model_training: Rank 0, Epoch 1508, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,411: INFO: model_training: Rank 0, Epoch 1508, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,412: INFO: model_training: Rank 0, Epoch 1508, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,414: INFO: model_training: Rank 0, Epoch 1508, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,415: INFO: model_training: Rank 0, Epoch 1509, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,416: INFO: model_training: Rank 0, Epoch 1509, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,418: INFO: model_training: Rank 0, Epoch 1509, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,420: INFO: model_training: Rank 0, Epoch 1509, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,422: INFO: model_training: Rank 0, Epoch 1509, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,423: INFO: model_training: Rank 0, Epoch 1510, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,424: INFO: model_training: Rank 0, Epoch 1510, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,425: INFO: model_training: Rank 0, Epoch 1510, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,427: INFO: model_training: Rank 0, Epoch 1510, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,428: INFO: model_training: Rank 0, Epoch 1510, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,429: INFO: model_training: Rank 0, Epoch 1511, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,431: INFO: model_training: Rank 0, Epoch 1511, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,432: INFO: model_training: Rank 0, Epoch 1511, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,433: INFO: model_training: Rank 0, Epoch 1511, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,434: INFO: model_training: Rank 0, Epoch 1511, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,436: INFO: model_training: Rank 0, Epoch 1512, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,437: INFO: model_training: Rank 0, Epoch 1512, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,438: INFO: model_training: Rank 0, Epoch 1512, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,439: INFO: model_training: Rank 0, Epoch 1512, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,441: INFO: model_training: Rank 0, Epoch 1512, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,442: INFO: model_training: Rank 0, Epoch 1513, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,443: INFO: model_training: Rank 0, Epoch 1513, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,444: INFO: model_training: Rank 0, Epoch 1513, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,445: INFO: model_training: Rank 0, Epoch 1513, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,446: INFO: model_training: Rank 0, Epoch 1513, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,447: INFO: model_training: Rank 0, Epoch 1514, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,449: INFO: model_training: Rank 0, Epoch 1514, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,450: INFO: model_training: Rank 0, Epoch 1514, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,451: INFO: model_training: Rank 0, Epoch 1514, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,452: INFO: model_training: Rank 0, Epoch 1514, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,454: INFO: model_training: Rank 0, Epoch 1515, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,455: INFO: model_training: Rank 0, Epoch 1515, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,457: INFO: model_training: Rank 0, Epoch 1515, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,458: INFO: model_training: Rank 0, Epoch 1515, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,459: INFO: model_training: Rank 0, Epoch 1515, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,460: INFO: model_training: Rank 0, Epoch 1516, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,463: INFO: model_training: Rank 0, Epoch 1516, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,464: INFO: model_training: Rank 0, Epoch 1516, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,465: INFO: model_training: Rank 0, Epoch 1516, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,466: INFO: model_training: Rank 0, Epoch 1516, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,467: INFO: model_training: Rank 0, Epoch 1517, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,468: INFO: model_training: Rank 0, Epoch 1517, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,469: INFO: model_training: Rank 0, Epoch 1517, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,470: INFO: model_training: Rank 0, Epoch 1517, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,471: INFO: model_training: Rank 0, Epoch 1517, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,472: INFO: model_training: Rank 0, Epoch 1518, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,473: INFO: model_training: Rank 0, Epoch 1518, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,474: INFO: model_training: Rank 0, Epoch 1518, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,476: INFO: model_training: Rank 0, Epoch 1518, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,477: INFO: model_training: Rank 0, Epoch 1518, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,478: INFO: model_training: Rank 0, Epoch 1519, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,479: INFO: model_training: Rank 0, Epoch 1519, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,481: INFO: model_training: Rank 0, Epoch 1519, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,485: INFO: model_training: Rank 0, Epoch 1519, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,487: INFO: model_training: Rank 0, Epoch 1519, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,490: INFO: model_training: Rank 0, Epoch 1520, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,494: INFO: model_training: Rank 0, Epoch 1520, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,495: INFO: model_training: Rank 0, Epoch 1520, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,497: INFO: model_training: Rank 0, Epoch 1520, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,499: INFO: model_training: Rank 0, Epoch 1520, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,501: INFO: model_training: Rank 0, Epoch 1521, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,502: INFO: model_training: Rank 0, Epoch 1521, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,503: INFO: model_training: Rank 0, Epoch 1521, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,505: INFO: model_training: Rank 0, Epoch 1521, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,507: INFO: model_training: Rank 0, Epoch 1521, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,509: INFO: model_training: Rank 0, Epoch 1522, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,510: INFO: model_training: Rank 0, Epoch 1522, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,512: INFO: model_training: Rank 0, Epoch 1522, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,514: INFO: model_training: Rank 0, Epoch 1522, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,516: INFO: model_training: Rank 0, Epoch 1522, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,518: INFO: model_training: Rank 0, Epoch 1523, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,519: INFO: model_training: Rank 0, Epoch 1523, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,520: INFO: model_training: Rank 0, Epoch 1523, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,522: INFO: model_training: Rank 0, Epoch 1523, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,524: INFO: model_training: Rank 0, Epoch 1523, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,525: INFO: model_training: Rank 0, Epoch 1524, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,526: INFO: model_training: Rank 0, Epoch 1524, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,527: INFO: model_training: Rank 0, Epoch 1524, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,529: INFO: model_training: Rank 0, Epoch 1524, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,530: INFO: model_training: Rank 0, Epoch 1524, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,531: INFO: model_training: Rank 0, Epoch 1525, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,532: INFO: model_training: Rank 0, Epoch 1525, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,533: INFO: model_training: Rank 0, Epoch 1525, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,534: INFO: model_training: Rank 0, Epoch 1525, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,535: INFO: model_training: Rank 0, Epoch 1525, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,536: INFO: model_training: Rank 0, Epoch 1526, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,537: INFO: model_training: Rank 0, Epoch 1526, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,539: INFO: model_training: Rank 0, Epoch 1526, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,540: INFO: model_training: Rank 0, Epoch 1526, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,541: INFO: model_training: Rank 0, Epoch 1526, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,542: INFO: model_training: Rank 0, Epoch 1527, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,543: INFO: model_training: Rank 0, Epoch 1527, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,544: INFO: model_training: Rank 0, Epoch 1527, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,545: INFO: model_training: Rank 0, Epoch 1527, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,546: INFO: model_training: Rank 0, Epoch 1527, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,547: INFO: model_training: Rank 0, Epoch 1528, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,548: INFO: model_training: Rank 0, Epoch 1528, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,549: INFO: model_training: Rank 0, Epoch 1528, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,550: INFO: model_training: Rank 0, Epoch 1528, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,551: INFO: model_training: Rank 0, Epoch 1528, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,552: INFO: model_training: Rank 0, Epoch 1529, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,553: INFO: model_training: Rank 0, Epoch 1529, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,554: INFO: model_training: Rank 0, Epoch 1529, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,555: INFO: model_training: Rank 0, Epoch 1529, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,556: INFO: model_training: Rank 0, Epoch 1529, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,558: INFO: model_training: Rank 0, Epoch 1530, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,559: INFO: model_training: Rank 0, Epoch 1530, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,560: INFO: model_training: Rank 0, Epoch 1530, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,561: INFO: model_training: Rank 0, Epoch 1530, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,562: INFO: model_training: Rank 0, Epoch 1530, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,563: INFO: model_training: Rank 0, Epoch 1531, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,564: INFO: model_training: Rank 0, Epoch 1531, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,566: INFO: model_training: Rank 0, Epoch 1531, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,567: INFO: model_training: Rank 0, Epoch 1531, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,568: INFO: model_training: Rank 0, Epoch 1531, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,569: INFO: model_training: Rank 0, Epoch 1532, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,570: INFO: model_training: Rank 0, Epoch 1532, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,571: INFO: model_training: Rank 0, Epoch 1532, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,572: INFO: model_training: Rank 0, Epoch 1532, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,573: INFO: model_training: Rank 0, Epoch 1532, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,574: INFO: model_training: Rank 0, Epoch 1533, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,576: INFO: model_training: Rank 0, Epoch 1533, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,577: INFO: model_training: Rank 0, Epoch 1533, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,578: INFO: model_training: Rank 0, Epoch 1533, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,579: INFO: model_training: Rank 0, Epoch 1533, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,580: INFO: model_training: Rank 0, Epoch 1534, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,581: INFO: model_training: Rank 0, Epoch 1534, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,583: INFO: model_training: Rank 0, Epoch 1534, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,584: INFO: model_training: Rank 0, Epoch 1534, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,585: INFO: model_training: Rank 0, Epoch 1534, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,586: INFO: model_training: Rank 0, Epoch 1535, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,587: INFO: model_training: Rank 0, Epoch 1535, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,588: INFO: model_training: Rank 0, Epoch 1535, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,589: INFO: model_training: Rank 0, Epoch 1535, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,591: INFO: model_training: Rank 0, Epoch 1535, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,592: INFO: model_training: Rank 0, Epoch 1536, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,593: INFO: model_training: Rank 0, Epoch 1536, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,595: INFO: model_training: Rank 0, Epoch 1536, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,596: INFO: model_training: Rank 0, Epoch 1536, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,597: INFO: model_training: Rank 0, Epoch 1536, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,598: INFO: model_training: Rank 0, Epoch 1537, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,599: INFO: model_training: Rank 0, Epoch 1537, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,600: INFO: model_training: Rank 0, Epoch 1537, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,601: INFO: model_training: Rank 0, Epoch 1537, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,602: INFO: model_training: Rank 0, Epoch 1537, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,603: INFO: model_training: Rank 0, Epoch 1538, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,604: INFO: model_training: Rank 0, Epoch 1538, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,606: INFO: model_training: Rank 0, Epoch 1538, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,607: INFO: model_training: Rank 0, Epoch 1538, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,608: INFO: model_training: Rank 0, Epoch 1538, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,609: INFO: model_training: Rank 0, Epoch 1539, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,610: INFO: model_training: Rank 0, Epoch 1539, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,611: INFO: model_training: Rank 0, Epoch 1539, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,612: INFO: model_training: Rank 0, Epoch 1539, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,613: INFO: model_training: Rank 0, Epoch 1539, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,614: INFO: model_training: Rank 0, Epoch 1540, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,615: INFO: model_training: Rank 0, Epoch 1540, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,616: INFO: model_training: Rank 0, Epoch 1540, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,618: INFO: model_training: Rank 0, Epoch 1540, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,619: INFO: model_training: Rank 0, Epoch 1540, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,621: INFO: model_training: Rank 0, Epoch 1541, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,622: INFO: model_training: Rank 0, Epoch 1541, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,623: INFO: model_training: Rank 0, Epoch 1541, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,624: INFO: model_training: Rank 0, Epoch 1541, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,625: INFO: model_training: Rank 0, Epoch 1541, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,626: INFO: model_training: Rank 0, Epoch 1542, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,627: INFO: model_training: Rank 0, Epoch 1542, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,628: INFO: model_training: Rank 0, Epoch 1542, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,629: INFO: model_training: Rank 0, Epoch 1542, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,631: INFO: model_training: Rank 0, Epoch 1542, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,631: INFO: model_training: Rank 0, Epoch 1543, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,632: INFO: model_training: Rank 0, Epoch 1543, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,634: INFO: model_training: Rank 0, Epoch 1543, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,635: INFO: model_training: Rank 0, Epoch 1543, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,636: INFO: model_training: Rank 0, Epoch 1543, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,637: INFO: model_training: Rank 0, Epoch 1544, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,638: INFO: model_training: Rank 0, Epoch 1544, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,639: INFO: model_training: Rank 0, Epoch 1544, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,640: INFO: model_training: Rank 0, Epoch 1544, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,641: INFO: model_training: Rank 0, Epoch 1544, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,642: INFO: model_training: Rank 0, Epoch 1545, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,643: INFO: model_training: Rank 0, Epoch 1545, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,644: INFO: model_training: Rank 0, Epoch 1545, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,645: INFO: model_training: Rank 0, Epoch 1545, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,646: INFO: model_training: Rank 0, Epoch 1545, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,647: INFO: model_training: Rank 0, Epoch 1546, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,648: INFO: model_training: Rank 0, Epoch 1546, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,649: INFO: model_training: Rank 0, Epoch 1546, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,650: INFO: model_training: Rank 0, Epoch 1546, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,651: INFO: model_training: Rank 0, Epoch 1546, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,652: INFO: model_training: Rank 0, Epoch 1547, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,653: INFO: model_training: Rank 0, Epoch 1547, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,654: INFO: model_training: Rank 0, Epoch 1547, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,655: INFO: model_training: Rank 0, Epoch 1547, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,656: INFO: model_training: Rank 0, Epoch 1547, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,657: INFO: model_training: Rank 0, Epoch 1548, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,658: INFO: model_training: Rank 0, Epoch 1548, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,659: INFO: model_training: Rank 0, Epoch 1548, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,660: INFO: model_training: Rank 0, Epoch 1548, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,661: INFO: model_training: Rank 0, Epoch 1548, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,662: INFO: model_training: Rank 0, Epoch 1549, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,663: INFO: model_training: Rank 0, Epoch 1549, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,664: INFO: model_training: Rank 0, Epoch 1549, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,665: INFO: model_training: Rank 0, Epoch 1549, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,666: INFO: model_training: Rank 0, Epoch 1549, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,667: INFO: model_training: Rank 0, Epoch 1550, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,668: INFO: model_training: Rank 0, Epoch 1550, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,669: INFO: model_training: Rank 0, Epoch 1550, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,670: INFO: model_training: Rank 0, Epoch 1550, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,671: INFO: model_training: Rank 0, Epoch 1550, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,672: INFO: model_training: Rank 0, Epoch 1551, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,673: INFO: model_training: Rank 0, Epoch 1551, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,674: INFO: model_training: Rank 0, Epoch 1551, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,675: INFO: model_training: Rank 0, Epoch 1551, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,676: INFO: model_training: Rank 0, Epoch 1551, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,677: INFO: model_training: Rank 0, Epoch 1552, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,679: INFO: model_training: Rank 0, Epoch 1552, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,680: INFO: model_training: Rank 0, Epoch 1552, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,681: INFO: model_training: Rank 0, Epoch 1552, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,683: INFO: model_training: Rank 0, Epoch 1552, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,684: INFO: model_training: Rank 0, Epoch 1553, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,685: INFO: model_training: Rank 0, Epoch 1553, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,686: INFO: model_training: Rank 0, Epoch 1553, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,687: INFO: model_training: Rank 0, Epoch 1553, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,688: INFO: model_training: Rank 0, Epoch 1553, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,690: INFO: model_training: Rank 0, Epoch 1554, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,691: INFO: model_training: Rank 0, Epoch 1554, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,692: INFO: model_training: Rank 0, Epoch 1554, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,693: INFO: model_training: Rank 0, Epoch 1554, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,694: INFO: model_training: Rank 0, Epoch 1554, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,694: INFO: model_training: Rank 0, Epoch 1555, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,695: INFO: model_training: Rank 0, Epoch 1555, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,696: INFO: model_training: Rank 0, Epoch 1555, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,698: INFO: model_training: Rank 0, Epoch 1555, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,699: INFO: model_training: Rank 0, Epoch 1555, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,700: INFO: model_training: Rank 0, Epoch 1556, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,701: INFO: model_training: Rank 0, Epoch 1556, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,701: INFO: model_training: Rank 0, Epoch 1556, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,702: INFO: model_training: Rank 0, Epoch 1556, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,703: INFO: model_training: Rank 0, Epoch 1556, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,705: INFO: model_training: Rank 0, Epoch 1557, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,706: INFO: model_training: Rank 0, Epoch 1557, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,707: INFO: model_training: Rank 0, Epoch 1557, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,708: INFO: model_training: Rank 0, Epoch 1557, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,709: INFO: model_training: Rank 0, Epoch 1557, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,710: INFO: model_training: Rank 0, Epoch 1558, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,711: INFO: model_training: Rank 0, Epoch 1558, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,712: INFO: model_training: Rank 0, Epoch 1558, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,713: INFO: model_training: Rank 0, Epoch 1558, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,714: INFO: model_training: Rank 0, Epoch 1558, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,715: INFO: model_training: Rank 0, Epoch 1559, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,716: INFO: model_training: Rank 0, Epoch 1559, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,716: INFO: model_training: Rank 0, Epoch 1559, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,718: INFO: model_training: Rank 0, Epoch 1559, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,719: INFO: model_training: Rank 0, Epoch 1559, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,720: INFO: model_training: Rank 0, Epoch 1560, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,721: INFO: model_training: Rank 0, Epoch 1560, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,722: INFO: model_training: Rank 0, Epoch 1560, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,723: INFO: model_training: Rank 0, Epoch 1560, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,724: INFO: model_training: Rank 0, Epoch 1560, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,725: INFO: model_training: Rank 0, Epoch 1561, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,726: INFO: model_training: Rank 0, Epoch 1561, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,727: INFO: model_training: Rank 0, Epoch 1561, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,728: INFO: model_training: Rank 0, Epoch 1561, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,729: INFO: model_training: Rank 0, Epoch 1561, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,731: INFO: model_training: Rank 0, Epoch 1562, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,732: INFO: model_training: Rank 0, Epoch 1562, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,733: INFO: model_training: Rank 0, Epoch 1562, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,734: INFO: model_training: Rank 0, Epoch 1562, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,735: INFO: model_training: Rank 0, Epoch 1562, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,736: INFO: model_training: Rank 0, Epoch 1563, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,738: INFO: model_training: Rank 0, Epoch 1563, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,739: INFO: model_training: Rank 0, Epoch 1563, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,740: INFO: model_training: Rank 0, Epoch 1563, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,741: INFO: model_training: Rank 0, Epoch 1563, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,742: INFO: model_training: Rank 0, Epoch 1564, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,743: INFO: model_training: Rank 0, Epoch 1564, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,744: INFO: model_training: Rank 0, Epoch 1564, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,745: INFO: model_training: Rank 0, Epoch 1564, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,746: INFO: model_training: Rank 0, Epoch 1564, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,747: INFO: model_training: Rank 0, Epoch 1565, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,749: INFO: model_training: Rank 0, Epoch 1565, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,751: INFO: model_training: Rank 0, Epoch 1565, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,752: INFO: model_training: Rank 0, Epoch 1565, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,754: INFO: model_training: Rank 0, Epoch 1565, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,755: INFO: model_training: Rank 0, Epoch 1566, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,756: INFO: model_training: Rank 0, Epoch 1566, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,757: INFO: model_training: Rank 0, Epoch 1566, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,759: INFO: model_training: Rank 0, Epoch 1566, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,760: INFO: model_training: Rank 0, Epoch 1566, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,760: INFO: model_training: Rank 0, Epoch 1567, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,761: INFO: model_training: Rank 0, Epoch 1567, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,762: INFO: model_training: Rank 0, Epoch 1567, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,764: INFO: model_training: Rank 0, Epoch 1567, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,765: INFO: model_training: Rank 0, Epoch 1567, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,766: INFO: model_training: Rank 0, Epoch 1568, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,768: INFO: model_training: Rank 0, Epoch 1568, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,769: INFO: model_training: Rank 0, Epoch 1568, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,770: INFO: model_training: Rank 0, Epoch 1568, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,771: INFO: model_training: Rank 0, Epoch 1568, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,772: INFO: model_training: Rank 0, Epoch 1569, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,773: INFO: model_training: Rank 0, Epoch 1569, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,774: INFO: model_training: Rank 0, Epoch 1569, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,775: INFO: model_training: Rank 0, Epoch 1569, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,777: INFO: model_training: Rank 0, Epoch 1569, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,778: INFO: model_training: Rank 0, Epoch 1570, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,779: INFO: model_training: Rank 0, Epoch 1570, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,780: INFO: model_training: Rank 0, Epoch 1570, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,782: INFO: model_training: Rank 0, Epoch 1570, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,783: INFO: model_training: Rank 0, Epoch 1570, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,784: INFO: model_training: Rank 0, Epoch 1571, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,785: INFO: model_training: Rank 0, Epoch 1571, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,786: INFO: model_training: Rank 0, Epoch 1571, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,787: INFO: model_training: Rank 0, Epoch 1571, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,788: INFO: model_training: Rank 0, Epoch 1571, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,790: INFO: model_training: Rank 0, Epoch 1572, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,791: INFO: model_training: Rank 0, Epoch 1572, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,792: INFO: model_training: Rank 0, Epoch 1572, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,793: INFO: model_training: Rank 0, Epoch 1572, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,794: INFO: model_training: Rank 0, Epoch 1572, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,795: INFO: model_training: Rank 0, Epoch 1573, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,796: INFO: model_training: Rank 0, Epoch 1573, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,797: INFO: model_training: Rank 0, Epoch 1573, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,799: INFO: model_training: Rank 0, Epoch 1573, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,800: INFO: model_training: Rank 0, Epoch 1573, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,801: INFO: model_training: Rank 0, Epoch 1574, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,802: INFO: model_training: Rank 0, Epoch 1574, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,803: INFO: model_training: Rank 0, Epoch 1574, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,804: INFO: model_training: Rank 0, Epoch 1574, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,806: INFO: model_training: Rank 0, Epoch 1574, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,807: INFO: model_training: Rank 0, Epoch 1575, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,808: INFO: model_training: Rank 0, Epoch 1575, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,809: INFO: model_training: Rank 0, Epoch 1575, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,810: INFO: model_training: Rank 0, Epoch 1575, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,811: INFO: model_training: Rank 0, Epoch 1575, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,812: INFO: model_training: Rank 0, Epoch 1576, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,813: INFO: model_training: Rank 0, Epoch 1576, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,815: INFO: model_training: Rank 0, Epoch 1576, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,816: INFO: model_training: Rank 0, Epoch 1576, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,817: INFO: model_training: Rank 0, Epoch 1576, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,818: INFO: model_training: Rank 0, Epoch 1577, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,819: INFO: model_training: Rank 0, Epoch 1577, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,820: INFO: model_training: Rank 0, Epoch 1577, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,821: INFO: model_training: Rank 0, Epoch 1577, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,822: INFO: model_training: Rank 0, Epoch 1577, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,823: INFO: model_training: Rank 0, Epoch 1578, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,824: INFO: model_training: Rank 0, Epoch 1578, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,825: INFO: model_training: Rank 0, Epoch 1578, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,827: INFO: model_training: Rank 0, Epoch 1578, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,828: INFO: model_training: Rank 0, Epoch 1578, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,829: INFO: model_training: Rank 0, Epoch 1579, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,830: INFO: model_training: Rank 0, Epoch 1579, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,831: INFO: model_training: Rank 0, Epoch 1579, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,833: INFO: model_training: Rank 0, Epoch 1579, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,834: INFO: model_training: Rank 0, Epoch 1579, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,835: INFO: model_training: Rank 0, Epoch 1580, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,836: INFO: model_training: Rank 0, Epoch 1580, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,838: INFO: model_training: Rank 0, Epoch 1580, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,839: INFO: model_training: Rank 0, Epoch 1580, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,840: INFO: model_training: Rank 0, Epoch 1580, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,842: INFO: model_training: Rank 0, Epoch 1581, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,842: INFO: model_training: Rank 0, Epoch 1581, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,843: INFO: model_training: Rank 0, Epoch 1581, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,845: INFO: model_training: Rank 0, Epoch 1581, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,846: INFO: model_training: Rank 0, Epoch 1581, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,847: INFO: model_training: Rank 0, Epoch 1582, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,848: INFO: model_training: Rank 0, Epoch 1582, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,849: INFO: model_training: Rank 0, Epoch 1582, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,850: INFO: model_training: Rank 0, Epoch 1582, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,851: INFO: model_training: Rank 0, Epoch 1582, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,852: INFO: model_training: Rank 0, Epoch 1583, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,853: INFO: model_training: Rank 0, Epoch 1583, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,854: INFO: model_training: Rank 0, Epoch 1583, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,856: INFO: model_training: Rank 0, Epoch 1583, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,857: INFO: model_training: Rank 0, Epoch 1583, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,858: INFO: model_training: Rank 0, Epoch 1584, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,859: INFO: model_training: Rank 0, Epoch 1584, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,860: INFO: model_training: Rank 0, Epoch 1584, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,862: INFO: model_training: Rank 0, Epoch 1584, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,862: INFO: model_training: Rank 0, Epoch 1584, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,864: INFO: model_training: Rank 0, Epoch 1585, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,865: INFO: model_training: Rank 0, Epoch 1585, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,866: INFO: model_training: Rank 0, Epoch 1585, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,867: INFO: model_training: Rank 0, Epoch 1585, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,868: INFO: model_training: Rank 0, Epoch 1585, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,870: INFO: model_training: Rank 0, Epoch 1586, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,871: INFO: model_training: Rank 0, Epoch 1586, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,873: INFO: model_training: Rank 0, Epoch 1586, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,874: INFO: model_training: Rank 0, Epoch 1586, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,875: INFO: model_training: Rank 0, Epoch 1586, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,876: INFO: model_training: Rank 0, Epoch 1587, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,877: INFO: model_training: Rank 0, Epoch 1587, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,878: INFO: model_training: Rank 0, Epoch 1587, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,879: INFO: model_training: Rank 0, Epoch 1587, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,880: INFO: model_training: Rank 0, Epoch 1587, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,881: INFO: model_training: Rank 0, Epoch 1588, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,882: INFO: model_training: Rank 0, Epoch 1588, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,883: INFO: model_training: Rank 0, Epoch 1588, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,885: INFO: model_training: Rank 0, Epoch 1588, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,886: INFO: model_training: Rank 0, Epoch 1588, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,886: INFO: model_training: Rank 0, Epoch 1589, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,887: INFO: model_training: Rank 0, Epoch 1589, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,889: INFO: model_training: Rank 0, Epoch 1589, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,890: INFO: model_training: Rank 0, Epoch 1589, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,891: INFO: model_training: Rank 0, Epoch 1589, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,892: INFO: model_training: Rank 0, Epoch 1590, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,893: INFO: model_training: Rank 0, Epoch 1590, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,894: INFO: model_training: Rank 0, Epoch 1590, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,895: INFO: model_training: Rank 0, Epoch 1590, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,896: INFO: model_training: Rank 0, Epoch 1590, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,897: INFO: model_training: Rank 0, Epoch 1591, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,899: INFO: model_training: Rank 0, Epoch 1591, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,900: INFO: model_training: Rank 0, Epoch 1591, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,901: INFO: model_training: Rank 0, Epoch 1591, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,902: INFO: model_training: Rank 0, Epoch 1591, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,903: INFO: model_training: Rank 0, Epoch 1592, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,904: INFO: model_training: Rank 0, Epoch 1592, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,905: INFO: model_training: Rank 0, Epoch 1592, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,907: INFO: model_training: Rank 0, Epoch 1592, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,908: INFO: model_training: Rank 0, Epoch 1592, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,909: INFO: model_training: Rank 0, Epoch 1593, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,910: INFO: model_training: Rank 0, Epoch 1593, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,911: INFO: model_training: Rank 0, Epoch 1593, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,913: INFO: model_training: Rank 0, Epoch 1593, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,914: INFO: model_training: Rank 0, Epoch 1593, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,916: INFO: model_training: Rank 0, Epoch 1594, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,917: INFO: model_training: Rank 0, Epoch 1594, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,918: INFO: model_training: Rank 0, Epoch 1594, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,919: INFO: model_training: Rank 0, Epoch 1594, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,920: INFO: model_training: Rank 0, Epoch 1594, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,922: INFO: model_training: Rank 0, Epoch 1595, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,923: INFO: model_training: Rank 0, Epoch 1595, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,924: INFO: model_training: Rank 0, Epoch 1595, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,925: INFO: model_training: Rank 0, Epoch 1595, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,926: INFO: model_training: Rank 0, Epoch 1595, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,928: INFO: model_training: Rank 0, Epoch 1596, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,929: INFO: model_training: Rank 0, Epoch 1596, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,930: INFO: model_training: Rank 0, Epoch 1596, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,931: INFO: model_training: Rank 0, Epoch 1596, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,932: INFO: model_training: Rank 0, Epoch 1596, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,934: INFO: model_training: Rank 0, Epoch 1597, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,935: INFO: model_training: Rank 0, Epoch 1597, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,936: INFO: model_training: Rank 0, Epoch 1597, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,937: INFO: model_training: Rank 0, Epoch 1597, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,939: INFO: model_training: Rank 0, Epoch 1597, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,940: INFO: model_training: Rank 0, Epoch 1598, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,942: INFO: model_training: Rank 0, Epoch 1598, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,943: INFO: model_training: Rank 0, Epoch 1598, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,944: INFO: model_training: Rank 0, Epoch 1598, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,946: INFO: model_training: Rank 0, Epoch 1598, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,947: INFO: model_training: Rank 0, Epoch 1599, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,949: INFO: model_training: Rank 0, Epoch 1599, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,950: INFO: model_training: Rank 0, Epoch 1599, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,951: INFO: model_training: Rank 0, Epoch 1599, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,953: INFO: model_training: Rank 0, Epoch 1599, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,954: INFO: model_training: Rank 0, Epoch 1600, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,956: INFO: model_training: Rank 0, Epoch 1600, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,959: INFO: model_training: Rank 0, Epoch 1600, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,962: INFO: model_training: Rank 0, Epoch 1600, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,964: INFO: model_training: Rank 0, Epoch 1600, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,966: INFO: model_training: Rank 0, Epoch 1601, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,969: INFO: model_training: Rank 0, Epoch 1601, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,970: INFO: model_training: Rank 0, Epoch 1601, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,972: INFO: model_training: Rank 0, Epoch 1601, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,974: INFO: model_training: Rank 0, Epoch 1601, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,975: INFO: model_training: Rank 0, Epoch 1602, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,977: INFO: model_training: Rank 0, Epoch 1602, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,978: INFO: model_training: Rank 0, Epoch 1602, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,980: INFO: model_training: Rank 0, Epoch 1602, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,982: INFO: model_training: Rank 0, Epoch 1602, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,983: INFO: model_training: Rank 0, Epoch 1603, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,984: INFO: model_training: Rank 0, Epoch 1603, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,986: INFO: model_training: Rank 0, Epoch 1603, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,987: INFO: model_training: Rank 0, Epoch 1603, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,988: INFO: model_training: Rank 0, Epoch 1603, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,989: INFO: model_training: Rank 0, Epoch 1604, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,991: INFO: model_training: Rank 0, Epoch 1604, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,992: INFO: model_training: Rank 0, Epoch 1604, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,993: INFO: model_training: Rank 0, Epoch 1604, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:35,994: INFO: model_training: Rank 0, Epoch 1604, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:35,995: INFO: model_training: Rank 0, Epoch 1605, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:35,997: INFO: model_training: Rank 0, Epoch 1605, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:35,998: INFO: model_training: Rank 0, Epoch 1605, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:35,999: INFO: model_training: Rank 0, Epoch 1605, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,001: INFO: model_training: Rank 0, Epoch 1605, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,002: INFO: model_training: Rank 0, Epoch 1606, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,003: INFO: model_training: Rank 0, Epoch 1606, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,005: INFO: model_training: Rank 0, Epoch 1606, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,007: INFO: model_training: Rank 0, Epoch 1606, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,008: INFO: model_training: Rank 0, Epoch 1606, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,009: INFO: model_training: Rank 0, Epoch 1607, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,011: INFO: model_training: Rank 0, Epoch 1607, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,012: INFO: model_training: Rank 0, Epoch 1607, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,013: INFO: model_training: Rank 0, Epoch 1607, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,014: INFO: model_training: Rank 0, Epoch 1607, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,015: INFO: model_training: Rank 0, Epoch 1608, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,017: INFO: model_training: Rank 0, Epoch 1608, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,018: INFO: model_training: Rank 0, Epoch 1608, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,019: INFO: model_training: Rank 0, Epoch 1608, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,020: INFO: model_training: Rank 0, Epoch 1608, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,021: INFO: model_training: Rank 0, Epoch 1609, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,023: INFO: model_training: Rank 0, Epoch 1609, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,024: INFO: model_training: Rank 0, Epoch 1609, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,025: INFO: model_training: Rank 0, Epoch 1609, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,027: INFO: model_training: Rank 0, Epoch 1609, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,028: INFO: model_training: Rank 0, Epoch 1610, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,029: INFO: model_training: Rank 0, Epoch 1610, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,030: INFO: model_training: Rank 0, Epoch 1610, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,032: INFO: model_training: Rank 0, Epoch 1610, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,033: INFO: model_training: Rank 0, Epoch 1610, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,034: INFO: model_training: Rank 0, Epoch 1611, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,035: INFO: model_training: Rank 0, Epoch 1611, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,036: INFO: model_training: Rank 0, Epoch 1611, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,038: INFO: model_training: Rank 0, Epoch 1611, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,039: INFO: model_training: Rank 0, Epoch 1611, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,041: INFO: model_training: Rank 0, Epoch 1612, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,042: INFO: model_training: Rank 0, Epoch 1612, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,043: INFO: model_training: Rank 0, Epoch 1612, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,045: INFO: model_training: Rank 0, Epoch 1612, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,046: INFO: model_training: Rank 0, Epoch 1612, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,047: INFO: model_training: Rank 0, Epoch 1613, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,048: INFO: model_training: Rank 0, Epoch 1613, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,049: INFO: model_training: Rank 0, Epoch 1613, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,050: INFO: model_training: Rank 0, Epoch 1613, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,052: INFO: model_training: Rank 0, Epoch 1613, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,053: INFO: model_training: Rank 0, Epoch 1614, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,054: INFO: model_training: Rank 0, Epoch 1614, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,055: INFO: model_training: Rank 0, Epoch 1614, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,057: INFO: model_training: Rank 0, Epoch 1614, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,058: INFO: model_training: Rank 0, Epoch 1614, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,060: INFO: model_training: Rank 0, Epoch 1615, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,061: INFO: model_training: Rank 0, Epoch 1615, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,062: INFO: model_training: Rank 0, Epoch 1615, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,063: INFO: model_training: Rank 0, Epoch 1615, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,065: INFO: model_training: Rank 0, Epoch 1615, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,067: INFO: model_training: Rank 0, Epoch 1616, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,069: INFO: model_training: Rank 0, Epoch 1616, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,071: INFO: model_training: Rank 0, Epoch 1616, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,077: INFO: model_training: Rank 0, Epoch 1616, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,081: INFO: model_training: Rank 0, Epoch 1616, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,083: INFO: model_training: Rank 0, Epoch 1617, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,085: INFO: model_training: Rank 0, Epoch 1617, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,087: INFO: model_training: Rank 0, Epoch 1617, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,089: INFO: model_training: Rank 0, Epoch 1617, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,091: INFO: model_training: Rank 0, Epoch 1617, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,092: INFO: model_training: Rank 0, Epoch 1618, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,093: INFO: model_training: Rank 0, Epoch 1618, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,094: INFO: model_training: Rank 0, Epoch 1618, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,096: INFO: model_training: Rank 0, Epoch 1618, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,097: INFO: model_training: Rank 0, Epoch 1618, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,098: INFO: model_training: Rank 0, Epoch 1619, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,100: INFO: model_training: Rank 0, Epoch 1619, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,101: INFO: model_training: Rank 0, Epoch 1619, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,102: INFO: model_training: Rank 0, Epoch 1619, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,103: INFO: model_training: Rank 0, Epoch 1619, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,105: INFO: model_training: Rank 0, Epoch 1620, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,106: INFO: model_training: Rank 0, Epoch 1620, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,107: INFO: model_training: Rank 0, Epoch 1620, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,108: INFO: model_training: Rank 0, Epoch 1620, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,109: INFO: model_training: Rank 0, Epoch 1620, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,111: INFO: model_training: Rank 0, Epoch 1621, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,112: INFO: model_training: Rank 0, Epoch 1621, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,113: INFO: model_training: Rank 0, Epoch 1621, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,115: INFO: model_training: Rank 0, Epoch 1621, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,116: INFO: model_training: Rank 0, Epoch 1621, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,118: INFO: model_training: Rank 0, Epoch 1622, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,119: INFO: model_training: Rank 0, Epoch 1622, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,120: INFO: model_training: Rank 0, Epoch 1622, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,122: INFO: model_training: Rank 0, Epoch 1622, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,123: INFO: model_training: Rank 0, Epoch 1622, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,125: INFO: model_training: Rank 0, Epoch 1623, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,126: INFO: model_training: Rank 0, Epoch 1623, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,127: INFO: model_training: Rank 0, Epoch 1623, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,129: INFO: model_training: Rank 0, Epoch 1623, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,130: INFO: model_training: Rank 0, Epoch 1623, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,132: INFO: model_training: Rank 0, Epoch 1624, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,133: INFO: model_training: Rank 0, Epoch 1624, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,134: INFO: model_training: Rank 0, Epoch 1624, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,136: INFO: model_training: Rank 0, Epoch 1624, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,137: INFO: model_training: Rank 0, Epoch 1624, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,138: INFO: model_training: Rank 0, Epoch 1625, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,139: INFO: model_training: Rank 0, Epoch 1625, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,141: INFO: model_training: Rank 0, Epoch 1625, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,142: INFO: model_training: Rank 0, Epoch 1625, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,143: INFO: model_training: Rank 0, Epoch 1625, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,144: INFO: model_training: Rank 0, Epoch 1626, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,145: INFO: model_training: Rank 0, Epoch 1626, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,147: INFO: model_training: Rank 0, Epoch 1626, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,148: INFO: model_training: Rank 0, Epoch 1626, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,150: INFO: model_training: Rank 0, Epoch 1626, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,151: INFO: model_training: Rank 0, Epoch 1627, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,152: INFO: model_training: Rank 0, Epoch 1627, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,153: INFO: model_training: Rank 0, Epoch 1627, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,155: INFO: model_training: Rank 0, Epoch 1627, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,156: INFO: model_training: Rank 0, Epoch 1627, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,157: INFO: model_training: Rank 0, Epoch 1628, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,159: INFO: model_training: Rank 0, Epoch 1628, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,160: INFO: model_training: Rank 0, Epoch 1628, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,161: INFO: model_training: Rank 0, Epoch 1628, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,162: INFO: model_training: Rank 0, Epoch 1628, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,164: INFO: model_training: Rank 0, Epoch 1629, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,165: INFO: model_training: Rank 0, Epoch 1629, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,166: INFO: model_training: Rank 0, Epoch 1629, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,167: INFO: model_training: Rank 0, Epoch 1629, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,169: INFO: model_training: Rank 0, Epoch 1629, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,170: INFO: model_training: Rank 0, Epoch 1630, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,171: INFO: model_training: Rank 0, Epoch 1630, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,172: INFO: model_training: Rank 0, Epoch 1630, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,173: INFO: model_training: Rank 0, Epoch 1630, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,175: INFO: model_training: Rank 0, Epoch 1630, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,176: INFO: model_training: Rank 0, Epoch 1631, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,177: INFO: model_training: Rank 0, Epoch 1631, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,179: INFO: model_training: Rank 0, Epoch 1631, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,180: INFO: model_training: Rank 0, Epoch 1631, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,182: INFO: model_training: Rank 0, Epoch 1631, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,183: INFO: model_training: Rank 0, Epoch 1632, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,184: INFO: model_training: Rank 0, Epoch 1632, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,185: INFO: model_training: Rank 0, Epoch 1632, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,186: INFO: model_training: Rank 0, Epoch 1632, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,188: INFO: model_training: Rank 0, Epoch 1632, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,189: INFO: model_training: Rank 0, Epoch 1633, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,190: INFO: model_training: Rank 0, Epoch 1633, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,192: INFO: model_training: Rank 0, Epoch 1633, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,193: INFO: model_training: Rank 0, Epoch 1633, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,194: INFO: model_training: Rank 0, Epoch 1633, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,195: INFO: model_training: Rank 0, Epoch 1634, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,196: INFO: model_training: Rank 0, Epoch 1634, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,198: INFO: model_training: Rank 0, Epoch 1634, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,199: INFO: model_training: Rank 0, Epoch 1634, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,200: INFO: model_training: Rank 0, Epoch 1634, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,202: INFO: model_training: Rank 0, Epoch 1635, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,203: INFO: model_training: Rank 0, Epoch 1635, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,204: INFO: model_training: Rank 0, Epoch 1635, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,205: INFO: model_training: Rank 0, Epoch 1635, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,206: INFO: model_training: Rank 0, Epoch 1635, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,207: INFO: model_training: Rank 0, Epoch 1636, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,209: INFO: model_training: Rank 0, Epoch 1636, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,210: INFO: model_training: Rank 0, Epoch 1636, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,211: INFO: model_training: Rank 0, Epoch 1636, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,213: INFO: model_training: Rank 0, Epoch 1636, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,214: INFO: model_training: Rank 0, Epoch 1637, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,215: INFO: model_training: Rank 0, Epoch 1637, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,216: INFO: model_training: Rank 0, Epoch 1637, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,217: INFO: model_training: Rank 0, Epoch 1637, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,219: INFO: model_training: Rank 0, Epoch 1637, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,220: INFO: model_training: Rank 0, Epoch 1638, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,222: INFO: model_training: Rank 0, Epoch 1638, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,223: INFO: model_training: Rank 0, Epoch 1638, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,224: INFO: model_training: Rank 0, Epoch 1638, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,226: INFO: model_training: Rank 0, Epoch 1638, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,227: INFO: model_training: Rank 0, Epoch 1639, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,228: INFO: model_training: Rank 0, Epoch 1639, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,230: INFO: model_training: Rank 0, Epoch 1639, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,231: INFO: model_training: Rank 0, Epoch 1639, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,232: INFO: model_training: Rank 0, Epoch 1639, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,234: INFO: model_training: Rank 0, Epoch 1640, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,235: INFO: model_training: Rank 0, Epoch 1640, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,236: INFO: model_training: Rank 0, Epoch 1640, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,237: INFO: model_training: Rank 0, Epoch 1640, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,239: INFO: model_training: Rank 0, Epoch 1640, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,240: INFO: model_training: Rank 0, Epoch 1641, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,242: INFO: model_training: Rank 0, Epoch 1641, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,244: INFO: model_training: Rank 0, Epoch 1641, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,246: INFO: model_training: Rank 0, Epoch 1641, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,247: INFO: model_training: Rank 0, Epoch 1641, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,249: INFO: model_training: Rank 0, Epoch 1642, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,250: INFO: model_training: Rank 0, Epoch 1642, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,252: INFO: model_training: Rank 0, Epoch 1642, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,254: INFO: model_training: Rank 0, Epoch 1642, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,256: INFO: model_training: Rank 0, Epoch 1642, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,259: INFO: model_training: Rank 0, Epoch 1643, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,262: INFO: model_training: Rank 0, Epoch 1643, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,263: INFO: model_training: Rank 0, Epoch 1643, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,266: INFO: model_training: Rank 0, Epoch 1643, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,269: INFO: model_training: Rank 0, Epoch 1643, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,272: INFO: model_training: Rank 0, Epoch 1644, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,275: INFO: model_training: Rank 0, Epoch 1644, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,277: INFO: model_training: Rank 0, Epoch 1644, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,279: INFO: model_training: Rank 0, Epoch 1644, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,280: INFO: model_training: Rank 0, Epoch 1644, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,282: INFO: model_training: Rank 0, Epoch 1645, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,283: INFO: model_training: Rank 0, Epoch 1645, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,285: INFO: model_training: Rank 0, Epoch 1645, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,286: INFO: model_training: Rank 0, Epoch 1645, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,288: INFO: model_training: Rank 0, Epoch 1645, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,290: INFO: model_training: Rank 0, Epoch 1646, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,293: INFO: model_training: Rank 0, Epoch 1646, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,294: INFO: model_training: Rank 0, Epoch 1646, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,296: INFO: model_training: Rank 0, Epoch 1646, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,298: INFO: model_training: Rank 0, Epoch 1646, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,300: INFO: model_training: Rank 0, Epoch 1647, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,302: INFO: model_training: Rank 0, Epoch 1647, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,304: INFO: model_training: Rank 0, Epoch 1647, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,307: INFO: model_training: Rank 0, Epoch 1647, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,309: INFO: model_training: Rank 0, Epoch 1647, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,311: INFO: model_training: Rank 0, Epoch 1648, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,313: INFO: model_training: Rank 0, Epoch 1648, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,316: INFO: model_training: Rank 0, Epoch 1648, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,319: INFO: model_training: Rank 0, Epoch 1648, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,321: INFO: model_training: Rank 0, Epoch 1648, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,324: INFO: model_training: Rank 0, Epoch 1649, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,325: INFO: model_training: Rank 0, Epoch 1649, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,327: INFO: model_training: Rank 0, Epoch 1649, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,328: INFO: model_training: Rank 0, Epoch 1649, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,330: INFO: model_training: Rank 0, Epoch 1649, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,332: INFO: model_training: Rank 0, Epoch 1650, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,334: INFO: model_training: Rank 0, Epoch 1650, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,335: INFO: model_training: Rank 0, Epoch 1650, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,337: INFO: model_training: Rank 0, Epoch 1650, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,339: INFO: model_training: Rank 0, Epoch 1650, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,340: INFO: model_training: Rank 0, Epoch 1651, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,342: INFO: model_training: Rank 0, Epoch 1651, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,343: INFO: model_training: Rank 0, Epoch 1651, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,347: INFO: model_training: Rank 0, Epoch 1651, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,349: INFO: model_training: Rank 0, Epoch 1651, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,351: INFO: model_training: Rank 0, Epoch 1652, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,353: INFO: model_training: Rank 0, Epoch 1652, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,354: INFO: model_training: Rank 0, Epoch 1652, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,356: INFO: model_training: Rank 0, Epoch 1652, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,358: INFO: model_training: Rank 0, Epoch 1652, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,360: INFO: model_training: Rank 0, Epoch 1653, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,362: INFO: model_training: Rank 0, Epoch 1653, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,363: INFO: model_training: Rank 0, Epoch 1653, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,364: INFO: model_training: Rank 0, Epoch 1653, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,365: INFO: model_training: Rank 0, Epoch 1653, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,367: INFO: model_training: Rank 0, Epoch 1654, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,369: INFO: model_training: Rank 0, Epoch 1654, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,370: INFO: model_training: Rank 0, Epoch 1654, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,371: INFO: model_training: Rank 0, Epoch 1654, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,372: INFO: model_training: Rank 0, Epoch 1654, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,373: INFO: model_training: Rank 0, Epoch 1655, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,374: INFO: model_training: Rank 0, Epoch 1655, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,376: INFO: model_training: Rank 0, Epoch 1655, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,377: INFO: model_training: Rank 0, Epoch 1655, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,378: INFO: model_training: Rank 0, Epoch 1655, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,379: INFO: model_training: Rank 0, Epoch 1656, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,380: INFO: model_training: Rank 0, Epoch 1656, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,382: INFO: model_training: Rank 0, Epoch 1656, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,383: INFO: model_training: Rank 0, Epoch 1656, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,384: INFO: model_training: Rank 0, Epoch 1656, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,385: INFO: model_training: Rank 0, Epoch 1657, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,386: INFO: model_training: Rank 0, Epoch 1657, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,387: INFO: model_training: Rank 0, Epoch 1657, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,388: INFO: model_training: Rank 0, Epoch 1657, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,389: INFO: model_training: Rank 0, Epoch 1657, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,391: INFO: model_training: Rank 0, Epoch 1658, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,392: INFO: model_training: Rank 0, Epoch 1658, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,393: INFO: model_training: Rank 0, Epoch 1658, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,394: INFO: model_training: Rank 0, Epoch 1658, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,395: INFO: model_training: Rank 0, Epoch 1658, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,396: INFO: model_training: Rank 0, Epoch 1659, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,397: INFO: model_training: Rank 0, Epoch 1659, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,398: INFO: model_training: Rank 0, Epoch 1659, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,399: INFO: model_training: Rank 0, Epoch 1659, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,400: INFO: model_training: Rank 0, Epoch 1659, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,401: INFO: model_training: Rank 0, Epoch 1660, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,402: INFO: model_training: Rank 0, Epoch 1660, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,403: INFO: model_training: Rank 0, Epoch 1660, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,405: INFO: model_training: Rank 0, Epoch 1660, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,406: INFO: model_training: Rank 0, Epoch 1660, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,407: INFO: model_training: Rank 0, Epoch 1661, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,409: INFO: model_training: Rank 0, Epoch 1661, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,410: INFO: model_training: Rank 0, Epoch 1661, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,411: INFO: model_training: Rank 0, Epoch 1661, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,412: INFO: model_training: Rank 0, Epoch 1661, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,413: INFO: model_training: Rank 0, Epoch 1662, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,414: INFO: model_training: Rank 0, Epoch 1662, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,415: INFO: model_training: Rank 0, Epoch 1662, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,416: INFO: model_training: Rank 0, Epoch 1662, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,417: INFO: model_training: Rank 0, Epoch 1662, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,419: INFO: model_training: Rank 0, Epoch 1663, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,420: INFO: model_training: Rank 0, Epoch 1663, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,421: INFO: model_training: Rank 0, Epoch 1663, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,422: INFO: model_training: Rank 0, Epoch 1663, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,423: INFO: model_training: Rank 0, Epoch 1663, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,424: INFO: model_training: Rank 0, Epoch 1664, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,425: INFO: model_training: Rank 0, Epoch 1664, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,426: INFO: model_training: Rank 0, Epoch 1664, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,427: INFO: model_training: Rank 0, Epoch 1664, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,428: INFO: model_training: Rank 0, Epoch 1664, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,429: INFO: model_training: Rank 0, Epoch 1665, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,430: INFO: model_training: Rank 0, Epoch 1665, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,431: INFO: model_training: Rank 0, Epoch 1665, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,432: INFO: model_training: Rank 0, Epoch 1665, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,433: INFO: model_training: Rank 0, Epoch 1665, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,434: INFO: model_training: Rank 0, Epoch 1666, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,435: INFO: model_training: Rank 0, Epoch 1666, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,436: INFO: model_training: Rank 0, Epoch 1666, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,437: INFO: model_training: Rank 0, Epoch 1666, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,439: INFO: model_training: Rank 0, Epoch 1666, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,440: INFO: model_training: Rank 0, Epoch 1667, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,441: INFO: model_training: Rank 0, Epoch 1667, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,442: INFO: model_training: Rank 0, Epoch 1667, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,443: INFO: model_training: Rank 0, Epoch 1667, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,444: INFO: model_training: Rank 0, Epoch 1667, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,445: INFO: model_training: Rank 0, Epoch 1668, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,446: INFO: model_training: Rank 0, Epoch 1668, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,447: INFO: model_training: Rank 0, Epoch 1668, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,448: INFO: model_training: Rank 0, Epoch 1668, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,449: INFO: model_training: Rank 0, Epoch 1668, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,450: INFO: model_training: Rank 0, Epoch 1669, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,451: INFO: model_training: Rank 0, Epoch 1669, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,452: INFO: model_training: Rank 0, Epoch 1669, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,453: INFO: model_training: Rank 0, Epoch 1669, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,454: INFO: model_training: Rank 0, Epoch 1669, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,455: INFO: model_training: Rank 0, Epoch 1670, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,456: INFO: model_training: Rank 0, Epoch 1670, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,457: INFO: model_training: Rank 0, Epoch 1670, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,457: INFO: model_training: Rank 0, Epoch 1670, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,459: INFO: model_training: Rank 0, Epoch 1670, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,460: INFO: model_training: Rank 0, Epoch 1671, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,461: INFO: model_training: Rank 0, Epoch 1671, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,462: INFO: model_training: Rank 0, Epoch 1671, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,463: INFO: model_training: Rank 0, Epoch 1671, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,465: INFO: model_training: Rank 0, Epoch 1671, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,466: INFO: model_training: Rank 0, Epoch 1672, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,467: INFO: model_training: Rank 0, Epoch 1672, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,469: INFO: model_training: Rank 0, Epoch 1672, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,470: INFO: model_training: Rank 0, Epoch 1672, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,470: INFO: model_training: Rank 0, Epoch 1672, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,471: INFO: model_training: Rank 0, Epoch 1673, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,472: INFO: model_training: Rank 0, Epoch 1673, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,473: INFO: model_training: Rank 0, Epoch 1673, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,474: INFO: model_training: Rank 0, Epoch 1673, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,475: INFO: model_training: Rank 0, Epoch 1673, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,477: INFO: model_training: Rank 0, Epoch 1674, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,478: INFO: model_training: Rank 0, Epoch 1674, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,479: INFO: model_training: Rank 0, Epoch 1674, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,480: INFO: model_training: Rank 0, Epoch 1674, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,481: INFO: model_training: Rank 0, Epoch 1674, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,482: INFO: model_training: Rank 0, Epoch 1675, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,483: INFO: model_training: Rank 0, Epoch 1675, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,484: INFO: model_training: Rank 0, Epoch 1675, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,485: INFO: model_training: Rank 0, Epoch 1675, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,486: INFO: model_training: Rank 0, Epoch 1675, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,487: INFO: model_training: Rank 0, Epoch 1676, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,488: INFO: model_training: Rank 0, Epoch 1676, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,489: INFO: model_training: Rank 0, Epoch 1676, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,489: INFO: model_training: Rank 0, Epoch 1676, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,491: INFO: model_training: Rank 0, Epoch 1676, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,492: INFO: model_training: Rank 0, Epoch 1677, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,493: INFO: model_training: Rank 0, Epoch 1677, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,494: INFO: model_training: Rank 0, Epoch 1677, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,495: INFO: model_training: Rank 0, Epoch 1677, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,496: INFO: model_training: Rank 0, Epoch 1677, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,497: INFO: model_training: Rank 0, Epoch 1678, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,499: INFO: model_training: Rank 0, Epoch 1678, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,500: INFO: model_training: Rank 0, Epoch 1678, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,501: INFO: model_training: Rank 0, Epoch 1678, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,501: INFO: model_training: Rank 0, Epoch 1678, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,502: INFO: model_training: Rank 0, Epoch 1679, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,503: INFO: model_training: Rank 0, Epoch 1679, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,504: INFO: model_training: Rank 0, Epoch 1679, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,505: INFO: model_training: Rank 0, Epoch 1679, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,506: INFO: model_training: Rank 0, Epoch 1679, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,507: INFO: model_training: Rank 0, Epoch 1680, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,508: INFO: model_training: Rank 0, Epoch 1680, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,509: INFO: model_training: Rank 0, Epoch 1680, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,510: INFO: model_training: Rank 0, Epoch 1680, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,511: INFO: model_training: Rank 0, Epoch 1680, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,512: INFO: model_training: Rank 0, Epoch 1681, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,513: INFO: model_training: Rank 0, Epoch 1681, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,514: INFO: model_training: Rank 0, Epoch 1681, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,515: INFO: model_training: Rank 0, Epoch 1681, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,516: INFO: model_training: Rank 0, Epoch 1681, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,517: INFO: model_training: Rank 0, Epoch 1682, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,518: INFO: model_training: Rank 0, Epoch 1682, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,519: INFO: model_training: Rank 0, Epoch 1682, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,520: INFO: model_training: Rank 0, Epoch 1682, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,521: INFO: model_training: Rank 0, Epoch 1682, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,522: INFO: model_training: Rank 0, Epoch 1683, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,523: INFO: model_training: Rank 0, Epoch 1683, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,524: INFO: model_training: Rank 0, Epoch 1683, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,525: INFO: model_training: Rank 0, Epoch 1683, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,526: INFO: model_training: Rank 0, Epoch 1683, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,527: INFO: model_training: Rank 0, Epoch 1684, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,528: INFO: model_training: Rank 0, Epoch 1684, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,529: INFO: model_training: Rank 0, Epoch 1684, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,530: INFO: model_training: Rank 0, Epoch 1684, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,531: INFO: model_training: Rank 0, Epoch 1684, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,532: INFO: model_training: Rank 0, Epoch 1685, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,533: INFO: model_training: Rank 0, Epoch 1685, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,534: INFO: model_training: Rank 0, Epoch 1685, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,535: INFO: model_training: Rank 0, Epoch 1685, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,536: INFO: model_training: Rank 0, Epoch 1685, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,537: INFO: model_training: Rank 0, Epoch 1686, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,538: INFO: model_training: Rank 0, Epoch 1686, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,539: INFO: model_training: Rank 0, Epoch 1686, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,540: INFO: model_training: Rank 0, Epoch 1686, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,541: INFO: model_training: Rank 0, Epoch 1686, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,542: INFO: model_training: Rank 0, Epoch 1687, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,543: INFO: model_training: Rank 0, Epoch 1687, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,544: INFO: model_training: Rank 0, Epoch 1687, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,545: INFO: model_training: Rank 0, Epoch 1687, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,546: INFO: model_training: Rank 0, Epoch 1687, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,547: INFO: model_training: Rank 0, Epoch 1688, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,548: INFO: model_training: Rank 0, Epoch 1688, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,549: INFO: model_training: Rank 0, Epoch 1688, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,550: INFO: model_training: Rank 0, Epoch 1688, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,551: INFO: model_training: Rank 0, Epoch 1688, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,552: INFO: model_training: Rank 0, Epoch 1689, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,553: INFO: model_training: Rank 0, Epoch 1689, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,555: INFO: model_training: Rank 0, Epoch 1689, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,556: INFO: model_training: Rank 0, Epoch 1689, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,557: INFO: model_training: Rank 0, Epoch 1689, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,558: INFO: model_training: Rank 0, Epoch 1690, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,559: INFO: model_training: Rank 0, Epoch 1690, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,560: INFO: model_training: Rank 0, Epoch 1690, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,561: INFO: model_training: Rank 0, Epoch 1690, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,562: INFO: model_training: Rank 0, Epoch 1690, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,564: INFO: model_training: Rank 0, Epoch 1691, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,565: INFO: model_training: Rank 0, Epoch 1691, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,566: INFO: model_training: Rank 0, Epoch 1691, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,567: INFO: model_training: Rank 0, Epoch 1691, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,568: INFO: model_training: Rank 0, Epoch 1691, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,569: INFO: model_training: Rank 0, Epoch 1692, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,570: INFO: model_training: Rank 0, Epoch 1692, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,572: INFO: model_training: Rank 0, Epoch 1692, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,573: INFO: model_training: Rank 0, Epoch 1692, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,574: INFO: model_training: Rank 0, Epoch 1692, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,575: INFO: model_training: Rank 0, Epoch 1693, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,576: INFO: model_training: Rank 0, Epoch 1693, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,577: INFO: model_training: Rank 0, Epoch 1693, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,578: INFO: model_training: Rank 0, Epoch 1693, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,580: INFO: model_training: Rank 0, Epoch 1693, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,581: INFO: model_training: Rank 0, Epoch 1694, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,582: INFO: model_training: Rank 0, Epoch 1694, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,583: INFO: model_training: Rank 0, Epoch 1694, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,584: INFO: model_training: Rank 0, Epoch 1694, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,585: INFO: model_training: Rank 0, Epoch 1694, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,586: INFO: model_training: Rank 0, Epoch 1695, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,587: INFO: model_training: Rank 0, Epoch 1695, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,588: INFO: model_training: Rank 0, Epoch 1695, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,589: INFO: model_training: Rank 0, Epoch 1695, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,590: INFO: model_training: Rank 0, Epoch 1695, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,591: INFO: model_training: Rank 0, Epoch 1696, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,592: INFO: model_training: Rank 0, Epoch 1696, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,593: INFO: model_training: Rank 0, Epoch 1696, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,594: INFO: model_training: Rank 0, Epoch 1696, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,595: INFO: model_training: Rank 0, Epoch 1696, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,596: INFO: model_training: Rank 0, Epoch 1697, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,597: INFO: model_training: Rank 0, Epoch 1697, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,599: INFO: model_training: Rank 0, Epoch 1697, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,599: INFO: model_training: Rank 0, Epoch 1697, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,600: INFO: model_training: Rank 0, Epoch 1697, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,602: INFO: model_training: Rank 0, Epoch 1698, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,603: INFO: model_training: Rank 0, Epoch 1698, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,604: INFO: model_training: Rank 0, Epoch 1698, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,605: INFO: model_training: Rank 0, Epoch 1698, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,606: INFO: model_training: Rank 0, Epoch 1698, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,607: INFO: model_training: Rank 0, Epoch 1699, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,608: INFO: model_training: Rank 0, Epoch 1699, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,609: INFO: model_training: Rank 0, Epoch 1699, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,610: INFO: model_training: Rank 0, Epoch 1699, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,611: INFO: model_training: Rank 0, Epoch 1699, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,612: INFO: model_training: Rank 0, Epoch 1700, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,614: INFO: model_training: Rank 0, Epoch 1700, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,615: INFO: model_training: Rank 0, Epoch 1700, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,616: INFO: model_training: Rank 0, Epoch 1700, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,617: INFO: model_training: Rank 0, Epoch 1700, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,618: INFO: model_training: Rank 0, Epoch 1701, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,620: INFO: model_training: Rank 0, Epoch 1701, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,622: INFO: model_training: Rank 0, Epoch 1701, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,624: INFO: model_training: Rank 0, Epoch 1701, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,627: INFO: model_training: Rank 0, Epoch 1701, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,628: INFO: model_training: Rank 0, Epoch 1702, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,629: INFO: model_training: Rank 0, Epoch 1702, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,630: INFO: model_training: Rank 0, Epoch 1702, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,631: INFO: model_training: Rank 0, Epoch 1702, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,632: INFO: model_training: Rank 0, Epoch 1702, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,633: INFO: model_training: Rank 0, Epoch 1703, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,635: INFO: model_training: Rank 0, Epoch 1703, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,636: INFO: model_training: Rank 0, Epoch 1703, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,638: INFO: model_training: Rank 0, Epoch 1703, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,640: INFO: model_training: Rank 0, Epoch 1703, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,642: INFO: model_training: Rank 0, Epoch 1704, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,644: INFO: model_training: Rank 0, Epoch 1704, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,646: INFO: model_training: Rank 0, Epoch 1704, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,647: INFO: model_training: Rank 0, Epoch 1704, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,649: INFO: model_training: Rank 0, Epoch 1704, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,651: INFO: model_training: Rank 0, Epoch 1705, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,652: INFO: model_training: Rank 0, Epoch 1705, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,654: INFO: model_training: Rank 0, Epoch 1705, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,655: INFO: model_training: Rank 0, Epoch 1705, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,656: INFO: model_training: Rank 0, Epoch 1705, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,657: INFO: model_training: Rank 0, Epoch 1706, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,659: INFO: model_training: Rank 0, Epoch 1706, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,661: INFO: model_training: Rank 0, Epoch 1706, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,662: INFO: model_training: Rank 0, Epoch 1706, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,663: INFO: model_training: Rank 0, Epoch 1706, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,665: INFO: model_training: Rank 0, Epoch 1707, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,666: INFO: model_training: Rank 0, Epoch 1707, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,668: INFO: model_training: Rank 0, Epoch 1707, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,669: INFO: model_training: Rank 0, Epoch 1707, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,670: INFO: model_training: Rank 0, Epoch 1707, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,672: INFO: model_training: Rank 0, Epoch 1708, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,674: INFO: model_training: Rank 0, Epoch 1708, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,676: INFO: model_training: Rank 0, Epoch 1708, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,677: INFO: model_training: Rank 0, Epoch 1708, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,680: INFO: model_training: Rank 0, Epoch 1708, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,682: INFO: model_training: Rank 0, Epoch 1709, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,684: INFO: model_training: Rank 0, Epoch 1709, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,686: INFO: model_training: Rank 0, Epoch 1709, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,689: INFO: model_training: Rank 0, Epoch 1709, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,690: INFO: model_training: Rank 0, Epoch 1709, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,692: INFO: model_training: Rank 0, Epoch 1710, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,693: INFO: model_training: Rank 0, Epoch 1710, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,695: INFO: model_training: Rank 0, Epoch 1710, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,697: INFO: model_training: Rank 0, Epoch 1710, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,699: INFO: model_training: Rank 0, Epoch 1710, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,700: INFO: model_training: Rank 0, Epoch 1711, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,702: INFO: model_training: Rank 0, Epoch 1711, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,703: INFO: model_training: Rank 0, Epoch 1711, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,706: INFO: model_training: Rank 0, Epoch 1711, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,707: INFO: model_training: Rank 0, Epoch 1711, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,709: INFO: model_training: Rank 0, Epoch 1712, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,710: INFO: model_training: Rank 0, Epoch 1712, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,711: INFO: model_training: Rank 0, Epoch 1712, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,713: INFO: model_training: Rank 0, Epoch 1712, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,714: INFO: model_training: Rank 0, Epoch 1712, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,716: INFO: model_training: Rank 0, Epoch 1713, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,717: INFO: model_training: Rank 0, Epoch 1713, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,719: INFO: model_training: Rank 0, Epoch 1713, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,721: INFO: model_training: Rank 0, Epoch 1713, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,722: INFO: model_training: Rank 0, Epoch 1713, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,724: INFO: model_training: Rank 0, Epoch 1714, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,726: INFO: model_training: Rank 0, Epoch 1714, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,727: INFO: model_training: Rank 0, Epoch 1714, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,729: INFO: model_training: Rank 0, Epoch 1714, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,730: INFO: model_training: Rank 0, Epoch 1714, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,734: INFO: model_training: Rank 0, Epoch 1715, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,736: INFO: model_training: Rank 0, Epoch 1715, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,739: INFO: model_training: Rank 0, Epoch 1715, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,742: INFO: model_training: Rank 0, Epoch 1715, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,745: INFO: model_training: Rank 0, Epoch 1715, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,747: INFO: model_training: Rank 0, Epoch 1716, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,749: INFO: model_training: Rank 0, Epoch 1716, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,751: INFO: model_training: Rank 0, Epoch 1716, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,752: INFO: model_training: Rank 0, Epoch 1716, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,754: INFO: model_training: Rank 0, Epoch 1716, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,755: INFO: model_training: Rank 0, Epoch 1717, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,758: INFO: model_training: Rank 0, Epoch 1717, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,761: INFO: model_training: Rank 0, Epoch 1717, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,764: INFO: model_training: Rank 0, Epoch 1717, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,768: INFO: model_training: Rank 0, Epoch 1717, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,771: INFO: model_training: Rank 0, Epoch 1718, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,775: INFO: model_training: Rank 0, Epoch 1718, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,777: INFO: model_training: Rank 0, Epoch 1718, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,780: INFO: model_training: Rank 0, Epoch 1718, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,783: INFO: model_training: Rank 0, Epoch 1718, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,785: INFO: model_training: Rank 0, Epoch 1719, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,788: INFO: model_training: Rank 0, Epoch 1719, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,792: INFO: model_training: Rank 0, Epoch 1719, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,794: INFO: model_training: Rank 0, Epoch 1719, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,797: INFO: model_training: Rank 0, Epoch 1719, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,800: INFO: model_training: Rank 0, Epoch 1720, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,802: INFO: model_training: Rank 0, Epoch 1720, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,804: INFO: model_training: Rank 0, Epoch 1720, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,805: INFO: model_training: Rank 0, Epoch 1720, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,807: INFO: model_training: Rank 0, Epoch 1720, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,809: INFO: model_training: Rank 0, Epoch 1721, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,812: INFO: model_training: Rank 0, Epoch 1721, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,814: INFO: model_training: Rank 0, Epoch 1721, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,816: INFO: model_training: Rank 0, Epoch 1721, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,818: INFO: model_training: Rank 0, Epoch 1721, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,819: INFO: model_training: Rank 0, Epoch 1722, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,821: INFO: model_training: Rank 0, Epoch 1722, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,824: INFO: model_training: Rank 0, Epoch 1722, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,825: INFO: model_training: Rank 0, Epoch 1722, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,827: INFO: model_training: Rank 0, Epoch 1722, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,829: INFO: model_training: Rank 0, Epoch 1723, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,830: INFO: model_training: Rank 0, Epoch 1723, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,832: INFO: model_training: Rank 0, Epoch 1723, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,833: INFO: model_training: Rank 0, Epoch 1723, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,834: INFO: model_training: Rank 0, Epoch 1723, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,836: INFO: model_training: Rank 0, Epoch 1724, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,837: INFO: model_training: Rank 0, Epoch 1724, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,839: INFO: model_training: Rank 0, Epoch 1724, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,841: INFO: model_training: Rank 0, Epoch 1724, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,843: INFO: model_training: Rank 0, Epoch 1724, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,845: INFO: model_training: Rank 0, Epoch 1725, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,846: INFO: model_training: Rank 0, Epoch 1725, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,847: INFO: model_training: Rank 0, Epoch 1725, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,849: INFO: model_training: Rank 0, Epoch 1725, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,850: INFO: model_training: Rank 0, Epoch 1725, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,851: INFO: model_training: Rank 0, Epoch 1726, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,852: INFO: model_training: Rank 0, Epoch 1726, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,853: INFO: model_training: Rank 0, Epoch 1726, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,855: INFO: model_training: Rank 0, Epoch 1726, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,856: INFO: model_training: Rank 0, Epoch 1726, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,857: INFO: model_training: Rank 0, Epoch 1727, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,859: INFO: model_training: Rank 0, Epoch 1727, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,860: INFO: model_training: Rank 0, Epoch 1727, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,861: INFO: model_training: Rank 0, Epoch 1727, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,863: INFO: model_training: Rank 0, Epoch 1727, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,864: INFO: model_training: Rank 0, Epoch 1728, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,865: INFO: model_training: Rank 0, Epoch 1728, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,866: INFO: model_training: Rank 0, Epoch 1728, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,868: INFO: model_training: Rank 0, Epoch 1728, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,869: INFO: model_training: Rank 0, Epoch 1728, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,870: INFO: model_training: Rank 0, Epoch 1729, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,871: INFO: model_training: Rank 0, Epoch 1729, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,872: INFO: model_training: Rank 0, Epoch 1729, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,874: INFO: model_training: Rank 0, Epoch 1729, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,876: INFO: model_training: Rank 0, Epoch 1729, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,877: INFO: model_training: Rank 0, Epoch 1730, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,878: INFO: model_training: Rank 0, Epoch 1730, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,879: INFO: model_training: Rank 0, Epoch 1730, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,881: INFO: model_training: Rank 0, Epoch 1730, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,883: INFO: model_training: Rank 0, Epoch 1730, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,884: INFO: model_training: Rank 0, Epoch 1731, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,886: INFO: model_training: Rank 0, Epoch 1731, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,887: INFO: model_training: Rank 0, Epoch 1731, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,889: INFO: model_training: Rank 0, Epoch 1731, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,890: INFO: model_training: Rank 0, Epoch 1731, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,892: INFO: model_training: Rank 0, Epoch 1732, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,893: INFO: model_training: Rank 0, Epoch 1732, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,894: INFO: model_training: Rank 0, Epoch 1732, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,896: INFO: model_training: Rank 0, Epoch 1732, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,897: INFO: model_training: Rank 0, Epoch 1732, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,899: INFO: model_training: Rank 0, Epoch 1733, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,901: INFO: model_training: Rank 0, Epoch 1733, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,902: INFO: model_training: Rank 0, Epoch 1733, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,904: INFO: model_training: Rank 0, Epoch 1733, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,906: INFO: model_training: Rank 0, Epoch 1733, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,908: INFO: model_training: Rank 0, Epoch 1734, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,909: INFO: model_training: Rank 0, Epoch 1734, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,911: INFO: model_training: Rank 0, Epoch 1734, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,912: INFO: model_training: Rank 0, Epoch 1734, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,914: INFO: model_training: Rank 0, Epoch 1734, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,916: INFO: model_training: Rank 0, Epoch 1735, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,918: INFO: model_training: Rank 0, Epoch 1735, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,919: INFO: model_training: Rank 0, Epoch 1735, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,921: INFO: model_training: Rank 0, Epoch 1735, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,922: INFO: model_training: Rank 0, Epoch 1735, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,924: INFO: model_training: Rank 0, Epoch 1736, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,925: INFO: model_training: Rank 0, Epoch 1736, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,927: INFO: model_training: Rank 0, Epoch 1736, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,929: INFO: model_training: Rank 0, Epoch 1736, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,931: INFO: model_training: Rank 0, Epoch 1736, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,933: INFO: model_training: Rank 0, Epoch 1737, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,934: INFO: model_training: Rank 0, Epoch 1737, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,936: INFO: model_training: Rank 0, Epoch 1737, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,939: INFO: model_training: Rank 0, Epoch 1737, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,941: INFO: model_training: Rank 0, Epoch 1737, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,942: INFO: model_training: Rank 0, Epoch 1738, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,944: INFO: model_training: Rank 0, Epoch 1738, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,946: INFO: model_training: Rank 0, Epoch 1738, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,948: INFO: model_training: Rank 0, Epoch 1738, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,950: INFO: model_training: Rank 0, Epoch 1738, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,952: INFO: model_training: Rank 0, Epoch 1739, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,953: INFO: model_training: Rank 0, Epoch 1739, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,955: INFO: model_training: Rank 0, Epoch 1739, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,956: INFO: model_training: Rank 0, Epoch 1739, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,958: INFO: model_training: Rank 0, Epoch 1739, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,960: INFO: model_training: Rank 0, Epoch 1740, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,962: INFO: model_training: Rank 0, Epoch 1740, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,964: INFO: model_training: Rank 0, Epoch 1740, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,965: INFO: model_training: Rank 0, Epoch 1740, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,967: INFO: model_training: Rank 0, Epoch 1740, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,968: INFO: model_training: Rank 0, Epoch 1741, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,970: INFO: model_training: Rank 0, Epoch 1741, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,972: INFO: model_training: Rank 0, Epoch 1741, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,973: INFO: model_training: Rank 0, Epoch 1741, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,975: INFO: model_training: Rank 0, Epoch 1741, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,977: INFO: model_training: Rank 0, Epoch 1742, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,979: INFO: model_training: Rank 0, Epoch 1742, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,980: INFO: model_training: Rank 0, Epoch 1742, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,982: INFO: model_training: Rank 0, Epoch 1742, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,983: INFO: model_training: Rank 0, Epoch 1742, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,985: INFO: model_training: Rank 0, Epoch 1743, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,986: INFO: model_training: Rank 0, Epoch 1743, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,987: INFO: model_training: Rank 0, Epoch 1743, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,989: INFO: model_training: Rank 0, Epoch 1743, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,990: INFO: model_training: Rank 0, Epoch 1743, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:36,991: INFO: model_training: Rank 0, Epoch 1744, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:36,993: INFO: model_training: Rank 0, Epoch 1744, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:36,995: INFO: model_training: Rank 0, Epoch 1744, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:36,996: INFO: model_training: Rank 0, Epoch 1744, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:36,998: INFO: model_training: Rank 0, Epoch 1744, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,000: INFO: model_training: Rank 0, Epoch 1745, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,003: INFO: model_training: Rank 0, Epoch 1745, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,004: INFO: model_training: Rank 0, Epoch 1745, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,006: INFO: model_training: Rank 0, Epoch 1745, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,007: INFO: model_training: Rank 0, Epoch 1745, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,009: INFO: model_training: Rank 0, Epoch 1746, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,011: INFO: model_training: Rank 0, Epoch 1746, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,012: INFO: model_training: Rank 0, Epoch 1746, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,013: INFO: model_training: Rank 0, Epoch 1746, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,015: INFO: model_training: Rank 0, Epoch 1746, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,016: INFO: model_training: Rank 0, Epoch 1747, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,018: INFO: model_training: Rank 0, Epoch 1747, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,019: INFO: model_training: Rank 0, Epoch 1747, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,020: INFO: model_training: Rank 0, Epoch 1747, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,022: INFO: model_training: Rank 0, Epoch 1747, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,024: INFO: model_training: Rank 0, Epoch 1748, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,025: INFO: model_training: Rank 0, Epoch 1748, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,027: INFO: model_training: Rank 0, Epoch 1748, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,028: INFO: model_training: Rank 0, Epoch 1748, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,031: INFO: model_training: Rank 0, Epoch 1748, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,033: INFO: model_training: Rank 0, Epoch 1749, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,034: INFO: model_training: Rank 0, Epoch 1749, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,036: INFO: model_training: Rank 0, Epoch 1749, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,037: INFO: model_training: Rank 0, Epoch 1749, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,040: INFO: model_training: Rank 0, Epoch 1749, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,042: INFO: model_training: Rank 0, Epoch 1750, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,044: INFO: model_training: Rank 0, Epoch 1750, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,045: INFO: model_training: Rank 0, Epoch 1750, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,047: INFO: model_training: Rank 0, Epoch 1750, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,050: INFO: model_training: Rank 0, Epoch 1750, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,052: INFO: model_training: Rank 0, Epoch 1751, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,054: INFO: model_training: Rank 0, Epoch 1751, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,056: INFO: model_training: Rank 0, Epoch 1751, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,058: INFO: model_training: Rank 0, Epoch 1751, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,060: INFO: model_training: Rank 0, Epoch 1751, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,062: INFO: model_training: Rank 0, Epoch 1752, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,063: INFO: model_training: Rank 0, Epoch 1752, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,065: INFO: model_training: Rank 0, Epoch 1752, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,067: INFO: model_training: Rank 0, Epoch 1752, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,069: INFO: model_training: Rank 0, Epoch 1752, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,071: INFO: model_training: Rank 0, Epoch 1753, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,073: INFO: model_training: Rank 0, Epoch 1753, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,075: INFO: model_training: Rank 0, Epoch 1753, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,076: INFO: model_training: Rank 0, Epoch 1753, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,079: INFO: model_training: Rank 0, Epoch 1753, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,081: INFO: model_training: Rank 0, Epoch 1754, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,083: INFO: model_training: Rank 0, Epoch 1754, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,084: INFO: model_training: Rank 0, Epoch 1754, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,086: INFO: model_training: Rank 0, Epoch 1754, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,088: INFO: model_training: Rank 0, Epoch 1754, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,091: INFO: model_training: Rank 0, Epoch 1755, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,093: INFO: model_training: Rank 0, Epoch 1755, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,095: INFO: model_training: Rank 0, Epoch 1755, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,096: INFO: model_training: Rank 0, Epoch 1755, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,097: INFO: model_training: Rank 0, Epoch 1755, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,099: INFO: model_training: Rank 0, Epoch 1756, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,101: INFO: model_training: Rank 0, Epoch 1756, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,103: INFO: model_training: Rank 0, Epoch 1756, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,105: INFO: model_training: Rank 0, Epoch 1756, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,107: INFO: model_training: Rank 0, Epoch 1756, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,108: INFO: model_training: Rank 0, Epoch 1757, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,110: INFO: model_training: Rank 0, Epoch 1757, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,111: INFO: model_training: Rank 0, Epoch 1757, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,113: INFO: model_training: Rank 0, Epoch 1757, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,114: INFO: model_training: Rank 0, Epoch 1757, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,116: INFO: model_training: Rank 0, Epoch 1758, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,117: INFO: model_training: Rank 0, Epoch 1758, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,118: INFO: model_training: Rank 0, Epoch 1758, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,120: INFO: model_training: Rank 0, Epoch 1758, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,121: INFO: model_training: Rank 0, Epoch 1758, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,123: INFO: model_training: Rank 0, Epoch 1759, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,124: INFO: model_training: Rank 0, Epoch 1759, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,126: INFO: model_training: Rank 0, Epoch 1759, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,127: INFO: model_training: Rank 0, Epoch 1759, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,129: INFO: model_training: Rank 0, Epoch 1759, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,130: INFO: model_training: Rank 0, Epoch 1760, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,132: INFO: model_training: Rank 0, Epoch 1760, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,133: INFO: model_training: Rank 0, Epoch 1760, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,135: INFO: model_training: Rank 0, Epoch 1760, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,136: INFO: model_training: Rank 0, Epoch 1760, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,137: INFO: model_training: Rank 0, Epoch 1761, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,139: INFO: model_training: Rank 0, Epoch 1761, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,141: INFO: model_training: Rank 0, Epoch 1761, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,143: INFO: model_training: Rank 0, Epoch 1761, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,145: INFO: model_training: Rank 0, Epoch 1761, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,147: INFO: model_training: Rank 0, Epoch 1762, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,149: INFO: model_training: Rank 0, Epoch 1762, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,150: INFO: model_training: Rank 0, Epoch 1762, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,153: INFO: model_training: Rank 0, Epoch 1762, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,156: INFO: model_training: Rank 0, Epoch 1762, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,158: INFO: model_training: Rank 0, Epoch 1763, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,160: INFO: model_training: Rank 0, Epoch 1763, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,162: INFO: model_training: Rank 0, Epoch 1763, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,164: INFO: model_training: Rank 0, Epoch 1763, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,165: INFO: model_training: Rank 0, Epoch 1763, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,167: INFO: model_training: Rank 0, Epoch 1764, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,169: INFO: model_training: Rank 0, Epoch 1764, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,170: INFO: model_training: Rank 0, Epoch 1764, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,172: INFO: model_training: Rank 0, Epoch 1764, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,174: INFO: model_training: Rank 0, Epoch 1764, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,176: INFO: model_training: Rank 0, Epoch 1765, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,177: INFO: model_training: Rank 0, Epoch 1765, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,178: INFO: model_training: Rank 0, Epoch 1765, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,180: INFO: model_training: Rank 0, Epoch 1765, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,181: INFO: model_training: Rank 0, Epoch 1765, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,183: INFO: model_training: Rank 0, Epoch 1766, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,185: INFO: model_training: Rank 0, Epoch 1766, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,186: INFO: model_training: Rank 0, Epoch 1766, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,189: INFO: model_training: Rank 0, Epoch 1766, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,191: INFO: model_training: Rank 0, Epoch 1766, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,192: INFO: model_training: Rank 0, Epoch 1767, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,194: INFO: model_training: Rank 0, Epoch 1767, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,195: INFO: model_training: Rank 0, Epoch 1767, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,197: INFO: model_training: Rank 0, Epoch 1767, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,198: INFO: model_training: Rank 0, Epoch 1767, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,200: INFO: model_training: Rank 0, Epoch 1768, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,201: INFO: model_training: Rank 0, Epoch 1768, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,202: INFO: model_training: Rank 0, Epoch 1768, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,204: INFO: model_training: Rank 0, Epoch 1768, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,205: INFO: model_training: Rank 0, Epoch 1768, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,207: INFO: model_training: Rank 0, Epoch 1769, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,208: INFO: model_training: Rank 0, Epoch 1769, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,211: INFO: model_training: Rank 0, Epoch 1769, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,212: INFO: model_training: Rank 0, Epoch 1769, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,213: INFO: model_training: Rank 0, Epoch 1769, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,214: INFO: model_training: Rank 0, Epoch 1770, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,216: INFO: model_training: Rank 0, Epoch 1770, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,217: INFO: model_training: Rank 0, Epoch 1770, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,219: INFO: model_training: Rank 0, Epoch 1770, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,220: INFO: model_training: Rank 0, Epoch 1770, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,222: INFO: model_training: Rank 0, Epoch 1771, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,223: INFO: model_training: Rank 0, Epoch 1771, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,225: INFO: model_training: Rank 0, Epoch 1771, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,226: INFO: model_training: Rank 0, Epoch 1771, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,227: INFO: model_training: Rank 0, Epoch 1771, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,228: INFO: model_training: Rank 0, Epoch 1772, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,229: INFO: model_training: Rank 0, Epoch 1772, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,231: INFO: model_training: Rank 0, Epoch 1772, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,232: INFO: model_training: Rank 0, Epoch 1772, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,234: INFO: model_training: Rank 0, Epoch 1772, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,237: INFO: model_training: Rank 0, Epoch 1773, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,238: INFO: model_training: Rank 0, Epoch 1773, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,240: INFO: model_training: Rank 0, Epoch 1773, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,241: INFO: model_training: Rank 0, Epoch 1773, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,243: INFO: model_training: Rank 0, Epoch 1773, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,245: INFO: model_training: Rank 0, Epoch 1774, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,246: INFO: model_training: Rank 0, Epoch 1774, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,247: INFO: model_training: Rank 0, Epoch 1774, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,249: INFO: model_training: Rank 0, Epoch 1774, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,250: INFO: model_training: Rank 0, Epoch 1774, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,252: INFO: model_training: Rank 0, Epoch 1775, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,253: INFO: model_training: Rank 0, Epoch 1775, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,254: INFO: model_training: Rank 0, Epoch 1775, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,255: INFO: model_training: Rank 0, Epoch 1775, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,256: INFO: model_training: Rank 0, Epoch 1775, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,257: INFO: model_training: Rank 0, Epoch 1776, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,259: INFO: model_training: Rank 0, Epoch 1776, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,260: INFO: model_training: Rank 0, Epoch 1776, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,261: INFO: model_training: Rank 0, Epoch 1776, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,263: INFO: model_training: Rank 0, Epoch 1776, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,264: INFO: model_training: Rank 0, Epoch 1777, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,266: INFO: model_training: Rank 0, Epoch 1777, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,268: INFO: model_training: Rank 0, Epoch 1777, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,269: INFO: model_training: Rank 0, Epoch 1777, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,271: INFO: model_training: Rank 0, Epoch 1777, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,272: INFO: model_training: Rank 0, Epoch 1778, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,273: INFO: model_training: Rank 0, Epoch 1778, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,274: INFO: model_training: Rank 0, Epoch 1778, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,276: INFO: model_training: Rank 0, Epoch 1778, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,278: INFO: model_training: Rank 0, Epoch 1778, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,279: INFO: model_training: Rank 0, Epoch 1779, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,280: INFO: model_training: Rank 0, Epoch 1779, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,281: INFO: model_training: Rank 0, Epoch 1779, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,282: INFO: model_training: Rank 0, Epoch 1779, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,283: INFO: model_training: Rank 0, Epoch 1779, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,284: INFO: model_training: Rank 0, Epoch 1780, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,286: INFO: model_training: Rank 0, Epoch 1780, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,287: INFO: model_training: Rank 0, Epoch 1780, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,288: INFO: model_training: Rank 0, Epoch 1780, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,290: INFO: model_training: Rank 0, Epoch 1780, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,291: INFO: model_training: Rank 0, Epoch 1781, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,292: INFO: model_training: Rank 0, Epoch 1781, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,293: INFO: model_training: Rank 0, Epoch 1781, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,294: INFO: model_training: Rank 0, Epoch 1781, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,295: INFO: model_training: Rank 0, Epoch 1781, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,296: INFO: model_training: Rank 0, Epoch 1782, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,297: INFO: model_training: Rank 0, Epoch 1782, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,299: INFO: model_training: Rank 0, Epoch 1782, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,300: INFO: model_training: Rank 0, Epoch 1782, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,301: INFO: model_training: Rank 0, Epoch 1782, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,303: INFO: model_training: Rank 0, Epoch 1783, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,305: INFO: model_training: Rank 0, Epoch 1783, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,306: INFO: model_training: Rank 0, Epoch 1783, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,307: INFO: model_training: Rank 0, Epoch 1783, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,308: INFO: model_training: Rank 0, Epoch 1783, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,310: INFO: model_training: Rank 0, Epoch 1784, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,311: INFO: model_training: Rank 0, Epoch 1784, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,313: INFO: model_training: Rank 0, Epoch 1784, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,315: INFO: model_training: Rank 0, Epoch 1784, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,317: INFO: model_training: Rank 0, Epoch 1784, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,318: INFO: model_training: Rank 0, Epoch 1785, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,320: INFO: model_training: Rank 0, Epoch 1785, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,322: INFO: model_training: Rank 0, Epoch 1785, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,324: INFO: model_training: Rank 0, Epoch 1785, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,325: INFO: model_training: Rank 0, Epoch 1785, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,326: INFO: model_training: Rank 0, Epoch 1786, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,328: INFO: model_training: Rank 0, Epoch 1786, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,329: INFO: model_training: Rank 0, Epoch 1786, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,331: INFO: model_training: Rank 0, Epoch 1786, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,332: INFO: model_training: Rank 0, Epoch 1786, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,334: INFO: model_training: Rank 0, Epoch 1787, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,335: INFO: model_training: Rank 0, Epoch 1787, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,337: INFO: model_training: Rank 0, Epoch 1787, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,339: INFO: model_training: Rank 0, Epoch 1787, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,340: INFO: model_training: Rank 0, Epoch 1787, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,342: INFO: model_training: Rank 0, Epoch 1788, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,343: INFO: model_training: Rank 0, Epoch 1788, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,345: INFO: model_training: Rank 0, Epoch 1788, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,347: INFO: model_training: Rank 0, Epoch 1788, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,349: INFO: model_training: Rank 0, Epoch 1788, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,350: INFO: model_training: Rank 0, Epoch 1789, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,352: INFO: model_training: Rank 0, Epoch 1789, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,354: INFO: model_training: Rank 0, Epoch 1789, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,355: INFO: model_training: Rank 0, Epoch 1789, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,357: INFO: model_training: Rank 0, Epoch 1789, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,359: INFO: model_training: Rank 0, Epoch 1790, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,361: INFO: model_training: Rank 0, Epoch 1790, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,363: INFO: model_training: Rank 0, Epoch 1790, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,364: INFO: model_training: Rank 0, Epoch 1790, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,365: INFO: model_training: Rank 0, Epoch 1790, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,367: INFO: model_training: Rank 0, Epoch 1791, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,370: INFO: model_training: Rank 0, Epoch 1791, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,372: INFO: model_training: Rank 0, Epoch 1791, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,374: INFO: model_training: Rank 0, Epoch 1791, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,377: INFO: model_training: Rank 0, Epoch 1791, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,379: INFO: model_training: Rank 0, Epoch 1792, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,382: INFO: model_training: Rank 0, Epoch 1792, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,384: INFO: model_training: Rank 0, Epoch 1792, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,386: INFO: model_training: Rank 0, Epoch 1792, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,388: INFO: model_training: Rank 0, Epoch 1792, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,390: INFO: model_training: Rank 0, Epoch 1793, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,393: INFO: model_training: Rank 0, Epoch 1793, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,395: INFO: model_training: Rank 0, Epoch 1793, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,397: INFO: model_training: Rank 0, Epoch 1793, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,400: INFO: model_training: Rank 0, Epoch 1793, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,402: INFO: model_training: Rank 0, Epoch 1794, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,403: INFO: model_training: Rank 0, Epoch 1794, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,404: INFO: model_training: Rank 0, Epoch 1794, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,406: INFO: model_training: Rank 0, Epoch 1794, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,408: INFO: model_training: Rank 0, Epoch 1794, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,409: INFO: model_training: Rank 0, Epoch 1795, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,411: INFO: model_training: Rank 0, Epoch 1795, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,412: INFO: model_training: Rank 0, Epoch 1795, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,414: INFO: model_training: Rank 0, Epoch 1795, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,415: INFO: model_training: Rank 0, Epoch 1795, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,417: INFO: model_training: Rank 0, Epoch 1796, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,418: INFO: model_training: Rank 0, Epoch 1796, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,420: INFO: model_training: Rank 0, Epoch 1796, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,421: INFO: model_training: Rank 0, Epoch 1796, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,422: INFO: model_training: Rank 0, Epoch 1796, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,424: INFO: model_training: Rank 0, Epoch 1797, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,425: INFO: model_training: Rank 0, Epoch 1797, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,427: INFO: model_training: Rank 0, Epoch 1797, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,429: INFO: model_training: Rank 0, Epoch 1797, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,432: INFO: model_training: Rank 0, Epoch 1797, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,434: INFO: model_training: Rank 0, Epoch 1798, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,436: INFO: model_training: Rank 0, Epoch 1798, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,438: INFO: model_training: Rank 0, Epoch 1798, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,440: INFO: model_training: Rank 0, Epoch 1798, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,441: INFO: model_training: Rank 0, Epoch 1798, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,442: INFO: model_training: Rank 0, Epoch 1799, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,444: INFO: model_training: Rank 0, Epoch 1799, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,446: INFO: model_training: Rank 0, Epoch 1799, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,449: INFO: model_training: Rank 0, Epoch 1799, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,450: INFO: model_training: Rank 0, Epoch 1799, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,451: INFO: model_training: Rank 0, Epoch 1800, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,453: INFO: model_training: Rank 0, Epoch 1800, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,454: INFO: model_training: Rank 0, Epoch 1800, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,456: INFO: model_training: Rank 0, Epoch 1800, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,458: INFO: model_training: Rank 0, Epoch 1800, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,461: INFO: model_training: Rank 0, Epoch 1801, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,463: INFO: model_training: Rank 0, Epoch 1801, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,465: INFO: model_training: Rank 0, Epoch 1801, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,466: INFO: model_training: Rank 0, Epoch 1801, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,468: INFO: model_training: Rank 0, Epoch 1801, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,470: INFO: model_training: Rank 0, Epoch 1802, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,473: INFO: model_training: Rank 0, Epoch 1802, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,474: INFO: model_training: Rank 0, Epoch 1802, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,477: INFO: model_training: Rank 0, Epoch 1802, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,478: INFO: model_training: Rank 0, Epoch 1802, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,479: INFO: model_training: Rank 0, Epoch 1803, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,481: INFO: model_training: Rank 0, Epoch 1803, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,482: INFO: model_training: Rank 0, Epoch 1803, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,483: INFO: model_training: Rank 0, Epoch 1803, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,484: INFO: model_training: Rank 0, Epoch 1803, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,486: INFO: model_training: Rank 0, Epoch 1804, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,487: INFO: model_training: Rank 0, Epoch 1804, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,488: INFO: model_training: Rank 0, Epoch 1804, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,489: INFO: model_training: Rank 0, Epoch 1804, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,490: INFO: model_training: Rank 0, Epoch 1804, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,491: INFO: model_training: Rank 0, Epoch 1805, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,492: INFO: model_training: Rank 0, Epoch 1805, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,493: INFO: model_training: Rank 0, Epoch 1805, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,494: INFO: model_training: Rank 0, Epoch 1805, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,495: INFO: model_training: Rank 0, Epoch 1805, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,496: INFO: model_training: Rank 0, Epoch 1806, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,499: INFO: model_training: Rank 0, Epoch 1806, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,500: INFO: model_training: Rank 0, Epoch 1806, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,502: INFO: model_training: Rank 0, Epoch 1806, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,503: INFO: model_training: Rank 0, Epoch 1806, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,504: INFO: model_training: Rank 0, Epoch 1807, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,505: INFO: model_training: Rank 0, Epoch 1807, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,507: INFO: model_training: Rank 0, Epoch 1807, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,508: INFO: model_training: Rank 0, Epoch 1807, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,509: INFO: model_training: Rank 0, Epoch 1807, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,511: INFO: model_training: Rank 0, Epoch 1808, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,512: INFO: model_training: Rank 0, Epoch 1808, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,513: INFO: model_training: Rank 0, Epoch 1808, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,514: INFO: model_training: Rank 0, Epoch 1808, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,515: INFO: model_training: Rank 0, Epoch 1808, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,516: INFO: model_training: Rank 0, Epoch 1809, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,517: INFO: model_training: Rank 0, Epoch 1809, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,518: INFO: model_training: Rank 0, Epoch 1809, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,519: INFO: model_training: Rank 0, Epoch 1809, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,520: INFO: model_training: Rank 0, Epoch 1809, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,521: INFO: model_training: Rank 0, Epoch 1810, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,523: INFO: model_training: Rank 0, Epoch 1810, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,524: INFO: model_training: Rank 0, Epoch 1810, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,525: INFO: model_training: Rank 0, Epoch 1810, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,526: INFO: model_training: Rank 0, Epoch 1810, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,527: INFO: model_training: Rank 0, Epoch 1811, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,528: INFO: model_training: Rank 0, Epoch 1811, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,529: INFO: model_training: Rank 0, Epoch 1811, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,530: INFO: model_training: Rank 0, Epoch 1811, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,531: INFO: model_training: Rank 0, Epoch 1811, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,532: INFO: model_training: Rank 0, Epoch 1812, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,533: INFO: model_training: Rank 0, Epoch 1812, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,534: INFO: model_training: Rank 0, Epoch 1812, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,536: INFO: model_training: Rank 0, Epoch 1812, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,537: INFO: model_training: Rank 0, Epoch 1812, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,538: INFO: model_training: Rank 0, Epoch 1813, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,539: INFO: model_training: Rank 0, Epoch 1813, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,540: INFO: model_training: Rank 0, Epoch 1813, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,541: INFO: model_training: Rank 0, Epoch 1813, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,542: INFO: model_training: Rank 0, Epoch 1813, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,543: INFO: model_training: Rank 0, Epoch 1814, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,544: INFO: model_training: Rank 0, Epoch 1814, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,545: INFO: model_training: Rank 0, Epoch 1814, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,546: INFO: model_training: Rank 0, Epoch 1814, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,547: INFO: model_training: Rank 0, Epoch 1814, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,548: INFO: model_training: Rank 0, Epoch 1815, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,551: INFO: model_training: Rank 0, Epoch 1815, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,552: INFO: model_training: Rank 0, Epoch 1815, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,553: INFO: model_training: Rank 0, Epoch 1815, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,554: INFO: model_training: Rank 0, Epoch 1815, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,555: INFO: model_training: Rank 0, Epoch 1816, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,556: INFO: model_training: Rank 0, Epoch 1816, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,558: INFO: model_training: Rank 0, Epoch 1816, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,559: INFO: model_training: Rank 0, Epoch 1816, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,560: INFO: model_training: Rank 0, Epoch 1816, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,561: INFO: model_training: Rank 0, Epoch 1817, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,562: INFO: model_training: Rank 0, Epoch 1817, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,563: INFO: model_training: Rank 0, Epoch 1817, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,564: INFO: model_training: Rank 0, Epoch 1817, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,565: INFO: model_training: Rank 0, Epoch 1817, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,566: INFO: model_training: Rank 0, Epoch 1818, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,568: INFO: model_training: Rank 0, Epoch 1818, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,569: INFO: model_training: Rank 0, Epoch 1818, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,571: INFO: model_training: Rank 0, Epoch 1818, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,572: INFO: model_training: Rank 0, Epoch 1818, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,573: INFO: model_training: Rank 0, Epoch 1819, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,574: INFO: model_training: Rank 0, Epoch 1819, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,575: INFO: model_training: Rank 0, Epoch 1819, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,576: INFO: model_training: Rank 0, Epoch 1819, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,577: INFO: model_training: Rank 0, Epoch 1819, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,579: INFO: model_training: Rank 0, Epoch 1820, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,580: INFO: model_training: Rank 0, Epoch 1820, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,581: INFO: model_training: Rank 0, Epoch 1820, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,582: INFO: model_training: Rank 0, Epoch 1820, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,583: INFO: model_training: Rank 0, Epoch 1820, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,584: INFO: model_training: Rank 0, Epoch 1821, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,585: INFO: model_training: Rank 0, Epoch 1821, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,586: INFO: model_training: Rank 0, Epoch 1821, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,587: INFO: model_training: Rank 0, Epoch 1821, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,589: INFO: model_training: Rank 0, Epoch 1821, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,590: INFO: model_training: Rank 0, Epoch 1822, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,591: INFO: model_training: Rank 0, Epoch 1822, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,592: INFO: model_training: Rank 0, Epoch 1822, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,593: INFO: model_training: Rank 0, Epoch 1822, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,594: INFO: model_training: Rank 0, Epoch 1822, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,595: INFO: model_training: Rank 0, Epoch 1823, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,596: INFO: model_training: Rank 0, Epoch 1823, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,597: INFO: model_training: Rank 0, Epoch 1823, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,599: INFO: model_training: Rank 0, Epoch 1823, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,600: INFO: model_training: Rank 0, Epoch 1823, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,601: INFO: model_training: Rank 0, Epoch 1824, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,602: INFO: model_training: Rank 0, Epoch 1824, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,603: INFO: model_training: Rank 0, Epoch 1824, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,604: INFO: model_training: Rank 0, Epoch 1824, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,605: INFO: model_training: Rank 0, Epoch 1824, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,606: INFO: model_training: Rank 0, Epoch 1825, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,607: INFO: model_training: Rank 0, Epoch 1825, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,608: INFO: model_training: Rank 0, Epoch 1825, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,609: INFO: model_training: Rank 0, Epoch 1825, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,610: INFO: model_training: Rank 0, Epoch 1825, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,610: INFO: model_training: Rank 0, Epoch 1826, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,612: INFO: model_training: Rank 0, Epoch 1826, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,613: INFO: model_training: Rank 0, Epoch 1826, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,614: INFO: model_training: Rank 0, Epoch 1826, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,615: INFO: model_training: Rank 0, Epoch 1826, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,616: INFO: model_training: Rank 0, Epoch 1827, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,617: INFO: model_training: Rank 0, Epoch 1827, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,618: INFO: model_training: Rank 0, Epoch 1827, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,620: INFO: model_training: Rank 0, Epoch 1827, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,621: INFO: model_training: Rank 0, Epoch 1827, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,622: INFO: model_training: Rank 0, Epoch 1828, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,623: INFO: model_training: Rank 0, Epoch 1828, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,624: INFO: model_training: Rank 0, Epoch 1828, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,625: INFO: model_training: Rank 0, Epoch 1828, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,626: INFO: model_training: Rank 0, Epoch 1828, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,628: INFO: model_training: Rank 0, Epoch 1829, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,630: INFO: model_training: Rank 0, Epoch 1829, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,631: INFO: model_training: Rank 0, Epoch 1829, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,633: INFO: model_training: Rank 0, Epoch 1829, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,634: INFO: model_training: Rank 0, Epoch 1829, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,636: INFO: model_training: Rank 0, Epoch 1830, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,637: INFO: model_training: Rank 0, Epoch 1830, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,638: INFO: model_training: Rank 0, Epoch 1830, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,639: INFO: model_training: Rank 0, Epoch 1830, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,640: INFO: model_training: Rank 0, Epoch 1830, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,641: INFO: model_training: Rank 0, Epoch 1831, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,642: INFO: model_training: Rank 0, Epoch 1831, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,643: INFO: model_training: Rank 0, Epoch 1831, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,644: INFO: model_training: Rank 0, Epoch 1831, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,646: INFO: model_training: Rank 0, Epoch 1831, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,647: INFO: model_training: Rank 0, Epoch 1832, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,648: INFO: model_training: Rank 0, Epoch 1832, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,649: INFO: model_training: Rank 0, Epoch 1832, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,650: INFO: model_training: Rank 0, Epoch 1832, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,651: INFO: model_training: Rank 0, Epoch 1832, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,652: INFO: model_training: Rank 0, Epoch 1833, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,653: INFO: model_training: Rank 0, Epoch 1833, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,654: INFO: model_training: Rank 0, Epoch 1833, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,655: INFO: model_training: Rank 0, Epoch 1833, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,657: INFO: model_training: Rank 0, Epoch 1833, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,658: INFO: model_training: Rank 0, Epoch 1834, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,659: INFO: model_training: Rank 0, Epoch 1834, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,660: INFO: model_training: Rank 0, Epoch 1834, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,661: INFO: model_training: Rank 0, Epoch 1834, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,662: INFO: model_training: Rank 0, Epoch 1834, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,663: INFO: model_training: Rank 0, Epoch 1835, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,664: INFO: model_training: Rank 0, Epoch 1835, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,665: INFO: model_training: Rank 0, Epoch 1835, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,666: INFO: model_training: Rank 0, Epoch 1835, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,667: INFO: model_training: Rank 0, Epoch 1835, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,669: INFO: model_training: Rank 0, Epoch 1836, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,670: INFO: model_training: Rank 0, Epoch 1836, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,671: INFO: model_training: Rank 0, Epoch 1836, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,672: INFO: model_training: Rank 0, Epoch 1836, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,673: INFO: model_training: Rank 0, Epoch 1836, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,674: INFO: model_training: Rank 0, Epoch 1837, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,675: INFO: model_training: Rank 0, Epoch 1837, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,676: INFO: model_training: Rank 0, Epoch 1837, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,677: INFO: model_training: Rank 0, Epoch 1837, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,678: INFO: model_training: Rank 0, Epoch 1837, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,679: INFO: model_training: Rank 0, Epoch 1838, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,680: INFO: model_training: Rank 0, Epoch 1838, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,681: INFO: model_training: Rank 0, Epoch 1838, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,683: INFO: model_training: Rank 0, Epoch 1838, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,684: INFO: model_training: Rank 0, Epoch 1838, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,685: INFO: model_training: Rank 0, Epoch 1839, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,686: INFO: model_training: Rank 0, Epoch 1839, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,687: INFO: model_training: Rank 0, Epoch 1839, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,689: INFO: model_training: Rank 0, Epoch 1839, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,690: INFO: model_training: Rank 0, Epoch 1839, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,691: INFO: model_training: Rank 0, Epoch 1840, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,692: INFO: model_training: Rank 0, Epoch 1840, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,693: INFO: model_training: Rank 0, Epoch 1840, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,694: INFO: model_training: Rank 0, Epoch 1840, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,695: INFO: model_training: Rank 0, Epoch 1840, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,696: INFO: model_training: Rank 0, Epoch 1841, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,697: INFO: model_training: Rank 0, Epoch 1841, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,698: INFO: model_training: Rank 0, Epoch 1841, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,699: INFO: model_training: Rank 0, Epoch 1841, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,700: INFO: model_training: Rank 0, Epoch 1841, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,701: INFO: model_training: Rank 0, Epoch 1842, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,702: INFO: model_training: Rank 0, Epoch 1842, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,703: INFO: model_training: Rank 0, Epoch 1842, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,704: INFO: model_training: Rank 0, Epoch 1842, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,705: INFO: model_training: Rank 0, Epoch 1842, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,706: INFO: model_training: Rank 0, Epoch 1843, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,707: INFO: model_training: Rank 0, Epoch 1843, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,709: INFO: model_training: Rank 0, Epoch 1843, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,711: INFO: model_training: Rank 0, Epoch 1843, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,712: INFO: model_training: Rank 0, Epoch 1843, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,713: INFO: model_training: Rank 0, Epoch 1844, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,714: INFO: model_training: Rank 0, Epoch 1844, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,715: INFO: model_training: Rank 0, Epoch 1844, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,716: INFO: model_training: Rank 0, Epoch 1844, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,717: INFO: model_training: Rank 0, Epoch 1844, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,718: INFO: model_training: Rank 0, Epoch 1845, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,719: INFO: model_training: Rank 0, Epoch 1845, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,720: INFO: model_training: Rank 0, Epoch 1845, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,721: INFO: model_training: Rank 0, Epoch 1845, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,722: INFO: model_training: Rank 0, Epoch 1845, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,724: INFO: model_training: Rank 0, Epoch 1846, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,725: INFO: model_training: Rank 0, Epoch 1846, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,726: INFO: model_training: Rank 0, Epoch 1846, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,727: INFO: model_training: Rank 0, Epoch 1846, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,728: INFO: model_training: Rank 0, Epoch 1846, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,729: INFO: model_training: Rank 0, Epoch 1847, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,730: INFO: model_training: Rank 0, Epoch 1847, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,731: INFO: model_training: Rank 0, Epoch 1847, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,732: INFO: model_training: Rank 0, Epoch 1847, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,734: INFO: model_training: Rank 0, Epoch 1847, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,736: INFO: model_training: Rank 0, Epoch 1848, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,738: INFO: model_training: Rank 0, Epoch 1848, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,740: INFO: model_training: Rank 0, Epoch 1848, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,742: INFO: model_training: Rank 0, Epoch 1848, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,744: INFO: model_training: Rank 0, Epoch 1848, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,746: INFO: model_training: Rank 0, Epoch 1849, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,747: INFO: model_training: Rank 0, Epoch 1849, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,749: INFO: model_training: Rank 0, Epoch 1849, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,750: INFO: model_training: Rank 0, Epoch 1849, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,752: INFO: model_training: Rank 0, Epoch 1849, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,753: INFO: model_training: Rank 0, Epoch 1850, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,755: INFO: model_training: Rank 0, Epoch 1850, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,756: INFO: model_training: Rank 0, Epoch 1850, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,757: INFO: model_training: Rank 0, Epoch 1850, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,759: INFO: model_training: Rank 0, Epoch 1850, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,761: INFO: model_training: Rank 0, Epoch 1851, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,762: INFO: model_training: Rank 0, Epoch 1851, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,764: INFO: model_training: Rank 0, Epoch 1851, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,765: INFO: model_training: Rank 0, Epoch 1851, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,766: INFO: model_training: Rank 0, Epoch 1851, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,768: INFO: model_training: Rank 0, Epoch 1852, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,769: INFO: model_training: Rank 0, Epoch 1852, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,770: INFO: model_training: Rank 0, Epoch 1852, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,771: INFO: model_training: Rank 0, Epoch 1852, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,772: INFO: model_training: Rank 0, Epoch 1852, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,773: INFO: model_training: Rank 0, Epoch 1853, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,774: INFO: model_training: Rank 0, Epoch 1853, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,776: INFO: model_training: Rank 0, Epoch 1853, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,777: INFO: model_training: Rank 0, Epoch 1853, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,778: INFO: model_training: Rank 0, Epoch 1853, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,780: INFO: model_training: Rank 0, Epoch 1854, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,780: INFO: model_training: Rank 0, Epoch 1854, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,781: INFO: model_training: Rank 0, Epoch 1854, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,783: INFO: model_training: Rank 0, Epoch 1854, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,784: INFO: model_training: Rank 0, Epoch 1854, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,786: INFO: model_training: Rank 0, Epoch 1855, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,788: INFO: model_training: Rank 0, Epoch 1855, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,789: INFO: model_training: Rank 0, Epoch 1855, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,790: INFO: model_training: Rank 0, Epoch 1855, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,791: INFO: model_training: Rank 0, Epoch 1855, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,792: INFO: model_training: Rank 0, Epoch 1856, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,794: INFO: model_training: Rank 0, Epoch 1856, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,795: INFO: model_training: Rank 0, Epoch 1856, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,796: INFO: model_training: Rank 0, Epoch 1856, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,797: INFO: model_training: Rank 0, Epoch 1856, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,798: INFO: model_training: Rank 0, Epoch 1857, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,799: INFO: model_training: Rank 0, Epoch 1857, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,800: INFO: model_training: Rank 0, Epoch 1857, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,801: INFO: model_training: Rank 0, Epoch 1857, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,803: INFO: model_training: Rank 0, Epoch 1857, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,804: INFO: model_training: Rank 0, Epoch 1858, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,805: INFO: model_training: Rank 0, Epoch 1858, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,805: INFO: model_training: Rank 0, Epoch 1858, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,806: INFO: model_training: Rank 0, Epoch 1858, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,807: INFO: model_training: Rank 0, Epoch 1858, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,809: INFO: model_training: Rank 0, Epoch 1859, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,810: INFO: model_training: Rank 0, Epoch 1859, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,811: INFO: model_training: Rank 0, Epoch 1859, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,812: INFO: model_training: Rank 0, Epoch 1859, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,813: INFO: model_training: Rank 0, Epoch 1859, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,814: INFO: model_training: Rank 0, Epoch 1860, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,815: INFO: model_training: Rank 0, Epoch 1860, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,816: INFO: model_training: Rank 0, Epoch 1860, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,817: INFO: model_training: Rank 0, Epoch 1860, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,819: INFO: model_training: Rank 0, Epoch 1860, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,820: INFO: model_training: Rank 0, Epoch 1861, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,821: INFO: model_training: Rank 0, Epoch 1861, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,822: INFO: model_training: Rank 0, Epoch 1861, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,823: INFO: model_training: Rank 0, Epoch 1861, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,824: INFO: model_training: Rank 0, Epoch 1861, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,825: INFO: model_training: Rank 0, Epoch 1862, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,826: INFO: model_training: Rank 0, Epoch 1862, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,827: INFO: model_training: Rank 0, Epoch 1862, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,828: INFO: model_training: Rank 0, Epoch 1862, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,829: INFO: model_training: Rank 0, Epoch 1862, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,830: INFO: model_training: Rank 0, Epoch 1863, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,831: INFO: model_training: Rank 0, Epoch 1863, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,832: INFO: model_training: Rank 0, Epoch 1863, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,833: INFO: model_training: Rank 0, Epoch 1863, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,834: INFO: model_training: Rank 0, Epoch 1863, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,835: INFO: model_training: Rank 0, Epoch 1864, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,836: INFO: model_training: Rank 0, Epoch 1864, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,837: INFO: model_training: Rank 0, Epoch 1864, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,839: INFO: model_training: Rank 0, Epoch 1864, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,840: INFO: model_training: Rank 0, Epoch 1864, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,841: INFO: model_training: Rank 0, Epoch 1865, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,842: INFO: model_training: Rank 0, Epoch 1865, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,843: INFO: model_training: Rank 0, Epoch 1865, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,844: INFO: model_training: Rank 0, Epoch 1865, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,845: INFO: model_training: Rank 0, Epoch 1865, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,846: INFO: model_training: Rank 0, Epoch 1866, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,847: INFO: model_training: Rank 0, Epoch 1866, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,849: INFO: model_training: Rank 0, Epoch 1866, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,850: INFO: model_training: Rank 0, Epoch 1866, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,851: INFO: model_training: Rank 0, Epoch 1866, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,852: INFO: model_training: Rank 0, Epoch 1867, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,853: INFO: model_training: Rank 0, Epoch 1867, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,854: INFO: model_training: Rank 0, Epoch 1867, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,855: INFO: model_training: Rank 0, Epoch 1867, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,855: INFO: model_training: Rank 0, Epoch 1867, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,856: INFO: model_training: Rank 0, Epoch 1868, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,858: INFO: model_training: Rank 0, Epoch 1868, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,859: INFO: model_training: Rank 0, Epoch 1868, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,860: INFO: model_training: Rank 0, Epoch 1868, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,861: INFO: model_training: Rank 0, Epoch 1868, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,862: INFO: model_training: Rank 0, Epoch 1869, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,864: INFO: model_training: Rank 0, Epoch 1869, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,865: INFO: model_training: Rank 0, Epoch 1869, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,866: INFO: model_training: Rank 0, Epoch 1869, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,868: INFO: model_training: Rank 0, Epoch 1869, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,870: INFO: model_training: Rank 0, Epoch 1870, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,871: INFO: model_training: Rank 0, Epoch 1870, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,873: INFO: model_training: Rank 0, Epoch 1870, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,874: INFO: model_training: Rank 0, Epoch 1870, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,875: INFO: model_training: Rank 0, Epoch 1870, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,876: INFO: model_training: Rank 0, Epoch 1871, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,877: INFO: model_training: Rank 0, Epoch 1871, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,878: INFO: model_training: Rank 0, Epoch 1871, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,879: INFO: model_training: Rank 0, Epoch 1871, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,880: INFO: model_training: Rank 0, Epoch 1871, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,881: INFO: model_training: Rank 0, Epoch 1872, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,882: INFO: model_training: Rank 0, Epoch 1872, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,883: INFO: model_training: Rank 0, Epoch 1872, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,884: INFO: model_training: Rank 0, Epoch 1872, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,885: INFO: model_training: Rank 0, Epoch 1872, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,886: INFO: model_training: Rank 0, Epoch 1873, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,887: INFO: model_training: Rank 0, Epoch 1873, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,888: INFO: model_training: Rank 0, Epoch 1873, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,889: INFO: model_training: Rank 0, Epoch 1873, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,890: INFO: model_training: Rank 0, Epoch 1873, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,891: INFO: model_training: Rank 0, Epoch 1874, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,892: INFO: model_training: Rank 0, Epoch 1874, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,893: INFO: model_training: Rank 0, Epoch 1874, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,894: INFO: model_training: Rank 0, Epoch 1874, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,895: INFO: model_training: Rank 0, Epoch 1874, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,896: INFO: model_training: Rank 0, Epoch 1875, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,897: INFO: model_training: Rank 0, Epoch 1875, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,898: INFO: model_training: Rank 0, Epoch 1875, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,900: INFO: model_training: Rank 0, Epoch 1875, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,901: INFO: model_training: Rank 0, Epoch 1875, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,902: INFO: model_training: Rank 0, Epoch 1876, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,903: INFO: model_training: Rank 0, Epoch 1876, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,904: INFO: model_training: Rank 0, Epoch 1876, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,905: INFO: model_training: Rank 0, Epoch 1876, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,906: INFO: model_training: Rank 0, Epoch 1876, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,907: INFO: model_training: Rank 0, Epoch 1877, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,909: INFO: model_training: Rank 0, Epoch 1877, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,909: INFO: model_training: Rank 0, Epoch 1877, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,910: INFO: model_training: Rank 0, Epoch 1877, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,911: INFO: model_training: Rank 0, Epoch 1877, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,912: INFO: model_training: Rank 0, Epoch 1878, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,914: INFO: model_training: Rank 0, Epoch 1878, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,915: INFO: model_training: Rank 0, Epoch 1878, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,916: INFO: model_training: Rank 0, Epoch 1878, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,917: INFO: model_training: Rank 0, Epoch 1878, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,918: INFO: model_training: Rank 0, Epoch 1879, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,919: INFO: model_training: Rank 0, Epoch 1879, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,920: INFO: model_training: Rank 0, Epoch 1879, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,921: INFO: model_training: Rank 0, Epoch 1879, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,922: INFO: model_training: Rank 0, Epoch 1879, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,923: INFO: model_training: Rank 0, Epoch 1880, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,924: INFO: model_training: Rank 0, Epoch 1880, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,924: INFO: model_training: Rank 0, Epoch 1880, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,925: INFO: model_training: Rank 0, Epoch 1880, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,927: INFO: model_training: Rank 0, Epoch 1880, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,928: INFO: model_training: Rank 0, Epoch 1881, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,929: INFO: model_training: Rank 0, Epoch 1881, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,930: INFO: model_training: Rank 0, Epoch 1881, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,931: INFO: model_training: Rank 0, Epoch 1881, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,932: INFO: model_training: Rank 0, Epoch 1881, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,933: INFO: model_training: Rank 0, Epoch 1882, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,934: INFO: model_training: Rank 0, Epoch 1882, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,935: INFO: model_training: Rank 0, Epoch 1882, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,936: INFO: model_training: Rank 0, Epoch 1882, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,937: INFO: model_training: Rank 0, Epoch 1882, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,939: INFO: model_training: Rank 0, Epoch 1883, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,940: INFO: model_training: Rank 0, Epoch 1883, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,941: INFO: model_training: Rank 0, Epoch 1883, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,942: INFO: model_training: Rank 0, Epoch 1883, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,943: INFO: model_training: Rank 0, Epoch 1883, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,944: INFO: model_training: Rank 0, Epoch 1884, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,945: INFO: model_training: Rank 0, Epoch 1884, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,946: INFO: model_training: Rank 0, Epoch 1884, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,947: INFO: model_training: Rank 0, Epoch 1884, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,948: INFO: model_training: Rank 0, Epoch 1884, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,949: INFO: model_training: Rank 0, Epoch 1885, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,951: INFO: model_training: Rank 0, Epoch 1885, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,952: INFO: model_training: Rank 0, Epoch 1885, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,953: INFO: model_training: Rank 0, Epoch 1885, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,954: INFO: model_training: Rank 0, Epoch 1885, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,955: INFO: model_training: Rank 0, Epoch 1886, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,956: INFO: model_training: Rank 0, Epoch 1886, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,957: INFO: model_training: Rank 0, Epoch 1886, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,958: INFO: model_training: Rank 0, Epoch 1886, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,959: INFO: model_training: Rank 0, Epoch 1886, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,960: INFO: model_training: Rank 0, Epoch 1887, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,961: INFO: model_training: Rank 0, Epoch 1887, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,962: INFO: model_training: Rank 0, Epoch 1887, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,963: INFO: model_training: Rank 0, Epoch 1887, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,964: INFO: model_training: Rank 0, Epoch 1887, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,965: INFO: model_training: Rank 0, Epoch 1888, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,966: INFO: model_training: Rank 0, Epoch 1888, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,967: INFO: model_training: Rank 0, Epoch 1888, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,968: INFO: model_training: Rank 0, Epoch 1888, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,969: INFO: model_training: Rank 0, Epoch 1888, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,970: INFO: model_training: Rank 0, Epoch 1889, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,971: INFO: model_training: Rank 0, Epoch 1889, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,973: INFO: model_training: Rank 0, Epoch 1889, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,974: INFO: model_training: Rank 0, Epoch 1889, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,975: INFO: model_training: Rank 0, Epoch 1889, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,976: INFO: model_training: Rank 0, Epoch 1890, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,977: INFO: model_training: Rank 0, Epoch 1890, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,978: INFO: model_training: Rank 0, Epoch 1890, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,979: INFO: model_training: Rank 0, Epoch 1890, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,980: INFO: model_training: Rank 0, Epoch 1890, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,981: INFO: model_training: Rank 0, Epoch 1891, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,983: INFO: model_training: Rank 0, Epoch 1891, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,984: INFO: model_training: Rank 0, Epoch 1891, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,985: INFO: model_training: Rank 0, Epoch 1891, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,986: INFO: model_training: Rank 0, Epoch 1891, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,987: INFO: model_training: Rank 0, Epoch 1892, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,988: INFO: model_training: Rank 0, Epoch 1892, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,989: INFO: model_training: Rank 0, Epoch 1892, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,990: INFO: model_training: Rank 0, Epoch 1892, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,991: INFO: model_training: Rank 0, Epoch 1892, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,992: INFO: model_training: Rank 0, Epoch 1893, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,993: INFO: model_training: Rank 0, Epoch 1893, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:37,994: INFO: model_training: Rank 0, Epoch 1893, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:37,995: INFO: model_training: Rank 0, Epoch 1893, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:37,997: INFO: model_training: Rank 0, Epoch 1893, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:37,998: INFO: model_training: Rank 0, Epoch 1894, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:37,999: INFO: model_training: Rank 0, Epoch 1894, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,000: INFO: model_training: Rank 0, Epoch 1894, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,001: INFO: model_training: Rank 0, Epoch 1894, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,002: INFO: model_training: Rank 0, Epoch 1894, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,003: INFO: model_training: Rank 0, Epoch 1895, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,003: INFO: model_training: Rank 0, Epoch 1895, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,004: INFO: model_training: Rank 0, Epoch 1895, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,005: INFO: model_training: Rank 0, Epoch 1895, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,006: INFO: model_training: Rank 0, Epoch 1895, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,007: INFO: model_training: Rank 0, Epoch 1896, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,009: INFO: model_training: Rank 0, Epoch 1896, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,010: INFO: model_training: Rank 0, Epoch 1896, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,011: INFO: model_training: Rank 0, Epoch 1896, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,012: INFO: model_training: Rank 0, Epoch 1896, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,013: INFO: model_training: Rank 0, Epoch 1897, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,014: INFO: model_training: Rank 0, Epoch 1897, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,015: INFO: model_training: Rank 0, Epoch 1897, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,016: INFO: model_training: Rank 0, Epoch 1897, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,017: INFO: model_training: Rank 0, Epoch 1897, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,018: INFO: model_training: Rank 0, Epoch 1898, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,020: INFO: model_training: Rank 0, Epoch 1898, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,021: INFO: model_training: Rank 0, Epoch 1898, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,022: INFO: model_training: Rank 0, Epoch 1898, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,023: INFO: model_training: Rank 0, Epoch 1898, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,024: INFO: model_training: Rank 0, Epoch 1899, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,025: INFO: model_training: Rank 0, Epoch 1899, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,026: INFO: model_training: Rank 0, Epoch 1899, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,027: INFO: model_training: Rank 0, Epoch 1899, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,028: INFO: model_training: Rank 0, Epoch 1899, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,029: INFO: model_training: Rank 0, Epoch 1900, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,030: INFO: model_training: Rank 0, Epoch 1900, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,031: INFO: model_training: Rank 0, Epoch 1900, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,032: INFO: model_training: Rank 0, Epoch 1900, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,033: INFO: model_training: Rank 0, Epoch 1900, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,034: INFO: model_training: Rank 0, Epoch 1901, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,035: INFO: model_training: Rank 0, Epoch 1901, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,036: INFO: model_training: Rank 0, Epoch 1901, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,037: INFO: model_training: Rank 0, Epoch 1901, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,039: INFO: model_training: Rank 0, Epoch 1901, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,040: INFO: model_training: Rank 0, Epoch 1902, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,041: INFO: model_training: Rank 0, Epoch 1902, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,042: INFO: model_training: Rank 0, Epoch 1902, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,043: INFO: model_training: Rank 0, Epoch 1902, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,044: INFO: model_training: Rank 0, Epoch 1902, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,045: INFO: model_training: Rank 0, Epoch 1903, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,046: INFO: model_training: Rank 0, Epoch 1903, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,048: INFO: model_training: Rank 0, Epoch 1903, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,049: INFO: model_training: Rank 0, Epoch 1903, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,050: INFO: model_training: Rank 0, Epoch 1903, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,051: INFO: model_training: Rank 0, Epoch 1904, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,052: INFO: model_training: Rank 0, Epoch 1904, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,053: INFO: model_training: Rank 0, Epoch 1904, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,054: INFO: model_training: Rank 0, Epoch 1904, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,055: INFO: model_training: Rank 0, Epoch 1904, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,056: INFO: model_training: Rank 0, Epoch 1905, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,057: INFO: model_training: Rank 0, Epoch 1905, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,058: INFO: model_training: Rank 0, Epoch 1905, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,059: INFO: model_training: Rank 0, Epoch 1905, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,060: INFO: model_training: Rank 0, Epoch 1905, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,061: INFO: model_training: Rank 0, Epoch 1906, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,062: INFO: model_training: Rank 0, Epoch 1906, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,064: INFO: model_training: Rank 0, Epoch 1906, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,065: INFO: model_training: Rank 0, Epoch 1906, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,066: INFO: model_training: Rank 0, Epoch 1906, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,067: INFO: model_training: Rank 0, Epoch 1907, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,068: INFO: model_training: Rank 0, Epoch 1907, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,069: INFO: model_training: Rank 0, Epoch 1907, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,070: INFO: model_training: Rank 0, Epoch 1907, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,071: INFO: model_training: Rank 0, Epoch 1907, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,072: INFO: model_training: Rank 0, Epoch 1908, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,073: INFO: model_training: Rank 0, Epoch 1908, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,074: INFO: model_training: Rank 0, Epoch 1908, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,075: INFO: model_training: Rank 0, Epoch 1908, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,076: INFO: model_training: Rank 0, Epoch 1908, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,077: INFO: model_training: Rank 0, Epoch 1909, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,079: INFO: model_training: Rank 0, Epoch 1909, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,080: INFO: model_training: Rank 0, Epoch 1909, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,082: INFO: model_training: Rank 0, Epoch 1909, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,084: INFO: model_training: Rank 0, Epoch 1909, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,087: INFO: model_training: Rank 0, Epoch 1910, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,089: INFO: model_training: Rank 0, Epoch 1910, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,090: INFO: model_training: Rank 0, Epoch 1910, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,091: INFO: model_training: Rank 0, Epoch 1910, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,092: INFO: model_training: Rank 0, Epoch 1910, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,094: INFO: model_training: Rank 0, Epoch 1911, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,095: INFO: model_training: Rank 0, Epoch 1911, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,096: INFO: model_training: Rank 0, Epoch 1911, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,097: INFO: model_training: Rank 0, Epoch 1911, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,098: INFO: model_training: Rank 0, Epoch 1911, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,099: INFO: model_training: Rank 0, Epoch 1912, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,100: INFO: model_training: Rank 0, Epoch 1912, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,101: INFO: model_training: Rank 0, Epoch 1912, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,103: INFO: model_training: Rank 0, Epoch 1912, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,103: INFO: model_training: Rank 0, Epoch 1912, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,104: INFO: model_training: Rank 0, Epoch 1913, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,105: INFO: model_training: Rank 0, Epoch 1913, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,106: INFO: model_training: Rank 0, Epoch 1913, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,107: INFO: model_training: Rank 0, Epoch 1913, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,109: INFO: model_training: Rank 0, Epoch 1913, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,110: INFO: model_training: Rank 0, Epoch 1914, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,111: INFO: model_training: Rank 0, Epoch 1914, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,112: INFO: model_training: Rank 0, Epoch 1914, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,113: INFO: model_training: Rank 0, Epoch 1914, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,114: INFO: model_training: Rank 0, Epoch 1914, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,115: INFO: model_training: Rank 0, Epoch 1915, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,116: INFO: model_training: Rank 0, Epoch 1915, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,117: INFO: model_training: Rank 0, Epoch 1915, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,118: INFO: model_training: Rank 0, Epoch 1915, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,119: INFO: model_training: Rank 0, Epoch 1915, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,120: INFO: model_training: Rank 0, Epoch 1916, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,122: INFO: model_training: Rank 0, Epoch 1916, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,123: INFO: model_training: Rank 0, Epoch 1916, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,124: INFO: model_training: Rank 0, Epoch 1916, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,125: INFO: model_training: Rank 0, Epoch 1916, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,127: INFO: model_training: Rank 0, Epoch 1917, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,127: INFO: model_training: Rank 0, Epoch 1917, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,129: INFO: model_training: Rank 0, Epoch 1917, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,130: INFO: model_training: Rank 0, Epoch 1917, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,131: INFO: model_training: Rank 0, Epoch 1917, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,132: INFO: model_training: Rank 0, Epoch 1918, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,133: INFO: model_training: Rank 0, Epoch 1918, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,134: INFO: model_training: Rank 0, Epoch 1918, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,135: INFO: model_training: Rank 0, Epoch 1918, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,136: INFO: model_training: Rank 0, Epoch 1918, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,137: INFO: model_training: Rank 0, Epoch 1919, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,137: INFO: model_training: Rank 0, Epoch 1919, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,139: INFO: model_training: Rank 0, Epoch 1919, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,140: INFO: model_training: Rank 0, Epoch 1919, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,141: INFO: model_training: Rank 0, Epoch 1919, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,142: INFO: model_training: Rank 0, Epoch 1920, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,143: INFO: model_training: Rank 0, Epoch 1920, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,144: INFO: model_training: Rank 0, Epoch 1920, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,145: INFO: model_training: Rank 0, Epoch 1920, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,146: INFO: model_training: Rank 0, Epoch 1920, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,147: INFO: model_training: Rank 0, Epoch 1921, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,148: INFO: model_training: Rank 0, Epoch 1921, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,149: INFO: model_training: Rank 0, Epoch 1921, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,150: INFO: model_training: Rank 0, Epoch 1921, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,151: INFO: model_training: Rank 0, Epoch 1921, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,152: INFO: model_training: Rank 0, Epoch 1922, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,153: INFO: model_training: Rank 0, Epoch 1922, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,154: INFO: model_training: Rank 0, Epoch 1922, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,155: INFO: model_training: Rank 0, Epoch 1922, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,156: INFO: model_training: Rank 0, Epoch 1922, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,157: INFO: model_training: Rank 0, Epoch 1923, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,158: INFO: model_training: Rank 0, Epoch 1923, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,160: INFO: model_training: Rank 0, Epoch 1923, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,161: INFO: model_training: Rank 0, Epoch 1923, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,162: INFO: model_training: Rank 0, Epoch 1923, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,163: INFO: model_training: Rank 0, Epoch 1924, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,164: INFO: model_training: Rank 0, Epoch 1924, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,165: INFO: model_training: Rank 0, Epoch 1924, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,166: INFO: model_training: Rank 0, Epoch 1924, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,167: INFO: model_training: Rank 0, Epoch 1924, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,168: INFO: model_training: Rank 0, Epoch 1925, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,169: INFO: model_training: Rank 0, Epoch 1925, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,170: INFO: model_training: Rank 0, Epoch 1925, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,171: INFO: model_training: Rank 0, Epoch 1925, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,172: INFO: model_training: Rank 0, Epoch 1925, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,173: INFO: model_training: Rank 0, Epoch 1926, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,174: INFO: model_training: Rank 0, Epoch 1926, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,174: INFO: model_training: Rank 0, Epoch 1926, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,175: INFO: model_training: Rank 0, Epoch 1926, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,176: INFO: model_training: Rank 0, Epoch 1926, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,177: INFO: model_training: Rank 0, Epoch 1927, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,179: INFO: model_training: Rank 0, Epoch 1927, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,179: INFO: model_training: Rank 0, Epoch 1927, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,180: INFO: model_training: Rank 0, Epoch 1927, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,181: INFO: model_training: Rank 0, Epoch 1927, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,182: INFO: model_training: Rank 0, Epoch 1928, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,183: INFO: model_training: Rank 0, Epoch 1928, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,184: INFO: model_training: Rank 0, Epoch 1928, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,185: INFO: model_training: Rank 0, Epoch 1928, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,186: INFO: model_training: Rank 0, Epoch 1928, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,187: INFO: model_training: Rank 0, Epoch 1929, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,188: INFO: model_training: Rank 0, Epoch 1929, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,190: INFO: model_training: Rank 0, Epoch 1929, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,191: INFO: model_training: Rank 0, Epoch 1929, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,192: INFO: model_training: Rank 0, Epoch 1929, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,193: INFO: model_training: Rank 0, Epoch 1930, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,194: INFO: model_training: Rank 0, Epoch 1930, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,195: INFO: model_training: Rank 0, Epoch 1930, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,196: INFO: model_training: Rank 0, Epoch 1930, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,197: INFO: model_training: Rank 0, Epoch 1930, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,198: INFO: model_training: Rank 0, Epoch 1931, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,199: INFO: model_training: Rank 0, Epoch 1931, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,201: INFO: model_training: Rank 0, Epoch 1931, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,202: INFO: model_training: Rank 0, Epoch 1931, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,203: INFO: model_training: Rank 0, Epoch 1931, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,204: INFO: model_training: Rank 0, Epoch 1932, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,205: INFO: model_training: Rank 0, Epoch 1932, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,206: INFO: model_training: Rank 0, Epoch 1932, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,207: INFO: model_training: Rank 0, Epoch 1932, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,208: INFO: model_training: Rank 0, Epoch 1932, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,210: INFO: model_training: Rank 0, Epoch 1933, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,210: INFO: model_training: Rank 0, Epoch 1933, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,211: INFO: model_training: Rank 0, Epoch 1933, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,212: INFO: model_training: Rank 0, Epoch 1933, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,213: INFO: model_training: Rank 0, Epoch 1933, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,214: INFO: model_training: Rank 0, Epoch 1934, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,215: INFO: model_training: Rank 0, Epoch 1934, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,216: INFO: model_training: Rank 0, Epoch 1934, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,217: INFO: model_training: Rank 0, Epoch 1934, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,219: INFO: model_training: Rank 0, Epoch 1934, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,220: INFO: model_training: Rank 0, Epoch 1935, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,221: INFO: model_training: Rank 0, Epoch 1935, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,221: INFO: model_training: Rank 0, Epoch 1935, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,222: INFO: model_training: Rank 0, Epoch 1935, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,223: INFO: model_training: Rank 0, Epoch 1935, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,224: INFO: model_training: Rank 0, Epoch 1936, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,225: INFO: model_training: Rank 0, Epoch 1936, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,226: INFO: model_training: Rank 0, Epoch 1936, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,228: INFO: model_training: Rank 0, Epoch 1936, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,229: INFO: model_training: Rank 0, Epoch 1936, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,230: INFO: model_training: Rank 0, Epoch 1937, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,231: INFO: model_training: Rank 0, Epoch 1937, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,232: INFO: model_training: Rank 0, Epoch 1937, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,233: INFO: model_training: Rank 0, Epoch 1937, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,234: INFO: model_training: Rank 0, Epoch 1937, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,235: INFO: model_training: Rank 0, Epoch 1938, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,236: INFO: model_training: Rank 0, Epoch 1938, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,237: INFO: model_training: Rank 0, Epoch 1938, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,238: INFO: model_training: Rank 0, Epoch 1938, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,239: INFO: model_training: Rank 0, Epoch 1938, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,240: INFO: model_training: Rank 0, Epoch 1939, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,241: INFO: model_training: Rank 0, Epoch 1939, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,242: INFO: model_training: Rank 0, Epoch 1939, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,243: INFO: model_training: Rank 0, Epoch 1939, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,244: INFO: model_training: Rank 0, Epoch 1939, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,245: INFO: model_training: Rank 0, Epoch 1940, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,246: INFO: model_training: Rank 0, Epoch 1940, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,247: INFO: model_training: Rank 0, Epoch 1940, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,248: INFO: model_training: Rank 0, Epoch 1940, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,249: INFO: model_training: Rank 0, Epoch 1940, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,250: INFO: model_training: Rank 0, Epoch 1941, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,251: INFO: model_training: Rank 0, Epoch 1941, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,252: INFO: model_training: Rank 0, Epoch 1941, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,253: INFO: model_training: Rank 0, Epoch 1941, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,254: INFO: model_training: Rank 0, Epoch 1941, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,255: INFO: model_training: Rank 0, Epoch 1942, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,256: INFO: model_training: Rank 0, Epoch 1942, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,257: INFO: model_training: Rank 0, Epoch 1942, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,258: INFO: model_training: Rank 0, Epoch 1942, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,259: INFO: model_training: Rank 0, Epoch 1942, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,260: INFO: model_training: Rank 0, Epoch 1943, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,261: INFO: model_training: Rank 0, Epoch 1943, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,263: INFO: model_training: Rank 0, Epoch 1943, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,264: INFO: model_training: Rank 0, Epoch 1943, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,265: INFO: model_training: Rank 0, Epoch 1943, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,266: INFO: model_training: Rank 0, Epoch 1944, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,267: INFO: model_training: Rank 0, Epoch 1944, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,268: INFO: model_training: Rank 0, Epoch 1944, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,269: INFO: model_training: Rank 0, Epoch 1944, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,270: INFO: model_training: Rank 0, Epoch 1944, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,271: INFO: model_training: Rank 0, Epoch 1945, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,272: INFO: model_training: Rank 0, Epoch 1945, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,273: INFO: model_training: Rank 0, Epoch 1945, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,275: INFO: model_training: Rank 0, Epoch 1945, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,276: INFO: model_training: Rank 0, Epoch 1945, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,277: INFO: model_training: Rank 0, Epoch 1946, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,277: INFO: model_training: Rank 0, Epoch 1946, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,279: INFO: model_training: Rank 0, Epoch 1946, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,280: INFO: model_training: Rank 0, Epoch 1946, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,282: INFO: model_training: Rank 0, Epoch 1946, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,283: INFO: model_training: Rank 0, Epoch 1947, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,285: INFO: model_training: Rank 0, Epoch 1947, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,286: INFO: model_training: Rank 0, Epoch 1947, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,287: INFO: model_training: Rank 0, Epoch 1947, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,288: INFO: model_training: Rank 0, Epoch 1947, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,289: INFO: model_training: Rank 0, Epoch 1948, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,290: INFO: model_training: Rank 0, Epoch 1948, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,291: INFO: model_training: Rank 0, Epoch 1948, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,292: INFO: model_training: Rank 0, Epoch 1948, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,293: INFO: model_training: Rank 0, Epoch 1948, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,294: INFO: model_training: Rank 0, Epoch 1949, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,295: INFO: model_training: Rank 0, Epoch 1949, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,296: INFO: model_training: Rank 0, Epoch 1949, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,297: INFO: model_training: Rank 0, Epoch 1949, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,299: INFO: model_training: Rank 0, Epoch 1949, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,300: INFO: model_training: Rank 0, Epoch 1950, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,301: INFO: model_training: Rank 0, Epoch 1950, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,302: INFO: model_training: Rank 0, Epoch 1950, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,303: INFO: model_training: Rank 0, Epoch 1950, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,304: INFO: model_training: Rank 0, Epoch 1950, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,305: INFO: model_training: Rank 0, Epoch 1951, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,306: INFO: model_training: Rank 0, Epoch 1951, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,307: INFO: model_training: Rank 0, Epoch 1951, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,308: INFO: model_training: Rank 0, Epoch 1951, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,309: INFO: model_training: Rank 0, Epoch 1951, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,310: INFO: model_training: Rank 0, Epoch 1952, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,311: INFO: model_training: Rank 0, Epoch 1952, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,312: INFO: model_training: Rank 0, Epoch 1952, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,313: INFO: model_training: Rank 0, Epoch 1952, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,314: INFO: model_training: Rank 0, Epoch 1952, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,315: INFO: model_training: Rank 0, Epoch 1953, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,316: INFO: model_training: Rank 0, Epoch 1953, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,318: INFO: model_training: Rank 0, Epoch 1953, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,319: INFO: model_training: Rank 0, Epoch 1953, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,320: INFO: model_training: Rank 0, Epoch 1953, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,321: INFO: model_training: Rank 0, Epoch 1954, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,322: INFO: model_training: Rank 0, Epoch 1954, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,323: INFO: model_training: Rank 0, Epoch 1954, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,325: INFO: model_training: Rank 0, Epoch 1954, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,326: INFO: model_training: Rank 0, Epoch 1954, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,328: INFO: model_training: Rank 0, Epoch 1955, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,329: INFO: model_training: Rank 0, Epoch 1955, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,330: INFO: model_training: Rank 0, Epoch 1955, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,332: INFO: model_training: Rank 0, Epoch 1955, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,334: INFO: model_training: Rank 0, Epoch 1955, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,335: INFO: model_training: Rank 0, Epoch 1956, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,336: INFO: model_training: Rank 0, Epoch 1956, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,337: INFO: model_training: Rank 0, Epoch 1956, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,339: INFO: model_training: Rank 0, Epoch 1956, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,340: INFO: model_training: Rank 0, Epoch 1956, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,341: INFO: model_training: Rank 0, Epoch 1957, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,342: INFO: model_training: Rank 0, Epoch 1957, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,343: INFO: model_training: Rank 0, Epoch 1957, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,344: INFO: model_training: Rank 0, Epoch 1957, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,345: INFO: model_training: Rank 0, Epoch 1957, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,346: INFO: model_training: Rank 0, Epoch 1958, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,347: INFO: model_training: Rank 0, Epoch 1958, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,349: INFO: model_training: Rank 0, Epoch 1958, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,350: INFO: model_training: Rank 0, Epoch 1958, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,351: INFO: model_training: Rank 0, Epoch 1958, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,352: INFO: model_training: Rank 0, Epoch 1959, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,354: INFO: model_training: Rank 0, Epoch 1959, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,355: INFO: model_training: Rank 0, Epoch 1959, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,357: INFO: model_training: Rank 0, Epoch 1959, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,358: INFO: model_training: Rank 0, Epoch 1959, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,361: INFO: model_training: Rank 0, Epoch 1960, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,362: INFO: model_training: Rank 0, Epoch 1960, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,364: INFO: model_training: Rank 0, Epoch 1960, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,366: INFO: model_training: Rank 0, Epoch 1960, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,367: INFO: model_training: Rank 0, Epoch 1960, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,369: INFO: model_training: Rank 0, Epoch 1961, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,370: INFO: model_training: Rank 0, Epoch 1961, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,372: INFO: model_training: Rank 0, Epoch 1961, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,373: INFO: model_training: Rank 0, Epoch 1961, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,375: INFO: model_training: Rank 0, Epoch 1961, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,376: INFO: model_training: Rank 0, Epoch 1962, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,379: INFO: model_training: Rank 0, Epoch 1962, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,380: INFO: model_training: Rank 0, Epoch 1962, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,382: INFO: model_training: Rank 0, Epoch 1962, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,384: INFO: model_training: Rank 0, Epoch 1962, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,385: INFO: model_training: Rank 0, Epoch 1963, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,386: INFO: model_training: Rank 0, Epoch 1963, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,388: INFO: model_training: Rank 0, Epoch 1963, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,389: INFO: model_training: Rank 0, Epoch 1963, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,390: INFO: model_training: Rank 0, Epoch 1963, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,392: INFO: model_training: Rank 0, Epoch 1964, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,393: INFO: model_training: Rank 0, Epoch 1964, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,394: INFO: model_training: Rank 0, Epoch 1964, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,395: INFO: model_training: Rank 0, Epoch 1964, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,397: INFO: model_training: Rank 0, Epoch 1964, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,399: INFO: model_training: Rank 0, Epoch 1965, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,400: INFO: model_training: Rank 0, Epoch 1965, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,402: INFO: model_training: Rank 0, Epoch 1965, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,403: INFO: model_training: Rank 0, Epoch 1965, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,404: INFO: model_training: Rank 0, Epoch 1965, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,406: INFO: model_training: Rank 0, Epoch 1966, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,409: INFO: model_training: Rank 0, Epoch 1966, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,411: INFO: model_training: Rank 0, Epoch 1966, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,413: INFO: model_training: Rank 0, Epoch 1966, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,415: INFO: model_training: Rank 0, Epoch 1966, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,418: INFO: model_training: Rank 0, Epoch 1967, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,419: INFO: model_training: Rank 0, Epoch 1967, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,421: INFO: model_training: Rank 0, Epoch 1967, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,422: INFO: model_training: Rank 0, Epoch 1967, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,424: INFO: model_training: Rank 0, Epoch 1967, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,425: INFO: model_training: Rank 0, Epoch 1968, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,427: INFO: model_training: Rank 0, Epoch 1968, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,428: INFO: model_training: Rank 0, Epoch 1968, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,430: INFO: model_training: Rank 0, Epoch 1968, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,432: INFO: model_training: Rank 0, Epoch 1968, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,433: INFO: model_training: Rank 0, Epoch 1969, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,435: INFO: model_training: Rank 0, Epoch 1969, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,436: INFO: model_training: Rank 0, Epoch 1969, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,438: INFO: model_training: Rank 0, Epoch 1969, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,439: INFO: model_training: Rank 0, Epoch 1969, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,440: INFO: model_training: Rank 0, Epoch 1970, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,442: INFO: model_training: Rank 0, Epoch 1970, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,443: INFO: model_training: Rank 0, Epoch 1970, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,444: INFO: model_training: Rank 0, Epoch 1970, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,445: INFO: model_training: Rank 0, Epoch 1970, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,447: INFO: model_training: Rank 0, Epoch 1971, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,448: INFO: model_training: Rank 0, Epoch 1971, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,449: INFO: model_training: Rank 0, Epoch 1971, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,450: INFO: model_training: Rank 0, Epoch 1971, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,452: INFO: model_training: Rank 0, Epoch 1971, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,453: INFO: model_training: Rank 0, Epoch 1972, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,454: INFO: model_training: Rank 0, Epoch 1972, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,456: INFO: model_training: Rank 0, Epoch 1972, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,457: INFO: model_training: Rank 0, Epoch 1972, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,459: INFO: model_training: Rank 0, Epoch 1972, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,460: INFO: model_training: Rank 0, Epoch 1973, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,462: INFO: model_training: Rank 0, Epoch 1973, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,464: INFO: model_training: Rank 0, Epoch 1973, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,466: INFO: model_training: Rank 0, Epoch 1973, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,467: INFO: model_training: Rank 0, Epoch 1973, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,469: INFO: model_training: Rank 0, Epoch 1974, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,471: INFO: model_training: Rank 0, Epoch 1974, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,472: INFO: model_training: Rank 0, Epoch 1974, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,473: INFO: model_training: Rank 0, Epoch 1974, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,474: INFO: model_training: Rank 0, Epoch 1974, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,475: INFO: model_training: Rank 0, Epoch 1975, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,476: INFO: model_training: Rank 0, Epoch 1975, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,478: INFO: model_training: Rank 0, Epoch 1975, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,480: INFO: model_training: Rank 0, Epoch 1975, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,481: INFO: model_training: Rank 0, Epoch 1975, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,482: INFO: model_training: Rank 0, Epoch 1976, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,484: INFO: model_training: Rank 0, Epoch 1976, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,485: INFO: model_training: Rank 0, Epoch 1976, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,486: INFO: model_training: Rank 0, Epoch 1976, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,487: INFO: model_training: Rank 0, Epoch 1976, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,489: INFO: model_training: Rank 0, Epoch 1977, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,490: INFO: model_training: Rank 0, Epoch 1977, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,491: INFO: model_training: Rank 0, Epoch 1977, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,493: INFO: model_training: Rank 0, Epoch 1977, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,494: INFO: model_training: Rank 0, Epoch 1977, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,495: INFO: model_training: Rank 0, Epoch 1978, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,497: INFO: model_training: Rank 0, Epoch 1978, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,498: INFO: model_training: Rank 0, Epoch 1978, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,499: INFO: model_training: Rank 0, Epoch 1978, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,501: INFO: model_training: Rank 0, Epoch 1978, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,502: INFO: model_training: Rank 0, Epoch 1979, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,503: INFO: model_training: Rank 0, Epoch 1979, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,504: INFO: model_training: Rank 0, Epoch 1979, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,506: INFO: model_training: Rank 0, Epoch 1979, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,507: INFO: model_training: Rank 0, Epoch 1979, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,509: INFO: model_training: Rank 0, Epoch 1980, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,510: INFO: model_training: Rank 0, Epoch 1980, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,512: INFO: model_training: Rank 0, Epoch 1980, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,514: INFO: model_training: Rank 0, Epoch 1980, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,516: INFO: model_training: Rank 0, Epoch 1980, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,518: INFO: model_training: Rank 0, Epoch 1981, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,519: INFO: model_training: Rank 0, Epoch 1981, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,520: INFO: model_training: Rank 0, Epoch 1981, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,521: INFO: model_training: Rank 0, Epoch 1981, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,522: INFO: model_training: Rank 0, Epoch 1981, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,524: INFO: model_training: Rank 0, Epoch 1982, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,525: INFO: model_training: Rank 0, Epoch 1982, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,527: INFO: model_training: Rank 0, Epoch 1982, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,528: INFO: model_training: Rank 0, Epoch 1982, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,529: INFO: model_training: Rank 0, Epoch 1982, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,531: INFO: model_training: Rank 0, Epoch 1983, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,532: INFO: model_training: Rank 0, Epoch 1983, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,533: INFO: model_training: Rank 0, Epoch 1983, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,534: INFO: model_training: Rank 0, Epoch 1983, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,535: INFO: model_training: Rank 0, Epoch 1983, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,536: INFO: model_training: Rank 0, Epoch 1984, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,537: INFO: model_training: Rank 0, Epoch 1984, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,538: INFO: model_training: Rank 0, Epoch 1984, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,539: INFO: model_training: Rank 0, Epoch 1984, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,540: INFO: model_training: Rank 0, Epoch 1984, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,541: INFO: model_training: Rank 0, Epoch 1985, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,542: INFO: model_training: Rank 0, Epoch 1985, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,544: INFO: model_training: Rank 0, Epoch 1985, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,545: INFO: model_training: Rank 0, Epoch 1985, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,546: INFO: model_training: Rank 0, Epoch 1985, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,547: INFO: model_training: Rank 0, Epoch 1986, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,548: INFO: model_training: Rank 0, Epoch 1986, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,549: INFO: model_training: Rank 0, Epoch 1986, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,551: INFO: model_training: Rank 0, Epoch 1986, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,552: INFO: model_training: Rank 0, Epoch 1986, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,553: INFO: model_training: Rank 0, Epoch 1987, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,554: INFO: model_training: Rank 0, Epoch 1987, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,555: INFO: model_training: Rank 0, Epoch 1987, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,556: INFO: model_training: Rank 0, Epoch 1987, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,558: INFO: model_training: Rank 0, Epoch 1987, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,559: INFO: model_training: Rank 0, Epoch 1988, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,560: INFO: model_training: Rank 0, Epoch 1988, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,561: INFO: model_training: Rank 0, Epoch 1988, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,562: INFO: model_training: Rank 0, Epoch 1988, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,563: INFO: model_training: Rank 0, Epoch 1988, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,565: INFO: model_training: Rank 0, Epoch 1989, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,566: INFO: model_training: Rank 0, Epoch 1989, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,567: INFO: model_training: Rank 0, Epoch 1989, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,568: INFO: model_training: Rank 0, Epoch 1989, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,569: INFO: model_training: Rank 0, Epoch 1989, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,570: INFO: model_training: Rank 0, Epoch 1990, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,571: INFO: model_training: Rank 0, Epoch 1990, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,572: INFO: model_training: Rank 0, Epoch 1990, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,573: INFO: model_training: Rank 0, Epoch 1990, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,574: INFO: model_training: Rank 0, Epoch 1990, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,575: INFO: model_training: Rank 0, Epoch 1991, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,576: INFO: model_training: Rank 0, Epoch 1991, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,577: INFO: model_training: Rank 0, Epoch 1991, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,578: INFO: model_training: Rank 0, Epoch 1991, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,580: INFO: model_training: Rank 0, Epoch 1991, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,580: INFO: model_training: Rank 0, Epoch 1992, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,581: INFO: model_training: Rank 0, Epoch 1992, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,582: INFO: model_training: Rank 0, Epoch 1992, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,583: INFO: model_training: Rank 0, Epoch 1992, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,584: INFO: model_training: Rank 0, Epoch 1992, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,585: INFO: model_training: Rank 0, Epoch 1993, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,587: INFO: model_training: Rank 0, Epoch 1993, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,587: INFO: model_training: Rank 0, Epoch 1993, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,589: INFO: model_training: Rank 0, Epoch 1993, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,590: INFO: model_training: Rank 0, Epoch 1993, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,591: INFO: model_training: Rank 0, Epoch 1994, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,592: INFO: model_training: Rank 0, Epoch 1994, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,593: INFO: model_training: Rank 0, Epoch 1994, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,594: INFO: model_training: Rank 0, Epoch 1994, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,595: INFO: model_training: Rank 0, Epoch 1994, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,596: INFO: model_training: Rank 0, Epoch 1995, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,597: INFO: model_training: Rank 0, Epoch 1995, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,599: INFO: model_training: Rank 0, Epoch 1995, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,600: INFO: model_training: Rank 0, Epoch 1995, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,601: INFO: model_training: Rank 0, Epoch 1995, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,602: INFO: model_training: Rank 0, Epoch 1996, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,603: INFO: model_training: Rank 0, Epoch 1996, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,604: INFO: model_training: Rank 0, Epoch 1996, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,605: INFO: model_training: Rank 0, Epoch 1996, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,606: INFO: model_training: Rank 0, Epoch 1996, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,607: INFO: model_training: Rank 0, Epoch 1997, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,609: INFO: model_training: Rank 0, Epoch 1997, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,610: INFO: model_training: Rank 0, Epoch 1997, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,611: INFO: model_training: Rank 0, Epoch 1997, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,612: INFO: model_training: Rank 0, Epoch 1997, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,614: INFO: model_training: Rank 0, Epoch 1998, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,615: INFO: model_training: Rank 0, Epoch 1998, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,616: INFO: model_training: Rank 0, Epoch 1998, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,617: INFO: model_training: Rank 0, Epoch 1998, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,619: INFO: model_training: Rank 0, Epoch 1998, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,620: INFO: model_training: Rank 0, Epoch 1999, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,621: INFO: model_training: Rank 0, Epoch 1999, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,622: INFO: model_training: Rank 0, Epoch 1999, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,623: INFO: model_training: Rank 0, Epoch 1999, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,624: INFO: model_training: Rank 0, Epoch 1999, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,625: INFO: model_training: Rank 0, Epoch 2000, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,626: INFO: model_training: Rank 0, Epoch 2000, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,627: INFO: model_training: Rank 0, Epoch 2000, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,628: INFO: model_training: Rank 0, Epoch 2000, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,629: INFO: model_training: Rank 0, Epoch 2000, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,631: INFO: model_training: Rank 0, Epoch 2001, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,632: INFO: model_training: Rank 0, Epoch 2001, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,633: INFO: model_training: Rank 0, Epoch 2001, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,635: INFO: model_training: Rank 0, Epoch 2001, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,636: INFO: model_training: Rank 0, Epoch 2001, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,637: INFO: model_training: Rank 0, Epoch 2002, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,638: INFO: model_training: Rank 0, Epoch 2002, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,639: INFO: model_training: Rank 0, Epoch 2002, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,640: INFO: model_training: Rank 0, Epoch 2002, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,642: INFO: model_training: Rank 0, Epoch 2002, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,643: INFO: model_training: Rank 0, Epoch 2003, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,644: INFO: model_training: Rank 0, Epoch 2003, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,645: INFO: model_training: Rank 0, Epoch 2003, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,646: INFO: model_training: Rank 0, Epoch 2003, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,647: INFO: model_training: Rank 0, Epoch 2003, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,649: INFO: model_training: Rank 0, Epoch 2004, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,650: INFO: model_training: Rank 0, Epoch 2004, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,651: INFO: model_training: Rank 0, Epoch 2004, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,652: INFO: model_training: Rank 0, Epoch 2004, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,653: INFO: model_training: Rank 0, Epoch 2004, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,654: INFO: model_training: Rank 0, Epoch 2005, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,655: INFO: model_training: Rank 0, Epoch 2005, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,656: INFO: model_training: Rank 0, Epoch 2005, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,658: INFO: model_training: Rank 0, Epoch 2005, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,659: INFO: model_training: Rank 0, Epoch 2005, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,660: INFO: model_training: Rank 0, Epoch 2006, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,661: INFO: model_training: Rank 0, Epoch 2006, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,663: INFO: model_training: Rank 0, Epoch 2006, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,664: INFO: model_training: Rank 0, Epoch 2006, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,665: INFO: model_training: Rank 0, Epoch 2006, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,667: INFO: model_training: Rank 0, Epoch 2007, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,668: INFO: model_training: Rank 0, Epoch 2007, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,670: INFO: model_training: Rank 0, Epoch 2007, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,671: INFO: model_training: Rank 0, Epoch 2007, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,674: INFO: model_training: Rank 0, Epoch 2007, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,676: INFO: model_training: Rank 0, Epoch 2008, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,678: INFO: model_training: Rank 0, Epoch 2008, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,680: INFO: model_training: Rank 0, Epoch 2008, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,681: INFO: model_training: Rank 0, Epoch 2008, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,682: INFO: model_training: Rank 0, Epoch 2008, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,684: INFO: model_training: Rank 0, Epoch 2009, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,685: INFO: model_training: Rank 0, Epoch 2009, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,686: INFO: model_training: Rank 0, Epoch 2009, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,688: INFO: model_training: Rank 0, Epoch 2009, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,689: INFO: model_training: Rank 0, Epoch 2009, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,691: INFO: model_training: Rank 0, Epoch 2010, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,692: INFO: model_training: Rank 0, Epoch 2010, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,693: INFO: model_training: Rank 0, Epoch 2010, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,694: INFO: model_training: Rank 0, Epoch 2010, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,695: INFO: model_training: Rank 0, Epoch 2010, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,696: INFO: model_training: Rank 0, Epoch 2011, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,697: INFO: model_training: Rank 0, Epoch 2011, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,699: INFO: model_training: Rank 0, Epoch 2011, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,701: INFO: model_training: Rank 0, Epoch 2011, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,703: INFO: model_training: Rank 0, Epoch 2011, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,705: INFO: model_training: Rank 0, Epoch 2012, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,706: INFO: model_training: Rank 0, Epoch 2012, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,708: INFO: model_training: Rank 0, Epoch 2012, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,710: INFO: model_training: Rank 0, Epoch 2012, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,711: INFO: model_training: Rank 0, Epoch 2012, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,713: INFO: model_training: Rank 0, Epoch 2013, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,715: INFO: model_training: Rank 0, Epoch 2013, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,717: INFO: model_training: Rank 0, Epoch 2013, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,719: INFO: model_training: Rank 0, Epoch 2013, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,721: INFO: model_training: Rank 0, Epoch 2013, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,722: INFO: model_training: Rank 0, Epoch 2014, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,723: INFO: model_training: Rank 0, Epoch 2014, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,725: INFO: model_training: Rank 0, Epoch 2014, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,727: INFO: model_training: Rank 0, Epoch 2014, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,729: INFO: model_training: Rank 0, Epoch 2014, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,731: INFO: model_training: Rank 0, Epoch 2015, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,733: INFO: model_training: Rank 0, Epoch 2015, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,734: INFO: model_training: Rank 0, Epoch 2015, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,736: INFO: model_training: Rank 0, Epoch 2015, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,738: INFO: model_training: Rank 0, Epoch 2015, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,740: INFO: model_training: Rank 0, Epoch 2016, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,742: INFO: model_training: Rank 0, Epoch 2016, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,743: INFO: model_training: Rank 0, Epoch 2016, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,745: INFO: model_training: Rank 0, Epoch 2016, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,746: INFO: model_training: Rank 0, Epoch 2016, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,747: INFO: model_training: Rank 0, Epoch 2017, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,749: INFO: model_training: Rank 0, Epoch 2017, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,750: INFO: model_training: Rank 0, Epoch 2017, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,752: INFO: model_training: Rank 0, Epoch 2017, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,753: INFO: model_training: Rank 0, Epoch 2017, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,755: INFO: model_training: Rank 0, Epoch 2018, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,757: INFO: model_training: Rank 0, Epoch 2018, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,759: INFO: model_training: Rank 0, Epoch 2018, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,760: INFO: model_training: Rank 0, Epoch 2018, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,762: INFO: model_training: Rank 0, Epoch 2018, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,763: INFO: model_training: Rank 0, Epoch 2019, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,764: INFO: model_training: Rank 0, Epoch 2019, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,766: INFO: model_training: Rank 0, Epoch 2019, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,767: INFO: model_training: Rank 0, Epoch 2019, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,769: INFO: model_training: Rank 0, Epoch 2019, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,770: INFO: model_training: Rank 0, Epoch 2020, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,771: INFO: model_training: Rank 0, Epoch 2020, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,773: INFO: model_training: Rank 0, Epoch 2020, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,775: INFO: model_training: Rank 0, Epoch 2020, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,776: INFO: model_training: Rank 0, Epoch 2020, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,778: INFO: model_training: Rank 0, Epoch 2021, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,779: INFO: model_training: Rank 0, Epoch 2021, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,781: INFO: model_training: Rank 0, Epoch 2021, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,782: INFO: model_training: Rank 0, Epoch 2021, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,784: INFO: model_training: Rank 0, Epoch 2021, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,785: INFO: model_training: Rank 0, Epoch 2022, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,786: INFO: model_training: Rank 0, Epoch 2022, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,788: INFO: model_training: Rank 0, Epoch 2022, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,790: INFO: model_training: Rank 0, Epoch 2022, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,791: INFO: model_training: Rank 0, Epoch 2022, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,793: INFO: model_training: Rank 0, Epoch 2023, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,794: INFO: model_training: Rank 0, Epoch 2023, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,795: INFO: model_training: Rank 0, Epoch 2023, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,796: INFO: model_training: Rank 0, Epoch 2023, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,798: INFO: model_training: Rank 0, Epoch 2023, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,799: INFO: model_training: Rank 0, Epoch 2024, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,801: INFO: model_training: Rank 0, Epoch 2024, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,802: INFO: model_training: Rank 0, Epoch 2024, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,804: INFO: model_training: Rank 0, Epoch 2024, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,805: INFO: model_training: Rank 0, Epoch 2024, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,806: INFO: model_training: Rank 0, Epoch 2025, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,808: INFO: model_training: Rank 0, Epoch 2025, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,809: INFO: model_training: Rank 0, Epoch 2025, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,811: INFO: model_training: Rank 0, Epoch 2025, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,812: INFO: model_training: Rank 0, Epoch 2025, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,814: INFO: model_training: Rank 0, Epoch 2026, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,818: INFO: model_training: Rank 0, Epoch 2026, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,820: INFO: model_training: Rank 0, Epoch 2026, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,822: INFO: model_training: Rank 0, Epoch 2026, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,823: INFO: model_training: Rank 0, Epoch 2026, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,825: INFO: model_training: Rank 0, Epoch 2027, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,827: INFO: model_training: Rank 0, Epoch 2027, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,829: INFO: model_training: Rank 0, Epoch 2027, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,831: INFO: model_training: Rank 0, Epoch 2027, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,833: INFO: model_training: Rank 0, Epoch 2027, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,835: INFO: model_training: Rank 0, Epoch 2028, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,837: INFO: model_training: Rank 0, Epoch 2028, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,839: INFO: model_training: Rank 0, Epoch 2028, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,840: INFO: model_training: Rank 0, Epoch 2028, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,842: INFO: model_training: Rank 0, Epoch 2028, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,844: INFO: model_training: Rank 0, Epoch 2029, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,846: INFO: model_training: Rank 0, Epoch 2029, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,847: INFO: model_training: Rank 0, Epoch 2029, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,849: INFO: model_training: Rank 0, Epoch 2029, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,851: INFO: model_training: Rank 0, Epoch 2029, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,852: INFO: model_training: Rank 0, Epoch 2030, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,853: INFO: model_training: Rank 0, Epoch 2030, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,855: INFO: model_training: Rank 0, Epoch 2030, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,856: INFO: model_training: Rank 0, Epoch 2030, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,858: INFO: model_training: Rank 0, Epoch 2030, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,860: INFO: model_training: Rank 0, Epoch 2031, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,861: INFO: model_training: Rank 0, Epoch 2031, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,863: INFO: model_training: Rank 0, Epoch 2031, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,864: INFO: model_training: Rank 0, Epoch 2031, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,866: INFO: model_training: Rank 0, Epoch 2031, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,867: INFO: model_training: Rank 0, Epoch 2032, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,868: INFO: model_training: Rank 0, Epoch 2032, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,869: INFO: model_training: Rank 0, Epoch 2032, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,872: INFO: model_training: Rank 0, Epoch 2032, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,873: INFO: model_training: Rank 0, Epoch 2032, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,875: INFO: model_training: Rank 0, Epoch 2033, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,878: INFO: model_training: Rank 0, Epoch 2033, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,879: INFO: model_training: Rank 0, Epoch 2033, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,881: INFO: model_training: Rank 0, Epoch 2033, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,883: INFO: model_training: Rank 0, Epoch 2033, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,885: INFO: model_training: Rank 0, Epoch 2034, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,887: INFO: model_training: Rank 0, Epoch 2034, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,888: INFO: model_training: Rank 0, Epoch 2034, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,890: INFO: model_training: Rank 0, Epoch 2034, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,892: INFO: model_training: Rank 0, Epoch 2034, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,894: INFO: model_training: Rank 0, Epoch 2035, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,896: INFO: model_training: Rank 0, Epoch 2035, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,898: INFO: model_training: Rank 0, Epoch 2035, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,900: INFO: model_training: Rank 0, Epoch 2035, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,901: INFO: model_training: Rank 0, Epoch 2035, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,903: INFO: model_training: Rank 0, Epoch 2036, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,904: INFO: model_training: Rank 0, Epoch 2036, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,906: INFO: model_training: Rank 0, Epoch 2036, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,908: INFO: model_training: Rank 0, Epoch 2036, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,910: INFO: model_training: Rank 0, Epoch 2036, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,912: INFO: model_training: Rank 0, Epoch 2037, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,914: INFO: model_training: Rank 0, Epoch 2037, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,916: INFO: model_training: Rank 0, Epoch 2037, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,918: INFO: model_training: Rank 0, Epoch 2037, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,919: INFO: model_training: Rank 0, Epoch 2037, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,921: INFO: model_training: Rank 0, Epoch 2038, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,922: INFO: model_training: Rank 0, Epoch 2038, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,923: INFO: model_training: Rank 0, Epoch 2038, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,924: INFO: model_training: Rank 0, Epoch 2038, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,926: INFO: model_training: Rank 0, Epoch 2038, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,927: INFO: model_training: Rank 0, Epoch 2039, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,929: INFO: model_training: Rank 0, Epoch 2039, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,931: INFO: model_training: Rank 0, Epoch 2039, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,933: INFO: model_training: Rank 0, Epoch 2039, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,936: INFO: model_training: Rank 0, Epoch 2039, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,938: INFO: model_training: Rank 0, Epoch 2040, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,940: INFO: model_training: Rank 0, Epoch 2040, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,941: INFO: model_training: Rank 0, Epoch 2040, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,943: INFO: model_training: Rank 0, Epoch 2040, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,944: INFO: model_training: Rank 0, Epoch 2040, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,946: INFO: model_training: Rank 0, Epoch 2041, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,947: INFO: model_training: Rank 0, Epoch 2041, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,949: INFO: model_training: Rank 0, Epoch 2041, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,952: INFO: model_training: Rank 0, Epoch 2041, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,956: INFO: model_training: Rank 0, Epoch 2041, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,957: INFO: model_training: Rank 0, Epoch 2042, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,959: INFO: model_training: Rank 0, Epoch 2042, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,961: INFO: model_training: Rank 0, Epoch 2042, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,962: INFO: model_training: Rank 0, Epoch 2042, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,964: INFO: model_training: Rank 0, Epoch 2042, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,966: INFO: model_training: Rank 0, Epoch 2043, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,968: INFO: model_training: Rank 0, Epoch 2043, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,970: INFO: model_training: Rank 0, Epoch 2043, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,972: INFO: model_training: Rank 0, Epoch 2043, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,974: INFO: model_training: Rank 0, Epoch 2043, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,976: INFO: model_training: Rank 0, Epoch 2044, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,978: INFO: model_training: Rank 0, Epoch 2044, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,980: INFO: model_training: Rank 0, Epoch 2044, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,982: INFO: model_training: Rank 0, Epoch 2044, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,984: INFO: model_training: Rank 0, Epoch 2044, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,986: INFO: model_training: Rank 0, Epoch 2045, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,987: INFO: model_training: Rank 0, Epoch 2045, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,989: INFO: model_training: Rank 0, Epoch 2045, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:38,991: INFO: model_training: Rank 0, Epoch 2045, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:38,992: INFO: model_training: Rank 0, Epoch 2045, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:38,994: INFO: model_training: Rank 0, Epoch 2046, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:38,995: INFO: model_training: Rank 0, Epoch 2046, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:38,997: INFO: model_training: Rank 0, Epoch 2046, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,000: INFO: model_training: Rank 0, Epoch 2046, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,002: INFO: model_training: Rank 0, Epoch 2046, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,003: INFO: model_training: Rank 0, Epoch 2047, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,005: INFO: model_training: Rank 0, Epoch 2047, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,007: INFO: model_training: Rank 0, Epoch 2047, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,010: INFO: model_training: Rank 0, Epoch 2047, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,012: INFO: model_training: Rank 0, Epoch 2047, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,014: INFO: model_training: Rank 0, Epoch 2048, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,015: INFO: model_training: Rank 0, Epoch 2048, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,017: INFO: model_training: Rank 0, Epoch 2048, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,019: INFO: model_training: Rank 0, Epoch 2048, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,021: INFO: model_training: Rank 0, Epoch 2048, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,023: INFO: model_training: Rank 0, Epoch 2049, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,024: INFO: model_training: Rank 0, Epoch 2049, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,026: INFO: model_training: Rank 0, Epoch 2049, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,028: INFO: model_training: Rank 0, Epoch 2049, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,031: INFO: model_training: Rank 0, Epoch 2049, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,033: INFO: model_training: Rank 0, Epoch 2050, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,035: INFO: model_training: Rank 0, Epoch 2050, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,036: INFO: model_training: Rank 0, Epoch 2050, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,038: INFO: model_training: Rank 0, Epoch 2050, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,040: INFO: model_training: Rank 0, Epoch 2050, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,042: INFO: model_training: Rank 0, Epoch 2051, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,044: INFO: model_training: Rank 0, Epoch 2051, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,045: INFO: model_training: Rank 0, Epoch 2051, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,046: INFO: model_training: Rank 0, Epoch 2051, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,047: INFO: model_training: Rank 0, Epoch 2051, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,050: INFO: model_training: Rank 0, Epoch 2052, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,051: INFO: model_training: Rank 0, Epoch 2052, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,053: INFO: model_training: Rank 0, Epoch 2052, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,055: INFO: model_training: Rank 0, Epoch 2052, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,057: INFO: model_training: Rank 0, Epoch 2052, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,060: INFO: model_training: Rank 0, Epoch 2053, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,061: INFO: model_training: Rank 0, Epoch 2053, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,063: INFO: model_training: Rank 0, Epoch 2053, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,064: INFO: model_training: Rank 0, Epoch 2053, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,066: INFO: model_training: Rank 0, Epoch 2053, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,067: INFO: model_training: Rank 0, Epoch 2054, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,069: INFO: model_training: Rank 0, Epoch 2054, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,071: INFO: model_training: Rank 0, Epoch 2054, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,072: INFO: model_training: Rank 0, Epoch 2054, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,074: INFO: model_training: Rank 0, Epoch 2054, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,075: INFO: model_training: Rank 0, Epoch 2055, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,077: INFO: model_training: Rank 0, Epoch 2055, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,078: INFO: model_training: Rank 0, Epoch 2055, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,079: INFO: model_training: Rank 0, Epoch 2055, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,080: INFO: model_training: Rank 0, Epoch 2055, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,082: INFO: model_training: Rank 0, Epoch 2056, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,083: INFO: model_training: Rank 0, Epoch 2056, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,084: INFO: model_training: Rank 0, Epoch 2056, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,086: INFO: model_training: Rank 0, Epoch 2056, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,087: INFO: model_training: Rank 0, Epoch 2056, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,088: INFO: model_training: Rank 0, Epoch 2057, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,089: INFO: model_training: Rank 0, Epoch 2057, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,091: INFO: model_training: Rank 0, Epoch 2057, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,092: INFO: model_training: Rank 0, Epoch 2057, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,093: INFO: model_training: Rank 0, Epoch 2057, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,095: INFO: model_training: Rank 0, Epoch 2058, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,096: INFO: model_training: Rank 0, Epoch 2058, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,098: INFO: model_training: Rank 0, Epoch 2058, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,099: INFO: model_training: Rank 0, Epoch 2058, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,100: INFO: model_training: Rank 0, Epoch 2058, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,101: INFO: model_training: Rank 0, Epoch 2059, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,102: INFO: model_training: Rank 0, Epoch 2059, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,104: INFO: model_training: Rank 0, Epoch 2059, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,105: INFO: model_training: Rank 0, Epoch 2059, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,106: INFO: model_training: Rank 0, Epoch 2059, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,107: INFO: model_training: Rank 0, Epoch 2060, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,109: INFO: model_training: Rank 0, Epoch 2060, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,110: INFO: model_training: Rank 0, Epoch 2060, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,111: INFO: model_training: Rank 0, Epoch 2060, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,113: INFO: model_training: Rank 0, Epoch 2060, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,114: INFO: model_training: Rank 0, Epoch 2061, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,115: INFO: model_training: Rank 0, Epoch 2061, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,117: INFO: model_training: Rank 0, Epoch 2061, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,118: INFO: model_training: Rank 0, Epoch 2061, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,119: INFO: model_training: Rank 0, Epoch 2061, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,120: INFO: model_training: Rank 0, Epoch 2062, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,122: INFO: model_training: Rank 0, Epoch 2062, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,123: INFO: model_training: Rank 0, Epoch 2062, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,125: INFO: model_training: Rank 0, Epoch 2062, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,126: INFO: model_training: Rank 0, Epoch 2062, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,127: INFO: model_training: Rank 0, Epoch 2063, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,129: INFO: model_training: Rank 0, Epoch 2063, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,130: INFO: model_training: Rank 0, Epoch 2063, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,132: INFO: model_training: Rank 0, Epoch 2063, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,134: INFO: model_training: Rank 0, Epoch 2063, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,136: INFO: model_training: Rank 0, Epoch 2064, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,138: INFO: model_training: Rank 0, Epoch 2064, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,140: INFO: model_training: Rank 0, Epoch 2064, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,141: INFO: model_training: Rank 0, Epoch 2064, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,143: INFO: model_training: Rank 0, Epoch 2064, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,144: INFO: model_training: Rank 0, Epoch 2065, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,145: INFO: model_training: Rank 0, Epoch 2065, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,146: INFO: model_training: Rank 0, Epoch 2065, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,148: INFO: model_training: Rank 0, Epoch 2065, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,149: INFO: model_training: Rank 0, Epoch 2065, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,151: INFO: model_training: Rank 0, Epoch 2066, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,152: INFO: model_training: Rank 0, Epoch 2066, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,153: INFO: model_training: Rank 0, Epoch 2066, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,154: INFO: model_training: Rank 0, Epoch 2066, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,156: INFO: model_training: Rank 0, Epoch 2066, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,158: INFO: model_training: Rank 0, Epoch 2067, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,161: INFO: model_training: Rank 0, Epoch 2067, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,162: INFO: model_training: Rank 0, Epoch 2067, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,165: INFO: model_training: Rank 0, Epoch 2067, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,166: INFO: model_training: Rank 0, Epoch 2067, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,167: INFO: model_training: Rank 0, Epoch 2068, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,169: INFO: model_training: Rank 0, Epoch 2068, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,170: INFO: model_training: Rank 0, Epoch 2068, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,171: INFO: model_training: Rank 0, Epoch 2068, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,173: INFO: model_training: Rank 0, Epoch 2068, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,174: INFO: model_training: Rank 0, Epoch 2069, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,176: INFO: model_training: Rank 0, Epoch 2069, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,177: INFO: model_training: Rank 0, Epoch 2069, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,178: INFO: model_training: Rank 0, Epoch 2069, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,179: INFO: model_training: Rank 0, Epoch 2069, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,181: INFO: model_training: Rank 0, Epoch 2070, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,183: INFO: model_training: Rank 0, Epoch 2070, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,184: INFO: model_training: Rank 0, Epoch 2070, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,186: INFO: model_training: Rank 0, Epoch 2070, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,187: INFO: model_training: Rank 0, Epoch 2070, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,188: INFO: model_training: Rank 0, Epoch 2071, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,189: INFO: model_training: Rank 0, Epoch 2071, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,191: INFO: model_training: Rank 0, Epoch 2071, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,192: INFO: model_training: Rank 0, Epoch 2071, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,193: INFO: model_training: Rank 0, Epoch 2071, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,195: INFO: model_training: Rank 0, Epoch 2072, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,196: INFO: model_training: Rank 0, Epoch 2072, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,198: INFO: model_training: Rank 0, Epoch 2072, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,199: INFO: model_training: Rank 0, Epoch 2072, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,200: INFO: model_training: Rank 0, Epoch 2072, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,202: INFO: model_training: Rank 0, Epoch 2073, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,203: INFO: model_training: Rank 0, Epoch 2073, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,204: INFO: model_training: Rank 0, Epoch 2073, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,206: INFO: model_training: Rank 0, Epoch 2073, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,207: INFO: model_training: Rank 0, Epoch 2073, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,209: INFO: model_training: Rank 0, Epoch 2074, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,210: INFO: model_training: Rank 0, Epoch 2074, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,212: INFO: model_training: Rank 0, Epoch 2074, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,213: INFO: model_training: Rank 0, Epoch 2074, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,215: INFO: model_training: Rank 0, Epoch 2074, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,216: INFO: model_training: Rank 0, Epoch 2075, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,217: INFO: model_training: Rank 0, Epoch 2075, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,219: INFO: model_training: Rank 0, Epoch 2075, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,220: INFO: model_training: Rank 0, Epoch 2075, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,221: INFO: model_training: Rank 0, Epoch 2075, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,222: INFO: model_training: Rank 0, Epoch 2076, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,224: INFO: model_training: Rank 0, Epoch 2076, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,225: INFO: model_training: Rank 0, Epoch 2076, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,226: INFO: model_training: Rank 0, Epoch 2076, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,228: INFO: model_training: Rank 0, Epoch 2076, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,229: INFO: model_training: Rank 0, Epoch 2077, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,230: INFO: model_training: Rank 0, Epoch 2077, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,231: INFO: model_training: Rank 0, Epoch 2077, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,233: INFO: model_training: Rank 0, Epoch 2077, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,234: INFO: model_training: Rank 0, Epoch 2077, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,235: INFO: model_training: Rank 0, Epoch 2078, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,236: INFO: model_training: Rank 0, Epoch 2078, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,237: INFO: model_training: Rank 0, Epoch 2078, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,238: INFO: model_training: Rank 0, Epoch 2078, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,239: INFO: model_training: Rank 0, Epoch 2078, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,240: INFO: model_training: Rank 0, Epoch 2079, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,241: INFO: model_training: Rank 0, Epoch 2079, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,243: INFO: model_training: Rank 0, Epoch 2079, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,244: INFO: model_training: Rank 0, Epoch 2079, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,245: INFO: model_training: Rank 0, Epoch 2079, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,246: INFO: model_training: Rank 0, Epoch 2080, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,247: INFO: model_training: Rank 0, Epoch 2080, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,248: INFO: model_training: Rank 0, Epoch 2080, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,249: INFO: model_training: Rank 0, Epoch 2080, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,251: INFO: model_training: Rank 0, Epoch 2080, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,253: INFO: model_training: Rank 0, Epoch 2081, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,254: INFO: model_training: Rank 0, Epoch 2081, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,256: INFO: model_training: Rank 0, Epoch 2081, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,258: INFO: model_training: Rank 0, Epoch 2081, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,260: INFO: model_training: Rank 0, Epoch 2081, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,262: INFO: model_training: Rank 0, Epoch 2082, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,263: INFO: model_training: Rank 0, Epoch 2082, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,266: INFO: model_training: Rank 0, Epoch 2082, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,267: INFO: model_training: Rank 0, Epoch 2082, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,269: INFO: model_training: Rank 0, Epoch 2082, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,270: INFO: model_training: Rank 0, Epoch 2083, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,272: INFO: model_training: Rank 0, Epoch 2083, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,273: INFO: model_training: Rank 0, Epoch 2083, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,275: INFO: model_training: Rank 0, Epoch 2083, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,276: INFO: model_training: Rank 0, Epoch 2083, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,277: INFO: model_training: Rank 0, Epoch 2084, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,279: INFO: model_training: Rank 0, Epoch 2084, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,280: INFO: model_training: Rank 0, Epoch 2084, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,281: INFO: model_training: Rank 0, Epoch 2084, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,283: INFO: model_training: Rank 0, Epoch 2084, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,284: INFO: model_training: Rank 0, Epoch 2085, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,286: INFO: model_training: Rank 0, Epoch 2085, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,287: INFO: model_training: Rank 0, Epoch 2085, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,288: INFO: model_training: Rank 0, Epoch 2085, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,289: INFO: model_training: Rank 0, Epoch 2085, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,291: INFO: model_training: Rank 0, Epoch 2086, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,292: INFO: model_training: Rank 0, Epoch 2086, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,294: INFO: model_training: Rank 0, Epoch 2086, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,295: INFO: model_training: Rank 0, Epoch 2086, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,297: INFO: model_training: Rank 0, Epoch 2086, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,298: INFO: model_training: Rank 0, Epoch 2087, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,299: INFO: model_training: Rank 0, Epoch 2087, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,301: INFO: model_training: Rank 0, Epoch 2087, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,302: INFO: model_training: Rank 0, Epoch 2087, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,303: INFO: model_training: Rank 0, Epoch 2087, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,304: INFO: model_training: Rank 0, Epoch 2088, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,306: INFO: model_training: Rank 0, Epoch 2088, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,307: INFO: model_training: Rank 0, Epoch 2088, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,309: INFO: model_training: Rank 0, Epoch 2088, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,310: INFO: model_training: Rank 0, Epoch 2088, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,312: INFO: model_training: Rank 0, Epoch 2089, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,314: INFO: model_training: Rank 0, Epoch 2089, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,315: INFO: model_training: Rank 0, Epoch 2089, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,316: INFO: model_training: Rank 0, Epoch 2089, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,319: INFO: model_training: Rank 0, Epoch 2089, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,320: INFO: model_training: Rank 0, Epoch 2090, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,322: INFO: model_training: Rank 0, Epoch 2090, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,324: INFO: model_training: Rank 0, Epoch 2090, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,325: INFO: model_training: Rank 0, Epoch 2090, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,329: INFO: model_training: Rank 0, Epoch 2090, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,331: INFO: model_training: Rank 0, Epoch 2091, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,333: INFO: model_training: Rank 0, Epoch 2091, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,334: INFO: model_training: Rank 0, Epoch 2091, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,337: INFO: model_training: Rank 0, Epoch 2091, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,339: INFO: model_training: Rank 0, Epoch 2091, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,341: INFO: model_training: Rank 0, Epoch 2092, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,343: INFO: model_training: Rank 0, Epoch 2092, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,344: INFO: model_training: Rank 0, Epoch 2092, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,346: INFO: model_training: Rank 0, Epoch 2092, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,347: INFO: model_training: Rank 0, Epoch 2092, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,349: INFO: model_training: Rank 0, Epoch 2093, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,350: INFO: model_training: Rank 0, Epoch 2093, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,351: INFO: model_training: Rank 0, Epoch 2093, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,353: INFO: model_training: Rank 0, Epoch 2093, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,354: INFO: model_training: Rank 0, Epoch 2093, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,356: INFO: model_training: Rank 0, Epoch 2094, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,357: INFO: model_training: Rank 0, Epoch 2094, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,358: INFO: model_training: Rank 0, Epoch 2094, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,360: INFO: model_training: Rank 0, Epoch 2094, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,361: INFO: model_training: Rank 0, Epoch 2094, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,363: INFO: model_training: Rank 0, Epoch 2095, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,364: INFO: model_training: Rank 0, Epoch 2095, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,365: INFO: model_training: Rank 0, Epoch 2095, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,367: INFO: model_training: Rank 0, Epoch 2095, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,368: INFO: model_training: Rank 0, Epoch 2095, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,369: INFO: model_training: Rank 0, Epoch 2096, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,371: INFO: model_training: Rank 0, Epoch 2096, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,372: INFO: model_training: Rank 0, Epoch 2096, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,374: INFO: model_training: Rank 0, Epoch 2096, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,375: INFO: model_training: Rank 0, Epoch 2096, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,376: INFO: model_training: Rank 0, Epoch 2097, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,377: INFO: model_training: Rank 0, Epoch 2097, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,379: INFO: model_training: Rank 0, Epoch 2097, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,380: INFO: model_training: Rank 0, Epoch 2097, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,381: INFO: model_training: Rank 0, Epoch 2097, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,383: INFO: model_training: Rank 0, Epoch 2098, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,384: INFO: model_training: Rank 0, Epoch 2098, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,386: INFO: model_training: Rank 0, Epoch 2098, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,388: INFO: model_training: Rank 0, Epoch 2098, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,390: INFO: model_training: Rank 0, Epoch 2098, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,391: INFO: model_training: Rank 0, Epoch 2099, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,392: INFO: model_training: Rank 0, Epoch 2099, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,394: INFO: model_training: Rank 0, Epoch 2099, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,396: INFO: model_training: Rank 0, Epoch 2099, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,398: INFO: model_training: Rank 0, Epoch 2099, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,399: INFO: model_training: Rank 0, Epoch 2100, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,401: INFO: model_training: Rank 0, Epoch 2100, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,403: INFO: model_training: Rank 0, Epoch 2100, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,405: INFO: model_training: Rank 0, Epoch 2100, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,407: INFO: model_training: Rank 0, Epoch 2100, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,411: INFO: model_training: Rank 0, Epoch 2101, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,415: INFO: model_training: Rank 0, Epoch 2101, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,417: INFO: model_training: Rank 0, Epoch 2101, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,419: INFO: model_training: Rank 0, Epoch 2101, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,422: INFO: model_training: Rank 0, Epoch 2101, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,424: INFO: model_training: Rank 0, Epoch 2102, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,426: INFO: model_training: Rank 0, Epoch 2102, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,427: INFO: model_training: Rank 0, Epoch 2102, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,428: INFO: model_training: Rank 0, Epoch 2102, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,430: INFO: model_training: Rank 0, Epoch 2102, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,431: INFO: model_training: Rank 0, Epoch 2103, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,432: INFO: model_training: Rank 0, Epoch 2103, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,434: INFO: model_training: Rank 0, Epoch 2103, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,435: INFO: model_training: Rank 0, Epoch 2103, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,437: INFO: model_training: Rank 0, Epoch 2103, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,438: INFO: model_training: Rank 0, Epoch 2104, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,439: INFO: model_training: Rank 0, Epoch 2104, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,441: INFO: model_training: Rank 0, Epoch 2104, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,442: INFO: model_training: Rank 0, Epoch 2104, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,444: INFO: model_training: Rank 0, Epoch 2104, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,445: INFO: model_training: Rank 0, Epoch 2105, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,446: INFO: model_training: Rank 0, Epoch 2105, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,448: INFO: model_training: Rank 0, Epoch 2105, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,449: INFO: model_training: Rank 0, Epoch 2105, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,452: INFO: model_training: Rank 0, Epoch 2105, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,453: INFO: model_training: Rank 0, Epoch 2106, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,455: INFO: model_training: Rank 0, Epoch 2106, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,456: INFO: model_training: Rank 0, Epoch 2106, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,458: INFO: model_training: Rank 0, Epoch 2106, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,459: INFO: model_training: Rank 0, Epoch 2106, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,460: INFO: model_training: Rank 0, Epoch 2107, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,461: INFO: model_training: Rank 0, Epoch 2107, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,462: INFO: model_training: Rank 0, Epoch 2107, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,464: INFO: model_training: Rank 0, Epoch 2107, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,465: INFO: model_training: Rank 0, Epoch 2107, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,466: INFO: model_training: Rank 0, Epoch 2108, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,467: INFO: model_training: Rank 0, Epoch 2108, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,469: INFO: model_training: Rank 0, Epoch 2108, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,470: INFO: model_training: Rank 0, Epoch 2108, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,471: INFO: model_training: Rank 0, Epoch 2108, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,472: INFO: model_training: Rank 0, Epoch 2109, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,474: INFO: model_training: Rank 0, Epoch 2109, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,475: INFO: model_training: Rank 0, Epoch 2109, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,476: INFO: model_training: Rank 0, Epoch 2109, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,477: INFO: model_training: Rank 0, Epoch 2109, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,479: INFO: model_training: Rank 0, Epoch 2110, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,481: INFO: model_training: Rank 0, Epoch 2110, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,483: INFO: model_training: Rank 0, Epoch 2110, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,485: INFO: model_training: Rank 0, Epoch 2110, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,487: INFO: model_training: Rank 0, Epoch 2110, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,488: INFO: model_training: Rank 0, Epoch 2111, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,490: INFO: model_training: Rank 0, Epoch 2111, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,491: INFO: model_training: Rank 0, Epoch 2111, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,492: INFO: model_training: Rank 0, Epoch 2111, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,494: INFO: model_training: Rank 0, Epoch 2111, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,496: INFO: model_training: Rank 0, Epoch 2112, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,497: INFO: model_training: Rank 0, Epoch 2112, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,499: INFO: model_training: Rank 0, Epoch 2112, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,502: INFO: model_training: Rank 0, Epoch 2112, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,505: INFO: model_training: Rank 0, Epoch 2112, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,507: INFO: model_training: Rank 0, Epoch 2113, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,508: INFO: model_training: Rank 0, Epoch 2113, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,509: INFO: model_training: Rank 0, Epoch 2113, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,511: INFO: model_training: Rank 0, Epoch 2113, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,512: INFO: model_training: Rank 0, Epoch 2113, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,514: INFO: model_training: Rank 0, Epoch 2114, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,515: INFO: model_training: Rank 0, Epoch 2114, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,517: INFO: model_training: Rank 0, Epoch 2114, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,519: INFO: model_training: Rank 0, Epoch 2114, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,520: INFO: model_training: Rank 0, Epoch 2114, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,522: INFO: model_training: Rank 0, Epoch 2115, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,524: INFO: model_training: Rank 0, Epoch 2115, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,525: INFO: model_training: Rank 0, Epoch 2115, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,526: INFO: model_training: Rank 0, Epoch 2115, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,527: INFO: model_training: Rank 0, Epoch 2115, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,529: INFO: model_training: Rank 0, Epoch 2116, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,530: INFO: model_training: Rank 0, Epoch 2116, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,531: INFO: model_training: Rank 0, Epoch 2116, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,533: INFO: model_training: Rank 0, Epoch 2116, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,534: INFO: model_training: Rank 0, Epoch 2116, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,535: INFO: model_training: Rank 0, Epoch 2117, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,537: INFO: model_training: Rank 0, Epoch 2117, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,538: INFO: model_training: Rank 0, Epoch 2117, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,539: INFO: model_training: Rank 0, Epoch 2117, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,541: INFO: model_training: Rank 0, Epoch 2117, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,542: INFO: model_training: Rank 0, Epoch 2118, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,543: INFO: model_training: Rank 0, Epoch 2118, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,545: INFO: model_training: Rank 0, Epoch 2118, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,546: INFO: model_training: Rank 0, Epoch 2118, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,548: INFO: model_training: Rank 0, Epoch 2118, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,549: INFO: model_training: Rank 0, Epoch 2119, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,550: INFO: model_training: Rank 0, Epoch 2119, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,552: INFO: model_training: Rank 0, Epoch 2119, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,553: INFO: model_training: Rank 0, Epoch 2119, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,555: INFO: model_training: Rank 0, Epoch 2119, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,556: INFO: model_training: Rank 0, Epoch 2120, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,557: INFO: model_training: Rank 0, Epoch 2120, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,559: INFO: model_training: Rank 0, Epoch 2120, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,560: INFO: model_training: Rank 0, Epoch 2120, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,562: INFO: model_training: Rank 0, Epoch 2120, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,563: INFO: model_training: Rank 0, Epoch 2121, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,564: INFO: model_training: Rank 0, Epoch 2121, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,566: INFO: model_training: Rank 0, Epoch 2121, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,568: INFO: model_training: Rank 0, Epoch 2121, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,569: INFO: model_training: Rank 0, Epoch 2121, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,571: INFO: model_training: Rank 0, Epoch 2122, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,573: INFO: model_training: Rank 0, Epoch 2122, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,574: INFO: model_training: Rank 0, Epoch 2122, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,575: INFO: model_training: Rank 0, Epoch 2122, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,577: INFO: model_training: Rank 0, Epoch 2122, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,579: INFO: model_training: Rank 0, Epoch 2123, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,580: INFO: model_training: Rank 0, Epoch 2123, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,582: INFO: model_training: Rank 0, Epoch 2123, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,584: INFO: model_training: Rank 0, Epoch 2123, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,586: INFO: model_training: Rank 0, Epoch 2123, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,590: INFO: model_training: Rank 0, Epoch 2124, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,593: INFO: model_training: Rank 0, Epoch 2124, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,594: INFO: model_training: Rank 0, Epoch 2124, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,596: INFO: model_training: Rank 0, Epoch 2124, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,597: INFO: model_training: Rank 0, Epoch 2124, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,598: INFO: model_training: Rank 0, Epoch 2125, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,600: INFO: model_training: Rank 0, Epoch 2125, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,601: INFO: model_training: Rank 0, Epoch 2125, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,603: INFO: model_training: Rank 0, Epoch 2125, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,604: INFO: model_training: Rank 0, Epoch 2125, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,605: INFO: model_training: Rank 0, Epoch 2126, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,606: INFO: model_training: Rank 0, Epoch 2126, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,608: INFO: model_training: Rank 0, Epoch 2126, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,609: INFO: model_training: Rank 0, Epoch 2126, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,610: INFO: model_training: Rank 0, Epoch 2126, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,611: INFO: model_training: Rank 0, Epoch 2127, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,613: INFO: model_training: Rank 0, Epoch 2127, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,614: INFO: model_training: Rank 0, Epoch 2127, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,616: INFO: model_training: Rank 0, Epoch 2127, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,617: INFO: model_training: Rank 0, Epoch 2127, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,618: INFO: model_training: Rank 0, Epoch 2128, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,619: INFO: model_training: Rank 0, Epoch 2128, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,621: INFO: model_training: Rank 0, Epoch 2128, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,622: INFO: model_training: Rank 0, Epoch 2128, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,624: INFO: model_training: Rank 0, Epoch 2128, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,625: INFO: model_training: Rank 0, Epoch 2129, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,627: INFO: model_training: Rank 0, Epoch 2129, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,628: INFO: model_training: Rank 0, Epoch 2129, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,629: INFO: model_training: Rank 0, Epoch 2129, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,630: INFO: model_training: Rank 0, Epoch 2129, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,632: INFO: model_training: Rank 0, Epoch 2130, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,633: INFO: model_training: Rank 0, Epoch 2130, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,635: INFO: model_training: Rank 0, Epoch 2130, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,636: INFO: model_training: Rank 0, Epoch 2130, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,637: INFO: model_training: Rank 0, Epoch 2130, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,638: INFO: model_training: Rank 0, Epoch 2131, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,640: INFO: model_training: Rank 0, Epoch 2131, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,641: INFO: model_training: Rank 0, Epoch 2131, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,643: INFO: model_training: Rank 0, Epoch 2131, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,644: INFO: model_training: Rank 0, Epoch 2131, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,645: INFO: model_training: Rank 0, Epoch 2132, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,647: INFO: model_training: Rank 0, Epoch 2132, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,648: INFO: model_training: Rank 0, Epoch 2132, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,649: INFO: model_training: Rank 0, Epoch 2132, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,651: INFO: model_training: Rank 0, Epoch 2132, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,652: INFO: model_training: Rank 0, Epoch 2133, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,654: INFO: model_training: Rank 0, Epoch 2133, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,656: INFO: model_training: Rank 0, Epoch 2133, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,658: INFO: model_training: Rank 0, Epoch 2133, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,659: INFO: model_training: Rank 0, Epoch 2133, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,662: INFO: model_training: Rank 0, Epoch 2134, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,663: INFO: model_training: Rank 0, Epoch 2134, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,665: INFO: model_training: Rank 0, Epoch 2134, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,666: INFO: model_training: Rank 0, Epoch 2134, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,668: INFO: model_training: Rank 0, Epoch 2134, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,670: INFO: model_training: Rank 0, Epoch 2135, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,671: INFO: model_training: Rank 0, Epoch 2135, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,673: INFO: model_training: Rank 0, Epoch 2135, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,675: INFO: model_training: Rank 0, Epoch 2135, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,676: INFO: model_training: Rank 0, Epoch 2135, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,678: INFO: model_training: Rank 0, Epoch 2136, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,679: INFO: model_training: Rank 0, Epoch 2136, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,681: INFO: model_training: Rank 0, Epoch 2136, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,682: INFO: model_training: Rank 0, Epoch 2136, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,684: INFO: model_training: Rank 0, Epoch 2136, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,685: INFO: model_training: Rank 0, Epoch 2137, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,687: INFO: model_training: Rank 0, Epoch 2137, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,690: INFO: model_training: Rank 0, Epoch 2137, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,692: INFO: model_training: Rank 0, Epoch 2137, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,693: INFO: model_training: Rank 0, Epoch 2137, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,695: INFO: model_training: Rank 0, Epoch 2138, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,699: INFO: model_training: Rank 0, Epoch 2138, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,701: INFO: model_training: Rank 0, Epoch 2138, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,702: INFO: model_training: Rank 0, Epoch 2138, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,703: INFO: model_training: Rank 0, Epoch 2138, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,705: INFO: model_training: Rank 0, Epoch 2139, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,706: INFO: model_training: Rank 0, Epoch 2139, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,708: INFO: model_training: Rank 0, Epoch 2139, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,709: INFO: model_training: Rank 0, Epoch 2139, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,711: INFO: model_training: Rank 0, Epoch 2139, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,712: INFO: model_training: Rank 0, Epoch 2140, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,714: INFO: model_training: Rank 0, Epoch 2140, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,716: INFO: model_training: Rank 0, Epoch 2140, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,717: INFO: model_training: Rank 0, Epoch 2140, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,719: INFO: model_training: Rank 0, Epoch 2140, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,720: INFO: model_training: Rank 0, Epoch 2141, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,722: INFO: model_training: Rank 0, Epoch 2141, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,723: INFO: model_training: Rank 0, Epoch 2141, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,724: INFO: model_training: Rank 0, Epoch 2141, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,726: INFO: model_training: Rank 0, Epoch 2141, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,727: INFO: model_training: Rank 0, Epoch 2142, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,729: INFO: model_training: Rank 0, Epoch 2142, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,731: INFO: model_training: Rank 0, Epoch 2142, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,732: INFO: model_training: Rank 0, Epoch 2142, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,734: INFO: model_training: Rank 0, Epoch 2142, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,735: INFO: model_training: Rank 0, Epoch 2143, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,738: INFO: model_training: Rank 0, Epoch 2143, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,739: INFO: model_training: Rank 0, Epoch 2143, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,741: INFO: model_training: Rank 0, Epoch 2143, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,742: INFO: model_training: Rank 0, Epoch 2143, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,744: INFO: model_training: Rank 0, Epoch 2144, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,745: INFO: model_training: Rank 0, Epoch 2144, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,748: INFO: model_training: Rank 0, Epoch 2144, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,750: INFO: model_training: Rank 0, Epoch 2144, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,753: INFO: model_training: Rank 0, Epoch 2144, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,756: INFO: model_training: Rank 0, Epoch 2145, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,759: INFO: model_training: Rank 0, Epoch 2145, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,762: INFO: model_training: Rank 0, Epoch 2145, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,765: INFO: model_training: Rank 0, Epoch 2145, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,767: INFO: model_training: Rank 0, Epoch 2145, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,770: INFO: model_training: Rank 0, Epoch 2146, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,772: INFO: model_training: Rank 0, Epoch 2146, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,774: INFO: model_training: Rank 0, Epoch 2146, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,775: INFO: model_training: Rank 0, Epoch 2146, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,777: INFO: model_training: Rank 0, Epoch 2146, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,779: INFO: model_training: Rank 0, Epoch 2147, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,780: INFO: model_training: Rank 0, Epoch 2147, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,782: INFO: model_training: Rank 0, Epoch 2147, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,784: INFO: model_training: Rank 0, Epoch 2147, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,786: INFO: model_training: Rank 0, Epoch 2147, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,788: INFO: model_training: Rank 0, Epoch 2148, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,790: INFO: model_training: Rank 0, Epoch 2148, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,792: INFO: model_training: Rank 0, Epoch 2148, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,793: INFO: model_training: Rank 0, Epoch 2148, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,795: INFO: model_training: Rank 0, Epoch 2148, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,797: INFO: model_training: Rank 0, Epoch 2149, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,799: INFO: model_training: Rank 0, Epoch 2149, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,800: INFO: model_training: Rank 0, Epoch 2149, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,802: INFO: model_training: Rank 0, Epoch 2149, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,803: INFO: model_training: Rank 0, Epoch 2149, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,806: INFO: model_training: Rank 0, Epoch 2150, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,808: INFO: model_training: Rank 0, Epoch 2150, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,810: INFO: model_training: Rank 0, Epoch 2150, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,813: INFO: model_training: Rank 0, Epoch 2150, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,816: INFO: model_training: Rank 0, Epoch 2150, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,819: INFO: model_training: Rank 0, Epoch 2151, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,821: INFO: model_training: Rank 0, Epoch 2151, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,823: INFO: model_training: Rank 0, Epoch 2151, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,825: INFO: model_training: Rank 0, Epoch 2151, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,828: INFO: model_training: Rank 0, Epoch 2151, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,829: INFO: model_training: Rank 0, Epoch 2152, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,830: INFO: model_training: Rank 0, Epoch 2152, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,832: INFO: model_training: Rank 0, Epoch 2152, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,834: INFO: model_training: Rank 0, Epoch 2152, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,835: INFO: model_training: Rank 0, Epoch 2152, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,837: INFO: model_training: Rank 0, Epoch 2153, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,838: INFO: model_training: Rank 0, Epoch 2153, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,840: INFO: model_training: Rank 0, Epoch 2153, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,842: INFO: model_training: Rank 0, Epoch 2153, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,843: INFO: model_training: Rank 0, Epoch 2153, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,844: INFO: model_training: Rank 0, Epoch 2154, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,846: INFO: model_training: Rank 0, Epoch 2154, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,847: INFO: model_training: Rank 0, Epoch 2154, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,848: INFO: model_training: Rank 0, Epoch 2154, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,850: INFO: model_training: Rank 0, Epoch 2154, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,851: INFO: model_training: Rank 0, Epoch 2155, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,852: INFO: model_training: Rank 0, Epoch 2155, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,853: INFO: model_training: Rank 0, Epoch 2155, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,855: INFO: model_training: Rank 0, Epoch 2155, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,856: INFO: model_training: Rank 0, Epoch 2155, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,857: INFO: model_training: Rank 0, Epoch 2156, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,859: INFO: model_training: Rank 0, Epoch 2156, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,860: INFO: model_training: Rank 0, Epoch 2156, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,862: INFO: model_training: Rank 0, Epoch 2156, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,864: INFO: model_training: Rank 0, Epoch 2156, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,865: INFO: model_training: Rank 0, Epoch 2157, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,866: INFO: model_training: Rank 0, Epoch 2157, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,868: INFO: model_training: Rank 0, Epoch 2157, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,869: INFO: model_training: Rank 0, Epoch 2157, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,870: INFO: model_training: Rank 0, Epoch 2157, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,871: INFO: model_training: Rank 0, Epoch 2158, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,873: INFO: model_training: Rank 0, Epoch 2158, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,874: INFO: model_training: Rank 0, Epoch 2158, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,875: INFO: model_training: Rank 0, Epoch 2158, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,876: INFO: model_training: Rank 0, Epoch 2158, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,878: INFO: model_training: Rank 0, Epoch 2159, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,879: INFO: model_training: Rank 0, Epoch 2159, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,880: INFO: model_training: Rank 0, Epoch 2159, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,882: INFO: model_training: Rank 0, Epoch 2159, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,883: INFO: model_training: Rank 0, Epoch 2159, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,885: INFO: model_training: Rank 0, Epoch 2160, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,887: INFO: model_training: Rank 0, Epoch 2160, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,888: INFO: model_training: Rank 0, Epoch 2160, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,890: INFO: model_training: Rank 0, Epoch 2160, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,892: INFO: model_training: Rank 0, Epoch 2160, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,894: INFO: model_training: Rank 0, Epoch 2161, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,896: INFO: model_training: Rank 0, Epoch 2161, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,898: INFO: model_training: Rank 0, Epoch 2161, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,899: INFO: model_training: Rank 0, Epoch 2161, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,900: INFO: model_training: Rank 0, Epoch 2161, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,901: INFO: model_training: Rank 0, Epoch 2162, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,903: INFO: model_training: Rank 0, Epoch 2162, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,904: INFO: model_training: Rank 0, Epoch 2162, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,905: INFO: model_training: Rank 0, Epoch 2162, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,907: INFO: model_training: Rank 0, Epoch 2162, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,908: INFO: model_training: Rank 0, Epoch 2163, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,910: INFO: model_training: Rank 0, Epoch 2163, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,911: INFO: model_training: Rank 0, Epoch 2163, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,912: INFO: model_training: Rank 0, Epoch 2163, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,913: INFO: model_training: Rank 0, Epoch 2163, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,915: INFO: model_training: Rank 0, Epoch 2164, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,916: INFO: model_training: Rank 0, Epoch 2164, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,917: INFO: model_training: Rank 0, Epoch 2164, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,919: INFO: model_training: Rank 0, Epoch 2164, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,921: INFO: model_training: Rank 0, Epoch 2164, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,922: INFO: model_training: Rank 0, Epoch 2165, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,924: INFO: model_training: Rank 0, Epoch 2165, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,925: INFO: model_training: Rank 0, Epoch 2165, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,927: INFO: model_training: Rank 0, Epoch 2165, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,928: INFO: model_training: Rank 0, Epoch 2165, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,929: INFO: model_training: Rank 0, Epoch 2166, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,930: INFO: model_training: Rank 0, Epoch 2166, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,932: INFO: model_training: Rank 0, Epoch 2166, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,933: INFO: model_training: Rank 0, Epoch 2166, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,934: INFO: model_training: Rank 0, Epoch 2166, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,936: INFO: model_training: Rank 0, Epoch 2167, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,937: INFO: model_training: Rank 0, Epoch 2167, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,938: INFO: model_training: Rank 0, Epoch 2167, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,939: INFO: model_training: Rank 0, Epoch 2167, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,941: INFO: model_training: Rank 0, Epoch 2167, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,942: INFO: model_training: Rank 0, Epoch 2168, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,943: INFO: model_training: Rank 0, Epoch 2168, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,945: INFO: model_training: Rank 0, Epoch 2168, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,947: INFO: model_training: Rank 0, Epoch 2168, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,948: INFO: model_training: Rank 0, Epoch 2168, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,949: INFO: model_training: Rank 0, Epoch 2169, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,951: INFO: model_training: Rank 0, Epoch 2169, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,952: INFO: model_training: Rank 0, Epoch 2169, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,954: INFO: model_training: Rank 0, Epoch 2169, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,955: INFO: model_training: Rank 0, Epoch 2169, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,956: INFO: model_training: Rank 0, Epoch 2170, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,957: INFO: model_training: Rank 0, Epoch 2170, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,959: INFO: model_training: Rank 0, Epoch 2170, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,961: INFO: model_training: Rank 0, Epoch 2170, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,962: INFO: model_training: Rank 0, Epoch 2170, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,963: INFO: model_training: Rank 0, Epoch 2171, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,964: INFO: model_training: Rank 0, Epoch 2171, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,966: INFO: model_training: Rank 0, Epoch 2171, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,967: INFO: model_training: Rank 0, Epoch 2171, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,969: INFO: model_training: Rank 0, Epoch 2171, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,970: INFO: model_training: Rank 0, Epoch 2172, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,972: INFO: model_training: Rank 0, Epoch 2172, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,973: INFO: model_training: Rank 0, Epoch 2172, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,975: INFO: model_training: Rank 0, Epoch 2172, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,976: INFO: model_training: Rank 0, Epoch 2172, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,978: INFO: model_training: Rank 0, Epoch 2173, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,979: INFO: model_training: Rank 0, Epoch 2173, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,981: INFO: model_training: Rank 0, Epoch 2173, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,982: INFO: model_training: Rank 0, Epoch 2173, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,984: INFO: model_training: Rank 0, Epoch 2173, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,985: INFO: model_training: Rank 0, Epoch 2174, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,986: INFO: model_training: Rank 0, Epoch 2174, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:39,988: INFO: model_training: Rank 0, Epoch 2174, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:39,990: INFO: model_training: Rank 0, Epoch 2174, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:39,992: INFO: model_training: Rank 0, Epoch 2174, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:39,994: INFO: model_training: Rank 0, Epoch 2175, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:39,996: INFO: model_training: Rank 0, Epoch 2175, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,000: INFO: model_training: Rank 0, Epoch 2175, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,002: INFO: model_training: Rank 0, Epoch 2175, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,005: INFO: model_training: Rank 0, Epoch 2175, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,007: INFO: model_training: Rank 0, Epoch 2176, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,010: INFO: model_training: Rank 0, Epoch 2176, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,011: INFO: model_training: Rank 0, Epoch 2176, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,013: INFO: model_training: Rank 0, Epoch 2176, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,014: INFO: model_training: Rank 0, Epoch 2176, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,015: INFO: model_training: Rank 0, Epoch 2177, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,017: INFO: model_training: Rank 0, Epoch 2177, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,019: INFO: model_training: Rank 0, Epoch 2177, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,021: INFO: model_training: Rank 0, Epoch 2177, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,023: INFO: model_training: Rank 0, Epoch 2177, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,024: INFO: model_training: Rank 0, Epoch 2178, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,026: INFO: model_training: Rank 0, Epoch 2178, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,027: INFO: model_training: Rank 0, Epoch 2178, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,029: INFO: model_training: Rank 0, Epoch 2178, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,030: INFO: model_training: Rank 0, Epoch 2178, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,031: INFO: model_training: Rank 0, Epoch 2179, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,033: INFO: model_training: Rank 0, Epoch 2179, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,034: INFO: model_training: Rank 0, Epoch 2179, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,036: INFO: model_training: Rank 0, Epoch 2179, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,037: INFO: model_training: Rank 0, Epoch 2179, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,038: INFO: model_training: Rank 0, Epoch 2180, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,040: INFO: model_training: Rank 0, Epoch 2180, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,041: INFO: model_training: Rank 0, Epoch 2180, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,042: INFO: model_training: Rank 0, Epoch 2180, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,044: INFO: model_training: Rank 0, Epoch 2180, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,045: INFO: model_training: Rank 0, Epoch 2181, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,046: INFO: model_training: Rank 0, Epoch 2181, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,048: INFO: model_training: Rank 0, Epoch 2181, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,049: INFO: model_training: Rank 0, Epoch 2181, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,050: INFO: model_training: Rank 0, Epoch 2181, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,052: INFO: model_training: Rank 0, Epoch 2182, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,053: INFO: model_training: Rank 0, Epoch 2182, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,054: INFO: model_training: Rank 0, Epoch 2182, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,056: INFO: model_training: Rank 0, Epoch 2182, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,057: INFO: model_training: Rank 0, Epoch 2182, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,059: INFO: model_training: Rank 0, Epoch 2183, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,060: INFO: model_training: Rank 0, Epoch 2183, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,061: INFO: model_training: Rank 0, Epoch 2183, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,062: INFO: model_training: Rank 0, Epoch 2183, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,063: INFO: model_training: Rank 0, Epoch 2183, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,065: INFO: model_training: Rank 0, Epoch 2184, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,066: INFO: model_training: Rank 0, Epoch 2184, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,067: INFO: model_training: Rank 0, Epoch 2184, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,068: INFO: model_training: Rank 0, Epoch 2184, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,070: INFO: model_training: Rank 0, Epoch 2184, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,071: INFO: model_training: Rank 0, Epoch 2185, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,072: INFO: model_training: Rank 0, Epoch 2185, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,073: INFO: model_training: Rank 0, Epoch 2185, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,075: INFO: model_training: Rank 0, Epoch 2185, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,076: INFO: model_training: Rank 0, Epoch 2185, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,077: INFO: model_training: Rank 0, Epoch 2186, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,078: INFO: model_training: Rank 0, Epoch 2186, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,080: INFO: model_training: Rank 0, Epoch 2186, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,081: INFO: model_training: Rank 0, Epoch 2186, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,083: INFO: model_training: Rank 0, Epoch 2186, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,084: INFO: model_training: Rank 0, Epoch 2187, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,086: INFO: model_training: Rank 0, Epoch 2187, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,087: INFO: model_training: Rank 0, Epoch 2187, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,089: INFO: model_training: Rank 0, Epoch 2187, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,090: INFO: model_training: Rank 0, Epoch 2187, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,091: INFO: model_training: Rank 0, Epoch 2188, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,092: INFO: model_training: Rank 0, Epoch 2188, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,094: INFO: model_training: Rank 0, Epoch 2188, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,095: INFO: model_training: Rank 0, Epoch 2188, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,096: INFO: model_training: Rank 0, Epoch 2188, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,097: INFO: model_training: Rank 0, Epoch 2189, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,099: INFO: model_training: Rank 0, Epoch 2189, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,101: INFO: model_training: Rank 0, Epoch 2189, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,102: INFO: model_training: Rank 0, Epoch 2189, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,103: INFO: model_training: Rank 0, Epoch 2189, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,104: INFO: model_training: Rank 0, Epoch 2190, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,106: INFO: model_training: Rank 0, Epoch 2190, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,108: INFO: model_training: Rank 0, Epoch 2190, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,110: INFO: model_training: Rank 0, Epoch 2190, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,111: INFO: model_training: Rank 0, Epoch 2190, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,113: INFO: model_training: Rank 0, Epoch 2191, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,114: INFO: model_training: Rank 0, Epoch 2191, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,115: INFO: model_training: Rank 0, Epoch 2191, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,116: INFO: model_training: Rank 0, Epoch 2191, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,118: INFO: model_training: Rank 0, Epoch 2191, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,120: INFO: model_training: Rank 0, Epoch 2192, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,121: INFO: model_training: Rank 0, Epoch 2192, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,123: INFO: model_training: Rank 0, Epoch 2192, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,124: INFO: model_training: Rank 0, Epoch 2192, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,126: INFO: model_training: Rank 0, Epoch 2192, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,127: INFO: model_training: Rank 0, Epoch 2193, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,129: INFO: model_training: Rank 0, Epoch 2193, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,132: INFO: model_training: Rank 0, Epoch 2193, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,134: INFO: model_training: Rank 0, Epoch 2193, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,137: INFO: model_training: Rank 0, Epoch 2193, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,140: INFO: model_training: Rank 0, Epoch 2194, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,142: INFO: model_training: Rank 0, Epoch 2194, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,143: INFO: model_training: Rank 0, Epoch 2194, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,145: INFO: model_training: Rank 0, Epoch 2194, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,147: INFO: model_training: Rank 0, Epoch 2194, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,149: INFO: model_training: Rank 0, Epoch 2195, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,151: INFO: model_training: Rank 0, Epoch 2195, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,153: INFO: model_training: Rank 0, Epoch 2195, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,155: INFO: model_training: Rank 0, Epoch 2195, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,157: INFO: model_training: Rank 0, Epoch 2195, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,160: INFO: model_training: Rank 0, Epoch 2196, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,162: INFO: model_training: Rank 0, Epoch 2196, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,165: INFO: model_training: Rank 0, Epoch 2196, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,167: INFO: model_training: Rank 0, Epoch 2196, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,168: INFO: model_training: Rank 0, Epoch 2196, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,170: INFO: model_training: Rank 0, Epoch 2197, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,171: INFO: model_training: Rank 0, Epoch 2197, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,173: INFO: model_training: Rank 0, Epoch 2197, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,175: INFO: model_training: Rank 0, Epoch 2197, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,176: INFO: model_training: Rank 0, Epoch 2197, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,178: INFO: model_training: Rank 0, Epoch 2198, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,179: INFO: model_training: Rank 0, Epoch 2198, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,181: INFO: model_training: Rank 0, Epoch 2198, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,182: INFO: model_training: Rank 0, Epoch 2198, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,183: INFO: model_training: Rank 0, Epoch 2198, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,185: INFO: model_training: Rank 0, Epoch 2199, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,186: INFO: model_training: Rank 0, Epoch 2199, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,187: INFO: model_training: Rank 0, Epoch 2199, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,188: INFO: model_training: Rank 0, Epoch 2199, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,191: INFO: model_training: Rank 0, Epoch 2199, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,192: INFO: model_training: Rank 0, Epoch 2200, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,194: INFO: model_training: Rank 0, Epoch 2200, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,196: INFO: model_training: Rank 0, Epoch 2200, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,197: INFO: model_training: Rank 0, Epoch 2200, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,198: INFO: model_training: Rank 0, Epoch 2200, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,200: INFO: model_training: Rank 0, Epoch 2201, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,202: INFO: model_training: Rank 0, Epoch 2201, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,203: INFO: model_training: Rank 0, Epoch 2201, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,204: INFO: model_training: Rank 0, Epoch 2201, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,206: INFO: model_training: Rank 0, Epoch 2201, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,207: INFO: model_training: Rank 0, Epoch 2202, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,209: INFO: model_training: Rank 0, Epoch 2202, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,210: INFO: model_training: Rank 0, Epoch 2202, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,212: INFO: model_training: Rank 0, Epoch 2202, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,213: INFO: model_training: Rank 0, Epoch 2202, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,215: INFO: model_training: Rank 0, Epoch 2203, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,217: INFO: model_training: Rank 0, Epoch 2203, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,218: INFO: model_training: Rank 0, Epoch 2203, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,219: INFO: model_training: Rank 0, Epoch 2203, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,221: INFO: model_training: Rank 0, Epoch 2203, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,222: INFO: model_training: Rank 0, Epoch 2204, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,224: INFO: model_training: Rank 0, Epoch 2204, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,226: INFO: model_training: Rank 0, Epoch 2204, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,228: INFO: model_training: Rank 0, Epoch 2204, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,229: INFO: model_training: Rank 0, Epoch 2204, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,231: INFO: model_training: Rank 0, Epoch 2205, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,232: INFO: model_training: Rank 0, Epoch 2205, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,234: INFO: model_training: Rank 0, Epoch 2205, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,236: INFO: model_training: Rank 0, Epoch 2205, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,237: INFO: model_training: Rank 0, Epoch 2205, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,241: INFO: model_training: Rank 0, Epoch 2206, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,244: INFO: model_training: Rank 0, Epoch 2206, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,247: INFO: model_training: Rank 0, Epoch 2206, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,250: INFO: model_training: Rank 0, Epoch 2206, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,252: INFO: model_training: Rank 0, Epoch 2206, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,254: INFO: model_training: Rank 0, Epoch 2207, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,255: INFO: model_training: Rank 0, Epoch 2207, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,256: INFO: model_training: Rank 0, Epoch 2207, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,257: INFO: model_training: Rank 0, Epoch 2207, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,259: INFO: model_training: Rank 0, Epoch 2207, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,261: INFO: model_training: Rank 0, Epoch 2208, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,262: INFO: model_training: Rank 0, Epoch 2208, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,264: INFO: model_training: Rank 0, Epoch 2208, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,266: INFO: model_training: Rank 0, Epoch 2208, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,267: INFO: model_training: Rank 0, Epoch 2208, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,269: INFO: model_training: Rank 0, Epoch 2209, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,270: INFO: model_training: Rank 0, Epoch 2209, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,271: INFO: model_training: Rank 0, Epoch 2209, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,272: INFO: model_training: Rank 0, Epoch 2209, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,274: INFO: model_training: Rank 0, Epoch 2209, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,275: INFO: model_training: Rank 0, Epoch 2210, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,276: INFO: model_training: Rank 0, Epoch 2210, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,278: INFO: model_training: Rank 0, Epoch 2210, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,279: INFO: model_training: Rank 0, Epoch 2210, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,281: INFO: model_training: Rank 0, Epoch 2210, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,282: INFO: model_training: Rank 0, Epoch 2211, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,284: INFO: model_training: Rank 0, Epoch 2211, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,285: INFO: model_training: Rank 0, Epoch 2211, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,286: INFO: model_training: Rank 0, Epoch 2211, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,288: INFO: model_training: Rank 0, Epoch 2211, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,289: INFO: model_training: Rank 0, Epoch 2212, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,290: INFO: model_training: Rank 0, Epoch 2212, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,292: INFO: model_training: Rank 0, Epoch 2212, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,293: INFO: model_training: Rank 0, Epoch 2212, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,294: INFO: model_training: Rank 0, Epoch 2212, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,295: INFO: model_training: Rank 0, Epoch 2213, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,297: INFO: model_training: Rank 0, Epoch 2213, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,298: INFO: model_training: Rank 0, Epoch 2213, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,299: INFO: model_training: Rank 0, Epoch 2213, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,300: INFO: model_training: Rank 0, Epoch 2213, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,301: INFO: model_training: Rank 0, Epoch 2214, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,303: INFO: model_training: Rank 0, Epoch 2214, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,304: INFO: model_training: Rank 0, Epoch 2214, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,306: INFO: model_training: Rank 0, Epoch 2214, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,307: INFO: model_training: Rank 0, Epoch 2214, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,308: INFO: model_training: Rank 0, Epoch 2215, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,309: INFO: model_training: Rank 0, Epoch 2215, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,311: INFO: model_training: Rank 0, Epoch 2215, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,312: INFO: model_training: Rank 0, Epoch 2215, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,313: INFO: model_training: Rank 0, Epoch 2215, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,315: INFO: model_training: Rank 0, Epoch 2216, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,316: INFO: model_training: Rank 0, Epoch 2216, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,317: INFO: model_training: Rank 0, Epoch 2216, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,318: INFO: model_training: Rank 0, Epoch 2216, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,320: INFO: model_training: Rank 0, Epoch 2216, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,321: INFO: model_training: Rank 0, Epoch 2217, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,322: INFO: model_training: Rank 0, Epoch 2217, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,323: INFO: model_training: Rank 0, Epoch 2217, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,325: INFO: model_training: Rank 0, Epoch 2217, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,326: INFO: model_training: Rank 0, Epoch 2217, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,327: INFO: model_training: Rank 0, Epoch 2218, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,328: INFO: model_training: Rank 0, Epoch 2218, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,329: INFO: model_training: Rank 0, Epoch 2218, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,330: INFO: model_training: Rank 0, Epoch 2218, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,332: INFO: model_training: Rank 0, Epoch 2218, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,333: INFO: model_training: Rank 0, Epoch 2219, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,334: INFO: model_training: Rank 0, Epoch 2219, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,335: INFO: model_training: Rank 0, Epoch 2219, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,337: INFO: model_training: Rank 0, Epoch 2219, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,338: INFO: model_training: Rank 0, Epoch 2219, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,340: INFO: model_training: Rank 0, Epoch 2220, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,342: INFO: model_training: Rank 0, Epoch 2220, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,343: INFO: model_training: Rank 0, Epoch 2220, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,344: INFO: model_training: Rank 0, Epoch 2220, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,345: INFO: model_training: Rank 0, Epoch 2220, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,346: INFO: model_training: Rank 0, Epoch 2221, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,348: INFO: model_training: Rank 0, Epoch 2221, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,349: INFO: model_training: Rank 0, Epoch 2221, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,350: INFO: model_training: Rank 0, Epoch 2221, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,351: INFO: model_training: Rank 0, Epoch 2221, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,353: INFO: model_training: Rank 0, Epoch 2222, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,355: INFO: model_training: Rank 0, Epoch 2222, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,356: INFO: model_training: Rank 0, Epoch 2222, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,358: INFO: model_training: Rank 0, Epoch 2222, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,359: INFO: model_training: Rank 0, Epoch 2222, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,361: INFO: model_training: Rank 0, Epoch 2223, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,362: INFO: model_training: Rank 0, Epoch 2223, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,363: INFO: model_training: Rank 0, Epoch 2223, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,365: INFO: model_training: Rank 0, Epoch 2223, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,366: INFO: model_training: Rank 0, Epoch 2223, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,368: INFO: model_training: Rank 0, Epoch 2224, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,370: INFO: model_training: Rank 0, Epoch 2224, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,371: INFO: model_training: Rank 0, Epoch 2224, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,372: INFO: model_training: Rank 0, Epoch 2224, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,374: INFO: model_training: Rank 0, Epoch 2224, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,375: INFO: model_training: Rank 0, Epoch 2225, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,377: INFO: model_training: Rank 0, Epoch 2225, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,378: INFO: model_training: Rank 0, Epoch 2225, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,379: INFO: model_training: Rank 0, Epoch 2225, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,381: INFO: model_training: Rank 0, Epoch 2225, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,382: INFO: model_training: Rank 0, Epoch 2226, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,383: INFO: model_training: Rank 0, Epoch 2226, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,385: INFO: model_training: Rank 0, Epoch 2226, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,386: INFO: model_training: Rank 0, Epoch 2226, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,387: INFO: model_training: Rank 0, Epoch 2226, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,389: INFO: model_training: Rank 0, Epoch 2227, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,391: INFO: model_training: Rank 0, Epoch 2227, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,394: INFO: model_training: Rank 0, Epoch 2227, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,397: INFO: model_training: Rank 0, Epoch 2227, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,400: INFO: model_training: Rank 0, Epoch 2227, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,403: INFO: model_training: Rank 0, Epoch 2228, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,405: INFO: model_training: Rank 0, Epoch 2228, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,408: INFO: model_training: Rank 0, Epoch 2228, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,410: INFO: model_training: Rank 0, Epoch 2228, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,412: INFO: model_training: Rank 0, Epoch 2228, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,413: INFO: model_training: Rank 0, Epoch 2229, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,414: INFO: model_training: Rank 0, Epoch 2229, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,416: INFO: model_training: Rank 0, Epoch 2229, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,418: INFO: model_training: Rank 0, Epoch 2229, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,420: INFO: model_training: Rank 0, Epoch 2229, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,422: INFO: model_training: Rank 0, Epoch 2230, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,423: INFO: model_training: Rank 0, Epoch 2230, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,426: INFO: model_training: Rank 0, Epoch 2230, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,427: INFO: model_training: Rank 0, Epoch 2230, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,429: INFO: model_training: Rank 0, Epoch 2230, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,430: INFO: model_training: Rank 0, Epoch 2231, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,431: INFO: model_training: Rank 0, Epoch 2231, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,432: INFO: model_training: Rank 0, Epoch 2231, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,434: INFO: model_training: Rank 0, Epoch 2231, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,435: INFO: model_training: Rank 0, Epoch 2231, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,436: INFO: model_training: Rank 0, Epoch 2232, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,437: INFO: model_training: Rank 0, Epoch 2232, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,439: INFO: model_training: Rank 0, Epoch 2232, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,440: INFO: model_training: Rank 0, Epoch 2232, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,442: INFO: model_training: Rank 0, Epoch 2232, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,443: INFO: model_training: Rank 0, Epoch 2233, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,444: INFO: model_training: Rank 0, Epoch 2233, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,445: INFO: model_training: Rank 0, Epoch 2233, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,446: INFO: model_training: Rank 0, Epoch 2233, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,447: INFO: model_training: Rank 0, Epoch 2233, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,449: INFO: model_training: Rank 0, Epoch 2234, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,450: INFO: model_training: Rank 0, Epoch 2234, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,451: INFO: model_training: Rank 0, Epoch 2234, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,452: INFO: model_training: Rank 0, Epoch 2234, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,453: INFO: model_training: Rank 0, Epoch 2234, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,454: INFO: model_training: Rank 0, Epoch 2235, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,455: INFO: model_training: Rank 0, Epoch 2235, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,456: INFO: model_training: Rank 0, Epoch 2235, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,457: INFO: model_training: Rank 0, Epoch 2235, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,458: INFO: model_training: Rank 0, Epoch 2235, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,459: INFO: model_training: Rank 0, Epoch 2236, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,461: INFO: model_training: Rank 0, Epoch 2236, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,462: INFO: model_training: Rank 0, Epoch 2236, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,463: INFO: model_training: Rank 0, Epoch 2236, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,464: INFO: model_training: Rank 0, Epoch 2236, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,466: INFO: model_training: Rank 0, Epoch 2237, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,467: INFO: model_training: Rank 0, Epoch 2237, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,468: INFO: model_training: Rank 0, Epoch 2237, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,469: INFO: model_training: Rank 0, Epoch 2237, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,470: INFO: model_training: Rank 0, Epoch 2237, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,472: INFO: model_training: Rank 0, Epoch 2238, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,473: INFO: model_training: Rank 0, Epoch 2238, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,474: INFO: model_training: Rank 0, Epoch 2238, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,475: INFO: model_training: Rank 0, Epoch 2238, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,476: INFO: model_training: Rank 0, Epoch 2238, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,477: INFO: model_training: Rank 0, Epoch 2239, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,478: INFO: model_training: Rank 0, Epoch 2239, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,479: INFO: model_training: Rank 0, Epoch 2239, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,480: INFO: model_training: Rank 0, Epoch 2239, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,482: INFO: model_training: Rank 0, Epoch 2239, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,483: INFO: model_training: Rank 0, Epoch 2240, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,484: INFO: model_training: Rank 0, Epoch 2240, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,485: INFO: model_training: Rank 0, Epoch 2240, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,486: INFO: model_training: Rank 0, Epoch 2240, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,487: INFO: model_training: Rank 0, Epoch 2240, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,489: INFO: model_training: Rank 0, Epoch 2241, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,490: INFO: model_training: Rank 0, Epoch 2241, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,491: INFO: model_training: Rank 0, Epoch 2241, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,492: INFO: model_training: Rank 0, Epoch 2241, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,493: INFO: model_training: Rank 0, Epoch 2241, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,494: INFO: model_training: Rank 0, Epoch 2242, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,495: INFO: model_training: Rank 0, Epoch 2242, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,496: INFO: model_training: Rank 0, Epoch 2242, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,497: INFO: model_training: Rank 0, Epoch 2242, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,498: INFO: model_training: Rank 0, Epoch 2242, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,499: INFO: model_training: Rank 0, Epoch 2243, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,501: INFO: model_training: Rank 0, Epoch 2243, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,502: INFO: model_training: Rank 0, Epoch 2243, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,504: INFO: model_training: Rank 0, Epoch 2243, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,505: INFO: model_training: Rank 0, Epoch 2243, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,506: INFO: model_training: Rank 0, Epoch 2244, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,507: INFO: model_training: Rank 0, Epoch 2244, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,508: INFO: model_training: Rank 0, Epoch 2244, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,510: INFO: model_training: Rank 0, Epoch 2244, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,511: INFO: model_training: Rank 0, Epoch 2244, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,512: INFO: model_training: Rank 0, Epoch 2245, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,513: INFO: model_training: Rank 0, Epoch 2245, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,514: INFO: model_training: Rank 0, Epoch 2245, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,515: INFO: model_training: Rank 0, Epoch 2245, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,516: INFO: model_training: Rank 0, Epoch 2245, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,517: INFO: model_training: Rank 0, Epoch 2246, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,518: INFO: model_training: Rank 0, Epoch 2246, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,519: INFO: model_training: Rank 0, Epoch 2246, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,520: INFO: model_training: Rank 0, Epoch 2246, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,521: INFO: model_training: Rank 0, Epoch 2246, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,522: INFO: model_training: Rank 0, Epoch 2247, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,523: INFO: model_training: Rank 0, Epoch 2247, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,525: INFO: model_training: Rank 0, Epoch 2247, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,526: INFO: model_training: Rank 0, Epoch 2247, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,528: INFO: model_training: Rank 0, Epoch 2247, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,529: INFO: model_training: Rank 0, Epoch 2248, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,530: INFO: model_training: Rank 0, Epoch 2248, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,531: INFO: model_training: Rank 0, Epoch 2248, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,532: INFO: model_training: Rank 0, Epoch 2248, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,533: INFO: model_training: Rank 0, Epoch 2248, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,534: INFO: model_training: Rank 0, Epoch 2249, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,535: INFO: model_training: Rank 0, Epoch 2249, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,536: INFO: model_training: Rank 0, Epoch 2249, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,537: INFO: model_training: Rank 0, Epoch 2249, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,539: INFO: model_training: Rank 0, Epoch 2249, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,540: INFO: model_training: Rank 0, Epoch 2250, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,542: INFO: model_training: Rank 0, Epoch 2250, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,543: INFO: model_training: Rank 0, Epoch 2250, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,544: INFO: model_training: Rank 0, Epoch 2250, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,545: INFO: model_training: Rank 0, Epoch 2250, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,546: INFO: model_training: Rank 0, Epoch 2251, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,548: INFO: model_training: Rank 0, Epoch 2251, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,549: INFO: model_training: Rank 0, Epoch 2251, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,550: INFO: model_training: Rank 0, Epoch 2251, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,551: INFO: model_training: Rank 0, Epoch 2251, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,552: INFO: model_training: Rank 0, Epoch 2252, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,553: INFO: model_training: Rank 0, Epoch 2252, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,554: INFO: model_training: Rank 0, Epoch 2252, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,555: INFO: model_training: Rank 0, Epoch 2252, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,557: INFO: model_training: Rank 0, Epoch 2252, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,558: INFO: model_training: Rank 0, Epoch 2253, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,559: INFO: model_training: Rank 0, Epoch 2253, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,560: INFO: model_training: Rank 0, Epoch 2253, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,561: INFO: model_training: Rank 0, Epoch 2253, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,562: INFO: model_training: Rank 0, Epoch 2253, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,563: INFO: model_training: Rank 0, Epoch 2254, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,564: INFO: model_training: Rank 0, Epoch 2254, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,565: INFO: model_training: Rank 0, Epoch 2254, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,566: INFO: model_training: Rank 0, Epoch 2254, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,568: INFO: model_training: Rank 0, Epoch 2254, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,569: INFO: model_training: Rank 0, Epoch 2255, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,570: INFO: model_training: Rank 0, Epoch 2255, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,572: INFO: model_training: Rank 0, Epoch 2255, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,573: INFO: model_training: Rank 0, Epoch 2255, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,574: INFO: model_training: Rank 0, Epoch 2255, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,575: INFO: model_training: Rank 0, Epoch 2256, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,576: INFO: model_training: Rank 0, Epoch 2256, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,577: INFO: model_training: Rank 0, Epoch 2256, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,579: INFO: model_training: Rank 0, Epoch 2256, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,580: INFO: model_training: Rank 0, Epoch 2256, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,581: INFO: model_training: Rank 0, Epoch 2257, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,582: INFO: model_training: Rank 0, Epoch 2257, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,583: INFO: model_training: Rank 0, Epoch 2257, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,585: INFO: model_training: Rank 0, Epoch 2257, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,586: INFO: model_training: Rank 0, Epoch 2257, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,587: INFO: model_training: Rank 0, Epoch 2258, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,589: INFO: model_training: Rank 0, Epoch 2258, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,592: INFO: model_training: Rank 0, Epoch 2258, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,593: INFO: model_training: Rank 0, Epoch 2258, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,594: INFO: model_training: Rank 0, Epoch 2258, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,596: INFO: model_training: Rank 0, Epoch 2259, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,598: INFO: model_training: Rank 0, Epoch 2259, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,600: INFO: model_training: Rank 0, Epoch 2259, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,601: INFO: model_training: Rank 0, Epoch 2259, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,603: INFO: model_training: Rank 0, Epoch 2259, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,605: INFO: model_training: Rank 0, Epoch 2260, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,606: INFO: model_training: Rank 0, Epoch 2260, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,607: INFO: model_training: Rank 0, Epoch 2260, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,609: INFO: model_training: Rank 0, Epoch 2260, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,610: INFO: model_training: Rank 0, Epoch 2260, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,611: INFO: model_training: Rank 0, Epoch 2261, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,612: INFO: model_training: Rank 0, Epoch 2261, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,613: INFO: model_training: Rank 0, Epoch 2261, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,614: INFO: model_training: Rank 0, Epoch 2261, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,616: INFO: model_training: Rank 0, Epoch 2261, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,618: INFO: model_training: Rank 0, Epoch 2262, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,620: INFO: model_training: Rank 0, Epoch 2262, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,621: INFO: model_training: Rank 0, Epoch 2262, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,622: INFO: model_training: Rank 0, Epoch 2262, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,624: INFO: model_training: Rank 0, Epoch 2262, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,626: INFO: model_training: Rank 0, Epoch 2263, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,627: INFO: model_training: Rank 0, Epoch 2263, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,629: INFO: model_training: Rank 0, Epoch 2263, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,630: INFO: model_training: Rank 0, Epoch 2263, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,631: INFO: model_training: Rank 0, Epoch 2263, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,633: INFO: model_training: Rank 0, Epoch 2264, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,636: INFO: model_training: Rank 0, Epoch 2264, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,637: INFO: model_training: Rank 0, Epoch 2264, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,639: INFO: model_training: Rank 0, Epoch 2264, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,641: INFO: model_training: Rank 0, Epoch 2264, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,643: INFO: model_training: Rank 0, Epoch 2265, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,645: INFO: model_training: Rank 0, Epoch 2265, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,647: INFO: model_training: Rank 0, Epoch 2265, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,648: INFO: model_training: Rank 0, Epoch 2265, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,652: INFO: model_training: Rank 0, Epoch 2265, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,654: INFO: model_training: Rank 0, Epoch 2266, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,656: INFO: model_training: Rank 0, Epoch 2266, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,657: INFO: model_training: Rank 0, Epoch 2266, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,658: INFO: model_training: Rank 0, Epoch 2266, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,659: INFO: model_training: Rank 0, Epoch 2266, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,661: INFO: model_training: Rank 0, Epoch 2267, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,662: INFO: model_training: Rank 0, Epoch 2267, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,663: INFO: model_training: Rank 0, Epoch 2267, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,664: INFO: model_training: Rank 0, Epoch 2267, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,666: INFO: model_training: Rank 0, Epoch 2267, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,668: INFO: model_training: Rank 0, Epoch 2268, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,669: INFO: model_training: Rank 0, Epoch 2268, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,671: INFO: model_training: Rank 0, Epoch 2268, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,673: INFO: model_training: Rank 0, Epoch 2268, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,674: INFO: model_training: Rank 0, Epoch 2268, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,675: INFO: model_training: Rank 0, Epoch 2269, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,677: INFO: model_training: Rank 0, Epoch 2269, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,678: INFO: model_training: Rank 0, Epoch 2269, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,680: INFO: model_training: Rank 0, Epoch 2269, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,681: INFO: model_training: Rank 0, Epoch 2269, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,683: INFO: model_training: Rank 0, Epoch 2270, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,684: INFO: model_training: Rank 0, Epoch 2270, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,687: INFO: model_training: Rank 0, Epoch 2270, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,689: INFO: model_training: Rank 0, Epoch 2270, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,691: INFO: model_training: Rank 0, Epoch 2270, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,692: INFO: model_training: Rank 0, Epoch 2271, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,694: INFO: model_training: Rank 0, Epoch 2271, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,695: INFO: model_training: Rank 0, Epoch 2271, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,696: INFO: model_training: Rank 0, Epoch 2271, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,697: INFO: model_training: Rank 0, Epoch 2271, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,700: INFO: model_training: Rank 0, Epoch 2272, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,701: INFO: model_training: Rank 0, Epoch 2272, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,703: INFO: model_training: Rank 0, Epoch 2272, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,705: INFO: model_training: Rank 0, Epoch 2272, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,707: INFO: model_training: Rank 0, Epoch 2272, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,709: INFO: model_training: Rank 0, Epoch 2273, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,710: INFO: model_training: Rank 0, Epoch 2273, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,711: INFO: model_training: Rank 0, Epoch 2273, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,712: INFO: model_training: Rank 0, Epoch 2273, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,713: INFO: model_training: Rank 0, Epoch 2273, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,715: INFO: model_training: Rank 0, Epoch 2274, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,716: INFO: model_training: Rank 0, Epoch 2274, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,719: INFO: model_training: Rank 0, Epoch 2274, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,720: INFO: model_training: Rank 0, Epoch 2274, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,722: INFO: model_training: Rank 0, Epoch 2274, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,724: INFO: model_training: Rank 0, Epoch 2275, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,725: INFO: model_training: Rank 0, Epoch 2275, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,727: INFO: model_training: Rank 0, Epoch 2275, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,728: INFO: model_training: Rank 0, Epoch 2275, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,729: INFO: model_training: Rank 0, Epoch 2275, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,730: INFO: model_training: Rank 0, Epoch 2276, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,731: INFO: model_training: Rank 0, Epoch 2276, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,733: INFO: model_training: Rank 0, Epoch 2276, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,735: INFO: model_training: Rank 0, Epoch 2276, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,736: INFO: model_training: Rank 0, Epoch 2276, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,738: INFO: model_training: Rank 0, Epoch 2277, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,739: INFO: model_training: Rank 0, Epoch 2277, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,741: INFO: model_training: Rank 0, Epoch 2277, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,742: INFO: model_training: Rank 0, Epoch 2277, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,743: INFO: model_training: Rank 0, Epoch 2277, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,744: INFO: model_training: Rank 0, Epoch 2278, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,746: INFO: model_training: Rank 0, Epoch 2278, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,747: INFO: model_training: Rank 0, Epoch 2278, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,748: INFO: model_training: Rank 0, Epoch 2278, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,750: INFO: model_training: Rank 0, Epoch 2278, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,752: INFO: model_training: Rank 0, Epoch 2279, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,754: INFO: model_training: Rank 0, Epoch 2279, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,755: INFO: model_training: Rank 0, Epoch 2279, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,758: INFO: model_training: Rank 0, Epoch 2279, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,759: INFO: model_training: Rank 0, Epoch 2279, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,760: INFO: model_training: Rank 0, Epoch 2280, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,762: INFO: model_training: Rank 0, Epoch 2280, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,763: INFO: model_training: Rank 0, Epoch 2280, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,765: INFO: model_training: Rank 0, Epoch 2280, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,767: INFO: model_training: Rank 0, Epoch 2280, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,769: INFO: model_training: Rank 0, Epoch 2281, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,771: INFO: model_training: Rank 0, Epoch 2281, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,773: INFO: model_training: Rank 0, Epoch 2281, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,774: INFO: model_training: Rank 0, Epoch 2281, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,775: INFO: model_training: Rank 0, Epoch 2281, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,776: INFO: model_training: Rank 0, Epoch 2282, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,778: INFO: model_training: Rank 0, Epoch 2282, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,779: INFO: model_training: Rank 0, Epoch 2282, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,781: INFO: model_training: Rank 0, Epoch 2282, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,782: INFO: model_training: Rank 0, Epoch 2282, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,784: INFO: model_training: Rank 0, Epoch 2283, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,786: INFO: model_training: Rank 0, Epoch 2283, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,787: INFO: model_training: Rank 0, Epoch 2283, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,789: INFO: model_training: Rank 0, Epoch 2283, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,791: INFO: model_training: Rank 0, Epoch 2283, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,792: INFO: model_training: Rank 0, Epoch 2284, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,793: INFO: model_training: Rank 0, Epoch 2284, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,794: INFO: model_training: Rank 0, Epoch 2284, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,795: INFO: model_training: Rank 0, Epoch 2284, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,796: INFO: model_training: Rank 0, Epoch 2284, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,798: INFO: model_training: Rank 0, Epoch 2285, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,799: INFO: model_training: Rank 0, Epoch 2285, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,802: INFO: model_training: Rank 0, Epoch 2285, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,805: INFO: model_training: Rank 0, Epoch 2285, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,807: INFO: model_training: Rank 0, Epoch 2285, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,808: INFO: model_training: Rank 0, Epoch 2286, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,809: INFO: model_training: Rank 0, Epoch 2286, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,811: INFO: model_training: Rank 0, Epoch 2286, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,812: INFO: model_training: Rank 0, Epoch 2286, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,813: INFO: model_training: Rank 0, Epoch 2286, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,815: INFO: model_training: Rank 0, Epoch 2287, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,816: INFO: model_training: Rank 0, Epoch 2287, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,818: INFO: model_training: Rank 0, Epoch 2287, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,820: INFO: model_training: Rank 0, Epoch 2287, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,822: INFO: model_training: Rank 0, Epoch 2287, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,824: INFO: model_training: Rank 0, Epoch 2288, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,825: INFO: model_training: Rank 0, Epoch 2288, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,827: INFO: model_training: Rank 0, Epoch 2288, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,828: INFO: model_training: Rank 0, Epoch 2288, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,829: INFO: model_training: Rank 0, Epoch 2288, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,830: INFO: model_training: Rank 0, Epoch 2289, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,833: INFO: model_training: Rank 0, Epoch 2289, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,835: INFO: model_training: Rank 0, Epoch 2289, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,836: INFO: model_training: Rank 0, Epoch 2289, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,838: INFO: model_training: Rank 0, Epoch 2289, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,840: INFO: model_training: Rank 0, Epoch 2290, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,841: INFO: model_training: Rank 0, Epoch 2290, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,842: INFO: model_training: Rank 0, Epoch 2290, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,843: INFO: model_training: Rank 0, Epoch 2290, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,844: INFO: model_training: Rank 0, Epoch 2290, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,846: INFO: model_training: Rank 0, Epoch 2291, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,847: INFO: model_training: Rank 0, Epoch 2291, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,849: INFO: model_training: Rank 0, Epoch 2291, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,850: INFO: model_training: Rank 0, Epoch 2291, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,851: INFO: model_training: Rank 0, Epoch 2291, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,853: INFO: model_training: Rank 0, Epoch 2292, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,854: INFO: model_training: Rank 0, Epoch 2292, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,855: INFO: model_training: Rank 0, Epoch 2292, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,856: INFO: model_training: Rank 0, Epoch 2292, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,857: INFO: model_training: Rank 0, Epoch 2292, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,859: INFO: model_training: Rank 0, Epoch 2293, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,860: INFO: model_training: Rank 0, Epoch 2293, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,861: INFO: model_training: Rank 0, Epoch 2293, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,862: INFO: model_training: Rank 0, Epoch 2293, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,864: INFO: model_training: Rank 0, Epoch 2293, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,865: INFO: model_training: Rank 0, Epoch 2294, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,866: INFO: model_training: Rank 0, Epoch 2294, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,867: INFO: model_training: Rank 0, Epoch 2294, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,868: INFO: model_training: Rank 0, Epoch 2294, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,870: INFO: model_training: Rank 0, Epoch 2294, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,871: INFO: model_training: Rank 0, Epoch 2295, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,872: INFO: model_training: Rank 0, Epoch 2295, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,873: INFO: model_training: Rank 0, Epoch 2295, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,874: INFO: model_training: Rank 0, Epoch 2295, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,875: INFO: model_training: Rank 0, Epoch 2295, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,876: INFO: model_training: Rank 0, Epoch 2296, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,877: INFO: model_training: Rank 0, Epoch 2296, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,878: INFO: model_training: Rank 0, Epoch 2296, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,880: INFO: model_training: Rank 0, Epoch 2296, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,881: INFO: model_training: Rank 0, Epoch 2296, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,882: INFO: model_training: Rank 0, Epoch 2297, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,884: INFO: model_training: Rank 0, Epoch 2297, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,885: INFO: model_training: Rank 0, Epoch 2297, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,886: INFO: model_training: Rank 0, Epoch 2297, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,887: INFO: model_training: Rank 0, Epoch 2297, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,889: INFO: model_training: Rank 0, Epoch 2298, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,890: INFO: model_training: Rank 0, Epoch 2298, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,891: INFO: model_training: Rank 0, Epoch 2298, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,892: INFO: model_training: Rank 0, Epoch 2298, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,893: INFO: model_training: Rank 0, Epoch 2298, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,894: INFO: model_training: Rank 0, Epoch 2299, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,895: INFO: model_training: Rank 0, Epoch 2299, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,897: INFO: model_training: Rank 0, Epoch 2299, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,898: INFO: model_training: Rank 0, Epoch 2299, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,899: INFO: model_training: Rank 0, Epoch 2299, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,900: INFO: model_training: Rank 0, Epoch 2300, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,902: INFO: model_training: Rank 0, Epoch 2300, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,903: INFO: model_training: Rank 0, Epoch 2300, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,904: INFO: model_training: Rank 0, Epoch 2300, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,906: INFO: model_training: Rank 0, Epoch 2300, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,907: INFO: model_training: Rank 0, Epoch 2301, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,908: INFO: model_training: Rank 0, Epoch 2301, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,909: INFO: model_training: Rank 0, Epoch 2301, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,910: INFO: model_training: Rank 0, Epoch 2301, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,911: INFO: model_training: Rank 0, Epoch 2301, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,913: INFO: model_training: Rank 0, Epoch 2302, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,914: INFO: model_training: Rank 0, Epoch 2302, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,915: INFO: model_training: Rank 0, Epoch 2302, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,916: INFO: model_training: Rank 0, Epoch 2302, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,917: INFO: model_training: Rank 0, Epoch 2302, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,918: INFO: model_training: Rank 0, Epoch 2303, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,919: INFO: model_training: Rank 0, Epoch 2303, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,921: INFO: model_training: Rank 0, Epoch 2303, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,922: INFO: model_training: Rank 0, Epoch 2303, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,923: INFO: model_training: Rank 0, Epoch 2303, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,924: INFO: model_training: Rank 0, Epoch 2304, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,925: INFO: model_training: Rank 0, Epoch 2304, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,926: INFO: model_training: Rank 0, Epoch 2304, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,927: INFO: model_training: Rank 0, Epoch 2304, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,929: INFO: model_training: Rank 0, Epoch 2304, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,930: INFO: model_training: Rank 0, Epoch 2305, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,931: INFO: model_training: Rank 0, Epoch 2305, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,932: INFO: model_training: Rank 0, Epoch 2305, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,933: INFO: model_training: Rank 0, Epoch 2305, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,934: INFO: model_training: Rank 0, Epoch 2305, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,935: INFO: model_training: Rank 0, Epoch 2306, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,937: INFO: model_training: Rank 0, Epoch 2306, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,938: INFO: model_training: Rank 0, Epoch 2306, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,939: INFO: model_training: Rank 0, Epoch 2306, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,940: INFO: model_training: Rank 0, Epoch 2306, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,941: INFO: model_training: Rank 0, Epoch 2307, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,942: INFO: model_training: Rank 0, Epoch 2307, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,943: INFO: model_training: Rank 0, Epoch 2307, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,944: INFO: model_training: Rank 0, Epoch 2307, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,945: INFO: model_training: Rank 0, Epoch 2307, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,946: INFO: model_training: Rank 0, Epoch 2308, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,948: INFO: model_training: Rank 0, Epoch 2308, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,949: INFO: model_training: Rank 0, Epoch 2308, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,951: INFO: model_training: Rank 0, Epoch 2308, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,952: INFO: model_training: Rank 0, Epoch 2308, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,953: INFO: model_training: Rank 0, Epoch 2309, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,954: INFO: model_training: Rank 0, Epoch 2309, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,956: INFO: model_training: Rank 0, Epoch 2309, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,957: INFO: model_training: Rank 0, Epoch 2309, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,957: INFO: model_training: Rank 0, Epoch 2309, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,959: INFO: model_training: Rank 0, Epoch 2310, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,960: INFO: model_training: Rank 0, Epoch 2310, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,961: INFO: model_training: Rank 0, Epoch 2310, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,962: INFO: model_training: Rank 0, Epoch 2310, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,963: INFO: model_training: Rank 0, Epoch 2310, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,964: INFO: model_training: Rank 0, Epoch 2311, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,965: INFO: model_training: Rank 0, Epoch 2311, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,966: INFO: model_training: Rank 0, Epoch 2311, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,967: INFO: model_training: Rank 0, Epoch 2311, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,968: INFO: model_training: Rank 0, Epoch 2311, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,969: INFO: model_training: Rank 0, Epoch 2312, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,971: INFO: model_training: Rank 0, Epoch 2312, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,972: INFO: model_training: Rank 0, Epoch 2312, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,973: INFO: model_training: Rank 0, Epoch 2312, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,974: INFO: model_training: Rank 0, Epoch 2312, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,976: INFO: model_training: Rank 0, Epoch 2313, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,977: INFO: model_training: Rank 0, Epoch 2313, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,978: INFO: model_training: Rank 0, Epoch 2313, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,979: INFO: model_training: Rank 0, Epoch 2313, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,980: INFO: model_training: Rank 0, Epoch 2313, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,981: INFO: model_training: Rank 0, Epoch 2314, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,982: INFO: model_training: Rank 0, Epoch 2314, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,984: INFO: model_training: Rank 0, Epoch 2314, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,984: INFO: model_training: Rank 0, Epoch 2314, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,986: INFO: model_training: Rank 0, Epoch 2314, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,987: INFO: model_training: Rank 0, Epoch 2315, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,988: INFO: model_training: Rank 0, Epoch 2315, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,989: INFO: model_training: Rank 0, Epoch 2315, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,990: INFO: model_training: Rank 0, Epoch 2315, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,991: INFO: model_training: Rank 0, Epoch 2315, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,992: INFO: model_training: Rank 0, Epoch 2316, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:40,994: INFO: model_training: Rank 0, Epoch 2316, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:40,995: INFO: model_training: Rank 0, Epoch 2316, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:40,996: INFO: model_training: Rank 0, Epoch 2316, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:40,997: INFO: model_training: Rank 0, Epoch 2316, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:40,999: INFO: model_training: Rank 0, Epoch 2317, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,000: INFO: model_training: Rank 0, Epoch 2317, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,001: INFO: model_training: Rank 0, Epoch 2317, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,002: INFO: model_training: Rank 0, Epoch 2317, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,003: INFO: model_training: Rank 0, Epoch 2317, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,004: INFO: model_training: Rank 0, Epoch 2318, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,006: INFO: model_training: Rank 0, Epoch 2318, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,007: INFO: model_training: Rank 0, Epoch 2318, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,008: INFO: model_training: Rank 0, Epoch 2318, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,009: INFO: model_training: Rank 0, Epoch 2318, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,010: INFO: model_training: Rank 0, Epoch 2319, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,011: INFO: model_training: Rank 0, Epoch 2319, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,012: INFO: model_training: Rank 0, Epoch 2319, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,013: INFO: model_training: Rank 0, Epoch 2319, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,015: INFO: model_training: Rank 0, Epoch 2319, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,017: INFO: model_training: Rank 0, Epoch 2320, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,019: INFO: model_training: Rank 0, Epoch 2320, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,020: INFO: model_training: Rank 0, Epoch 2320, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,021: INFO: model_training: Rank 0, Epoch 2320, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,022: INFO: model_training: Rank 0, Epoch 2320, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,023: INFO: model_training: Rank 0, Epoch 2321, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,024: INFO: model_training: Rank 0, Epoch 2321, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,025: INFO: model_training: Rank 0, Epoch 2321, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,026: INFO: model_training: Rank 0, Epoch 2321, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,027: INFO: model_training: Rank 0, Epoch 2321, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,029: INFO: model_training: Rank 0, Epoch 2322, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,030: INFO: model_training: Rank 0, Epoch 2322, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,031: INFO: model_training: Rank 0, Epoch 2322, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,032: INFO: model_training: Rank 0, Epoch 2322, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,034: INFO: model_training: Rank 0, Epoch 2322, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,035: INFO: model_training: Rank 0, Epoch 2323, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,036: INFO: model_training: Rank 0, Epoch 2323, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,037: INFO: model_training: Rank 0, Epoch 2323, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,039: INFO: model_training: Rank 0, Epoch 2323, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,040: INFO: model_training: Rank 0, Epoch 2323, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,041: INFO: model_training: Rank 0, Epoch 2324, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,042: INFO: model_training: Rank 0, Epoch 2324, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,044: INFO: model_training: Rank 0, Epoch 2324, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,044: INFO: model_training: Rank 0, Epoch 2324, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,046: INFO: model_training: Rank 0, Epoch 2324, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,047: INFO: model_training: Rank 0, Epoch 2325, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,048: INFO: model_training: Rank 0, Epoch 2325, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,049: INFO: model_training: Rank 0, Epoch 2325, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,050: INFO: model_training: Rank 0, Epoch 2325, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,051: INFO: model_training: Rank 0, Epoch 2325, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,053: INFO: model_training: Rank 0, Epoch 2326, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,054: INFO: model_training: Rank 0, Epoch 2326, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,055: INFO: model_training: Rank 0, Epoch 2326, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,056: INFO: model_training: Rank 0, Epoch 2326, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,057: INFO: model_training: Rank 0, Epoch 2326, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,058: INFO: model_training: Rank 0, Epoch 2327, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,059: INFO: model_training: Rank 0, Epoch 2327, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,060: INFO: model_training: Rank 0, Epoch 2327, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,061: INFO: model_training: Rank 0, Epoch 2327, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,063: INFO: model_training: Rank 0, Epoch 2327, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,064: INFO: model_training: Rank 0, Epoch 2328, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,065: INFO: model_training: Rank 0, Epoch 2328, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,066: INFO: model_training: Rank 0, Epoch 2328, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,067: INFO: model_training: Rank 0, Epoch 2328, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,068: INFO: model_training: Rank 0, Epoch 2328, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,070: INFO: model_training: Rank 0, Epoch 2329, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,071: INFO: model_training: Rank 0, Epoch 2329, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,072: INFO: model_training: Rank 0, Epoch 2329, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,073: INFO: model_training: Rank 0, Epoch 2329, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,075: INFO: model_training: Rank 0, Epoch 2329, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,076: INFO: model_training: Rank 0, Epoch 2330, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,077: INFO: model_training: Rank 0, Epoch 2330, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,079: INFO: model_training: Rank 0, Epoch 2330, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,081: INFO: model_training: Rank 0, Epoch 2330, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,082: INFO: model_training: Rank 0, Epoch 2330, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,084: INFO: model_training: Rank 0, Epoch 2331, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,085: INFO: model_training: Rank 0, Epoch 2331, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,086: INFO: model_training: Rank 0, Epoch 2331, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,088: INFO: model_training: Rank 0, Epoch 2331, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,090: INFO: model_training: Rank 0, Epoch 2331, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,091: INFO: model_training: Rank 0, Epoch 2332, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,092: INFO: model_training: Rank 0, Epoch 2332, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,093: INFO: model_training: Rank 0, Epoch 2332, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,094: INFO: model_training: Rank 0, Epoch 2332, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,096: INFO: model_training: Rank 0, Epoch 2332, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,097: INFO: model_training: Rank 0, Epoch 2333, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,098: INFO: model_training: Rank 0, Epoch 2333, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,099: INFO: model_training: Rank 0, Epoch 2333, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,100: INFO: model_training: Rank 0, Epoch 2333, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,102: INFO: model_training: Rank 0, Epoch 2333, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,103: INFO: model_training: Rank 0, Epoch 2334, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,104: INFO: model_training: Rank 0, Epoch 2334, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,105: INFO: model_training: Rank 0, Epoch 2334, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,106: INFO: model_training: Rank 0, Epoch 2334, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,107: INFO: model_training: Rank 0, Epoch 2334, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,109: INFO: model_training: Rank 0, Epoch 2335, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,111: INFO: model_training: Rank 0, Epoch 2335, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,112: INFO: model_training: Rank 0, Epoch 2335, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,113: INFO: model_training: Rank 0, Epoch 2335, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,115: INFO: model_training: Rank 0, Epoch 2335, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,117: INFO: model_training: Rank 0, Epoch 2336, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,119: INFO: model_training: Rank 0, Epoch 2336, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,122: INFO: model_training: Rank 0, Epoch 2336, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,125: INFO: model_training: Rank 0, Epoch 2336, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,127: INFO: model_training: Rank 0, Epoch 2336, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,128: INFO: model_training: Rank 0, Epoch 2337, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,130: INFO: model_training: Rank 0, Epoch 2337, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,132: INFO: model_training: Rank 0, Epoch 2337, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,134: INFO: model_training: Rank 0, Epoch 2337, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,136: INFO: model_training: Rank 0, Epoch 2337, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,137: INFO: model_training: Rank 0, Epoch 2338, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,139: INFO: model_training: Rank 0, Epoch 2338, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,140: INFO: model_training: Rank 0, Epoch 2338, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,142: INFO: model_training: Rank 0, Epoch 2338, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,144: INFO: model_training: Rank 0, Epoch 2338, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,145: INFO: model_training: Rank 0, Epoch 2339, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,146: INFO: model_training: Rank 0, Epoch 2339, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,149: INFO: model_training: Rank 0, Epoch 2339, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,150: INFO: model_training: Rank 0, Epoch 2339, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,151: INFO: model_training: Rank 0, Epoch 2339, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,152: INFO: model_training: Rank 0, Epoch 2340, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,153: INFO: model_training: Rank 0, Epoch 2340, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,155: INFO: model_training: Rank 0, Epoch 2340, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,156: INFO: model_training: Rank 0, Epoch 2340, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,157: INFO: model_training: Rank 0, Epoch 2340, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,158: INFO: model_training: Rank 0, Epoch 2341, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,160: INFO: model_training: Rank 0, Epoch 2341, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,161: INFO: model_training: Rank 0, Epoch 2341, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,162: INFO: model_training: Rank 0, Epoch 2341, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,163: INFO: model_training: Rank 0, Epoch 2341, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,164: INFO: model_training: Rank 0, Epoch 2342, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,166: INFO: model_training: Rank 0, Epoch 2342, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,167: INFO: model_training: Rank 0, Epoch 2342, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,168: INFO: model_training: Rank 0, Epoch 2342, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,169: INFO: model_training: Rank 0, Epoch 2342, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,172: INFO: model_training: Rank 0, Epoch 2343, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,173: INFO: model_training: Rank 0, Epoch 2343, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,174: INFO: model_training: Rank 0, Epoch 2343, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,176: INFO: model_training: Rank 0, Epoch 2343, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,177: INFO: model_training: Rank 0, Epoch 2343, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,178: INFO: model_training: Rank 0, Epoch 2344, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,179: INFO: model_training: Rank 0, Epoch 2344, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,180: INFO: model_training: Rank 0, Epoch 2344, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,181: INFO: model_training: Rank 0, Epoch 2344, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,182: INFO: model_training: Rank 0, Epoch 2344, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,184: INFO: model_training: Rank 0, Epoch 2345, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,185: INFO: model_training: Rank 0, Epoch 2345, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,186: INFO: model_training: Rank 0, Epoch 2345, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,187: INFO: model_training: Rank 0, Epoch 2345, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,188: INFO: model_training: Rank 0, Epoch 2345, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,190: INFO: model_training: Rank 0, Epoch 2346, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,191: INFO: model_training: Rank 0, Epoch 2346, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,192: INFO: model_training: Rank 0, Epoch 2346, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,193: INFO: model_training: Rank 0, Epoch 2346, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,195: INFO: model_training: Rank 0, Epoch 2346, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,196: INFO: model_training: Rank 0, Epoch 2347, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,197: INFO: model_training: Rank 0, Epoch 2347, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,199: INFO: model_training: Rank 0, Epoch 2347, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,200: INFO: model_training: Rank 0, Epoch 2347, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,202: INFO: model_training: Rank 0, Epoch 2347, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,203: INFO: model_training: Rank 0, Epoch 2348, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,205: INFO: model_training: Rank 0, Epoch 2348, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,206: INFO: model_training: Rank 0, Epoch 2348, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,207: INFO: model_training: Rank 0, Epoch 2348, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,208: INFO: model_training: Rank 0, Epoch 2348, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,210: INFO: model_training: Rank 0, Epoch 2349, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,211: INFO: model_training: Rank 0, Epoch 2349, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,212: INFO: model_training: Rank 0, Epoch 2349, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,213: INFO: model_training: Rank 0, Epoch 2349, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,214: INFO: model_training: Rank 0, Epoch 2349, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,215: INFO: model_training: Rank 0, Epoch 2350, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,217: INFO: model_training: Rank 0, Epoch 2350, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,218: INFO: model_training: Rank 0, Epoch 2350, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,220: INFO: model_training: Rank 0, Epoch 2350, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,221: INFO: model_training: Rank 0, Epoch 2350, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,222: INFO: model_training: Rank 0, Epoch 2351, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,223: INFO: model_training: Rank 0, Epoch 2351, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,224: INFO: model_training: Rank 0, Epoch 2351, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,226: INFO: model_training: Rank 0, Epoch 2351, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,227: INFO: model_training: Rank 0, Epoch 2351, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,228: INFO: model_training: Rank 0, Epoch 2352, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,229: INFO: model_training: Rank 0, Epoch 2352, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,230: INFO: model_training: Rank 0, Epoch 2352, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,232: INFO: model_training: Rank 0, Epoch 2352, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,233: INFO: model_training: Rank 0, Epoch 2352, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,234: INFO: model_training: Rank 0, Epoch 2353, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,235: INFO: model_training: Rank 0, Epoch 2353, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,236: INFO: model_training: Rank 0, Epoch 2353, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,237: INFO: model_training: Rank 0, Epoch 2353, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,238: INFO: model_training: Rank 0, Epoch 2353, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,240: INFO: model_training: Rank 0, Epoch 2354, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,241: INFO: model_training: Rank 0, Epoch 2354, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,242: INFO: model_training: Rank 0, Epoch 2354, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,244: INFO: model_training: Rank 0, Epoch 2354, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,245: INFO: model_training: Rank 0, Epoch 2354, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,246: INFO: model_training: Rank 0, Epoch 2355, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,247: INFO: model_training: Rank 0, Epoch 2355, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,247: INFO: model_training: Rank 0, Epoch 2355, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,249: INFO: model_training: Rank 0, Epoch 2355, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,250: INFO: model_training: Rank 0, Epoch 2355, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,251: INFO: model_training: Rank 0, Epoch 2356, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,252: INFO: model_training: Rank 0, Epoch 2356, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,253: INFO: model_training: Rank 0, Epoch 2356, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,254: INFO: model_training: Rank 0, Epoch 2356, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,255: INFO: model_training: Rank 0, Epoch 2356, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,256: INFO: model_training: Rank 0, Epoch 2357, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,258: INFO: model_training: Rank 0, Epoch 2357, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,259: INFO: model_training: Rank 0, Epoch 2357, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,260: INFO: model_training: Rank 0, Epoch 2357, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,262: INFO: model_training: Rank 0, Epoch 2357, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,263: INFO: model_training: Rank 0, Epoch 2358, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,264: INFO: model_training: Rank 0, Epoch 2358, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,265: INFO: model_training: Rank 0, Epoch 2358, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,267: INFO: model_training: Rank 0, Epoch 2358, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,268: INFO: model_training: Rank 0, Epoch 2358, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,270: INFO: model_training: Rank 0, Epoch 2359, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,271: INFO: model_training: Rank 0, Epoch 2359, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,272: INFO: model_training: Rank 0, Epoch 2359, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,273: INFO: model_training: Rank 0, Epoch 2359, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,274: INFO: model_training: Rank 0, Epoch 2359, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,275: INFO: model_training: Rank 0, Epoch 2360, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,277: INFO: model_training: Rank 0, Epoch 2360, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,278: INFO: model_training: Rank 0, Epoch 2360, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,279: INFO: model_training: Rank 0, Epoch 2360, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,281: INFO: model_training: Rank 0, Epoch 2360, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,282: INFO: model_training: Rank 0, Epoch 2361, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,283: INFO: model_training: Rank 0, Epoch 2361, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,285: INFO: model_training: Rank 0, Epoch 2361, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,287: INFO: model_training: Rank 0, Epoch 2361, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,288: INFO: model_training: Rank 0, Epoch 2361, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,289: INFO: model_training: Rank 0, Epoch 2362, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,290: INFO: model_training: Rank 0, Epoch 2362, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,292: INFO: model_training: Rank 0, Epoch 2362, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,293: INFO: model_training: Rank 0, Epoch 2362, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,294: INFO: model_training: Rank 0, Epoch 2362, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,295: INFO: model_training: Rank 0, Epoch 2363, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,297: INFO: model_training: Rank 0, Epoch 2363, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,298: INFO: model_training: Rank 0, Epoch 2363, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,300: INFO: model_training: Rank 0, Epoch 2363, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,301: INFO: model_training: Rank 0, Epoch 2363, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,303: INFO: model_training: Rank 0, Epoch 2364, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,304: INFO: model_training: Rank 0, Epoch 2364, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,305: INFO: model_training: Rank 0, Epoch 2364, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,307: INFO: model_training: Rank 0, Epoch 2364, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,308: INFO: model_training: Rank 0, Epoch 2364, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,309: INFO: model_training: Rank 0, Epoch 2365, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,310: INFO: model_training: Rank 0, Epoch 2365, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,312: INFO: model_training: Rank 0, Epoch 2365, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,313: INFO: model_training: Rank 0, Epoch 2365, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,314: INFO: model_training: Rank 0, Epoch 2365, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,316: INFO: model_training: Rank 0, Epoch 2366, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,317: INFO: model_training: Rank 0, Epoch 2366, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,317: INFO: model_training: Rank 0, Epoch 2366, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,319: INFO: model_training: Rank 0, Epoch 2366, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,320: INFO: model_training: Rank 0, Epoch 2366, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,321: INFO: model_training: Rank 0, Epoch 2367, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,322: INFO: model_training: Rank 0, Epoch 2367, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,323: INFO: model_training: Rank 0, Epoch 2367, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,324: INFO: model_training: Rank 0, Epoch 2367, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,325: INFO: model_training: Rank 0, Epoch 2367, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,326: INFO: model_training: Rank 0, Epoch 2368, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,327: INFO: model_training: Rank 0, Epoch 2368, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,329: INFO: model_training: Rank 0, Epoch 2368, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,330: INFO: model_training: Rank 0, Epoch 2368, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,331: INFO: model_training: Rank 0, Epoch 2368, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,332: INFO: model_training: Rank 0, Epoch 2369, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,333: INFO: model_training: Rank 0, Epoch 2369, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,334: INFO: model_training: Rank 0, Epoch 2369, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,335: INFO: model_training: Rank 0, Epoch 2369, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,337: INFO: model_training: Rank 0, Epoch 2369, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,338: INFO: model_training: Rank 0, Epoch 2370, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,339: INFO: model_training: Rank 0, Epoch 2370, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,340: INFO: model_training: Rank 0, Epoch 2370, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,341: INFO: model_training: Rank 0, Epoch 2370, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,342: INFO: model_training: Rank 0, Epoch 2370, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,343: INFO: model_training: Rank 0, Epoch 2371, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,344: INFO: model_training: Rank 0, Epoch 2371, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,345: INFO: model_training: Rank 0, Epoch 2371, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,346: INFO: model_training: Rank 0, Epoch 2371, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,347: INFO: model_training: Rank 0, Epoch 2371, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,349: INFO: model_training: Rank 0, Epoch 2372, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,350: INFO: model_training: Rank 0, Epoch 2372, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,351: INFO: model_training: Rank 0, Epoch 2372, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,352: INFO: model_training: Rank 0, Epoch 2372, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,354: INFO: model_training: Rank 0, Epoch 2372, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,355: INFO: model_training: Rank 0, Epoch 2373, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,357: INFO: model_training: Rank 0, Epoch 2373, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,359: INFO: model_training: Rank 0, Epoch 2373, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,361: INFO: model_training: Rank 0, Epoch 2373, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,362: INFO: model_training: Rank 0, Epoch 2373, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,363: INFO: model_training: Rank 0, Epoch 2374, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,364: INFO: model_training: Rank 0, Epoch 2374, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,365: INFO: model_training: Rank 0, Epoch 2374, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,366: INFO: model_training: Rank 0, Epoch 2374, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,367: INFO: model_training: Rank 0, Epoch 2374, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,369: INFO: model_training: Rank 0, Epoch 2375, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,370: INFO: model_training: Rank 0, Epoch 2375, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,371: INFO: model_training: Rank 0, Epoch 2375, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,372: INFO: model_training: Rank 0, Epoch 2375, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,373: INFO: model_training: Rank 0, Epoch 2375, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,374: INFO: model_training: Rank 0, Epoch 2376, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,375: INFO: model_training: Rank 0, Epoch 2376, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,377: INFO: model_training: Rank 0, Epoch 2376, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,378: INFO: model_training: Rank 0, Epoch 2376, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,379: INFO: model_training: Rank 0, Epoch 2376, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,381: INFO: model_training: Rank 0, Epoch 2377, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,382: INFO: model_training: Rank 0, Epoch 2377, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,383: INFO: model_training: Rank 0, Epoch 2377, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,384: INFO: model_training: Rank 0, Epoch 2377, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,385: INFO: model_training: Rank 0, Epoch 2377, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,386: INFO: model_training: Rank 0, Epoch 2378, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,387: INFO: model_training: Rank 0, Epoch 2378, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,388: INFO: model_training: Rank 0, Epoch 2378, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,389: INFO: model_training: Rank 0, Epoch 2378, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,390: INFO: model_training: Rank 0, Epoch 2378, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,391: INFO: model_training: Rank 0, Epoch 2379, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,392: INFO: model_training: Rank 0, Epoch 2379, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,394: INFO: model_training: Rank 0, Epoch 2379, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,395: INFO: model_training: Rank 0, Epoch 2379, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,396: INFO: model_training: Rank 0, Epoch 2379, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,397: INFO: model_training: Rank 0, Epoch 2380, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,398: INFO: model_training: Rank 0, Epoch 2380, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,399: INFO: model_training: Rank 0, Epoch 2380, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,400: INFO: model_training: Rank 0, Epoch 2380, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,402: INFO: model_training: Rank 0, Epoch 2380, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,403: INFO: model_training: Rank 0, Epoch 2381, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,404: INFO: model_training: Rank 0, Epoch 2381, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,405: INFO: model_training: Rank 0, Epoch 2381, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,406: INFO: model_training: Rank 0, Epoch 2381, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,407: INFO: model_training: Rank 0, Epoch 2381, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,408: INFO: model_training: Rank 0, Epoch 2382, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,409: INFO: model_training: Rank 0, Epoch 2382, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,410: INFO: model_training: Rank 0, Epoch 2382, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,411: INFO: model_training: Rank 0, Epoch 2382, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,412: INFO: model_training: Rank 0, Epoch 2382, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,413: INFO: model_training: Rank 0, Epoch 2383, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,415: INFO: model_training: Rank 0, Epoch 2383, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,416: INFO: model_training: Rank 0, Epoch 2383, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,417: INFO: model_training: Rank 0, Epoch 2383, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,418: INFO: model_training: Rank 0, Epoch 2383, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,419: INFO: model_training: Rank 0, Epoch 2384, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,420: INFO: model_training: Rank 0, Epoch 2384, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,421: INFO: model_training: Rank 0, Epoch 2384, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,423: INFO: model_training: Rank 0, Epoch 2384, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,424: INFO: model_training: Rank 0, Epoch 2384, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,425: INFO: model_training: Rank 0, Epoch 2385, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,426: INFO: model_training: Rank 0, Epoch 2385, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,427: INFO: model_training: Rank 0, Epoch 2385, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,428: INFO: model_training: Rank 0, Epoch 2385, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,429: INFO: model_training: Rank 0, Epoch 2385, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,430: INFO: model_training: Rank 0, Epoch 2386, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,431: INFO: model_training: Rank 0, Epoch 2386, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,432: INFO: model_training: Rank 0, Epoch 2386, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,433: INFO: model_training: Rank 0, Epoch 2386, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,434: INFO: model_training: Rank 0, Epoch 2386, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,435: INFO: model_training: Rank 0, Epoch 2387, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,436: INFO: model_training: Rank 0, Epoch 2387, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,437: INFO: model_training: Rank 0, Epoch 2387, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,438: INFO: model_training: Rank 0, Epoch 2387, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,439: INFO: model_training: Rank 0, Epoch 2387, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,440: INFO: model_training: Rank 0, Epoch 2388, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,441: INFO: model_training: Rank 0, Epoch 2388, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,443: INFO: model_training: Rank 0, Epoch 2388, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,444: INFO: model_training: Rank 0, Epoch 2388, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,445: INFO: model_training: Rank 0, Epoch 2388, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,446: INFO: model_training: Rank 0, Epoch 2389, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,447: INFO: model_training: Rank 0, Epoch 2389, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,449: INFO: model_training: Rank 0, Epoch 2389, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,450: INFO: model_training: Rank 0, Epoch 2389, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,451: INFO: model_training: Rank 0, Epoch 2389, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,452: INFO: model_training: Rank 0, Epoch 2390, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,453: INFO: model_training: Rank 0, Epoch 2390, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,454: INFO: model_training: Rank 0, Epoch 2390, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,455: INFO: model_training: Rank 0, Epoch 2390, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,456: INFO: model_training: Rank 0, Epoch 2390, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,457: INFO: model_training: Rank 0, Epoch 2391, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,458: INFO: model_training: Rank 0, Epoch 2391, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,459: INFO: model_training: Rank 0, Epoch 2391, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,461: INFO: model_training: Rank 0, Epoch 2391, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,462: INFO: model_training: Rank 0, Epoch 2391, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,463: INFO: model_training: Rank 0, Epoch 2392, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,464: INFO: model_training: Rank 0, Epoch 2392, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,466: INFO: model_training: Rank 0, Epoch 2392, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,468: INFO: model_training: Rank 0, Epoch 2392, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,469: INFO: model_training: Rank 0, Epoch 2392, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,470: INFO: model_training: Rank 0, Epoch 2393, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,471: INFO: model_training: Rank 0, Epoch 2393, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,472: INFO: model_training: Rank 0, Epoch 2393, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,473: INFO: model_training: Rank 0, Epoch 2393, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,474: INFO: model_training: Rank 0, Epoch 2393, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,476: INFO: model_training: Rank 0, Epoch 2394, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,477: INFO: model_training: Rank 0, Epoch 2394, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,478: INFO: model_training: Rank 0, Epoch 2394, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,479: INFO: model_training: Rank 0, Epoch 2394, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,480: INFO: model_training: Rank 0, Epoch 2394, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,481: INFO: model_training: Rank 0, Epoch 2395, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,482: INFO: model_training: Rank 0, Epoch 2395, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,483: INFO: model_training: Rank 0, Epoch 2395, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,484: INFO: model_training: Rank 0, Epoch 2395, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,485: INFO: model_training: Rank 0, Epoch 2395, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,486: INFO: model_training: Rank 0, Epoch 2396, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,488: INFO: model_training: Rank 0, Epoch 2396, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,489: INFO: model_training: Rank 0, Epoch 2396, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,490: INFO: model_training: Rank 0, Epoch 2396, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,491: INFO: model_training: Rank 0, Epoch 2396, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,492: INFO: model_training: Rank 0, Epoch 2397, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,493: INFO: model_training: Rank 0, Epoch 2397, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,494: INFO: model_training: Rank 0, Epoch 2397, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,495: INFO: model_training: Rank 0, Epoch 2397, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,496: INFO: model_training: Rank 0, Epoch 2397, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,497: INFO: model_training: Rank 0, Epoch 2398, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,498: INFO: model_training: Rank 0, Epoch 2398, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,499: INFO: model_training: Rank 0, Epoch 2398, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,500: INFO: model_training: Rank 0, Epoch 2398, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,501: INFO: model_training: Rank 0, Epoch 2398, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,502: INFO: model_training: Rank 0, Epoch 2399, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,503: INFO: model_training: Rank 0, Epoch 2399, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,505: INFO: model_training: Rank 0, Epoch 2399, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,506: INFO: model_training: Rank 0, Epoch 2399, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,507: INFO: model_training: Rank 0, Epoch 2399, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,509: INFO: model_training: Rank 0, Epoch 2400, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,510: INFO: model_training: Rank 0, Epoch 2400, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,511: INFO: model_training: Rank 0, Epoch 2400, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,512: INFO: model_training: Rank 0, Epoch 2400, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,513: INFO: model_training: Rank 0, Epoch 2400, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,514: INFO: model_training: Rank 0, Epoch 2401, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,515: INFO: model_training: Rank 0, Epoch 2401, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,516: INFO: model_training: Rank 0, Epoch 2401, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,517: INFO: model_training: Rank 0, Epoch 2401, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,518: INFO: model_training: Rank 0, Epoch 2401, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,519: INFO: model_training: Rank 0, Epoch 2402, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,520: INFO: model_training: Rank 0, Epoch 2402, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,521: INFO: model_training: Rank 0, Epoch 2402, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,522: INFO: model_training: Rank 0, Epoch 2402, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,523: INFO: model_training: Rank 0, Epoch 2402, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,524: INFO: model_training: Rank 0, Epoch 2403, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,525: INFO: model_training: Rank 0, Epoch 2403, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,527: INFO: model_training: Rank 0, Epoch 2403, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,528: INFO: model_training: Rank 0, Epoch 2403, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,529: INFO: model_training: Rank 0, Epoch 2403, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,531: INFO: model_training: Rank 0, Epoch 2404, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,532: INFO: model_training: Rank 0, Epoch 2404, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,532: INFO: model_training: Rank 0, Epoch 2404, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,534: INFO: model_training: Rank 0, Epoch 2404, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,535: INFO: model_training: Rank 0, Epoch 2404, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,536: INFO: model_training: Rank 0, Epoch 2405, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,537: INFO: model_training: Rank 0, Epoch 2405, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,538: INFO: model_training: Rank 0, Epoch 2405, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,539: INFO: model_training: Rank 0, Epoch 2405, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,540: INFO: model_training: Rank 0, Epoch 2405, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,541: INFO: model_training: Rank 0, Epoch 2406, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,542: INFO: model_training: Rank 0, Epoch 2406, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,543: INFO: model_training: Rank 0, Epoch 2406, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,544: INFO: model_training: Rank 0, Epoch 2406, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,545: INFO: model_training: Rank 0, Epoch 2406, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,546: INFO: model_training: Rank 0, Epoch 2407, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,547: INFO: model_training: Rank 0, Epoch 2407, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,548: INFO: model_training: Rank 0, Epoch 2407, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,550: INFO: model_training: Rank 0, Epoch 2407, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,551: INFO: model_training: Rank 0, Epoch 2407, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,552: INFO: model_training: Rank 0, Epoch 2408, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,553: INFO: model_training: Rank 0, Epoch 2408, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,554: INFO: model_training: Rank 0, Epoch 2408, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,555: INFO: model_training: Rank 0, Epoch 2408, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,556: INFO: model_training: Rank 0, Epoch 2408, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,557: INFO: model_training: Rank 0, Epoch 2409, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,559: INFO: model_training: Rank 0, Epoch 2409, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,560: INFO: model_training: Rank 0, Epoch 2409, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,561: INFO: model_training: Rank 0, Epoch 2409, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,562: INFO: model_training: Rank 0, Epoch 2409, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,563: INFO: model_training: Rank 0, Epoch 2410, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,564: INFO: model_training: Rank 0, Epoch 2410, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,565: INFO: model_training: Rank 0, Epoch 2410, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,566: INFO: model_training: Rank 0, Epoch 2410, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,567: INFO: model_training: Rank 0, Epoch 2410, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,568: INFO: model_training: Rank 0, Epoch 2411, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,569: INFO: model_training: Rank 0, Epoch 2411, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,571: INFO: model_training: Rank 0, Epoch 2411, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,572: INFO: model_training: Rank 0, Epoch 2411, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,573: INFO: model_training: Rank 0, Epoch 2411, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,575: INFO: model_training: Rank 0, Epoch 2412, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,576: INFO: model_training: Rank 0, Epoch 2412, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,577: INFO: model_training: Rank 0, Epoch 2412, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,578: INFO: model_training: Rank 0, Epoch 2412, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,579: INFO: model_training: Rank 0, Epoch 2412, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,580: INFO: model_training: Rank 0, Epoch 2413, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,581: INFO: model_training: Rank 0, Epoch 2413, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,582: INFO: model_training: Rank 0, Epoch 2413, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,583: INFO: model_training: Rank 0, Epoch 2413, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,584: INFO: model_training: Rank 0, Epoch 2413, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,585: INFO: model_training: Rank 0, Epoch 2414, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,586: INFO: model_training: Rank 0, Epoch 2414, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,587: INFO: model_training: Rank 0, Epoch 2414, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,588: INFO: model_training: Rank 0, Epoch 2414, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,589: INFO: model_training: Rank 0, Epoch 2414, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,591: INFO: model_training: Rank 0, Epoch 2415, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,592: INFO: model_training: Rank 0, Epoch 2415, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,594: INFO: model_training: Rank 0, Epoch 2415, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,594: INFO: model_training: Rank 0, Epoch 2415, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,596: INFO: model_training: Rank 0, Epoch 2415, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,596: INFO: model_training: Rank 0, Epoch 2416, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,598: INFO: model_training: Rank 0, Epoch 2416, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,599: INFO: model_training: Rank 0, Epoch 2416, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,600: INFO: model_training: Rank 0, Epoch 2416, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,601: INFO: model_training: Rank 0, Epoch 2416, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,602: INFO: model_training: Rank 0, Epoch 2417, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,603: INFO: model_training: Rank 0, Epoch 2417, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,604: INFO: model_training: Rank 0, Epoch 2417, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,605: INFO: model_training: Rank 0, Epoch 2417, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,606: INFO: model_training: Rank 0, Epoch 2417, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,607: INFO: model_training: Rank 0, Epoch 2418, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,608: INFO: model_training: Rank 0, Epoch 2418, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,609: INFO: model_training: Rank 0, Epoch 2418, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,610: INFO: model_training: Rank 0, Epoch 2418, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,611: INFO: model_training: Rank 0, Epoch 2418, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,613: INFO: model_training: Rank 0, Epoch 2419, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,614: INFO: model_training: Rank 0, Epoch 2419, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,615: INFO: model_training: Rank 0, Epoch 2419, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,616: INFO: model_training: Rank 0, Epoch 2419, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,617: INFO: model_training: Rank 0, Epoch 2419, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,618: INFO: model_training: Rank 0, Epoch 2420, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,619: INFO: model_training: Rank 0, Epoch 2420, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,620: INFO: model_training: Rank 0, Epoch 2420, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,621: INFO: model_training: Rank 0, Epoch 2420, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,623: INFO: model_training: Rank 0, Epoch 2420, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,623: INFO: model_training: Rank 0, Epoch 2421, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,625: INFO: model_training: Rank 0, Epoch 2421, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,626: INFO: model_training: Rank 0, Epoch 2421, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,626: INFO: model_training: Rank 0, Epoch 2421, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,628: INFO: model_training: Rank 0, Epoch 2421, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,629: INFO: model_training: Rank 0, Epoch 2422, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,630: INFO: model_training: Rank 0, Epoch 2422, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,631: INFO: model_training: Rank 0, Epoch 2422, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,633: INFO: model_training: Rank 0, Epoch 2422, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,634: INFO: model_training: Rank 0, Epoch 2422, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,635: INFO: model_training: Rank 0, Epoch 2423, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,636: INFO: model_training: Rank 0, Epoch 2423, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,637: INFO: model_training: Rank 0, Epoch 2423, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,638: INFO: model_training: Rank 0, Epoch 2423, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,639: INFO: model_training: Rank 0, Epoch 2423, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,640: INFO: model_training: Rank 0, Epoch 2424, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,641: INFO: model_training: Rank 0, Epoch 2424, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,642: INFO: model_training: Rank 0, Epoch 2424, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,643: INFO: model_training: Rank 0, Epoch 2424, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,644: INFO: model_training: Rank 0, Epoch 2424, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,645: INFO: model_training: Rank 0, Epoch 2425, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,646: INFO: model_training: Rank 0, Epoch 2425, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,647: INFO: model_training: Rank 0, Epoch 2425, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,648: INFO: model_training: Rank 0, Epoch 2425, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,649: INFO: model_training: Rank 0, Epoch 2425, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,650: INFO: model_training: Rank 0, Epoch 2426, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,651: INFO: model_training: Rank 0, Epoch 2426, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,652: INFO: model_training: Rank 0, Epoch 2426, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,654: INFO: model_training: Rank 0, Epoch 2426, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,655: INFO: model_training: Rank 0, Epoch 2426, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,656: INFO: model_training: Rank 0, Epoch 2427, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,657: INFO: model_training: Rank 0, Epoch 2427, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,658: INFO: model_training: Rank 0, Epoch 2427, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,659: INFO: model_training: Rank 0, Epoch 2427, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,660: INFO: model_training: Rank 0, Epoch 2427, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,661: INFO: model_training: Rank 0, Epoch 2428, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,662: INFO: model_training: Rank 0, Epoch 2428, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,663: INFO: model_training: Rank 0, Epoch 2428, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,664: INFO: model_training: Rank 0, Epoch 2428, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,665: INFO: model_training: Rank 0, Epoch 2428, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,666: INFO: model_training: Rank 0, Epoch 2429, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,667: INFO: model_training: Rank 0, Epoch 2429, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,668: INFO: model_training: Rank 0, Epoch 2429, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,669: INFO: model_training: Rank 0, Epoch 2429, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,671: INFO: model_training: Rank 0, Epoch 2429, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,672: INFO: model_training: Rank 0, Epoch 2430, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,673: INFO: model_training: Rank 0, Epoch 2430, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,674: INFO: model_training: Rank 0, Epoch 2430, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,675: INFO: model_training: Rank 0, Epoch 2430, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,676: INFO: model_training: Rank 0, Epoch 2430, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,677: INFO: model_training: Rank 0, Epoch 2431, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,679: INFO: model_training: Rank 0, Epoch 2431, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,680: INFO: model_training: Rank 0, Epoch 2431, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,681: INFO: model_training: Rank 0, Epoch 2431, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,682: INFO: model_training: Rank 0, Epoch 2431, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,683: INFO: model_training: Rank 0, Epoch 2432, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,684: INFO: model_training: Rank 0, Epoch 2432, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,685: INFO: model_training: Rank 0, Epoch 2432, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,686: INFO: model_training: Rank 0, Epoch 2432, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,687: INFO: model_training: Rank 0, Epoch 2432, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,688: INFO: model_training: Rank 0, Epoch 2433, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,689: INFO: model_training: Rank 0, Epoch 2433, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,690: INFO: model_training: Rank 0, Epoch 2433, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,691: INFO: model_training: Rank 0, Epoch 2433, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,692: INFO: model_training: Rank 0, Epoch 2433, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,693: INFO: model_training: Rank 0, Epoch 2434, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,695: INFO: model_training: Rank 0, Epoch 2434, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,696: INFO: model_training: Rank 0, Epoch 2434, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,697: INFO: model_training: Rank 0, Epoch 2434, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,698: INFO: model_training: Rank 0, Epoch 2434, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,700: INFO: model_training: Rank 0, Epoch 2435, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,701: INFO: model_training: Rank 0, Epoch 2435, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,702: INFO: model_training: Rank 0, Epoch 2435, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,703: INFO: model_training: Rank 0, Epoch 2435, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,704: INFO: model_training: Rank 0, Epoch 2435, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,705: INFO: model_training: Rank 0, Epoch 2436, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,706: INFO: model_training: Rank 0, Epoch 2436, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,707: INFO: model_training: Rank 0, Epoch 2436, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,708: INFO: model_training: Rank 0, Epoch 2436, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,709: INFO: model_training: Rank 0, Epoch 2436, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,710: INFO: model_training: Rank 0, Epoch 2437, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,711: INFO: model_training: Rank 0, Epoch 2437, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,712: INFO: model_training: Rank 0, Epoch 2437, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,713: INFO: model_training: Rank 0, Epoch 2437, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,714: INFO: model_training: Rank 0, Epoch 2437, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,715: INFO: model_training: Rank 0, Epoch 2438, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,716: INFO: model_training: Rank 0, Epoch 2438, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,717: INFO: model_training: Rank 0, Epoch 2438, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,719: INFO: model_training: Rank 0, Epoch 2438, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,720: INFO: model_training: Rank 0, Epoch 2438, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,721: INFO: model_training: Rank 0, Epoch 2439, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,723: INFO: model_training: Rank 0, Epoch 2439, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,724: INFO: model_training: Rank 0, Epoch 2439, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,725: INFO: model_training: Rank 0, Epoch 2439, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,726: INFO: model_training: Rank 0, Epoch 2439, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,727: INFO: model_training: Rank 0, Epoch 2440, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,728: INFO: model_training: Rank 0, Epoch 2440, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,729: INFO: model_training: Rank 0, Epoch 2440, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,730: INFO: model_training: Rank 0, Epoch 2440, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,731: INFO: model_training: Rank 0, Epoch 2440, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,732: INFO: model_training: Rank 0, Epoch 2441, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,733: INFO: model_training: Rank 0, Epoch 2441, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,734: INFO: model_training: Rank 0, Epoch 2441, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,736: INFO: model_training: Rank 0, Epoch 2441, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,737: INFO: model_training: Rank 0, Epoch 2441, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,738: INFO: model_training: Rank 0, Epoch 2442, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,739: INFO: model_training: Rank 0, Epoch 2442, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,740: INFO: model_training: Rank 0, Epoch 2442, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,741: INFO: model_training: Rank 0, Epoch 2442, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,743: INFO: model_training: Rank 0, Epoch 2442, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,744: INFO: model_training: Rank 0, Epoch 2443, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,745: INFO: model_training: Rank 0, Epoch 2443, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,746: INFO: model_training: Rank 0, Epoch 2443, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,747: INFO: model_training: Rank 0, Epoch 2443, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,748: INFO: model_training: Rank 0, Epoch 2443, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,749: INFO: model_training: Rank 0, Epoch 2444, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,750: INFO: model_training: Rank 0, Epoch 2444, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,751: INFO: model_training: Rank 0, Epoch 2444, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,752: INFO: model_training: Rank 0, Epoch 2444, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,753: INFO: model_training: Rank 0, Epoch 2444, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,755: INFO: model_training: Rank 0, Epoch 2445, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,756: INFO: model_training: Rank 0, Epoch 2445, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,757: INFO: model_training: Rank 0, Epoch 2445, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,758: INFO: model_training: Rank 0, Epoch 2445, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,760: INFO: model_training: Rank 0, Epoch 2445, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,761: INFO: model_training: Rank 0, Epoch 2446, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,764: INFO: model_training: Rank 0, Epoch 2446, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,765: INFO: model_training: Rank 0, Epoch 2446, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,767: INFO: model_training: Rank 0, Epoch 2446, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,769: INFO: model_training: Rank 0, Epoch 2446, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,770: INFO: model_training: Rank 0, Epoch 2447, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,771: INFO: model_training: Rank 0, Epoch 2447, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,772: INFO: model_training: Rank 0, Epoch 2447, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,773: INFO: model_training: Rank 0, Epoch 2447, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,774: INFO: model_training: Rank 0, Epoch 2447, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,776: INFO: model_training: Rank 0, Epoch 2448, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,777: INFO: model_training: Rank 0, Epoch 2448, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,779: INFO: model_training: Rank 0, Epoch 2448, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,781: INFO: model_training: Rank 0, Epoch 2448, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,782: INFO: model_training: Rank 0, Epoch 2448, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,784: INFO: model_training: Rank 0, Epoch 2449, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,785: INFO: model_training: Rank 0, Epoch 2449, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,787: INFO: model_training: Rank 0, Epoch 2449, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,789: INFO: model_training: Rank 0, Epoch 2449, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,790: INFO: model_training: Rank 0, Epoch 2449, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,792: INFO: model_training: Rank 0, Epoch 2450, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,794: INFO: model_training: Rank 0, Epoch 2450, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,796: INFO: model_training: Rank 0, Epoch 2450, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,797: INFO: model_training: Rank 0, Epoch 2450, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,799: INFO: model_training: Rank 0, Epoch 2450, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,800: INFO: model_training: Rank 0, Epoch 2451, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,801: INFO: model_training: Rank 0, Epoch 2451, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,802: INFO: model_training: Rank 0, Epoch 2451, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,803: INFO: model_training: Rank 0, Epoch 2451, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,804: INFO: model_training: Rank 0, Epoch 2451, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,806: INFO: model_training: Rank 0, Epoch 2452, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,807: INFO: model_training: Rank 0, Epoch 2452, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,809: INFO: model_training: Rank 0, Epoch 2452, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,811: INFO: model_training: Rank 0, Epoch 2452, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,814: INFO: model_training: Rank 0, Epoch 2452, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,815: INFO: model_training: Rank 0, Epoch 2453, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,816: INFO: model_training: Rank 0, Epoch 2453, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,817: INFO: model_training: Rank 0, Epoch 2453, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,818: INFO: model_training: Rank 0, Epoch 2453, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,819: INFO: model_training: Rank 0, Epoch 2453, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,821: INFO: model_training: Rank 0, Epoch 2454, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,822: INFO: model_training: Rank 0, Epoch 2454, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,823: INFO: model_training: Rank 0, Epoch 2454, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,824: INFO: model_training: Rank 0, Epoch 2454, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,826: INFO: model_training: Rank 0, Epoch 2454, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,828: INFO: model_training: Rank 0, Epoch 2455, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,830: INFO: model_training: Rank 0, Epoch 2455, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,831: INFO: model_training: Rank 0, Epoch 2455, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,832: INFO: model_training: Rank 0, Epoch 2455, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,833: INFO: model_training: Rank 0, Epoch 2455, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,835: INFO: model_training: Rank 0, Epoch 2456, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,836: INFO: model_training: Rank 0, Epoch 2456, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,837: INFO: model_training: Rank 0, Epoch 2456, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,839: INFO: model_training: Rank 0, Epoch 2456, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,840: INFO: model_training: Rank 0, Epoch 2456, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,842: INFO: model_training: Rank 0, Epoch 2457, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,844: INFO: model_training: Rank 0, Epoch 2457, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,846: INFO: model_training: Rank 0, Epoch 2457, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,848: INFO: model_training: Rank 0, Epoch 2457, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,849: INFO: model_training: Rank 0, Epoch 2457, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,850: INFO: model_training: Rank 0, Epoch 2458, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,851: INFO: model_training: Rank 0, Epoch 2458, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,853: INFO: model_training: Rank 0, Epoch 2458, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,854: INFO: model_training: Rank 0, Epoch 2458, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,855: INFO: model_training: Rank 0, Epoch 2458, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,856: INFO: model_training: Rank 0, Epoch 2459, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,858: INFO: model_training: Rank 0, Epoch 2459, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,860: INFO: model_training: Rank 0, Epoch 2459, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,861: INFO: model_training: Rank 0, Epoch 2459, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,863: INFO: model_training: Rank 0, Epoch 2459, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,865: INFO: model_training: Rank 0, Epoch 2460, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,866: INFO: model_training: Rank 0, Epoch 2460, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,867: INFO: model_training: Rank 0, Epoch 2460, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,868: INFO: model_training: Rank 0, Epoch 2460, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,870: INFO: model_training: Rank 0, Epoch 2460, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,871: INFO: model_training: Rank 0, Epoch 2461, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,872: INFO: model_training: Rank 0, Epoch 2461, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,873: INFO: model_training: Rank 0, Epoch 2461, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,875: INFO: model_training: Rank 0, Epoch 2461, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,877: INFO: model_training: Rank 0, Epoch 2461, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,879: INFO: model_training: Rank 0, Epoch 2462, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,881: INFO: model_training: Rank 0, Epoch 2462, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,882: INFO: model_training: Rank 0, Epoch 2462, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,884: INFO: model_training: Rank 0, Epoch 2462, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,885: INFO: model_training: Rank 0, Epoch 2462, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,886: INFO: model_training: Rank 0, Epoch 2463, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,887: INFO: model_training: Rank 0, Epoch 2463, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,888: INFO: model_training: Rank 0, Epoch 2463, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,889: INFO: model_training: Rank 0, Epoch 2463, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,891: INFO: model_training: Rank 0, Epoch 2463, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,893: INFO: model_training: Rank 0, Epoch 2464, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,895: INFO: model_training: Rank 0, Epoch 2464, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,897: INFO: model_training: Rank 0, Epoch 2464, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,898: INFO: model_training: Rank 0, Epoch 2464, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,899: INFO: model_training: Rank 0, Epoch 2464, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,901: INFO: model_training: Rank 0, Epoch 2465, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,902: INFO: model_training: Rank 0, Epoch 2465, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,902: INFO: model_training: Rank 0, Epoch 2465, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,904: INFO: model_training: Rank 0, Epoch 2465, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,905: INFO: model_training: Rank 0, Epoch 2465, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,906: INFO: model_training: Rank 0, Epoch 2466, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,908: INFO: model_training: Rank 0, Epoch 2466, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,910: INFO: model_training: Rank 0, Epoch 2466, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,912: INFO: model_training: Rank 0, Epoch 2466, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,914: INFO: model_training: Rank 0, Epoch 2466, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,915: INFO: model_training: Rank 0, Epoch 2467, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,917: INFO: model_training: Rank 0, Epoch 2467, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,918: INFO: model_training: Rank 0, Epoch 2467, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,919: INFO: model_training: Rank 0, Epoch 2467, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,920: INFO: model_training: Rank 0, Epoch 2467, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,921: INFO: model_training: Rank 0, Epoch 2468, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,922: INFO: model_training: Rank 0, Epoch 2468, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,924: INFO: model_training: Rank 0, Epoch 2468, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,925: INFO: model_training: Rank 0, Epoch 2468, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,927: INFO: model_training: Rank 0, Epoch 2468, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,929: INFO: model_training: Rank 0, Epoch 2469, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,931: INFO: model_training: Rank 0, Epoch 2469, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,932: INFO: model_training: Rank 0, Epoch 2469, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,934: INFO: model_training: Rank 0, Epoch 2469, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,935: INFO: model_training: Rank 0, Epoch 2469, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,936: INFO: model_training: Rank 0, Epoch 2470, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,937: INFO: model_training: Rank 0, Epoch 2470, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,938: INFO: model_training: Rank 0, Epoch 2470, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,939: INFO: model_training: Rank 0, Epoch 2470, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,940: INFO: model_training: Rank 0, Epoch 2470, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,942: INFO: model_training: Rank 0, Epoch 2471, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,943: INFO: model_training: Rank 0, Epoch 2471, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,945: INFO: model_training: Rank 0, Epoch 2471, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,946: INFO: model_training: Rank 0, Epoch 2471, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,947: INFO: model_training: Rank 0, Epoch 2471, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,948: INFO: model_training: Rank 0, Epoch 2472, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,949: INFO: model_training: Rank 0, Epoch 2472, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,950: INFO: model_training: Rank 0, Epoch 2472, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,952: INFO: model_training: Rank 0, Epoch 2472, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,953: INFO: model_training: Rank 0, Epoch 2472, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,954: INFO: model_training: Rank 0, Epoch 2473, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,955: INFO: model_training: Rank 0, Epoch 2473, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,957: INFO: model_training: Rank 0, Epoch 2473, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,959: INFO: model_training: Rank 0, Epoch 2473, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,960: INFO: model_training: Rank 0, Epoch 2473, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,961: INFO: model_training: Rank 0, Epoch 2474, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,963: INFO: model_training: Rank 0, Epoch 2474, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,964: INFO: model_training: Rank 0, Epoch 2474, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,965: INFO: model_training: Rank 0, Epoch 2474, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,966: INFO: model_training: Rank 0, Epoch 2474, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,967: INFO: model_training: Rank 0, Epoch 2475, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,969: INFO: model_training: Rank 0, Epoch 2475, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,970: INFO: model_training: Rank 0, Epoch 2475, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,971: INFO: model_training: Rank 0, Epoch 2475, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,972: INFO: model_training: Rank 0, Epoch 2475, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,974: INFO: model_training: Rank 0, Epoch 2476, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,975: INFO: model_training: Rank 0, Epoch 2476, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,976: INFO: model_training: Rank 0, Epoch 2476, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,977: INFO: model_training: Rank 0, Epoch 2476, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,979: INFO: model_training: Rank 0, Epoch 2476, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,980: INFO: model_training: Rank 0, Epoch 2477, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,981: INFO: model_training: Rank 0, Epoch 2477, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,982: INFO: model_training: Rank 0, Epoch 2477, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,983: INFO: model_training: Rank 0, Epoch 2477, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,985: INFO: model_training: Rank 0, Epoch 2477, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,986: INFO: model_training: Rank 0, Epoch 2478, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,987: INFO: model_training: Rank 0, Epoch 2478, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,988: INFO: model_training: Rank 0, Epoch 2478, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,990: INFO: model_training: Rank 0, Epoch 2478, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,991: INFO: model_training: Rank 0, Epoch 2478, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,992: INFO: model_training: Rank 0, Epoch 2479, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,993: INFO: model_training: Rank 0, Epoch 2479, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:41,994: INFO: model_training: Rank 0, Epoch 2479, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:41,995: INFO: model_training: Rank 0, Epoch 2479, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:41,996: INFO: model_training: Rank 0, Epoch 2479, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:41,997: INFO: model_training: Rank 0, Epoch 2480, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:41,999: INFO: model_training: Rank 0, Epoch 2480, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,000: INFO: model_training: Rank 0, Epoch 2480, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,001: INFO: model_training: Rank 0, Epoch 2480, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,002: INFO: model_training: Rank 0, Epoch 2480, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,003: INFO: model_training: Rank 0, Epoch 2481, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,004: INFO: model_training: Rank 0, Epoch 2481, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,006: INFO: model_training: Rank 0, Epoch 2481, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,008: INFO: model_training: Rank 0, Epoch 2481, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,009: INFO: model_training: Rank 0, Epoch 2481, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,011: INFO: model_training: Rank 0, Epoch 2482, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,012: INFO: model_training: Rank 0, Epoch 2482, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,013: INFO: model_training: Rank 0, Epoch 2482, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,014: INFO: model_training: Rank 0, Epoch 2482, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,015: INFO: model_training: Rank 0, Epoch 2482, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,016: INFO: model_training: Rank 0, Epoch 2483, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,017: INFO: model_training: Rank 0, Epoch 2483, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,019: INFO: model_training: Rank 0, Epoch 2483, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,020: INFO: model_training: Rank 0, Epoch 2483, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,021: INFO: model_training: Rank 0, Epoch 2483, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,022: INFO: model_training: Rank 0, Epoch 2484, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,023: INFO: model_training: Rank 0, Epoch 2484, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,024: INFO: model_training: Rank 0, Epoch 2484, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,025: INFO: model_training: Rank 0, Epoch 2484, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,026: INFO: model_training: Rank 0, Epoch 2484, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,028: INFO: model_training: Rank 0, Epoch 2485, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,029: INFO: model_training: Rank 0, Epoch 2485, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,031: INFO: model_training: Rank 0, Epoch 2485, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,032: INFO: model_training: Rank 0, Epoch 2485, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,033: INFO: model_training: Rank 0, Epoch 2485, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,034: INFO: model_training: Rank 0, Epoch 2486, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,035: INFO: model_training: Rank 0, Epoch 2486, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,036: INFO: model_training: Rank 0, Epoch 2486, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,037: INFO: model_training: Rank 0, Epoch 2486, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,039: INFO: model_training: Rank 0, Epoch 2486, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,040: INFO: model_training: Rank 0, Epoch 2487, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,041: INFO: model_training: Rank 0, Epoch 2487, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,042: INFO: model_training: Rank 0, Epoch 2487, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,043: INFO: model_training: Rank 0, Epoch 2487, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,044: INFO: model_training: Rank 0, Epoch 2487, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,045: INFO: model_training: Rank 0, Epoch 2488, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,046: INFO: model_training: Rank 0, Epoch 2488, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,048: INFO: model_training: Rank 0, Epoch 2488, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,049: INFO: model_training: Rank 0, Epoch 2488, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,050: INFO: model_training: Rank 0, Epoch 2488, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,051: INFO: model_training: Rank 0, Epoch 2489, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,053: INFO: model_training: Rank 0, Epoch 2489, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,054: INFO: model_training: Rank 0, Epoch 2489, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,055: INFO: model_training: Rank 0, Epoch 2489, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,056: INFO: model_training: Rank 0, Epoch 2489, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,057: INFO: model_training: Rank 0, Epoch 2490, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,059: INFO: model_training: Rank 0, Epoch 2490, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,060: INFO: model_training: Rank 0, Epoch 2490, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,061: INFO: model_training: Rank 0, Epoch 2490, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,062: INFO: model_training: Rank 0, Epoch 2490, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,063: INFO: model_training: Rank 0, Epoch 2491, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,065: INFO: model_training: Rank 0, Epoch 2491, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,066: INFO: model_training: Rank 0, Epoch 2491, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,067: INFO: model_training: Rank 0, Epoch 2491, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,069: INFO: model_training: Rank 0, Epoch 2491, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,070: INFO: model_training: Rank 0, Epoch 2492, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,071: INFO: model_training: Rank 0, Epoch 2492, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,072: INFO: model_training: Rank 0, Epoch 2492, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,074: INFO: model_training: Rank 0, Epoch 2492, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,075: INFO: model_training: Rank 0, Epoch 2492, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,076: INFO: model_training: Rank 0, Epoch 2493, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,078: INFO: model_training: Rank 0, Epoch 2493, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,079: INFO: model_training: Rank 0, Epoch 2493, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,080: INFO: model_training: Rank 0, Epoch 2493, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,082: INFO: model_training: Rank 0, Epoch 2493, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,083: INFO: model_training: Rank 0, Epoch 2494, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,084: INFO: model_training: Rank 0, Epoch 2494, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,085: INFO: model_training: Rank 0, Epoch 2494, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,086: INFO: model_training: Rank 0, Epoch 2494, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,087: INFO: model_training: Rank 0, Epoch 2494, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,088: INFO: model_training: Rank 0, Epoch 2495, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,089: INFO: model_training: Rank 0, Epoch 2495, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,091: INFO: model_training: Rank 0, Epoch 2495, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,092: INFO: model_training: Rank 0, Epoch 2495, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,094: INFO: model_training: Rank 0, Epoch 2495, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,095: INFO: model_training: Rank 0, Epoch 2496, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,096: INFO: model_training: Rank 0, Epoch 2496, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,097: INFO: model_training: Rank 0, Epoch 2496, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,098: INFO: model_training: Rank 0, Epoch 2496, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,099: INFO: model_training: Rank 0, Epoch 2496, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,101: INFO: model_training: Rank 0, Epoch 2497, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,102: INFO: model_training: Rank 0, Epoch 2497, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,103: INFO: model_training: Rank 0, Epoch 2497, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,104: INFO: model_training: Rank 0, Epoch 2497, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,106: INFO: model_training: Rank 0, Epoch 2497, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,107: INFO: model_training: Rank 0, Epoch 2498, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,108: INFO: model_training: Rank 0, Epoch 2498, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,109: INFO: model_training: Rank 0, Epoch 2498, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,110: INFO: model_training: Rank 0, Epoch 2498, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,111: INFO: model_training: Rank 0, Epoch 2498, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,112: INFO: model_training: Rank 0, Epoch 2499, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,114: INFO: model_training: Rank 0, Epoch 2499, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,115: INFO: model_training: Rank 0, Epoch 2499, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,116: INFO: model_training: Rank 0, Epoch 2499, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,117: INFO: model_training: Rank 0, Epoch 2499, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,118: INFO: model_training: Rank 0, Epoch 2500, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,119: INFO: model_training: Rank 0, Epoch 2500, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,120: INFO: model_training: Rank 0, Epoch 2500, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,121: INFO: model_training: Rank 0, Epoch 2500, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,122: INFO: model_training: Rank 0, Epoch 2500, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,123: INFO: model_training: Rank 0, Epoch 2501, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,125: INFO: model_training: Rank 0, Epoch 2501, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,126: INFO: model_training: Rank 0, Epoch 2501, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,127: INFO: model_training: Rank 0, Epoch 2501, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,128: INFO: model_training: Rank 0, Epoch 2501, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,129: INFO: model_training: Rank 0, Epoch 2502, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,130: INFO: model_training: Rank 0, Epoch 2502, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,131: INFO: model_training: Rank 0, Epoch 2502, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,132: INFO: model_training: Rank 0, Epoch 2502, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,133: INFO: model_training: Rank 0, Epoch 2502, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,134: INFO: model_training: Rank 0, Epoch 2503, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,135: INFO: model_training: Rank 0, Epoch 2503, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,136: INFO: model_training: Rank 0, Epoch 2503, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,137: INFO: model_training: Rank 0, Epoch 2503, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,139: INFO: model_training: Rank 0, Epoch 2503, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,140: INFO: model_training: Rank 0, Epoch 2504, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,141: INFO: model_training: Rank 0, Epoch 2504, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,142: INFO: model_training: Rank 0, Epoch 2504, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,143: INFO: model_training: Rank 0, Epoch 2504, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,144: INFO: model_training: Rank 0, Epoch 2504, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,145: INFO: model_training: Rank 0, Epoch 2505, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,146: INFO: model_training: Rank 0, Epoch 2505, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,148: INFO: model_training: Rank 0, Epoch 2505, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,149: INFO: model_training: Rank 0, Epoch 2505, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,150: INFO: model_training: Rank 0, Epoch 2505, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,151: INFO: model_training: Rank 0, Epoch 2506, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,152: INFO: model_training: Rank 0, Epoch 2506, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,153: INFO: model_training: Rank 0, Epoch 2506, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,154: INFO: model_training: Rank 0, Epoch 2506, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,155: INFO: model_training: Rank 0, Epoch 2506, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,156: INFO: model_training: Rank 0, Epoch 2507, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,157: INFO: model_training: Rank 0, Epoch 2507, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,158: INFO: model_training: Rank 0, Epoch 2507, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,159: INFO: model_training: Rank 0, Epoch 2507, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,160: INFO: model_training: Rank 0, Epoch 2507, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,161: INFO: model_training: Rank 0, Epoch 2508, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,162: INFO: model_training: Rank 0, Epoch 2508, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,164: INFO: model_training: Rank 0, Epoch 2508, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,165: INFO: model_training: Rank 0, Epoch 2508, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,166: INFO: model_training: Rank 0, Epoch 2508, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,167: INFO: model_training: Rank 0, Epoch 2509, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,169: INFO: model_training: Rank 0, Epoch 2509, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,170: INFO: model_training: Rank 0, Epoch 2509, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,171: INFO: model_training: Rank 0, Epoch 2509, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,172: INFO: model_training: Rank 0, Epoch 2509, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,173: INFO: model_training: Rank 0, Epoch 2510, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,174: INFO: model_training: Rank 0, Epoch 2510, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,175: INFO: model_training: Rank 0, Epoch 2510, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,176: INFO: model_training: Rank 0, Epoch 2510, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,177: INFO: model_training: Rank 0, Epoch 2510, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,178: INFO: model_training: Rank 0, Epoch 2511, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,179: INFO: model_training: Rank 0, Epoch 2511, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,180: INFO: model_training: Rank 0, Epoch 2511, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,181: INFO: model_training: Rank 0, Epoch 2511, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,182: INFO: model_training: Rank 0, Epoch 2511, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,183: INFO: model_training: Rank 0, Epoch 2512, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,184: INFO: model_training: Rank 0, Epoch 2512, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,185: INFO: model_training: Rank 0, Epoch 2512, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,186: INFO: model_training: Rank 0, Epoch 2512, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,187: INFO: model_training: Rank 0, Epoch 2512, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,189: INFO: model_training: Rank 0, Epoch 2513, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,190: INFO: model_training: Rank 0, Epoch 2513, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,191: INFO: model_training: Rank 0, Epoch 2513, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,193: INFO: model_training: Rank 0, Epoch 2513, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,194: INFO: model_training: Rank 0, Epoch 2513, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,195: INFO: model_training: Rank 0, Epoch 2514, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,196: INFO: model_training: Rank 0, Epoch 2514, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,197: INFO: model_training: Rank 0, Epoch 2514, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,198: INFO: model_training: Rank 0, Epoch 2514, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,199: INFO: model_training: Rank 0, Epoch 2514, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,200: INFO: model_training: Rank 0, Epoch 2515, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,201: INFO: model_training: Rank 0, Epoch 2515, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,202: INFO: model_training: Rank 0, Epoch 2515, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,203: INFO: model_training: Rank 0, Epoch 2515, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,204: INFO: model_training: Rank 0, Epoch 2515, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,205: INFO: model_training: Rank 0, Epoch 2516, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,207: INFO: model_training: Rank 0, Epoch 2516, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,208: INFO: model_training: Rank 0, Epoch 2516, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,209: INFO: model_training: Rank 0, Epoch 2516, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,210: INFO: model_training: Rank 0, Epoch 2516, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,211: INFO: model_training: Rank 0, Epoch 2517, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,213: INFO: model_training: Rank 0, Epoch 2517, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,214: INFO: model_training: Rank 0, Epoch 2517, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,215: INFO: model_training: Rank 0, Epoch 2517, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,216: INFO: model_training: Rank 0, Epoch 2517, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,217: INFO: model_training: Rank 0, Epoch 2518, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,218: INFO: model_training: Rank 0, Epoch 2518, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,219: INFO: model_training: Rank 0, Epoch 2518, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,220: INFO: model_training: Rank 0, Epoch 2518, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,221: INFO: model_training: Rank 0, Epoch 2518, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,222: INFO: model_training: Rank 0, Epoch 2519, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,223: INFO: model_training: Rank 0, Epoch 2519, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,225: INFO: model_training: Rank 0, Epoch 2519, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,226: INFO: model_training: Rank 0, Epoch 2519, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,227: INFO: model_training: Rank 0, Epoch 2519, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,228: INFO: model_training: Rank 0, Epoch 2520, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,229: INFO: model_training: Rank 0, Epoch 2520, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,230: INFO: model_training: Rank 0, Epoch 2520, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,231: INFO: model_training: Rank 0, Epoch 2520, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,233: INFO: model_training: Rank 0, Epoch 2520, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,234: INFO: model_training: Rank 0, Epoch 2521, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,235: INFO: model_training: Rank 0, Epoch 2521, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,236: INFO: model_training: Rank 0, Epoch 2521, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,237: INFO: model_training: Rank 0, Epoch 2521, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,238: INFO: model_training: Rank 0, Epoch 2521, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,240: INFO: model_training: Rank 0, Epoch 2522, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,241: INFO: model_training: Rank 0, Epoch 2522, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,242: INFO: model_training: Rank 0, Epoch 2522, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,243: INFO: model_training: Rank 0, Epoch 2522, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,244: INFO: model_training: Rank 0, Epoch 2522, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,245: INFO: model_training: Rank 0, Epoch 2523, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,246: INFO: model_training: Rank 0, Epoch 2523, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,247: INFO: model_training: Rank 0, Epoch 2523, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,248: INFO: model_training: Rank 0, Epoch 2523, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,249: INFO: model_training: Rank 0, Epoch 2523, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,250: INFO: model_training: Rank 0, Epoch 2524, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,251: INFO: model_training: Rank 0, Epoch 2524, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,253: INFO: model_training: Rank 0, Epoch 2524, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,254: INFO: model_training: Rank 0, Epoch 2524, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,255: INFO: model_training: Rank 0, Epoch 2524, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,256: INFO: model_training: Rank 0, Epoch 2525, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,257: INFO: model_training: Rank 0, Epoch 2525, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,259: INFO: model_training: Rank 0, Epoch 2525, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,260: INFO: model_training: Rank 0, Epoch 2525, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,260: INFO: model_training: Rank 0, Epoch 2525, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,262: INFO: model_training: Rank 0, Epoch 2526, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,262: INFO: model_training: Rank 0, Epoch 2526, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,264: INFO: model_training: Rank 0, Epoch 2526, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,265: INFO: model_training: Rank 0, Epoch 2526, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,266: INFO: model_training: Rank 0, Epoch 2526, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,267: INFO: model_training: Rank 0, Epoch 2527, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,268: INFO: model_training: Rank 0, Epoch 2527, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,269: INFO: model_training: Rank 0, Epoch 2527, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,270: INFO: model_training: Rank 0, Epoch 2527, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,271: INFO: model_training: Rank 0, Epoch 2527, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,272: INFO: model_training: Rank 0, Epoch 2528, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,274: INFO: model_training: Rank 0, Epoch 2528, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,275: INFO: model_training: Rank 0, Epoch 2528, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,276: INFO: model_training: Rank 0, Epoch 2528, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,277: INFO: model_training: Rank 0, Epoch 2528, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,278: INFO: model_training: Rank 0, Epoch 2529, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,279: INFO: model_training: Rank 0, Epoch 2529, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,281: INFO: model_training: Rank 0, Epoch 2529, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,282: INFO: model_training: Rank 0, Epoch 2529, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,283: INFO: model_training: Rank 0, Epoch 2529, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,284: INFO: model_training: Rank 0, Epoch 2530, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,285: INFO: model_training: Rank 0, Epoch 2530, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,286: INFO: model_training: Rank 0, Epoch 2530, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,287: INFO: model_training: Rank 0, Epoch 2530, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,289: INFO: model_training: Rank 0, Epoch 2530, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,290: INFO: model_training: Rank 0, Epoch 2531, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,291: INFO: model_training: Rank 0, Epoch 2531, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,292: INFO: model_training: Rank 0, Epoch 2531, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,293: INFO: model_training: Rank 0, Epoch 2531, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,295: INFO: model_training: Rank 0, Epoch 2531, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,296: INFO: model_training: Rank 0, Epoch 2532, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,297: INFO: model_training: Rank 0, Epoch 2532, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,298: INFO: model_training: Rank 0, Epoch 2532, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,299: INFO: model_training: Rank 0, Epoch 2532, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,300: INFO: model_training: Rank 0, Epoch 2532, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,301: INFO: model_training: Rank 0, Epoch 2533, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,302: INFO: model_training: Rank 0, Epoch 2533, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,303: INFO: model_training: Rank 0, Epoch 2533, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,304: INFO: model_training: Rank 0, Epoch 2533, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,305: INFO: model_training: Rank 0, Epoch 2533, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,307: INFO: model_training: Rank 0, Epoch 2534, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,308: INFO: model_training: Rank 0, Epoch 2534, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,309: INFO: model_training: Rank 0, Epoch 2534, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,310: INFO: model_training: Rank 0, Epoch 2534, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,311: INFO: model_training: Rank 0, Epoch 2534, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,312: INFO: model_training: Rank 0, Epoch 2535, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,313: INFO: model_training: Rank 0, Epoch 2535, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,315: INFO: model_training: Rank 0, Epoch 2535, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,316: INFO: model_training: Rank 0, Epoch 2535, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,317: INFO: model_training: Rank 0, Epoch 2535, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,319: INFO: model_training: Rank 0, Epoch 2536, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,320: INFO: model_training: Rank 0, Epoch 2536, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,321: INFO: model_training: Rank 0, Epoch 2536, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,322: INFO: model_training: Rank 0, Epoch 2536, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,323: INFO: model_training: Rank 0, Epoch 2536, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,324: INFO: model_training: Rank 0, Epoch 2537, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,325: INFO: model_training: Rank 0, Epoch 2537, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,326: INFO: model_training: Rank 0, Epoch 2537, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,328: INFO: model_training: Rank 0, Epoch 2537, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,329: INFO: model_training: Rank 0, Epoch 2537, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,331: INFO: model_training: Rank 0, Epoch 2538, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,332: INFO: model_training: Rank 0, Epoch 2538, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,334: INFO: model_training: Rank 0, Epoch 2538, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,335: INFO: model_training: Rank 0, Epoch 2538, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,337: INFO: model_training: Rank 0, Epoch 2538, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,339: INFO: model_training: Rank 0, Epoch 2539, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,340: INFO: model_training: Rank 0, Epoch 2539, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,341: INFO: model_training: Rank 0, Epoch 2539, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,343: INFO: model_training: Rank 0, Epoch 2539, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,344: INFO: model_training: Rank 0, Epoch 2539, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,345: INFO: model_training: Rank 0, Epoch 2540, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,347: INFO: model_training: Rank 0, Epoch 2540, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,348: INFO: model_training: Rank 0, Epoch 2540, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,349: INFO: model_training: Rank 0, Epoch 2540, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,351: INFO: model_training: Rank 0, Epoch 2540, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,352: INFO: model_training: Rank 0, Epoch 2541, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,354: INFO: model_training: Rank 0, Epoch 2541, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,355: INFO: model_training: Rank 0, Epoch 2541, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,356: INFO: model_training: Rank 0, Epoch 2541, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,358: INFO: model_training: Rank 0, Epoch 2541, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,359: INFO: model_training: Rank 0, Epoch 2542, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,360: INFO: model_training: Rank 0, Epoch 2542, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,362: INFO: model_training: Rank 0, Epoch 2542, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,363: INFO: model_training: Rank 0, Epoch 2542, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,364: INFO: model_training: Rank 0, Epoch 2542, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,366: INFO: model_training: Rank 0, Epoch 2543, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,367: INFO: model_training: Rank 0, Epoch 2543, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,368: INFO: model_training: Rank 0, Epoch 2543, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,369: INFO: model_training: Rank 0, Epoch 2543, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,371: INFO: model_training: Rank 0, Epoch 2543, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,373: INFO: model_training: Rank 0, Epoch 2544, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,375: INFO: model_training: Rank 0, Epoch 2544, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,377: INFO: model_training: Rank 0, Epoch 2544, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,380: INFO: model_training: Rank 0, Epoch 2544, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,382: INFO: model_training: Rank 0, Epoch 2544, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,384: INFO: model_training: Rank 0, Epoch 2545, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,385: INFO: model_training: Rank 0, Epoch 2545, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,388: INFO: model_training: Rank 0, Epoch 2545, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,389: INFO: model_training: Rank 0, Epoch 2545, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,392: INFO: model_training: Rank 0, Epoch 2545, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,394: INFO: model_training: Rank 0, Epoch 2546, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,397: INFO: model_training: Rank 0, Epoch 2546, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,399: INFO: model_training: Rank 0, Epoch 2546, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,400: INFO: model_training: Rank 0, Epoch 2546, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,401: INFO: model_training: Rank 0, Epoch 2546, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,403: INFO: model_training: Rank 0, Epoch 2547, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,404: INFO: model_training: Rank 0, Epoch 2547, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,405: INFO: model_training: Rank 0, Epoch 2547, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,406: INFO: model_training: Rank 0, Epoch 2547, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,407: INFO: model_training: Rank 0, Epoch 2547, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,408: INFO: model_training: Rank 0, Epoch 2548, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,410: INFO: model_training: Rank 0, Epoch 2548, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,412: INFO: model_training: Rank 0, Epoch 2548, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,413: INFO: model_training: Rank 0, Epoch 2548, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,414: INFO: model_training: Rank 0, Epoch 2548, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,416: INFO: model_training: Rank 0, Epoch 2549, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,417: INFO: model_training: Rank 0, Epoch 2549, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,418: INFO: model_training: Rank 0, Epoch 2549, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,420: INFO: model_training: Rank 0, Epoch 2549, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,421: INFO: model_training: Rank 0, Epoch 2549, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,422: INFO: model_training: Rank 0, Epoch 2550, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,423: INFO: model_training: Rank 0, Epoch 2550, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,425: INFO: model_training: Rank 0, Epoch 2550, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,426: INFO: model_training: Rank 0, Epoch 2550, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,427: INFO: model_training: Rank 0, Epoch 2550, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,429: INFO: model_training: Rank 0, Epoch 2551, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,430: INFO: model_training: Rank 0, Epoch 2551, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,431: INFO: model_training: Rank 0, Epoch 2551, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,432: INFO: model_training: Rank 0, Epoch 2551, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,433: INFO: model_training: Rank 0, Epoch 2551, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,435: INFO: model_training: Rank 0, Epoch 2552, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,436: INFO: model_training: Rank 0, Epoch 2552, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,437: INFO: model_training: Rank 0, Epoch 2552, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,438: INFO: model_training: Rank 0, Epoch 2552, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,440: INFO: model_training: Rank 0, Epoch 2552, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,441: INFO: model_training: Rank 0, Epoch 2553, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,442: INFO: model_training: Rank 0, Epoch 2553, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,444: INFO: model_training: Rank 0, Epoch 2553, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,446: INFO: model_training: Rank 0, Epoch 2553, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,447: INFO: model_training: Rank 0, Epoch 2553, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,448: INFO: model_training: Rank 0, Epoch 2554, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,449: INFO: model_training: Rank 0, Epoch 2554, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,451: INFO: model_training: Rank 0, Epoch 2554, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,452: INFO: model_training: Rank 0, Epoch 2554, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,453: INFO: model_training: Rank 0, Epoch 2554, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,454: INFO: model_training: Rank 0, Epoch 2555, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,455: INFO: model_training: Rank 0, Epoch 2555, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,456: INFO: model_training: Rank 0, Epoch 2555, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,457: INFO: model_training: Rank 0, Epoch 2555, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,459: INFO: model_training: Rank 0, Epoch 2555, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,461: INFO: model_training: Rank 0, Epoch 2556, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,462: INFO: model_training: Rank 0, Epoch 2556, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,463: INFO: model_training: Rank 0, Epoch 2556, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,464: INFO: model_training: Rank 0, Epoch 2556, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,465: INFO: model_training: Rank 0, Epoch 2556, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,466: INFO: model_training: Rank 0, Epoch 2557, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,467: INFO: model_training: Rank 0, Epoch 2557, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,469: INFO: model_training: Rank 0, Epoch 2557, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,470: INFO: model_training: Rank 0, Epoch 2557, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,471: INFO: model_training: Rank 0, Epoch 2557, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,472: INFO: model_training: Rank 0, Epoch 2558, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,473: INFO: model_training: Rank 0, Epoch 2558, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,475: INFO: model_training: Rank 0, Epoch 2558, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,476: INFO: model_training: Rank 0, Epoch 2558, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,478: INFO: model_training: Rank 0, Epoch 2558, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,480: INFO: model_training: Rank 0, Epoch 2559, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,481: INFO: model_training: Rank 0, Epoch 2559, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,482: INFO: model_training: Rank 0, Epoch 2559, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,483: INFO: model_training: Rank 0, Epoch 2559, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,484: INFO: model_training: Rank 0, Epoch 2559, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,485: INFO: model_training: Rank 0, Epoch 2560, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,486: INFO: model_training: Rank 0, Epoch 2560, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,487: INFO: model_training: Rank 0, Epoch 2560, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,489: INFO: model_training: Rank 0, Epoch 2560, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,490: INFO: model_training: Rank 0, Epoch 2560, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,491: INFO: model_training: Rank 0, Epoch 2561, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,493: INFO: model_training: Rank 0, Epoch 2561, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,494: INFO: model_training: Rank 0, Epoch 2561, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,496: INFO: model_training: Rank 0, Epoch 2561, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,497: INFO: model_training: Rank 0, Epoch 2561, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,498: INFO: model_training: Rank 0, Epoch 2562, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,500: INFO: model_training: Rank 0, Epoch 2562, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,501: INFO: model_training: Rank 0, Epoch 2562, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,502: INFO: model_training: Rank 0, Epoch 2562, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,503: INFO: model_training: Rank 0, Epoch 2562, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,504: INFO: model_training: Rank 0, Epoch 2563, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,505: INFO: model_training: Rank 0, Epoch 2563, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,507: INFO: model_training: Rank 0, Epoch 2563, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,508: INFO: model_training: Rank 0, Epoch 2563, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,510: INFO: model_training: Rank 0, Epoch 2563, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,511: INFO: model_training: Rank 0, Epoch 2564, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,512: INFO: model_training: Rank 0, Epoch 2564, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,513: INFO: model_training: Rank 0, Epoch 2564, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,515: INFO: model_training: Rank 0, Epoch 2564, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,516: INFO: model_training: Rank 0, Epoch 2564, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,517: INFO: model_training: Rank 0, Epoch 2565, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,518: INFO: model_training: Rank 0, Epoch 2565, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,519: INFO: model_training: Rank 0, Epoch 2565, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,520: INFO: model_training: Rank 0, Epoch 2565, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,521: INFO: model_training: Rank 0, Epoch 2565, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,522: INFO: model_training: Rank 0, Epoch 2566, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,524: INFO: model_training: Rank 0, Epoch 2566, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,525: INFO: model_training: Rank 0, Epoch 2566, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,527: INFO: model_training: Rank 0, Epoch 2566, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,528: INFO: model_training: Rank 0, Epoch 2566, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,529: INFO: model_training: Rank 0, Epoch 2567, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,530: INFO: model_training: Rank 0, Epoch 2567, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,531: INFO: model_training: Rank 0, Epoch 2567, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,533: INFO: model_training: Rank 0, Epoch 2567, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,534: INFO: model_training: Rank 0, Epoch 2567, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,535: INFO: model_training: Rank 0, Epoch 2568, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,536: INFO: model_training: Rank 0, Epoch 2568, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,537: INFO: model_training: Rank 0, Epoch 2568, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,538: INFO: model_training: Rank 0, Epoch 2568, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,540: INFO: model_training: Rank 0, Epoch 2568, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,541: INFO: model_training: Rank 0, Epoch 2569, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,543: INFO: model_training: Rank 0, Epoch 2569, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,544: INFO: model_training: Rank 0, Epoch 2569, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,545: INFO: model_training: Rank 0, Epoch 2569, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,546: INFO: model_training: Rank 0, Epoch 2569, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,547: INFO: model_training: Rank 0, Epoch 2570, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,549: INFO: model_training: Rank 0, Epoch 2570, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,550: INFO: model_training: Rank 0, Epoch 2570, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,551: INFO: model_training: Rank 0, Epoch 2570, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,552: INFO: model_training: Rank 0, Epoch 2570, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,554: INFO: model_training: Rank 0, Epoch 2571, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,555: INFO: model_training: Rank 0, Epoch 2571, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,556: INFO: model_training: Rank 0, Epoch 2571, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,557: INFO: model_training: Rank 0, Epoch 2571, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,559: INFO: model_training: Rank 0, Epoch 2571, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,561: INFO: model_training: Rank 0, Epoch 2572, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,562: INFO: model_training: Rank 0, Epoch 2572, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,564: INFO: model_training: Rank 0, Epoch 2572, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,565: INFO: model_training: Rank 0, Epoch 2572, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,566: INFO: model_training: Rank 0, Epoch 2572, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,567: INFO: model_training: Rank 0, Epoch 2573, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,568: INFO: model_training: Rank 0, Epoch 2573, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,570: INFO: model_training: Rank 0, Epoch 2573, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,571: INFO: model_training: Rank 0, Epoch 2573, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,572: INFO: model_training: Rank 0, Epoch 2573, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,573: INFO: model_training: Rank 0, Epoch 2574, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,575: INFO: model_training: Rank 0, Epoch 2574, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,578: INFO: model_training: Rank 0, Epoch 2574, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,580: INFO: model_training: Rank 0, Epoch 2574, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,581: INFO: model_training: Rank 0, Epoch 2574, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,583: INFO: model_training: Rank 0, Epoch 2575, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,584: INFO: model_training: Rank 0, Epoch 2575, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,586: INFO: model_training: Rank 0, Epoch 2575, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,587: INFO: model_training: Rank 0, Epoch 2575, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,589: INFO: model_training: Rank 0, Epoch 2575, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,590: INFO: model_training: Rank 0, Epoch 2576, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,593: INFO: model_training: Rank 0, Epoch 2576, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,595: INFO: model_training: Rank 0, Epoch 2576, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,596: INFO: model_training: Rank 0, Epoch 2576, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,598: INFO: model_training: Rank 0, Epoch 2576, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,599: INFO: model_training: Rank 0, Epoch 2577, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,601: INFO: model_training: Rank 0, Epoch 2577, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,602: INFO: model_training: Rank 0, Epoch 2577, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,603: INFO: model_training: Rank 0, Epoch 2577, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,604: INFO: model_training: Rank 0, Epoch 2577, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,606: INFO: model_training: Rank 0, Epoch 2578, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,608: INFO: model_training: Rank 0, Epoch 2578, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,610: INFO: model_training: Rank 0, Epoch 2578, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,612: INFO: model_training: Rank 0, Epoch 2578, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,613: INFO: model_training: Rank 0, Epoch 2578, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,615: INFO: model_training: Rank 0, Epoch 2579, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,616: INFO: model_training: Rank 0, Epoch 2579, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,618: INFO: model_training: Rank 0, Epoch 2579, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,619: INFO: model_training: Rank 0, Epoch 2579, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,621: INFO: model_training: Rank 0, Epoch 2579, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,623: INFO: model_training: Rank 0, Epoch 2580, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,625: INFO: model_training: Rank 0, Epoch 2580, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,626: INFO: model_training: Rank 0, Epoch 2580, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,628: INFO: model_training: Rank 0, Epoch 2580, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,629: INFO: model_training: Rank 0, Epoch 2580, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,630: INFO: model_training: Rank 0, Epoch 2581, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,632: INFO: model_training: Rank 0, Epoch 2581, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,633: INFO: model_training: Rank 0, Epoch 2581, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,634: INFO: model_training: Rank 0, Epoch 2581, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,635: INFO: model_training: Rank 0, Epoch 2581, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,637: INFO: model_training: Rank 0, Epoch 2582, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,639: INFO: model_training: Rank 0, Epoch 2582, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,641: INFO: model_training: Rank 0, Epoch 2582, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,642: INFO: model_training: Rank 0, Epoch 2582, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,644: INFO: model_training: Rank 0, Epoch 2582, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,645: INFO: model_training: Rank 0, Epoch 2583, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,646: INFO: model_training: Rank 0, Epoch 2583, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,647: INFO: model_training: Rank 0, Epoch 2583, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,649: INFO: model_training: Rank 0, Epoch 2583, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,650: INFO: model_training: Rank 0, Epoch 2583, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,651: INFO: model_training: Rank 0, Epoch 2584, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,652: INFO: model_training: Rank 0, Epoch 2584, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,654: INFO: model_training: Rank 0, Epoch 2584, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,656: INFO: model_training: Rank 0, Epoch 2584, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,658: INFO: model_training: Rank 0, Epoch 2584, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,660: INFO: model_training: Rank 0, Epoch 2585, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,661: INFO: model_training: Rank 0, Epoch 2585, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,662: INFO: model_training: Rank 0, Epoch 2585, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,663: INFO: model_training: Rank 0, Epoch 2585, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,665: INFO: model_training: Rank 0, Epoch 2585, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,666: INFO: model_training: Rank 0, Epoch 2586, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,667: INFO: model_training: Rank 0, Epoch 2586, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,668: INFO: model_training: Rank 0, Epoch 2586, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,670: INFO: model_training: Rank 0, Epoch 2586, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,671: INFO: model_training: Rank 0, Epoch 2586, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,673: INFO: model_training: Rank 0, Epoch 2587, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,675: INFO: model_training: Rank 0, Epoch 2587, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,677: INFO: model_training: Rank 0, Epoch 2587, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,678: INFO: model_training: Rank 0, Epoch 2587, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,679: INFO: model_training: Rank 0, Epoch 2587, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,681: INFO: model_training: Rank 0, Epoch 2588, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,682: INFO: model_training: Rank 0, Epoch 2588, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,683: INFO: model_training: Rank 0, Epoch 2588, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,684: INFO: model_training: Rank 0, Epoch 2588, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,685: INFO: model_training: Rank 0, Epoch 2588, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,686: INFO: model_training: Rank 0, Epoch 2589, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,688: INFO: model_training: Rank 0, Epoch 2589, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,690: INFO: model_training: Rank 0, Epoch 2589, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,692: INFO: model_training: Rank 0, Epoch 2589, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,694: INFO: model_training: Rank 0, Epoch 2589, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,696: INFO: model_training: Rank 0, Epoch 2590, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,697: INFO: model_training: Rank 0, Epoch 2590, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,699: INFO: model_training: Rank 0, Epoch 2590, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,700: INFO: model_training: Rank 0, Epoch 2590, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,701: INFO: model_training: Rank 0, Epoch 2590, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,702: INFO: model_training: Rank 0, Epoch 2591, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,704: INFO: model_training: Rank 0, Epoch 2591, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,706: INFO: model_training: Rank 0, Epoch 2591, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,708: INFO: model_training: Rank 0, Epoch 2591, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,710: INFO: model_training: Rank 0, Epoch 2591, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,711: INFO: model_training: Rank 0, Epoch 2592, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,713: INFO: model_training: Rank 0, Epoch 2592, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,714: INFO: model_training: Rank 0, Epoch 2592, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,715: INFO: model_training: Rank 0, Epoch 2592, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,716: INFO: model_training: Rank 0, Epoch 2592, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,717: INFO: model_training: Rank 0, Epoch 2593, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,719: INFO: model_training: Rank 0, Epoch 2593, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,721: INFO: model_training: Rank 0, Epoch 2593, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,723: INFO: model_training: Rank 0, Epoch 2593, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,724: INFO: model_training: Rank 0, Epoch 2593, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,726: INFO: model_training: Rank 0, Epoch 2594, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,728: INFO: model_training: Rank 0, Epoch 2594, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,729: INFO: model_training: Rank 0, Epoch 2594, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,730: INFO: model_training: Rank 0, Epoch 2594, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,731: INFO: model_training: Rank 0, Epoch 2594, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,732: INFO: model_training: Rank 0, Epoch 2595, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,734: INFO: model_training: Rank 0, Epoch 2595, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,734: INFO: model_training: Rank 0, Epoch 2595, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,736: INFO: model_training: Rank 0, Epoch 2595, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,737: INFO: model_training: Rank 0, Epoch 2595, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,739: INFO: model_training: Rank 0, Epoch 2596, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,742: INFO: model_training: Rank 0, Epoch 2596, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,743: INFO: model_training: Rank 0, Epoch 2596, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,745: INFO: model_training: Rank 0, Epoch 2596, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,746: INFO: model_training: Rank 0, Epoch 2596, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,747: INFO: model_training: Rank 0, Epoch 2597, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,748: INFO: model_training: Rank 0, Epoch 2597, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,749: INFO: model_training: Rank 0, Epoch 2597, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,750: INFO: model_training: Rank 0, Epoch 2597, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,751: INFO: model_training: Rank 0, Epoch 2597, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,753: INFO: model_training: Rank 0, Epoch 2598, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,754: INFO: model_training: Rank 0, Epoch 2598, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,755: INFO: model_training: Rank 0, Epoch 2598, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,756: INFO: model_training: Rank 0, Epoch 2598, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,757: INFO: model_training: Rank 0, Epoch 2598, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,759: INFO: model_training: Rank 0, Epoch 2599, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,760: INFO: model_training: Rank 0, Epoch 2599, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,761: INFO: model_training: Rank 0, Epoch 2599, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,763: INFO: model_training: Rank 0, Epoch 2599, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,764: INFO: model_training: Rank 0, Epoch 2599, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,765: INFO: model_training: Rank 0, Epoch 2600, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,766: INFO: model_training: Rank 0, Epoch 2600, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,768: INFO: model_training: Rank 0, Epoch 2600, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,769: INFO: model_training: Rank 0, Epoch 2600, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,770: INFO: model_training: Rank 0, Epoch 2600, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,771: INFO: model_training: Rank 0, Epoch 2601, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,772: INFO: model_training: Rank 0, Epoch 2601, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,773: INFO: model_training: Rank 0, Epoch 2601, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,775: INFO: model_training: Rank 0, Epoch 2601, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,776: INFO: model_training: Rank 0, Epoch 2601, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,777: INFO: model_training: Rank 0, Epoch 2602, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,779: INFO: model_training: Rank 0, Epoch 2602, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,780: INFO: model_training: Rank 0, Epoch 2602, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,781: INFO: model_training: Rank 0, Epoch 2602, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,782: INFO: model_training: Rank 0, Epoch 2602, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,783: INFO: model_training: Rank 0, Epoch 2603, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,784: INFO: model_training: Rank 0, Epoch 2603, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,786: INFO: model_training: Rank 0, Epoch 2603, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,787: INFO: model_training: Rank 0, Epoch 2603, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,788: INFO: model_training: Rank 0, Epoch 2603, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,789: INFO: model_training: Rank 0, Epoch 2604, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,790: INFO: model_training: Rank 0, Epoch 2604, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,791: INFO: model_training: Rank 0, Epoch 2604, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,792: INFO: model_training: Rank 0, Epoch 2604, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,793: INFO: model_training: Rank 0, Epoch 2604, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,794: INFO: model_training: Rank 0, Epoch 2605, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,796: INFO: model_training: Rank 0, Epoch 2605, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,797: INFO: model_training: Rank 0, Epoch 2605, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,799: INFO: model_training: Rank 0, Epoch 2605, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,800: INFO: model_training: Rank 0, Epoch 2605, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,801: INFO: model_training: Rank 0, Epoch 2606, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,802: INFO: model_training: Rank 0, Epoch 2606, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,803: INFO: model_training: Rank 0, Epoch 2606, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,804: INFO: model_training: Rank 0, Epoch 2606, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,805: INFO: model_training: Rank 0, Epoch 2606, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,806: INFO: model_training: Rank 0, Epoch 2607, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,807: INFO: model_training: Rank 0, Epoch 2607, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,809: INFO: model_training: Rank 0, Epoch 2607, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,810: INFO: model_training: Rank 0, Epoch 2607, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,811: INFO: model_training: Rank 0, Epoch 2607, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,812: INFO: model_training: Rank 0, Epoch 2608, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,813: INFO: model_training: Rank 0, Epoch 2608, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,815: INFO: model_training: Rank 0, Epoch 2608, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,816: INFO: model_training: Rank 0, Epoch 2608, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,817: INFO: model_training: Rank 0, Epoch 2608, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,818: INFO: model_training: Rank 0, Epoch 2609, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,819: INFO: model_training: Rank 0, Epoch 2609, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,820: INFO: model_training: Rank 0, Epoch 2609, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,821: INFO: model_training: Rank 0, Epoch 2609, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,823: INFO: model_training: Rank 0, Epoch 2609, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,824: INFO: model_training: Rank 0, Epoch 2610, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,825: INFO: model_training: Rank 0, Epoch 2610, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,826: INFO: model_training: Rank 0, Epoch 2610, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,827: INFO: model_training: Rank 0, Epoch 2610, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,829: INFO: model_training: Rank 0, Epoch 2610, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,830: INFO: model_training: Rank 0, Epoch 2611, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,832: INFO: model_training: Rank 0, Epoch 2611, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,833: INFO: model_training: Rank 0, Epoch 2611, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,834: INFO: model_training: Rank 0, Epoch 2611, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,835: INFO: model_training: Rank 0, Epoch 2611, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,836: INFO: model_training: Rank 0, Epoch 2612, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,837: INFO: model_training: Rank 0, Epoch 2612, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,838: INFO: model_training: Rank 0, Epoch 2612, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,839: INFO: model_training: Rank 0, Epoch 2612, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,840: INFO: model_training: Rank 0, Epoch 2612, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,842: INFO: model_training: Rank 0, Epoch 2613, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,842: INFO: model_training: Rank 0, Epoch 2613, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,844: INFO: model_training: Rank 0, Epoch 2613, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,845: INFO: model_training: Rank 0, Epoch 2613, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,846: INFO: model_training: Rank 0, Epoch 2613, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,847: INFO: model_training: Rank 0, Epoch 2614, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,848: INFO: model_training: Rank 0, Epoch 2614, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,850: INFO: model_training: Rank 0, Epoch 2614, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,851: INFO: model_training: Rank 0, Epoch 2614, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,852: INFO: model_training: Rank 0, Epoch 2614, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,853: INFO: model_training: Rank 0, Epoch 2615, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,854: INFO: model_training: Rank 0, Epoch 2615, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,855: INFO: model_training: Rank 0, Epoch 2615, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,856: INFO: model_training: Rank 0, Epoch 2615, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,857: INFO: model_training: Rank 0, Epoch 2615, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,858: INFO: model_training: Rank 0, Epoch 2616, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,859: INFO: model_training: Rank 0, Epoch 2616, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,860: INFO: model_training: Rank 0, Epoch 2616, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,861: INFO: model_training: Rank 0, Epoch 2616, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,862: INFO: model_training: Rank 0, Epoch 2616, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,863: INFO: model_training: Rank 0, Epoch 2617, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,864: INFO: model_training: Rank 0, Epoch 2617, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,865: INFO: model_training: Rank 0, Epoch 2617, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,867: INFO: model_training: Rank 0, Epoch 2617, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,868: INFO: model_training: Rank 0, Epoch 2617, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,869: INFO: model_training: Rank 0, Epoch 2618, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,871: INFO: model_training: Rank 0, Epoch 2618, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,872: INFO: model_training: Rank 0, Epoch 2618, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,873: INFO: model_training: Rank 0, Epoch 2618, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,874: INFO: model_training: Rank 0, Epoch 2618, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,875: INFO: model_training: Rank 0, Epoch 2619, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,876: INFO: model_training: Rank 0, Epoch 2619, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,877: INFO: model_training: Rank 0, Epoch 2619, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,879: INFO: model_training: Rank 0, Epoch 2619, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,880: INFO: model_training: Rank 0, Epoch 2619, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,881: INFO: model_training: Rank 0, Epoch 2620, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,882: INFO: model_training: Rank 0, Epoch 2620, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,883: INFO: model_training: Rank 0, Epoch 2620, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,885: INFO: model_training: Rank 0, Epoch 2620, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,887: INFO: model_training: Rank 0, Epoch 2620, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,888: INFO: model_training: Rank 0, Epoch 2621, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,889: INFO: model_training: Rank 0, Epoch 2621, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,890: INFO: model_training: Rank 0, Epoch 2621, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,891: INFO: model_training: Rank 0, Epoch 2621, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,893: INFO: model_training: Rank 0, Epoch 2621, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,894: INFO: model_training: Rank 0, Epoch 2622, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,895: INFO: model_training: Rank 0, Epoch 2622, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,896: INFO: model_training: Rank 0, Epoch 2622, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,897: INFO: model_training: Rank 0, Epoch 2622, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,898: INFO: model_training: Rank 0, Epoch 2622, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,899: INFO: model_training: Rank 0, Epoch 2623, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,900: INFO: model_training: Rank 0, Epoch 2623, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,901: INFO: model_training: Rank 0, Epoch 2623, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,903: INFO: model_training: Rank 0, Epoch 2623, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,904: INFO: model_training: Rank 0, Epoch 2623, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,905: INFO: model_training: Rank 0, Epoch 2624, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,906: INFO: model_training: Rank 0, Epoch 2624, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,907: INFO: model_training: Rank 0, Epoch 2624, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,909: INFO: model_training: Rank 0, Epoch 2624, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,910: INFO: model_training: Rank 0, Epoch 2624, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,911: INFO: model_training: Rank 0, Epoch 2625, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,912: INFO: model_training: Rank 0, Epoch 2625, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,913: INFO: model_training: Rank 0, Epoch 2625, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,914: INFO: model_training: Rank 0, Epoch 2625, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,915: INFO: model_training: Rank 0, Epoch 2625, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,916: INFO: model_training: Rank 0, Epoch 2626, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,917: INFO: model_training: Rank 0, Epoch 2626, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,918: INFO: model_training: Rank 0, Epoch 2626, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,920: INFO: model_training: Rank 0, Epoch 2626, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,921: INFO: model_training: Rank 0, Epoch 2626, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,922: INFO: model_training: Rank 0, Epoch 2627, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,923: INFO: model_training: Rank 0, Epoch 2627, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,925: INFO: model_training: Rank 0, Epoch 2627, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,926: INFO: model_training: Rank 0, Epoch 2627, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,927: INFO: model_training: Rank 0, Epoch 2627, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,928: INFO: model_training: Rank 0, Epoch 2628, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,929: INFO: model_training: Rank 0, Epoch 2628, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,930: INFO: model_training: Rank 0, Epoch 2628, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,931: INFO: model_training: Rank 0, Epoch 2628, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,932: INFO: model_training: Rank 0, Epoch 2628, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,933: INFO: model_training: Rank 0, Epoch 2629, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,934: INFO: model_training: Rank 0, Epoch 2629, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,936: INFO: model_training: Rank 0, Epoch 2629, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,937: INFO: model_training: Rank 0, Epoch 2629, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,938: INFO: model_training: Rank 0, Epoch 2629, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,939: INFO: model_training: Rank 0, Epoch 2630, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,940: INFO: model_training: Rank 0, Epoch 2630, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,941: INFO: model_training: Rank 0, Epoch 2630, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,942: INFO: model_training: Rank 0, Epoch 2630, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,943: INFO: model_training: Rank 0, Epoch 2630, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,944: INFO: model_training: Rank 0, Epoch 2631, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,945: INFO: model_training: Rank 0, Epoch 2631, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,946: INFO: model_training: Rank 0, Epoch 2631, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,947: INFO: model_training: Rank 0, Epoch 2631, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,949: INFO: model_training: Rank 0, Epoch 2631, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,950: INFO: model_training: Rank 0, Epoch 2632, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,951: INFO: model_training: Rank 0, Epoch 2632, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,952: INFO: model_training: Rank 0, Epoch 2632, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,953: INFO: model_training: Rank 0, Epoch 2632, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,955: INFO: model_training: Rank 0, Epoch 2632, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,956: INFO: model_training: Rank 0, Epoch 2633, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,957: INFO: model_training: Rank 0, Epoch 2633, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,959: INFO: model_training: Rank 0, Epoch 2633, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,960: INFO: model_training: Rank 0, Epoch 2633, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,961: INFO: model_training: Rank 0, Epoch 2633, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,962: INFO: model_training: Rank 0, Epoch 2634, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,963: INFO: model_training: Rank 0, Epoch 2634, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,964: INFO: model_training: Rank 0, Epoch 2634, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,965: INFO: model_training: Rank 0, Epoch 2634, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,966: INFO: model_training: Rank 0, Epoch 2634, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,967: INFO: model_training: Rank 0, Epoch 2635, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,969: INFO: model_training: Rank 0, Epoch 2635, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,970: INFO: model_training: Rank 0, Epoch 2635, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,972: INFO: model_training: Rank 0, Epoch 2635, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,973: INFO: model_training: Rank 0, Epoch 2635, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,974: INFO: model_training: Rank 0, Epoch 2636, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,975: INFO: model_training: Rank 0, Epoch 2636, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,976: INFO: model_training: Rank 0, Epoch 2636, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,977: INFO: model_training: Rank 0, Epoch 2636, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,978: INFO: model_training: Rank 0, Epoch 2636, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,979: INFO: model_training: Rank 0, Epoch 2637, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,981: INFO: model_training: Rank 0, Epoch 2637, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,982: INFO: model_training: Rank 0, Epoch 2637, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,983: INFO: model_training: Rank 0, Epoch 2637, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,984: INFO: model_training: Rank 0, Epoch 2637, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,985: INFO: model_training: Rank 0, Epoch 2638, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,986: INFO: model_training: Rank 0, Epoch 2638, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,987: INFO: model_training: Rank 0, Epoch 2638, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,989: INFO: model_training: Rank 0, Epoch 2638, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,990: INFO: model_training: Rank 0, Epoch 2638, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,992: INFO: model_training: Rank 0, Epoch 2639, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,993: INFO: model_training: Rank 0, Epoch 2639, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,994: INFO: model_training: Rank 0, Epoch 2639, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:42,995: INFO: model_training: Rank 0, Epoch 2639, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:42,996: INFO: model_training: Rank 0, Epoch 2639, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:42,997: INFO: model_training: Rank 0, Epoch 2640, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:42,998: INFO: model_training: Rank 0, Epoch 2640, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:42,999: INFO: model_training: Rank 0, Epoch 2640, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,000: INFO: model_training: Rank 0, Epoch 2640, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,001: INFO: model_training: Rank 0, Epoch 2640, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,002: INFO: model_training: Rank 0, Epoch 2641, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,003: INFO: model_training: Rank 0, Epoch 2641, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,004: INFO: model_training: Rank 0, Epoch 2641, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,006: INFO: model_training: Rank 0, Epoch 2641, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,007: INFO: model_training: Rank 0, Epoch 2641, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,009: INFO: model_training: Rank 0, Epoch 2642, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,010: INFO: model_training: Rank 0, Epoch 2642, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,011: INFO: model_training: Rank 0, Epoch 2642, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,012: INFO: model_training: Rank 0, Epoch 2642, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,013: INFO: model_training: Rank 0, Epoch 2642, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,014: INFO: model_training: Rank 0, Epoch 2643, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,015: INFO: model_training: Rank 0, Epoch 2643, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,016: INFO: model_training: Rank 0, Epoch 2643, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,017: INFO: model_training: Rank 0, Epoch 2643, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,019: INFO: model_training: Rank 0, Epoch 2643, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,020: INFO: model_training: Rank 0, Epoch 2644, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,022: INFO: model_training: Rank 0, Epoch 2644, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,023: INFO: model_training: Rank 0, Epoch 2644, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,024: INFO: model_training: Rank 0, Epoch 2644, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,025: INFO: model_training: Rank 0, Epoch 2644, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,026: INFO: model_training: Rank 0, Epoch 2645, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,027: INFO: model_training: Rank 0, Epoch 2645, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,028: INFO: model_training: Rank 0, Epoch 2645, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,029: INFO: model_training: Rank 0, Epoch 2645, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,030: INFO: model_training: Rank 0, Epoch 2645, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,031: INFO: model_training: Rank 0, Epoch 2646, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,033: INFO: model_training: Rank 0, Epoch 2646, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,034: INFO: model_training: Rank 0, Epoch 2646, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,035: INFO: model_training: Rank 0, Epoch 2646, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,036: INFO: model_training: Rank 0, Epoch 2646, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,037: INFO: model_training: Rank 0, Epoch 2647, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,039: INFO: model_training: Rank 0, Epoch 2647, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,040: INFO: model_training: Rank 0, Epoch 2647, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,041: INFO: model_training: Rank 0, Epoch 2647, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,042: INFO: model_training: Rank 0, Epoch 2647, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,043: INFO: model_training: Rank 0, Epoch 2648, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,044: INFO: model_training: Rank 0, Epoch 2648, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,045: INFO: model_training: Rank 0, Epoch 2648, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,047: INFO: model_training: Rank 0, Epoch 2648, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,048: INFO: model_training: Rank 0, Epoch 2648, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,049: INFO: model_training: Rank 0, Epoch 2649, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,050: INFO: model_training: Rank 0, Epoch 2649, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,051: INFO: model_training: Rank 0, Epoch 2649, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,052: INFO: model_training: Rank 0, Epoch 2649, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,053: INFO: model_training: Rank 0, Epoch 2649, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,054: INFO: model_training: Rank 0, Epoch 2650, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,056: INFO: model_training: Rank 0, Epoch 2650, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,057: INFO: model_training: Rank 0, Epoch 2650, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,058: INFO: model_training: Rank 0, Epoch 2650, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,059: INFO: model_training: Rank 0, Epoch 2650, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,060: INFO: model_training: Rank 0, Epoch 2651, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,061: INFO: model_training: Rank 0, Epoch 2651, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,063: INFO: model_training: Rank 0, Epoch 2651, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,064: INFO: model_training: Rank 0, Epoch 2651, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,065: INFO: model_training: Rank 0, Epoch 2651, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,066: INFO: model_training: Rank 0, Epoch 2652, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,067: INFO: model_training: Rank 0, Epoch 2652, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,068: INFO: model_training: Rank 0, Epoch 2652, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,069: INFO: model_training: Rank 0, Epoch 2652, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,070: INFO: model_training: Rank 0, Epoch 2652, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,072: INFO: model_training: Rank 0, Epoch 2653, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,073: INFO: model_training: Rank 0, Epoch 2653, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,074: INFO: model_training: Rank 0, Epoch 2653, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,076: INFO: model_training: Rank 0, Epoch 2653, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,077: INFO: model_training: Rank 0, Epoch 2653, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,078: INFO: model_training: Rank 0, Epoch 2654, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,079: INFO: model_training: Rank 0, Epoch 2654, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,080: INFO: model_training: Rank 0, Epoch 2654, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,081: INFO: model_training: Rank 0, Epoch 2654, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,082: INFO: model_training: Rank 0, Epoch 2654, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,083: INFO: model_training: Rank 0, Epoch 2655, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,084: INFO: model_training: Rank 0, Epoch 2655, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,085: INFO: model_training: Rank 0, Epoch 2655, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,086: INFO: model_training: Rank 0, Epoch 2655, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,087: INFO: model_training: Rank 0, Epoch 2655, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,089: INFO: model_training: Rank 0, Epoch 2656, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,090: INFO: model_training: Rank 0, Epoch 2656, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,091: INFO: model_training: Rank 0, Epoch 2656, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,092: INFO: model_training: Rank 0, Epoch 2656, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,094: INFO: model_training: Rank 0, Epoch 2656, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,095: INFO: model_training: Rank 0, Epoch 2657, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,096: INFO: model_training: Rank 0, Epoch 2657, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,097: INFO: model_training: Rank 0, Epoch 2657, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,099: INFO: model_training: Rank 0, Epoch 2657, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,100: INFO: model_training: Rank 0, Epoch 2657, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,101: INFO: model_training: Rank 0, Epoch 2658, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,102: INFO: model_training: Rank 0, Epoch 2658, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,103: INFO: model_training: Rank 0, Epoch 2658, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,104: INFO: model_training: Rank 0, Epoch 2658, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,105: INFO: model_training: Rank 0, Epoch 2658, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,107: INFO: model_training: Rank 0, Epoch 2659, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,110: INFO: model_training: Rank 0, Epoch 2659, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,112: INFO: model_training: Rank 0, Epoch 2659, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,113: INFO: model_training: Rank 0, Epoch 2659, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,114: INFO: model_training: Rank 0, Epoch 2659, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,115: INFO: model_training: Rank 0, Epoch 2660, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,117: INFO: model_training: Rank 0, Epoch 2660, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,118: INFO: model_training: Rank 0, Epoch 2660, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,119: INFO: model_training: Rank 0, Epoch 2660, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,120: INFO: model_training: Rank 0, Epoch 2660, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,122: INFO: model_training: Rank 0, Epoch 2661, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,123: INFO: model_training: Rank 0, Epoch 2661, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,124: INFO: model_training: Rank 0, Epoch 2661, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,125: INFO: model_training: Rank 0, Epoch 2661, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,127: INFO: model_training: Rank 0, Epoch 2661, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,128: INFO: model_training: Rank 0, Epoch 2662, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,130: INFO: model_training: Rank 0, Epoch 2662, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,131: INFO: model_training: Rank 0, Epoch 2662, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,132: INFO: model_training: Rank 0, Epoch 2662, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,134: INFO: model_training: Rank 0, Epoch 2662, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,135: INFO: model_training: Rank 0, Epoch 2663, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,136: INFO: model_training: Rank 0, Epoch 2663, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,137: INFO: model_training: Rank 0, Epoch 2663, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,139: INFO: model_training: Rank 0, Epoch 2663, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,140: INFO: model_training: Rank 0, Epoch 2663, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,142: INFO: model_training: Rank 0, Epoch 2664, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,143: INFO: model_training: Rank 0, Epoch 2664, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,145: INFO: model_training: Rank 0, Epoch 2664, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,146: INFO: model_training: Rank 0, Epoch 2664, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,148: INFO: model_training: Rank 0, Epoch 2664, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,151: INFO: model_training: Rank 0, Epoch 2665, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,153: INFO: model_training: Rank 0, Epoch 2665, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,154: INFO: model_training: Rank 0, Epoch 2665, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,156: INFO: model_training: Rank 0, Epoch 2665, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,158: INFO: model_training: Rank 0, Epoch 2665, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,160: INFO: model_training: Rank 0, Epoch 2666, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,162: INFO: model_training: Rank 0, Epoch 2666, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,163: INFO: model_training: Rank 0, Epoch 2666, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,164: INFO: model_training: Rank 0, Epoch 2666, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,166: INFO: model_training: Rank 0, Epoch 2666, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,168: INFO: model_training: Rank 0, Epoch 2667, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,169: INFO: model_training: Rank 0, Epoch 2667, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,170: INFO: model_training: Rank 0, Epoch 2667, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,171: INFO: model_training: Rank 0, Epoch 2667, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,173: INFO: model_training: Rank 0, Epoch 2667, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,174: INFO: model_training: Rank 0, Epoch 2668, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,175: INFO: model_training: Rank 0, Epoch 2668, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,176: INFO: model_training: Rank 0, Epoch 2668, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,177: INFO: model_training: Rank 0, Epoch 2668, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,179: INFO: model_training: Rank 0, Epoch 2668, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,180: INFO: model_training: Rank 0, Epoch 2669, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,181: INFO: model_training: Rank 0, Epoch 2669, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,182: INFO: model_training: Rank 0, Epoch 2669, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,183: INFO: model_training: Rank 0, Epoch 2669, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,184: INFO: model_training: Rank 0, Epoch 2669, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,186: INFO: model_training: Rank 0, Epoch 2670, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,187: INFO: model_training: Rank 0, Epoch 2670, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,188: INFO: model_training: Rank 0, Epoch 2670, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,189: INFO: model_training: Rank 0, Epoch 2670, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,190: INFO: model_training: Rank 0, Epoch 2670, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,193: INFO: model_training: Rank 0, Epoch 2671, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,194: INFO: model_training: Rank 0, Epoch 2671, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,196: INFO: model_training: Rank 0, Epoch 2671, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,197: INFO: model_training: Rank 0, Epoch 2671, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,198: INFO: model_training: Rank 0, Epoch 2671, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,200: INFO: model_training: Rank 0, Epoch 2672, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,201: INFO: model_training: Rank 0, Epoch 2672, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,202: INFO: model_training: Rank 0, Epoch 2672, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,203: INFO: model_training: Rank 0, Epoch 2672, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,205: INFO: model_training: Rank 0, Epoch 2672, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,207: INFO: model_training: Rank 0, Epoch 2673, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,208: INFO: model_training: Rank 0, Epoch 2673, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,209: INFO: model_training: Rank 0, Epoch 2673, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,211: INFO: model_training: Rank 0, Epoch 2673, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,213: INFO: model_training: Rank 0, Epoch 2673, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,215: INFO: model_training: Rank 0, Epoch 2674, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,217: INFO: model_training: Rank 0, Epoch 2674, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,219: INFO: model_training: Rank 0, Epoch 2674, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,220: INFO: model_training: Rank 0, Epoch 2674, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,221: INFO: model_training: Rank 0, Epoch 2674, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,223: INFO: model_training: Rank 0, Epoch 2675, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,224: INFO: model_training: Rank 0, Epoch 2675, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,225: INFO: model_training: Rank 0, Epoch 2675, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,227: INFO: model_training: Rank 0, Epoch 2675, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,228: INFO: model_training: Rank 0, Epoch 2675, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,229: INFO: model_training: Rank 0, Epoch 2676, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,230: INFO: model_training: Rank 0, Epoch 2676, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,232: INFO: model_training: Rank 0, Epoch 2676, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,233: INFO: model_training: Rank 0, Epoch 2676, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,234: INFO: model_training: Rank 0, Epoch 2676, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,236: INFO: model_training: Rank 0, Epoch 2677, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,237: INFO: model_training: Rank 0, Epoch 2677, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,239: INFO: model_training: Rank 0, Epoch 2677, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,240: INFO: model_training: Rank 0, Epoch 2677, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,241: INFO: model_training: Rank 0, Epoch 2677, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,243: INFO: model_training: Rank 0, Epoch 2678, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,244: INFO: model_training: Rank 0, Epoch 2678, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,245: INFO: model_training: Rank 0, Epoch 2678, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,246: INFO: model_training: Rank 0, Epoch 2678, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,247: INFO: model_training: Rank 0, Epoch 2678, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,249: INFO: model_training: Rank 0, Epoch 2679, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,250: INFO: model_training: Rank 0, Epoch 2679, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,251: INFO: model_training: Rank 0, Epoch 2679, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,253: INFO: model_training: Rank 0, Epoch 2679, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,254: INFO: model_training: Rank 0, Epoch 2679, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,255: INFO: model_training: Rank 0, Epoch 2680, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,257: INFO: model_training: Rank 0, Epoch 2680, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,259: INFO: model_training: Rank 0, Epoch 2680, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,260: INFO: model_training: Rank 0, Epoch 2680, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,262: INFO: model_training: Rank 0, Epoch 2680, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,263: INFO: model_training: Rank 0, Epoch 2681, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,265: INFO: model_training: Rank 0, Epoch 2681, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,266: INFO: model_training: Rank 0, Epoch 2681, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,267: INFO: model_training: Rank 0, Epoch 2681, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,269: INFO: model_training: Rank 0, Epoch 2681, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,271: INFO: model_training: Rank 0, Epoch 2682, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,273: INFO: model_training: Rank 0, Epoch 2682, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,275: INFO: model_training: Rank 0, Epoch 2682, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,277: INFO: model_training: Rank 0, Epoch 2682, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,278: INFO: model_training: Rank 0, Epoch 2682, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,280: INFO: model_training: Rank 0, Epoch 2683, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,282: INFO: model_training: Rank 0, Epoch 2683, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,283: INFO: model_training: Rank 0, Epoch 2683, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,285: INFO: model_training: Rank 0, Epoch 2683, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,286: INFO: model_training: Rank 0, Epoch 2683, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,288: INFO: model_training: Rank 0, Epoch 2684, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,290: INFO: model_training: Rank 0, Epoch 2684, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,292: INFO: model_training: Rank 0, Epoch 2684, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,293: INFO: model_training: Rank 0, Epoch 2684, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,294: INFO: model_training: Rank 0, Epoch 2684, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,296: INFO: model_training: Rank 0, Epoch 2685, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,298: INFO: model_training: Rank 0, Epoch 2685, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,299: INFO: model_training: Rank 0, Epoch 2685, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,301: INFO: model_training: Rank 0, Epoch 2685, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,302: INFO: model_training: Rank 0, Epoch 2685, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,305: INFO: model_training: Rank 0, Epoch 2686, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,307: INFO: model_training: Rank 0, Epoch 2686, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,308: INFO: model_training: Rank 0, Epoch 2686, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,310: INFO: model_training: Rank 0, Epoch 2686, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,311: INFO: model_training: Rank 0, Epoch 2686, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,313: INFO: model_training: Rank 0, Epoch 2687, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,314: INFO: model_training: Rank 0, Epoch 2687, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,316: INFO: model_training: Rank 0, Epoch 2687, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,317: INFO: model_training: Rank 0, Epoch 2687, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,318: INFO: model_training: Rank 0, Epoch 2687, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,321: INFO: model_training: Rank 0, Epoch 2688, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,323: INFO: model_training: Rank 0, Epoch 2688, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,324: INFO: model_training: Rank 0, Epoch 2688, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,327: INFO: model_training: Rank 0, Epoch 2688, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,329: INFO: model_training: Rank 0, Epoch 2688, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,330: INFO: model_training: Rank 0, Epoch 2689, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,332: INFO: model_training: Rank 0, Epoch 2689, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,333: INFO: model_training: Rank 0, Epoch 2689, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,334: INFO: model_training: Rank 0, Epoch 2689, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,336: INFO: model_training: Rank 0, Epoch 2689, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,338: INFO: model_training: Rank 0, Epoch 2690, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,340: INFO: model_training: Rank 0, Epoch 2690, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,342: INFO: model_training: Rank 0, Epoch 2690, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,343: INFO: model_training: Rank 0, Epoch 2690, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,344: INFO: model_training: Rank 0, Epoch 2690, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,346: INFO: model_training: Rank 0, Epoch 2691, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,348: INFO: model_training: Rank 0, Epoch 2691, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,350: INFO: model_training: Rank 0, Epoch 2691, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,352: INFO: model_training: Rank 0, Epoch 2691, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,355: INFO: model_training: Rank 0, Epoch 2691, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,358: INFO: model_training: Rank 0, Epoch 2692, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,360: INFO: model_training: Rank 0, Epoch 2692, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,362: INFO: model_training: Rank 0, Epoch 2692, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,364: INFO: model_training: Rank 0, Epoch 2692, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,366: INFO: model_training: Rank 0, Epoch 2692, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,367: INFO: model_training: Rank 0, Epoch 2693, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,369: INFO: model_training: Rank 0, Epoch 2693, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,372: INFO: model_training: Rank 0, Epoch 2693, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,374: INFO: model_training: Rank 0, Epoch 2693, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,375: INFO: model_training: Rank 0, Epoch 2693, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,377: INFO: model_training: Rank 0, Epoch 2694, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,378: INFO: model_training: Rank 0, Epoch 2694, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,380: INFO: model_training: Rank 0, Epoch 2694, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,381: INFO: model_training: Rank 0, Epoch 2694, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,382: INFO: model_training: Rank 0, Epoch 2694, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,383: INFO: model_training: Rank 0, Epoch 2695, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,384: INFO: model_training: Rank 0, Epoch 2695, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,386: INFO: model_training: Rank 0, Epoch 2695, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,388: INFO: model_training: Rank 0, Epoch 2695, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,389: INFO: model_training: Rank 0, Epoch 2695, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,391: INFO: model_training: Rank 0, Epoch 2696, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,392: INFO: model_training: Rank 0, Epoch 2696, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,393: INFO: model_training: Rank 0, Epoch 2696, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,395: INFO: model_training: Rank 0, Epoch 2696, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,397: INFO: model_training: Rank 0, Epoch 2696, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,398: INFO: model_training: Rank 0, Epoch 2697, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,399: INFO: model_training: Rank 0, Epoch 2697, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,400: INFO: model_training: Rank 0, Epoch 2697, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,402: INFO: model_training: Rank 0, Epoch 2697, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,403: INFO: model_training: Rank 0, Epoch 2697, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,405: INFO: model_training: Rank 0, Epoch 2698, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,406: INFO: model_training: Rank 0, Epoch 2698, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,408: INFO: model_training: Rank 0, Epoch 2698, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,409: INFO: model_training: Rank 0, Epoch 2698, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,410: INFO: model_training: Rank 0, Epoch 2698, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,411: INFO: model_training: Rank 0, Epoch 2699, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,413: INFO: model_training: Rank 0, Epoch 2699, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,415: INFO: model_training: Rank 0, Epoch 2699, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,416: INFO: model_training: Rank 0, Epoch 2699, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,417: INFO: model_training: Rank 0, Epoch 2699, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,419: INFO: model_training: Rank 0, Epoch 2700, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,421: INFO: model_training: Rank 0, Epoch 2700, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,422: INFO: model_training: Rank 0, Epoch 2700, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,424: INFO: model_training: Rank 0, Epoch 2700, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,426: INFO: model_training: Rank 0, Epoch 2700, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,427: INFO: model_training: Rank 0, Epoch 2701, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,430: INFO: model_training: Rank 0, Epoch 2701, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,432: INFO: model_training: Rank 0, Epoch 2701, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,433: INFO: model_training: Rank 0, Epoch 2701, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,434: INFO: model_training: Rank 0, Epoch 2701, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,436: INFO: model_training: Rank 0, Epoch 2702, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,438: INFO: model_training: Rank 0, Epoch 2702, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,440: INFO: model_training: Rank 0, Epoch 2702, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,441: INFO: model_training: Rank 0, Epoch 2702, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,442: INFO: model_training: Rank 0, Epoch 2702, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,443: INFO: model_training: Rank 0, Epoch 2703, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,445: INFO: model_training: Rank 0, Epoch 2703, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,447: INFO: model_training: Rank 0, Epoch 2703, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,448: INFO: model_training: Rank 0, Epoch 2703, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,449: INFO: model_training: Rank 0, Epoch 2703, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,450: INFO: model_training: Rank 0, Epoch 2704, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,452: INFO: model_training: Rank 0, Epoch 2704, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,453: INFO: model_training: Rank 0, Epoch 2704, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,455: INFO: model_training: Rank 0, Epoch 2704, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,456: INFO: model_training: Rank 0, Epoch 2704, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,458: INFO: model_training: Rank 0, Epoch 2705, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,460: INFO: model_training: Rank 0, Epoch 2705, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,461: INFO: model_training: Rank 0, Epoch 2705, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,463: INFO: model_training: Rank 0, Epoch 2705, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,464: INFO: model_training: Rank 0, Epoch 2705, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,465: INFO: model_training: Rank 0, Epoch 2706, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,466: INFO: model_training: Rank 0, Epoch 2706, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,467: INFO: model_training: Rank 0, Epoch 2706, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,469: INFO: model_training: Rank 0, Epoch 2706, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,471: INFO: model_training: Rank 0, Epoch 2706, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,473: INFO: model_training: Rank 0, Epoch 2707, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,475: INFO: model_training: Rank 0, Epoch 2707, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,477: INFO: model_training: Rank 0, Epoch 2707, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,479: INFO: model_training: Rank 0, Epoch 2707, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,481: INFO: model_training: Rank 0, Epoch 2707, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,482: INFO: model_training: Rank 0, Epoch 2708, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,484: INFO: model_training: Rank 0, Epoch 2708, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,485: INFO: model_training: Rank 0, Epoch 2708, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,486: INFO: model_training: Rank 0, Epoch 2708, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,489: INFO: model_training: Rank 0, Epoch 2708, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,491: INFO: model_training: Rank 0, Epoch 2709, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,492: INFO: model_training: Rank 0, Epoch 2709, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,493: INFO: model_training: Rank 0, Epoch 2709, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,495: INFO: model_training: Rank 0, Epoch 2709, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,496: INFO: model_training: Rank 0, Epoch 2709, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,498: INFO: model_training: Rank 0, Epoch 2710, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,500: INFO: model_training: Rank 0, Epoch 2710, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,501: INFO: model_training: Rank 0, Epoch 2710, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,502: INFO: model_training: Rank 0, Epoch 2710, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,503: INFO: model_training: Rank 0, Epoch 2710, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,505: INFO: model_training: Rank 0, Epoch 2711, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,507: INFO: model_training: Rank 0, Epoch 2711, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,509: INFO: model_training: Rank 0, Epoch 2711, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,510: INFO: model_training: Rank 0, Epoch 2711, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,511: INFO: model_training: Rank 0, Epoch 2711, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,512: INFO: model_training: Rank 0, Epoch 2712, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,514: INFO: model_training: Rank 0, Epoch 2712, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,515: INFO: model_training: Rank 0, Epoch 2712, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,516: INFO: model_training: Rank 0, Epoch 2712, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,517: INFO: model_training: Rank 0, Epoch 2712, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,519: INFO: model_training: Rank 0, Epoch 2713, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,521: INFO: model_training: Rank 0, Epoch 2713, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,523: INFO: model_training: Rank 0, Epoch 2713, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,524: INFO: model_training: Rank 0, Epoch 2713, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,526: INFO: model_training: Rank 0, Epoch 2713, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,527: INFO: model_training: Rank 0, Epoch 2714, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,528: INFO: model_training: Rank 0, Epoch 2714, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,530: INFO: model_training: Rank 0, Epoch 2714, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,531: INFO: model_training: Rank 0, Epoch 2714, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,532: INFO: model_training: Rank 0, Epoch 2714, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,533: INFO: model_training: Rank 0, Epoch 2715, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,534: INFO: model_training: Rank 0, Epoch 2715, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,536: INFO: model_training: Rank 0, Epoch 2715, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,538: INFO: model_training: Rank 0, Epoch 2715, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,541: INFO: model_training: Rank 0, Epoch 2715, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,542: INFO: model_training: Rank 0, Epoch 2716, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,543: INFO: model_training: Rank 0, Epoch 2716, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,544: INFO: model_training: Rank 0, Epoch 2716, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,545: INFO: model_training: Rank 0, Epoch 2716, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,547: INFO: model_training: Rank 0, Epoch 2716, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,548: INFO: model_training: Rank 0, Epoch 2717, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,549: INFO: model_training: Rank 0, Epoch 2717, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,550: INFO: model_training: Rank 0, Epoch 2717, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,551: INFO: model_training: Rank 0, Epoch 2717, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,553: INFO: model_training: Rank 0, Epoch 2717, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,555: INFO: model_training: Rank 0, Epoch 2718, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,556: INFO: model_training: Rank 0, Epoch 2718, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,558: INFO: model_training: Rank 0, Epoch 2718, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,560: INFO: model_training: Rank 0, Epoch 2718, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,561: INFO: model_training: Rank 0, Epoch 2718, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,563: INFO: model_training: Rank 0, Epoch 2719, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,565: INFO: model_training: Rank 0, Epoch 2719, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,566: INFO: model_training: Rank 0, Epoch 2719, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,567: INFO: model_training: Rank 0, Epoch 2719, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,569: INFO: model_training: Rank 0, Epoch 2719, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,571: INFO: model_training: Rank 0, Epoch 2720, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,572: INFO: model_training: Rank 0, Epoch 2720, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,574: INFO: model_training: Rank 0, Epoch 2720, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,575: INFO: model_training: Rank 0, Epoch 2720, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,577: INFO: model_training: Rank 0, Epoch 2720, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,578: INFO: model_training: Rank 0, Epoch 2721, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,579: INFO: model_training: Rank 0, Epoch 2721, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,581: INFO: model_training: Rank 0, Epoch 2721, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,582: INFO: model_training: Rank 0, Epoch 2721, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,583: INFO: model_training: Rank 0, Epoch 2721, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,585: INFO: model_training: Rank 0, Epoch 2722, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,586: INFO: model_training: Rank 0, Epoch 2722, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,588: INFO: model_training: Rank 0, Epoch 2722, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,590: INFO: model_training: Rank 0, Epoch 2722, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,591: INFO: model_training: Rank 0, Epoch 2722, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,592: INFO: model_training: Rank 0, Epoch 2723, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,593: INFO: model_training: Rank 0, Epoch 2723, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,595: INFO: model_training: Rank 0, Epoch 2723, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,596: INFO: model_training: Rank 0, Epoch 2723, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,598: INFO: model_training: Rank 0, Epoch 2723, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,599: INFO: model_training: Rank 0, Epoch 2724, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,601: INFO: model_training: Rank 0, Epoch 2724, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,602: INFO: model_training: Rank 0, Epoch 2724, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,604: INFO: model_training: Rank 0, Epoch 2724, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,605: INFO: model_training: Rank 0, Epoch 2724, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,607: INFO: model_training: Rank 0, Epoch 2725, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,608: INFO: model_training: Rank 0, Epoch 2725, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,609: INFO: model_training: Rank 0, Epoch 2725, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,611: INFO: model_training: Rank 0, Epoch 2725, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,612: INFO: model_training: Rank 0, Epoch 2725, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,613: INFO: model_training: Rank 0, Epoch 2726, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,615: INFO: model_training: Rank 0, Epoch 2726, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,617: INFO: model_training: Rank 0, Epoch 2726, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,618: INFO: model_training: Rank 0, Epoch 2726, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,620: INFO: model_training: Rank 0, Epoch 2726, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,621: INFO: model_training: Rank 0, Epoch 2727, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,622: INFO: model_training: Rank 0, Epoch 2727, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,623: INFO: model_training: Rank 0, Epoch 2727, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,625: INFO: model_training: Rank 0, Epoch 2727, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,626: INFO: model_training: Rank 0, Epoch 2727, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,627: INFO: model_training: Rank 0, Epoch 2728, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,628: INFO: model_training: Rank 0, Epoch 2728, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,630: INFO: model_training: Rank 0, Epoch 2728, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,631: INFO: model_training: Rank 0, Epoch 2728, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,633: INFO: model_training: Rank 0, Epoch 2728, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,634: INFO: model_training: Rank 0, Epoch 2729, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,635: INFO: model_training: Rank 0, Epoch 2729, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,637: INFO: model_training: Rank 0, Epoch 2729, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,639: INFO: model_training: Rank 0, Epoch 2729, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,641: INFO: model_training: Rank 0, Epoch 2729, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,642: INFO: model_training: Rank 0, Epoch 2730, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,643: INFO: model_training: Rank 0, Epoch 2730, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,644: INFO: model_training: Rank 0, Epoch 2730, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,646: INFO: model_training: Rank 0, Epoch 2730, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,647: INFO: model_training: Rank 0, Epoch 2730, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,648: INFO: model_training: Rank 0, Epoch 2731, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,649: INFO: model_training: Rank 0, Epoch 2731, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,651: INFO: model_training: Rank 0, Epoch 2731, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,652: INFO: model_training: Rank 0, Epoch 2731, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,654: INFO: model_training: Rank 0, Epoch 2731, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,655: INFO: model_training: Rank 0, Epoch 2732, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,656: INFO: model_training: Rank 0, Epoch 2732, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,658: INFO: model_training: Rank 0, Epoch 2732, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,659: INFO: model_training: Rank 0, Epoch 2732, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,661: INFO: model_training: Rank 0, Epoch 2732, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,662: INFO: model_training: Rank 0, Epoch 2733, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,663: INFO: model_training: Rank 0, Epoch 2733, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,665: INFO: model_training: Rank 0, Epoch 2733, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,666: INFO: model_training: Rank 0, Epoch 2733, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,668: INFO: model_training: Rank 0, Epoch 2733, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,669: INFO: model_training: Rank 0, Epoch 2734, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,671: INFO: model_training: Rank 0, Epoch 2734, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,672: INFO: model_training: Rank 0, Epoch 2734, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,674: INFO: model_training: Rank 0, Epoch 2734, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,675: INFO: model_training: Rank 0, Epoch 2734, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,676: INFO: model_training: Rank 0, Epoch 2735, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,677: INFO: model_training: Rank 0, Epoch 2735, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,678: INFO: model_training: Rank 0, Epoch 2735, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,679: INFO: model_training: Rank 0, Epoch 2735, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,681: INFO: model_training: Rank 0, Epoch 2735, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,683: INFO: model_training: Rank 0, Epoch 2736, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,685: INFO: model_training: Rank 0, Epoch 2736, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,686: INFO: model_training: Rank 0, Epoch 2736, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,687: INFO: model_training: Rank 0, Epoch 2736, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,688: INFO: model_training: Rank 0, Epoch 2736, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,689: INFO: model_training: Rank 0, Epoch 2737, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,690: INFO: model_training: Rank 0, Epoch 2737, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,692: INFO: model_training: Rank 0, Epoch 2737, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,693: INFO: model_training: Rank 0, Epoch 2737, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,695: INFO: model_training: Rank 0, Epoch 2737, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,697: INFO: model_training: Rank 0, Epoch 2738, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,699: INFO: model_training: Rank 0, Epoch 2738, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,700: INFO: model_training: Rank 0, Epoch 2738, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,702: INFO: model_training: Rank 0, Epoch 2738, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,703: INFO: model_training: Rank 0, Epoch 2738, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,705: INFO: model_training: Rank 0, Epoch 2739, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,706: INFO: model_training: Rank 0, Epoch 2739, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,707: INFO: model_training: Rank 0, Epoch 2739, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,708: INFO: model_training: Rank 0, Epoch 2739, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,709: INFO: model_training: Rank 0, Epoch 2739, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,711: INFO: model_training: Rank 0, Epoch 2740, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,712: INFO: model_training: Rank 0, Epoch 2740, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,715: INFO: model_training: Rank 0, Epoch 2740, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,716: INFO: model_training: Rank 0, Epoch 2740, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,717: INFO: model_training: Rank 0, Epoch 2740, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,718: INFO: model_training: Rank 0, Epoch 2741, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,720: INFO: model_training: Rank 0, Epoch 2741, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,722: INFO: model_training: Rank 0, Epoch 2741, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,723: INFO: model_training: Rank 0, Epoch 2741, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,725: INFO: model_training: Rank 0, Epoch 2741, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,726: INFO: model_training: Rank 0, Epoch 2742, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,728: INFO: model_training: Rank 0, Epoch 2742, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,730: INFO: model_training: Rank 0, Epoch 2742, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,732: INFO: model_training: Rank 0, Epoch 2742, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,734: INFO: model_training: Rank 0, Epoch 2742, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,735: INFO: model_training: Rank 0, Epoch 2743, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,737: INFO: model_training: Rank 0, Epoch 2743, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,738: INFO: model_training: Rank 0, Epoch 2743, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,739: INFO: model_training: Rank 0, Epoch 2743, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,741: INFO: model_training: Rank 0, Epoch 2743, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,743: INFO: model_training: Rank 0, Epoch 2744, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,744: INFO: model_training: Rank 0, Epoch 2744, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,746: INFO: model_training: Rank 0, Epoch 2744, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,747: INFO: model_training: Rank 0, Epoch 2744, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,749: INFO: model_training: Rank 0, Epoch 2744, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,750: INFO: model_training: Rank 0, Epoch 2745, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,752: INFO: model_training: Rank 0, Epoch 2745, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,753: INFO: model_training: Rank 0, Epoch 2745, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,755: INFO: model_training: Rank 0, Epoch 2745, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,756: INFO: model_training: Rank 0, Epoch 2745, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,757: INFO: model_training: Rank 0, Epoch 2746, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,758: INFO: model_training: Rank 0, Epoch 2746, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,759: INFO: model_training: Rank 0, Epoch 2746, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,761: INFO: model_training: Rank 0, Epoch 2746, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,763: INFO: model_training: Rank 0, Epoch 2746, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,765: INFO: model_training: Rank 0, Epoch 2747, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,766: INFO: model_training: Rank 0, Epoch 2747, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,767: INFO: model_training: Rank 0, Epoch 2747, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,769: INFO: model_training: Rank 0, Epoch 2747, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,770: INFO: model_training: Rank 0, Epoch 2747, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,772: INFO: model_training: Rank 0, Epoch 2748, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,773: INFO: model_training: Rank 0, Epoch 2748, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,774: INFO: model_training: Rank 0, Epoch 2748, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,775: INFO: model_training: Rank 0, Epoch 2748, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,777: INFO: model_training: Rank 0, Epoch 2748, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,778: INFO: model_training: Rank 0, Epoch 2749, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,781: INFO: model_training: Rank 0, Epoch 2749, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,782: INFO: model_training: Rank 0, Epoch 2749, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,783: INFO: model_training: Rank 0, Epoch 2749, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,784: INFO: model_training: Rank 0, Epoch 2749, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,785: INFO: model_training: Rank 0, Epoch 2750, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,786: INFO: model_training: Rank 0, Epoch 2750, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,788: INFO: model_training: Rank 0, Epoch 2750, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,789: INFO: model_training: Rank 0, Epoch 2750, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,791: INFO: model_training: Rank 0, Epoch 2750, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,792: INFO: model_training: Rank 0, Epoch 2751, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,793: INFO: model_training: Rank 0, Epoch 2751, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,794: INFO: model_training: Rank 0, Epoch 2751, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,796: INFO: model_training: Rank 0, Epoch 2751, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,797: INFO: model_training: Rank 0, Epoch 2751, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,799: INFO: model_training: Rank 0, Epoch 2752, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,801: INFO: model_training: Rank 0, Epoch 2752, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,802: INFO: model_training: Rank 0, Epoch 2752, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,803: INFO: model_training: Rank 0, Epoch 2752, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,804: INFO: model_training: Rank 0, Epoch 2752, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,806: INFO: model_training: Rank 0, Epoch 2753, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,807: INFO: model_training: Rank 0, Epoch 2753, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,809: INFO: model_training: Rank 0, Epoch 2753, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,810: INFO: model_training: Rank 0, Epoch 2753, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,811: INFO: model_training: Rank 0, Epoch 2753, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,813: INFO: model_training: Rank 0, Epoch 2754, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,814: INFO: model_training: Rank 0, Epoch 2754, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,816: INFO: model_training: Rank 0, Epoch 2754, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,817: INFO: model_training: Rank 0, Epoch 2754, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,819: INFO: model_training: Rank 0, Epoch 2754, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,820: INFO: model_training: Rank 0, Epoch 2755, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,822: INFO: model_training: Rank 0, Epoch 2755, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,823: INFO: model_training: Rank 0, Epoch 2755, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,824: INFO: model_training: Rank 0, Epoch 2755, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,825: INFO: model_training: Rank 0, Epoch 2755, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,827: INFO: model_training: Rank 0, Epoch 2756, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,828: INFO: model_training: Rank 0, Epoch 2756, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,830: INFO: model_training: Rank 0, Epoch 2756, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,831: INFO: model_training: Rank 0, Epoch 2756, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,832: INFO: model_training: Rank 0, Epoch 2756, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,834: INFO: model_training: Rank 0, Epoch 2757, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,835: INFO: model_training: Rank 0, Epoch 2757, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,836: INFO: model_training: Rank 0, Epoch 2757, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,838: INFO: model_training: Rank 0, Epoch 2757, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,840: INFO: model_training: Rank 0, Epoch 2757, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,842: INFO: model_training: Rank 0, Epoch 2758, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,843: INFO: model_training: Rank 0, Epoch 2758, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,844: INFO: model_training: Rank 0, Epoch 2758, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,846: INFO: model_training: Rank 0, Epoch 2758, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,847: INFO: model_training: Rank 0, Epoch 2758, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,849: INFO: model_training: Rank 0, Epoch 2759, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,850: INFO: model_training: Rank 0, Epoch 2759, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,851: INFO: model_training: Rank 0, Epoch 2759, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,852: INFO: model_training: Rank 0, Epoch 2759, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,855: INFO: model_training: Rank 0, Epoch 2759, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,856: INFO: model_training: Rank 0, Epoch 2760, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,857: INFO: model_training: Rank 0, Epoch 2760, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,858: INFO: model_training: Rank 0, Epoch 2760, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,859: INFO: model_training: Rank 0, Epoch 2760, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,860: INFO: model_training: Rank 0, Epoch 2760, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,862: INFO: model_training: Rank 0, Epoch 2761, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,864: INFO: model_training: Rank 0, Epoch 2761, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,865: INFO: model_training: Rank 0, Epoch 2761, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,867: INFO: model_training: Rank 0, Epoch 2761, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,868: INFO: model_training: Rank 0, Epoch 2761, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,869: INFO: model_training: Rank 0, Epoch 2762, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,870: INFO: model_training: Rank 0, Epoch 2762, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,872: INFO: model_training: Rank 0, Epoch 2762, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,873: INFO: model_training: Rank 0, Epoch 2762, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,874: INFO: model_training: Rank 0, Epoch 2762, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,876: INFO: model_training: Rank 0, Epoch 2763, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,876: INFO: model_training: Rank 0, Epoch 2763, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,878: INFO: model_training: Rank 0, Epoch 2763, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,879: INFO: model_training: Rank 0, Epoch 2763, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,880: INFO: model_training: Rank 0, Epoch 2763, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,881: INFO: model_training: Rank 0, Epoch 2764, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,882: INFO: model_training: Rank 0, Epoch 2764, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,883: INFO: model_training: Rank 0, Epoch 2764, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,884: INFO: model_training: Rank 0, Epoch 2764, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,885: INFO: model_training: Rank 0, Epoch 2764, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,887: INFO: model_training: Rank 0, Epoch 2765, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,888: INFO: model_training: Rank 0, Epoch 2765, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,890: INFO: model_training: Rank 0, Epoch 2765, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,891: INFO: model_training: Rank 0, Epoch 2765, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,892: INFO: model_training: Rank 0, Epoch 2765, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,894: INFO: model_training: Rank 0, Epoch 2766, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,895: INFO: model_training: Rank 0, Epoch 2766, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,896: INFO: model_training: Rank 0, Epoch 2766, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,897: INFO: model_training: Rank 0, Epoch 2766, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,899: INFO: model_training: Rank 0, Epoch 2766, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,900: INFO: model_training: Rank 0, Epoch 2767, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,901: INFO: model_training: Rank 0, Epoch 2767, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,902: INFO: model_training: Rank 0, Epoch 2767, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,903: INFO: model_training: Rank 0, Epoch 2767, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,904: INFO: model_training: Rank 0, Epoch 2767, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,905: INFO: model_training: Rank 0, Epoch 2768, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,907: INFO: model_training: Rank 0, Epoch 2768, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,909: INFO: model_training: Rank 0, Epoch 2768, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,910: INFO: model_training: Rank 0, Epoch 2768, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,911: INFO: model_training: Rank 0, Epoch 2768, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,912: INFO: model_training: Rank 0, Epoch 2769, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,913: INFO: model_training: Rank 0, Epoch 2769, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,914: INFO: model_training: Rank 0, Epoch 2769, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,915: INFO: model_training: Rank 0, Epoch 2769, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,916: INFO: model_training: Rank 0, Epoch 2769, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,918: INFO: model_training: Rank 0, Epoch 2770, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,919: INFO: model_training: Rank 0, Epoch 2770, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,920: INFO: model_training: Rank 0, Epoch 2770, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,921: INFO: model_training: Rank 0, Epoch 2770, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,922: INFO: model_training: Rank 0, Epoch 2770, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,923: INFO: model_training: Rank 0, Epoch 2771, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,925: INFO: model_training: Rank 0, Epoch 2771, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,926: INFO: model_training: Rank 0, Epoch 2771, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,928: INFO: model_training: Rank 0, Epoch 2771, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,929: INFO: model_training: Rank 0, Epoch 2771, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,930: INFO: model_training: Rank 0, Epoch 2772, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,931: INFO: model_training: Rank 0, Epoch 2772, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,932: INFO: model_training: Rank 0, Epoch 2772, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,934: INFO: model_training: Rank 0, Epoch 2772, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,935: INFO: model_training: Rank 0, Epoch 2772, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,936: INFO: model_training: Rank 0, Epoch 2773, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,937: INFO: model_training: Rank 0, Epoch 2773, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,938: INFO: model_training: Rank 0, Epoch 2773, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,939: INFO: model_training: Rank 0, Epoch 2773, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,940: INFO: model_training: Rank 0, Epoch 2773, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,942: INFO: model_training: Rank 0, Epoch 2774, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,943: INFO: model_training: Rank 0, Epoch 2774, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,944: INFO: model_training: Rank 0, Epoch 2774, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,945: INFO: model_training: Rank 0, Epoch 2774, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,947: INFO: model_training: Rank 0, Epoch 2774, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,948: INFO: model_training: Rank 0, Epoch 2775, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,949: INFO: model_training: Rank 0, Epoch 2775, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,950: INFO: model_training: Rank 0, Epoch 2775, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,952: INFO: model_training: Rank 0, Epoch 2775, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,955: INFO: model_training: Rank 0, Epoch 2775, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,957: INFO: model_training: Rank 0, Epoch 2776, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,959: INFO: model_training: Rank 0, Epoch 2776, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,961: INFO: model_training: Rank 0, Epoch 2776, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,963: INFO: model_training: Rank 0, Epoch 2776, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,965: INFO: model_training: Rank 0, Epoch 2776, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,968: INFO: model_training: Rank 0, Epoch 2777, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,971: INFO: model_training: Rank 0, Epoch 2777, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,973: INFO: model_training: Rank 0, Epoch 2777, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,976: INFO: model_training: Rank 0, Epoch 2777, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,978: INFO: model_training: Rank 0, Epoch 2777, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,980: INFO: model_training: Rank 0, Epoch 2778, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,983: INFO: model_training: Rank 0, Epoch 2778, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,985: INFO: model_training: Rank 0, Epoch 2778, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,987: INFO: model_training: Rank 0, Epoch 2778, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:43,990: INFO: model_training: Rank 0, Epoch 2778, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:43,992: INFO: model_training: Rank 0, Epoch 2779, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:43,994: INFO: model_training: Rank 0, Epoch 2779, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:43,996: INFO: model_training: Rank 0, Epoch 2779, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:43,998: INFO: model_training: Rank 0, Epoch 2779, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,000: INFO: model_training: Rank 0, Epoch 2779, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,001: INFO: model_training: Rank 0, Epoch 2780, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,003: INFO: model_training: Rank 0, Epoch 2780, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,004: INFO: model_training: Rank 0, Epoch 2780, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,006: INFO: model_training: Rank 0, Epoch 2780, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,007: INFO: model_training: Rank 0, Epoch 2780, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,009: INFO: model_training: Rank 0, Epoch 2781, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,010: INFO: model_training: Rank 0, Epoch 2781, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,012: INFO: model_training: Rank 0, Epoch 2781, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,013: INFO: model_training: Rank 0, Epoch 2781, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,015: INFO: model_training: Rank 0, Epoch 2781, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,017: INFO: model_training: Rank 0, Epoch 2782, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,018: INFO: model_training: Rank 0, Epoch 2782, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,021: INFO: model_training: Rank 0, Epoch 2782, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,022: INFO: model_training: Rank 0, Epoch 2782, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,024: INFO: model_training: Rank 0, Epoch 2782, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,026: INFO: model_training: Rank 0, Epoch 2783, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,027: INFO: model_training: Rank 0, Epoch 2783, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,028: INFO: model_training: Rank 0, Epoch 2783, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,030: INFO: model_training: Rank 0, Epoch 2783, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,032: INFO: model_training: Rank 0, Epoch 2783, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,033: INFO: model_training: Rank 0, Epoch 2784, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,035: INFO: model_training: Rank 0, Epoch 2784, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,036: INFO: model_training: Rank 0, Epoch 2784, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,037: INFO: model_training: Rank 0, Epoch 2784, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,039: INFO: model_training: Rank 0, Epoch 2784, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,041: INFO: model_training: Rank 0, Epoch 2785, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,043: INFO: model_training: Rank 0, Epoch 2785, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,045: INFO: model_training: Rank 0, Epoch 2785, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,047: INFO: model_training: Rank 0, Epoch 2785, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,048: INFO: model_training: Rank 0, Epoch 2785, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,050: INFO: model_training: Rank 0, Epoch 2786, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,051: INFO: model_training: Rank 0, Epoch 2786, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,053: INFO: model_training: Rank 0, Epoch 2786, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,054: INFO: model_training: Rank 0, Epoch 2786, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,055: INFO: model_training: Rank 0, Epoch 2786, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,056: INFO: model_training: Rank 0, Epoch 2787, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,057: INFO: model_training: Rank 0, Epoch 2787, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,060: INFO: model_training: Rank 0, Epoch 2787, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,062: INFO: model_training: Rank 0, Epoch 2787, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,063: INFO: model_training: Rank 0, Epoch 2787, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,064: INFO: model_training: Rank 0, Epoch 2788, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,065: INFO: model_training: Rank 0, Epoch 2788, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,068: INFO: model_training: Rank 0, Epoch 2788, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,069: INFO: model_training: Rank 0, Epoch 2788, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,070: INFO: model_training: Rank 0, Epoch 2788, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,072: INFO: model_training: Rank 0, Epoch 2789, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,073: INFO: model_training: Rank 0, Epoch 2789, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,075: INFO: model_training: Rank 0, Epoch 2789, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,076: INFO: model_training: Rank 0, Epoch 2789, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,078: INFO: model_training: Rank 0, Epoch 2789, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,079: INFO: model_training: Rank 0, Epoch 2790, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,080: INFO: model_training: Rank 0, Epoch 2790, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,082: INFO: model_training: Rank 0, Epoch 2790, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,083: INFO: model_training: Rank 0, Epoch 2790, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,084: INFO: model_training: Rank 0, Epoch 2790, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,086: INFO: model_training: Rank 0, Epoch 2791, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,087: INFO: model_training: Rank 0, Epoch 2791, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,089: INFO: model_training: Rank 0, Epoch 2791, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,091: INFO: model_training: Rank 0, Epoch 2791, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,093: INFO: model_training: Rank 0, Epoch 2791, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,094: INFO: model_training: Rank 0, Epoch 2792, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,096: INFO: model_training: Rank 0, Epoch 2792, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,097: INFO: model_training: Rank 0, Epoch 2792, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,098: INFO: model_training: Rank 0, Epoch 2792, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,100: INFO: model_training: Rank 0, Epoch 2792, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,101: INFO: model_training: Rank 0, Epoch 2793, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,103: INFO: model_training: Rank 0, Epoch 2793, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,104: INFO: model_training: Rank 0, Epoch 2793, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,105: INFO: model_training: Rank 0, Epoch 2793, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,106: INFO: model_training: Rank 0, Epoch 2793, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,108: INFO: model_training: Rank 0, Epoch 2794, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,111: INFO: model_training: Rank 0, Epoch 2794, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,112: INFO: model_training: Rank 0, Epoch 2794, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,114: INFO: model_training: Rank 0, Epoch 2794, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,115: INFO: model_training: Rank 0, Epoch 2794, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,116: INFO: model_training: Rank 0, Epoch 2795, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,118: INFO: model_training: Rank 0, Epoch 2795, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,120: INFO: model_training: Rank 0, Epoch 2795, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,121: INFO: model_training: Rank 0, Epoch 2795, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,122: INFO: model_training: Rank 0, Epoch 2795, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,123: INFO: model_training: Rank 0, Epoch 2796, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,125: INFO: model_training: Rank 0, Epoch 2796, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,127: INFO: model_training: Rank 0, Epoch 2796, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,129: INFO: model_training: Rank 0, Epoch 2796, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,130: INFO: model_training: Rank 0, Epoch 2796, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,132: INFO: model_training: Rank 0, Epoch 2797, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,134: INFO: model_training: Rank 0, Epoch 2797, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,135: INFO: model_training: Rank 0, Epoch 2797, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,136: INFO: model_training: Rank 0, Epoch 2797, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,138: INFO: model_training: Rank 0, Epoch 2797, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,139: INFO: model_training: Rank 0, Epoch 2798, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,140: INFO: model_training: Rank 0, Epoch 2798, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,141: INFO: model_training: Rank 0, Epoch 2798, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,143: INFO: model_training: Rank 0, Epoch 2798, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,145: INFO: model_training: Rank 0, Epoch 2798, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,146: INFO: model_training: Rank 0, Epoch 2799, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,147: INFO: model_training: Rank 0, Epoch 2799, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,149: INFO: model_training: Rank 0, Epoch 2799, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,150: INFO: model_training: Rank 0, Epoch 2799, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,151: INFO: model_training: Rank 0, Epoch 2799, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,152: INFO: model_training: Rank 0, Epoch 2800, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,154: INFO: model_training: Rank 0, Epoch 2800, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,156: INFO: model_training: Rank 0, Epoch 2800, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,157: INFO: model_training: Rank 0, Epoch 2800, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,159: INFO: model_training: Rank 0, Epoch 2800, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,161: INFO: model_training: Rank 0, Epoch 2801, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,162: INFO: model_training: Rank 0, Epoch 2801, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,163: INFO: model_training: Rank 0, Epoch 2801, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,164: INFO: model_training: Rank 0, Epoch 2801, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,166: INFO: model_training: Rank 0, Epoch 2801, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,167: INFO: model_training: Rank 0, Epoch 2802, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,168: INFO: model_training: Rank 0, Epoch 2802, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,169: INFO: model_training: Rank 0, Epoch 2802, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,171: INFO: model_training: Rank 0, Epoch 2802, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,172: INFO: model_training: Rank 0, Epoch 2802, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,174: INFO: model_training: Rank 0, Epoch 2803, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,176: INFO: model_training: Rank 0, Epoch 2803, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,177: INFO: model_training: Rank 0, Epoch 2803, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,179: INFO: model_training: Rank 0, Epoch 2803, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,180: INFO: model_training: Rank 0, Epoch 2803, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,181: INFO: model_training: Rank 0, Epoch 2804, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,182: INFO: model_training: Rank 0, Epoch 2804, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,183: INFO: model_training: Rank 0, Epoch 2804, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,184: INFO: model_training: Rank 0, Epoch 2804, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,186: INFO: model_training: Rank 0, Epoch 2804, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,187: INFO: model_training: Rank 0, Epoch 2805, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,188: INFO: model_training: Rank 0, Epoch 2805, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,189: INFO: model_training: Rank 0, Epoch 2805, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,191: INFO: model_training: Rank 0, Epoch 2805, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,192: INFO: model_training: Rank 0, Epoch 2805, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,193: INFO: model_training: Rank 0, Epoch 2806, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,195: INFO: model_training: Rank 0, Epoch 2806, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,196: INFO: model_training: Rank 0, Epoch 2806, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,197: INFO: model_training: Rank 0, Epoch 2806, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,199: INFO: model_training: Rank 0, Epoch 2806, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,200: INFO: model_training: Rank 0, Epoch 2807, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,201: INFO: model_training: Rank 0, Epoch 2807, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,202: INFO: model_training: Rank 0, Epoch 2807, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,203: INFO: model_training: Rank 0, Epoch 2807, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,204: INFO: model_training: Rank 0, Epoch 2807, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,206: INFO: model_training: Rank 0, Epoch 2808, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,207: INFO: model_training: Rank 0, Epoch 2808, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,208: INFO: model_training: Rank 0, Epoch 2808, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,209: INFO: model_training: Rank 0, Epoch 2808, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,210: INFO: model_training: Rank 0, Epoch 2808, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,211: INFO: model_training: Rank 0, Epoch 2809, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,213: INFO: model_training: Rank 0, Epoch 2809, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,214: INFO: model_training: Rank 0, Epoch 2809, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,216: INFO: model_training: Rank 0, Epoch 2809, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,217: INFO: model_training: Rank 0, Epoch 2809, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,218: INFO: model_training: Rank 0, Epoch 2810, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,219: INFO: model_training: Rank 0, Epoch 2810, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,220: INFO: model_training: Rank 0, Epoch 2810, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,222: INFO: model_training: Rank 0, Epoch 2810, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,223: INFO: model_training: Rank 0, Epoch 2810, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,224: INFO: model_training: Rank 0, Epoch 2811, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,225: INFO: model_training: Rank 0, Epoch 2811, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,227: INFO: model_training: Rank 0, Epoch 2811, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,228: INFO: model_training: Rank 0, Epoch 2811, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,229: INFO: model_training: Rank 0, Epoch 2811, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,231: INFO: model_training: Rank 0, Epoch 2812, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,232: INFO: model_training: Rank 0, Epoch 2812, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,233: INFO: model_training: Rank 0, Epoch 2812, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,234: INFO: model_training: Rank 0, Epoch 2812, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,236: INFO: model_training: Rank 0, Epoch 2812, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,237: INFO: model_training: Rank 0, Epoch 2813, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,238: INFO: model_training: Rank 0, Epoch 2813, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,239: INFO: model_training: Rank 0, Epoch 2813, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,240: INFO: model_training: Rank 0, Epoch 2813, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,241: INFO: model_training: Rank 0, Epoch 2813, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,242: INFO: model_training: Rank 0, Epoch 2814, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,243: INFO: model_training: Rank 0, Epoch 2814, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,244: INFO: model_training: Rank 0, Epoch 2814, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,246: INFO: model_training: Rank 0, Epoch 2814, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,247: INFO: model_training: Rank 0, Epoch 2814, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,248: INFO: model_training: Rank 0, Epoch 2815, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,249: INFO: model_training: Rank 0, Epoch 2815, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,250: INFO: model_training: Rank 0, Epoch 2815, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,252: INFO: model_training: Rank 0, Epoch 2815, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,253: INFO: model_training: Rank 0, Epoch 2815, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,254: INFO: model_training: Rank 0, Epoch 2816, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,255: INFO: model_training: Rank 0, Epoch 2816, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,256: INFO: model_training: Rank 0, Epoch 2816, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,257: INFO: model_training: Rank 0, Epoch 2816, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,258: INFO: model_training: Rank 0, Epoch 2816, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,260: INFO: model_training: Rank 0, Epoch 2817, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,262: INFO: model_training: Rank 0, Epoch 2817, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,263: INFO: model_training: Rank 0, Epoch 2817, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,265: INFO: model_training: Rank 0, Epoch 2817, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,266: INFO: model_training: Rank 0, Epoch 2817, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,267: INFO: model_training: Rank 0, Epoch 2818, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,270: INFO: model_training: Rank 0, Epoch 2818, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,271: INFO: model_training: Rank 0, Epoch 2818, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,273: INFO: model_training: Rank 0, Epoch 2818, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,275: INFO: model_training: Rank 0, Epoch 2818, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,276: INFO: model_training: Rank 0, Epoch 2819, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,277: INFO: model_training: Rank 0, Epoch 2819, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,279: INFO: model_training: Rank 0, Epoch 2819, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,280: INFO: model_training: Rank 0, Epoch 2819, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,281: INFO: model_training: Rank 0, Epoch 2819, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,283: INFO: model_training: Rank 0, Epoch 2820, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,284: INFO: model_training: Rank 0, Epoch 2820, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,285: INFO: model_training: Rank 0, Epoch 2820, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,286: INFO: model_training: Rank 0, Epoch 2820, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,287: INFO: model_training: Rank 0, Epoch 2820, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,288: INFO: model_training: Rank 0, Epoch 2821, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,289: INFO: model_training: Rank 0, Epoch 2821, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,291: INFO: model_training: Rank 0, Epoch 2821, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,292: INFO: model_training: Rank 0, Epoch 2821, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,293: INFO: model_training: Rank 0, Epoch 2821, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,294: INFO: model_training: Rank 0, Epoch 2822, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,296: INFO: model_training: Rank 0, Epoch 2822, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,297: INFO: model_training: Rank 0, Epoch 2822, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,298: INFO: model_training: Rank 0, Epoch 2822, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,299: INFO: model_training: Rank 0, Epoch 2822, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,300: INFO: model_training: Rank 0, Epoch 2823, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,301: INFO: model_training: Rank 0, Epoch 2823, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,302: INFO: model_training: Rank 0, Epoch 2823, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,303: INFO: model_training: Rank 0, Epoch 2823, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,305: INFO: model_training: Rank 0, Epoch 2823, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,306: INFO: model_training: Rank 0, Epoch 2824, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,307: INFO: model_training: Rank 0, Epoch 2824, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,308: INFO: model_training: Rank 0, Epoch 2824, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,310: INFO: model_training: Rank 0, Epoch 2824, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,311: INFO: model_training: Rank 0, Epoch 2824, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,312: INFO: model_training: Rank 0, Epoch 2825, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,313: INFO: model_training: Rank 0, Epoch 2825, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,314: INFO: model_training: Rank 0, Epoch 2825, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,315: INFO: model_training: Rank 0, Epoch 2825, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,316: INFO: model_training: Rank 0, Epoch 2825, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,318: INFO: model_training: Rank 0, Epoch 2826, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,319: INFO: model_training: Rank 0, Epoch 2826, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,320: INFO: model_training: Rank 0, Epoch 2826, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,321: INFO: model_training: Rank 0, Epoch 2826, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,322: INFO: model_training: Rank 0, Epoch 2826, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,323: INFO: model_training: Rank 0, Epoch 2827, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,325: INFO: model_training: Rank 0, Epoch 2827, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,326: INFO: model_training: Rank 0, Epoch 2827, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,327: INFO: model_training: Rank 0, Epoch 2827, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,328: INFO: model_training: Rank 0, Epoch 2827, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,329: INFO: model_training: Rank 0, Epoch 2828, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,330: INFO: model_training: Rank 0, Epoch 2828, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,332: INFO: model_training: Rank 0, Epoch 2828, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,333: INFO: model_training: Rank 0, Epoch 2828, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,334: INFO: model_training: Rank 0, Epoch 2828, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,335: INFO: model_training: Rank 0, Epoch 2829, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,336: INFO: model_training: Rank 0, Epoch 2829, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,337: INFO: model_training: Rank 0, Epoch 2829, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,338: INFO: model_training: Rank 0, Epoch 2829, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,340: INFO: model_training: Rank 0, Epoch 2829, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,341: INFO: model_training: Rank 0, Epoch 2830, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,342: INFO: model_training: Rank 0, Epoch 2830, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,343: INFO: model_training: Rank 0, Epoch 2830, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,345: INFO: model_training: Rank 0, Epoch 2830, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,346: INFO: model_training: Rank 0, Epoch 2830, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,347: INFO: model_training: Rank 0, Epoch 2831, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,348: INFO: model_training: Rank 0, Epoch 2831, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,349: INFO: model_training: Rank 0, Epoch 2831, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,350: INFO: model_training: Rank 0, Epoch 2831, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,351: INFO: model_training: Rank 0, Epoch 2831, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,352: INFO: model_training: Rank 0, Epoch 2832, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,353: INFO: model_training: Rank 0, Epoch 2832, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,354: INFO: model_training: Rank 0, Epoch 2832, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,355: INFO: model_training: Rank 0, Epoch 2832, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,357: INFO: model_training: Rank 0, Epoch 2832, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,358: INFO: model_training: Rank 0, Epoch 2833, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,359: INFO: model_training: Rank 0, Epoch 2833, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,361: INFO: model_training: Rank 0, Epoch 2833, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,362: INFO: model_training: Rank 0, Epoch 2833, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,363: INFO: model_training: Rank 0, Epoch 2833, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,364: INFO: model_training: Rank 0, Epoch 2834, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,365: INFO: model_training: Rank 0, Epoch 2834, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,366: INFO: model_training: Rank 0, Epoch 2834, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,367: INFO: model_training: Rank 0, Epoch 2834, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,369: INFO: model_training: Rank 0, Epoch 2834, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,370: INFO: model_training: Rank 0, Epoch 2835, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,371: INFO: model_training: Rank 0, Epoch 2835, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,372: INFO: model_training: Rank 0, Epoch 2835, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,373: INFO: model_training: Rank 0, Epoch 2835, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,375: INFO: model_training: Rank 0, Epoch 2835, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,376: INFO: model_training: Rank 0, Epoch 2836, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,378: INFO: model_training: Rank 0, Epoch 2836, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,379: INFO: model_training: Rank 0, Epoch 2836, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,380: INFO: model_training: Rank 0, Epoch 2836, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,381: INFO: model_training: Rank 0, Epoch 2836, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,382: INFO: model_training: Rank 0, Epoch 2837, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,383: INFO: model_training: Rank 0, Epoch 2837, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,384: INFO: model_training: Rank 0, Epoch 2837, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,385: INFO: model_training: Rank 0, Epoch 2837, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,386: INFO: model_training: Rank 0, Epoch 2837, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,387: INFO: model_training: Rank 0, Epoch 2838, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,388: INFO: model_training: Rank 0, Epoch 2838, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,390: INFO: model_training: Rank 0, Epoch 2838, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,391: INFO: model_training: Rank 0, Epoch 2838, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,393: INFO: model_training: Rank 0, Epoch 2838, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,394: INFO: model_training: Rank 0, Epoch 2839, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,395: INFO: model_training: Rank 0, Epoch 2839, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,396: INFO: model_training: Rank 0, Epoch 2839, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,397: INFO: model_training: Rank 0, Epoch 2839, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,398: INFO: model_training: Rank 0, Epoch 2839, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,399: INFO: model_training: Rank 0, Epoch 2840, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,400: INFO: model_training: Rank 0, Epoch 2840, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,401: INFO: model_training: Rank 0, Epoch 2840, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,402: INFO: model_training: Rank 0, Epoch 2840, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,404: INFO: model_training: Rank 0, Epoch 2840, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,405: INFO: model_training: Rank 0, Epoch 2841, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,406: INFO: model_training: Rank 0, Epoch 2841, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,407: INFO: model_training: Rank 0, Epoch 2841, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,408: INFO: model_training: Rank 0, Epoch 2841, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,410: INFO: model_training: Rank 0, Epoch 2841, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,411: INFO: model_training: Rank 0, Epoch 2842, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,412: INFO: model_training: Rank 0, Epoch 2842, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,413: INFO: model_training: Rank 0, Epoch 2842, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,414: INFO: model_training: Rank 0, Epoch 2842, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,415: INFO: model_training: Rank 0, Epoch 2842, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,417: INFO: model_training: Rank 0, Epoch 2843, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,418: INFO: model_training: Rank 0, Epoch 2843, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,419: INFO: model_training: Rank 0, Epoch 2843, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,420: INFO: model_training: Rank 0, Epoch 2843, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,421: INFO: model_training: Rank 0, Epoch 2843, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,422: INFO: model_training: Rank 0, Epoch 2844, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,424: INFO: model_training: Rank 0, Epoch 2844, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,425: INFO: model_training: Rank 0, Epoch 2844, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,426: INFO: model_training: Rank 0, Epoch 2844, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,427: INFO: model_training: Rank 0, Epoch 2844, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,429: INFO: model_training: Rank 0, Epoch 2845, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,430: INFO: model_training: Rank 0, Epoch 2845, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,431: INFO: model_training: Rank 0, Epoch 2845, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,432: INFO: model_training: Rank 0, Epoch 2845, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,433: INFO: model_training: Rank 0, Epoch 2845, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,435: INFO: model_training: Rank 0, Epoch 2846, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,436: INFO: model_training: Rank 0, Epoch 2846, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,437: INFO: model_training: Rank 0, Epoch 2846, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,438: INFO: model_training: Rank 0, Epoch 2846, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,439: INFO: model_training: Rank 0, Epoch 2846, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,440: INFO: model_training: Rank 0, Epoch 2847, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,441: INFO: model_training: Rank 0, Epoch 2847, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,443: INFO: model_training: Rank 0, Epoch 2847, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,444: INFO: model_training: Rank 0, Epoch 2847, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,445: INFO: model_training: Rank 0, Epoch 2847, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,446: INFO: model_training: Rank 0, Epoch 2848, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,447: INFO: model_training: Rank 0, Epoch 2848, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,449: INFO: model_training: Rank 0, Epoch 2848, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,450: INFO: model_training: Rank 0, Epoch 2848, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,451: INFO: model_training: Rank 0, Epoch 2848, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,452: INFO: model_training: Rank 0, Epoch 2849, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,453: INFO: model_training: Rank 0, Epoch 2849, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,454: INFO: model_training: Rank 0, Epoch 2849, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,456: INFO: model_training: Rank 0, Epoch 2849, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,457: INFO: model_training: Rank 0, Epoch 2849, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,458: INFO: model_training: Rank 0, Epoch 2850, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,459: INFO: model_training: Rank 0, Epoch 2850, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,461: INFO: model_training: Rank 0, Epoch 2850, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,462: INFO: model_training: Rank 0, Epoch 2850, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,463: INFO: model_training: Rank 0, Epoch 2850, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,464: INFO: model_training: Rank 0, Epoch 2851, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,465: INFO: model_training: Rank 0, Epoch 2851, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,466: INFO: model_training: Rank 0, Epoch 2851, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,467: INFO: model_training: Rank 0, Epoch 2851, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,468: INFO: model_training: Rank 0, Epoch 2851, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,470: INFO: model_training: Rank 0, Epoch 2852, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,471: INFO: model_training: Rank 0, Epoch 2852, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,472: INFO: model_training: Rank 0, Epoch 2852, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,473: INFO: model_training: Rank 0, Epoch 2852, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,474: INFO: model_training: Rank 0, Epoch 2852, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,476: INFO: model_training: Rank 0, Epoch 2853, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,477: INFO: model_training: Rank 0, Epoch 2853, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,478: INFO: model_training: Rank 0, Epoch 2853, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,479: INFO: model_training: Rank 0, Epoch 2853, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,481: INFO: model_training: Rank 0, Epoch 2853, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,482: INFO: model_training: Rank 0, Epoch 2854, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,483: INFO: model_training: Rank 0, Epoch 2854, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,485: INFO: model_training: Rank 0, Epoch 2854, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,486: INFO: model_training: Rank 0, Epoch 2854, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,487: INFO: model_training: Rank 0, Epoch 2854, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,488: INFO: model_training: Rank 0, Epoch 2855, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,490: INFO: model_training: Rank 0, Epoch 2855, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,491: INFO: model_training: Rank 0, Epoch 2855, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,492: INFO: model_training: Rank 0, Epoch 2855, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,493: INFO: model_training: Rank 0, Epoch 2855, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,494: INFO: model_training: Rank 0, Epoch 2856, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,496: INFO: model_training: Rank 0, Epoch 2856, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,498: INFO: model_training: Rank 0, Epoch 2856, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,499: INFO: model_training: Rank 0, Epoch 2856, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,501: INFO: model_training: Rank 0, Epoch 2856, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,503: INFO: model_training: Rank 0, Epoch 2857, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,504: INFO: model_training: Rank 0, Epoch 2857, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,505: INFO: model_training: Rank 0, Epoch 2857, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,507: INFO: model_training: Rank 0, Epoch 2857, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,508: INFO: model_training: Rank 0, Epoch 2857, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,509: INFO: model_training: Rank 0, Epoch 2858, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,511: INFO: model_training: Rank 0, Epoch 2858, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,513: INFO: model_training: Rank 0, Epoch 2858, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,515: INFO: model_training: Rank 0, Epoch 2858, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,516: INFO: model_training: Rank 0, Epoch 2858, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,517: INFO: model_training: Rank 0, Epoch 2859, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,519: INFO: model_training: Rank 0, Epoch 2859, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,520: INFO: model_training: Rank 0, Epoch 2859, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,522: INFO: model_training: Rank 0, Epoch 2859, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,523: INFO: model_training: Rank 0, Epoch 2859, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,524: INFO: model_training: Rank 0, Epoch 2860, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,526: INFO: model_training: Rank 0, Epoch 2860, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,527: INFO: model_training: Rank 0, Epoch 2860, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,528: INFO: model_training: Rank 0, Epoch 2860, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,530: INFO: model_training: Rank 0, Epoch 2860, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,532: INFO: model_training: Rank 0, Epoch 2861, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,533: INFO: model_training: Rank 0, Epoch 2861, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,534: INFO: model_training: Rank 0, Epoch 2861, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,536: INFO: model_training: Rank 0, Epoch 2861, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,537: INFO: model_training: Rank 0, Epoch 2861, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,538: INFO: model_training: Rank 0, Epoch 2862, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,539: INFO: model_training: Rank 0, Epoch 2862, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,541: INFO: model_training: Rank 0, Epoch 2862, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,542: INFO: model_training: Rank 0, Epoch 2862, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,543: INFO: model_training: Rank 0, Epoch 2862, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,545: INFO: model_training: Rank 0, Epoch 2863, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,547: INFO: model_training: Rank 0, Epoch 2863, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,548: INFO: model_training: Rank 0, Epoch 2863, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,551: INFO: model_training: Rank 0, Epoch 2863, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,554: INFO: model_training: Rank 0, Epoch 2863, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,557: INFO: model_training: Rank 0, Epoch 2864, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,559: INFO: model_training: Rank 0, Epoch 2864, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,561: INFO: model_training: Rank 0, Epoch 2864, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,563: INFO: model_training: Rank 0, Epoch 2864, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,565: INFO: model_training: Rank 0, Epoch 2864, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,567: INFO: model_training: Rank 0, Epoch 2865, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,570: INFO: model_training: Rank 0, Epoch 2865, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,572: INFO: model_training: Rank 0, Epoch 2865, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,574: INFO: model_training: Rank 0, Epoch 2865, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,576: INFO: model_training: Rank 0, Epoch 2865, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,578: INFO: model_training: Rank 0, Epoch 2866, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,580: INFO: model_training: Rank 0, Epoch 2866, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,582: INFO: model_training: Rank 0, Epoch 2866, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,583: INFO: model_training: Rank 0, Epoch 2866, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,585: INFO: model_training: Rank 0, Epoch 2866, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,587: INFO: model_training: Rank 0, Epoch 2867, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,588: INFO: model_training: Rank 0, Epoch 2867, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,589: INFO: model_training: Rank 0, Epoch 2867, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,590: INFO: model_training: Rank 0, Epoch 2867, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,591: INFO: model_training: Rank 0, Epoch 2867, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,593: INFO: model_training: Rank 0, Epoch 2868, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,595: INFO: model_training: Rank 0, Epoch 2868, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,596: INFO: model_training: Rank 0, Epoch 2868, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,597: INFO: model_training: Rank 0, Epoch 2868, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,599: INFO: model_training: Rank 0, Epoch 2868, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,601: INFO: model_training: Rank 0, Epoch 2869, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,602: INFO: model_training: Rank 0, Epoch 2869, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,603: INFO: model_training: Rank 0, Epoch 2869, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,604: INFO: model_training: Rank 0, Epoch 2869, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,605: INFO: model_training: Rank 0, Epoch 2869, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,607: INFO: model_training: Rank 0, Epoch 2870, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,608: INFO: model_training: Rank 0, Epoch 2870, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,609: INFO: model_training: Rank 0, Epoch 2870, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,610: INFO: model_training: Rank 0, Epoch 2870, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,611: INFO: model_training: Rank 0, Epoch 2870, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,613: INFO: model_training: Rank 0, Epoch 2871, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,615: INFO: model_training: Rank 0, Epoch 2871, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,617: INFO: model_training: Rank 0, Epoch 2871, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,618: INFO: model_training: Rank 0, Epoch 2871, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,620: INFO: model_training: Rank 0, Epoch 2871, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,621: INFO: model_training: Rank 0, Epoch 2872, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,622: INFO: model_training: Rank 0, Epoch 2872, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,624: INFO: model_training: Rank 0, Epoch 2872, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,625: INFO: model_training: Rank 0, Epoch 2872, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,627: INFO: model_training: Rank 0, Epoch 2872, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,628: INFO: model_training: Rank 0, Epoch 2873, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,629: INFO: model_training: Rank 0, Epoch 2873, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,630: INFO: model_training: Rank 0, Epoch 2873, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,632: INFO: model_training: Rank 0, Epoch 2873, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,634: INFO: model_training: Rank 0, Epoch 2873, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,635: INFO: model_training: Rank 0, Epoch 2874, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,637: INFO: model_training: Rank 0, Epoch 2874, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,638: INFO: model_training: Rank 0, Epoch 2874, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,639: INFO: model_training: Rank 0, Epoch 2874, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,640: INFO: model_training: Rank 0, Epoch 2874, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,641: INFO: model_training: Rank 0, Epoch 2875, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,643: INFO: model_training: Rank 0, Epoch 2875, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,644: INFO: model_training: Rank 0, Epoch 2875, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,645: INFO: model_training: Rank 0, Epoch 2875, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,647: INFO: model_training: Rank 0, Epoch 2875, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,648: INFO: model_training: Rank 0, Epoch 2876, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,649: INFO: model_training: Rank 0, Epoch 2876, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,651: INFO: model_training: Rank 0, Epoch 2876, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,652: INFO: model_training: Rank 0, Epoch 2876, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,654: INFO: model_training: Rank 0, Epoch 2876, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,655: INFO: model_training: Rank 0, Epoch 2877, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,657: INFO: model_training: Rank 0, Epoch 2877, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,658: INFO: model_training: Rank 0, Epoch 2877, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,659: INFO: model_training: Rank 0, Epoch 2877, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,660: INFO: model_training: Rank 0, Epoch 2877, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,662: INFO: model_training: Rank 0, Epoch 2878, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,663: INFO: model_training: Rank 0, Epoch 2878, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,664: INFO: model_training: Rank 0, Epoch 2878, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,665: INFO: model_training: Rank 0, Epoch 2878, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,667: INFO: model_training: Rank 0, Epoch 2878, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,668: INFO: model_training: Rank 0, Epoch 2879, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,670: INFO: model_training: Rank 0, Epoch 2879, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,672: INFO: model_training: Rank 0, Epoch 2879, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,673: INFO: model_training: Rank 0, Epoch 2879, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,674: INFO: model_training: Rank 0, Epoch 2879, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,676: INFO: model_training: Rank 0, Epoch 2880, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,677: INFO: model_training: Rank 0, Epoch 2880, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,678: INFO: model_training: Rank 0, Epoch 2880, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,680: INFO: model_training: Rank 0, Epoch 2880, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,681: INFO: model_training: Rank 0, Epoch 2880, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,683: INFO: model_training: Rank 0, Epoch 2881, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,685: INFO: model_training: Rank 0, Epoch 2881, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,686: INFO: model_training: Rank 0, Epoch 2881, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,688: INFO: model_training: Rank 0, Epoch 2881, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,689: INFO: model_training: Rank 0, Epoch 2881, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,690: INFO: model_training: Rank 0, Epoch 2882, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,692: INFO: model_training: Rank 0, Epoch 2882, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,693: INFO: model_training: Rank 0, Epoch 2882, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,694: INFO: model_training: Rank 0, Epoch 2882, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,695: INFO: model_training: Rank 0, Epoch 2882, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,696: INFO: model_training: Rank 0, Epoch 2883, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,698: INFO: model_training: Rank 0, Epoch 2883, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,699: INFO: model_training: Rank 0, Epoch 2883, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,701: INFO: model_training: Rank 0, Epoch 2883, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,703: INFO: model_training: Rank 0, Epoch 2883, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,704: INFO: model_training: Rank 0, Epoch 2884, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,706: INFO: model_training: Rank 0, Epoch 2884, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,707: INFO: model_training: Rank 0, Epoch 2884, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,709: INFO: model_training: Rank 0, Epoch 2884, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,711: INFO: model_training: Rank 0, Epoch 2884, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,712: INFO: model_training: Rank 0, Epoch 2885, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,714: INFO: model_training: Rank 0, Epoch 2885, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,715: INFO: model_training: Rank 0, Epoch 2885, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,716: INFO: model_training: Rank 0, Epoch 2885, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,718: INFO: model_training: Rank 0, Epoch 2885, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,720: INFO: model_training: Rank 0, Epoch 2886, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,721: INFO: model_training: Rank 0, Epoch 2886, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,723: INFO: model_training: Rank 0, Epoch 2886, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,724: INFO: model_training: Rank 0, Epoch 2886, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,725: INFO: model_training: Rank 0, Epoch 2886, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,727: INFO: model_training: Rank 0, Epoch 2887, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,728: INFO: model_training: Rank 0, Epoch 2887, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,730: INFO: model_training: Rank 0, Epoch 2887, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,731: INFO: model_training: Rank 0, Epoch 2887, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,732: INFO: model_training: Rank 0, Epoch 2887, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,734: INFO: model_training: Rank 0, Epoch 2888, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,735: INFO: model_training: Rank 0, Epoch 2888, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,737: INFO: model_training: Rank 0, Epoch 2888, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,738: INFO: model_training: Rank 0, Epoch 2888, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,739: INFO: model_training: Rank 0, Epoch 2888, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,742: INFO: model_training: Rank 0, Epoch 2889, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,743: INFO: model_training: Rank 0, Epoch 2889, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,744: INFO: model_training: Rank 0, Epoch 2889, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,746: INFO: model_training: Rank 0, Epoch 2889, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,748: INFO: model_training: Rank 0, Epoch 2889, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,750: INFO: model_training: Rank 0, Epoch 2890, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,753: INFO: model_training: Rank 0, Epoch 2890, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,755: INFO: model_training: Rank 0, Epoch 2890, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,757: INFO: model_training: Rank 0, Epoch 2890, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,760: INFO: model_training: Rank 0, Epoch 2890, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,761: INFO: model_training: Rank 0, Epoch 2891, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,763: INFO: model_training: Rank 0, Epoch 2891, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,764: INFO: model_training: Rank 0, Epoch 2891, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,766: INFO: model_training: Rank 0, Epoch 2891, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,768: INFO: model_training: Rank 0, Epoch 2891, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,770: INFO: model_training: Rank 0, Epoch 2892, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,771: INFO: model_training: Rank 0, Epoch 2892, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,773: INFO: model_training: Rank 0, Epoch 2892, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,774: INFO: model_training: Rank 0, Epoch 2892, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,776: INFO: model_training: Rank 0, Epoch 2892, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,777: INFO: model_training: Rank 0, Epoch 2893, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,779: INFO: model_training: Rank 0, Epoch 2893, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,781: INFO: model_training: Rank 0, Epoch 2893, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,783: INFO: model_training: Rank 0, Epoch 2893, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,784: INFO: model_training: Rank 0, Epoch 2893, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,785: INFO: model_training: Rank 0, Epoch 2894, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,787: INFO: model_training: Rank 0, Epoch 2894, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,788: INFO: model_training: Rank 0, Epoch 2894, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,790: INFO: model_training: Rank 0, Epoch 2894, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,792: INFO: model_training: Rank 0, Epoch 2894, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,793: INFO: model_training: Rank 0, Epoch 2895, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,794: INFO: model_training: Rank 0, Epoch 2895, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,795: INFO: model_training: Rank 0, Epoch 2895, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,797: INFO: model_training: Rank 0, Epoch 2895, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,799: INFO: model_training: Rank 0, Epoch 2895, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,800: INFO: model_training: Rank 0, Epoch 2896, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,802: INFO: model_training: Rank 0, Epoch 2896, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,803: INFO: model_training: Rank 0, Epoch 2896, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,804: INFO: model_training: Rank 0, Epoch 2896, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,806: INFO: model_training: Rank 0, Epoch 2896, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,807: INFO: model_training: Rank 0, Epoch 2897, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,809: INFO: model_training: Rank 0, Epoch 2897, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,810: INFO: model_training: Rank 0, Epoch 2897, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,812: INFO: model_training: Rank 0, Epoch 2897, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,813: INFO: model_training: Rank 0, Epoch 2897, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,814: INFO: model_training: Rank 0, Epoch 2898, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,815: INFO: model_training: Rank 0, Epoch 2898, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,817: INFO: model_training: Rank 0, Epoch 2898, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,818: INFO: model_training: Rank 0, Epoch 2898, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,820: INFO: model_training: Rank 0, Epoch 2898, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,821: INFO: model_training: Rank 0, Epoch 2899, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,823: INFO: model_training: Rank 0, Epoch 2899, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,824: INFO: model_training: Rank 0, Epoch 2899, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,825: INFO: model_training: Rank 0, Epoch 2899, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,826: INFO: model_training: Rank 0, Epoch 2899, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,827: INFO: model_training: Rank 0, Epoch 2900, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,829: INFO: model_training: Rank 0, Epoch 2900, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,830: INFO: model_training: Rank 0, Epoch 2900, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,832: INFO: model_training: Rank 0, Epoch 2900, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,833: INFO: model_training: Rank 0, Epoch 2900, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,835: INFO: model_training: Rank 0, Epoch 2901, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,837: INFO: model_training: Rank 0, Epoch 2901, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,838: INFO: model_training: Rank 0, Epoch 2901, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,840: INFO: model_training: Rank 0, Epoch 2901, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,841: INFO: model_training: Rank 0, Epoch 2901, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,842: INFO: model_training: Rank 0, Epoch 2902, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,844: INFO: model_training: Rank 0, Epoch 2902, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,845: INFO: model_training: Rank 0, Epoch 2902, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,846: INFO: model_training: Rank 0, Epoch 2902, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,847: INFO: model_training: Rank 0, Epoch 2902, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,849: INFO: model_training: Rank 0, Epoch 2903, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,851: INFO: model_training: Rank 0, Epoch 2903, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,853: INFO: model_training: Rank 0, Epoch 2903, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,854: INFO: model_training: Rank 0, Epoch 2903, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,856: INFO: model_training: Rank 0, Epoch 2903, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,858: INFO: model_training: Rank 0, Epoch 2904, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,859: INFO: model_training: Rank 0, Epoch 2904, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,860: INFO: model_training: Rank 0, Epoch 2904, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,861: INFO: model_training: Rank 0, Epoch 2904, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,863: INFO: model_training: Rank 0, Epoch 2904, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,865: INFO: model_training: Rank 0, Epoch 2905, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,867: INFO: model_training: Rank 0, Epoch 2905, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,868: INFO: model_training: Rank 0, Epoch 2905, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,869: INFO: model_training: Rank 0, Epoch 2905, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,871: INFO: model_training: Rank 0, Epoch 2905, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,873: INFO: model_training: Rank 0, Epoch 2906, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,874: INFO: model_training: Rank 0, Epoch 2906, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,875: INFO: model_training: Rank 0, Epoch 2906, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,876: INFO: model_training: Rank 0, Epoch 2906, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,877: INFO: model_training: Rank 0, Epoch 2906, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,879: INFO: model_training: Rank 0, Epoch 2907, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,880: INFO: model_training: Rank 0, Epoch 2907, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,882: INFO: model_training: Rank 0, Epoch 2907, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,884: INFO: model_training: Rank 0, Epoch 2907, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,886: INFO: model_training: Rank 0, Epoch 2907, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,887: INFO: model_training: Rank 0, Epoch 2908, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,889: INFO: model_training: Rank 0, Epoch 2908, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,891: INFO: model_training: Rank 0, Epoch 2908, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,892: INFO: model_training: Rank 0, Epoch 2908, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,893: INFO: model_training: Rank 0, Epoch 2908, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,894: INFO: model_training: Rank 0, Epoch 2909, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,895: INFO: model_training: Rank 0, Epoch 2909, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,897: INFO: model_training: Rank 0, Epoch 2909, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,899: INFO: model_training: Rank 0, Epoch 2909, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,900: INFO: model_training: Rank 0, Epoch 2909, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,902: INFO: model_training: Rank 0, Epoch 2910, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,903: INFO: model_training: Rank 0, Epoch 2910, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,905: INFO: model_training: Rank 0, Epoch 2910, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,906: INFO: model_training: Rank 0, Epoch 2910, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,907: INFO: model_training: Rank 0, Epoch 2910, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,908: INFO: model_training: Rank 0, Epoch 2911, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,910: INFO: model_training: Rank 0, Epoch 2911, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,911: INFO: model_training: Rank 0, Epoch 2911, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,912: INFO: model_training: Rank 0, Epoch 2911, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,914: INFO: model_training: Rank 0, Epoch 2911, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,915: INFO: model_training: Rank 0, Epoch 2912, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,916: INFO: model_training: Rank 0, Epoch 2912, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,917: INFO: model_training: Rank 0, Epoch 2912, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,919: INFO: model_training: Rank 0, Epoch 2912, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,921: INFO: model_training: Rank 0, Epoch 2912, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,922: INFO: model_training: Rank 0, Epoch 2913, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,923: INFO: model_training: Rank 0, Epoch 2913, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,924: INFO: model_training: Rank 0, Epoch 2913, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,926: INFO: model_training: Rank 0, Epoch 2913, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,927: INFO: model_training: Rank 0, Epoch 2913, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,928: INFO: model_training: Rank 0, Epoch 2914, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,929: INFO: model_training: Rank 0, Epoch 2914, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,930: INFO: model_training: Rank 0, Epoch 2914, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,932: INFO: model_training: Rank 0, Epoch 2914, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,933: INFO: model_training: Rank 0, Epoch 2914, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,935: INFO: model_training: Rank 0, Epoch 2915, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,936: INFO: model_training: Rank 0, Epoch 2915, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,937: INFO: model_training: Rank 0, Epoch 2915, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,939: INFO: model_training: Rank 0, Epoch 2915, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,940: INFO: model_training: Rank 0, Epoch 2915, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,941: INFO: model_training: Rank 0, Epoch 2916, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,942: INFO: model_training: Rank 0, Epoch 2916, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,944: INFO: model_training: Rank 0, Epoch 2916, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,945: INFO: model_training: Rank 0, Epoch 2916, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,946: INFO: model_training: Rank 0, Epoch 2916, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,947: INFO: model_training: Rank 0, Epoch 2917, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,949: INFO: model_training: Rank 0, Epoch 2917, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,950: INFO: model_training: Rank 0, Epoch 2917, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,951: INFO: model_training: Rank 0, Epoch 2917, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,953: INFO: model_training: Rank 0, Epoch 2917, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,954: INFO: model_training: Rank 0, Epoch 2918, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,955: INFO: model_training: Rank 0, Epoch 2918, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,956: INFO: model_training: Rank 0, Epoch 2918, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,957: INFO: model_training: Rank 0, Epoch 2918, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,958: INFO: model_training: Rank 0, Epoch 2918, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,960: INFO: model_training: Rank 0, Epoch 2919, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,961: INFO: model_training: Rank 0, Epoch 2919, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,963: INFO: model_training: Rank 0, Epoch 2919, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,964: INFO: model_training: Rank 0, Epoch 2919, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,965: INFO: model_training: Rank 0, Epoch 2919, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,966: INFO: model_training: Rank 0, Epoch 2920, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,967: INFO: model_training: Rank 0, Epoch 2920, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,968: INFO: model_training: Rank 0, Epoch 2920, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,969: INFO: model_training: Rank 0, Epoch 2920, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,970: INFO: model_training: Rank 0, Epoch 2920, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,971: INFO: model_training: Rank 0, Epoch 2921, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,972: INFO: model_training: Rank 0, Epoch 2921, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,974: INFO: model_training: Rank 0, Epoch 2921, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,975: INFO: model_training: Rank 0, Epoch 2921, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,976: INFO: model_training: Rank 0, Epoch 2921, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,977: INFO: model_training: Rank 0, Epoch 2922, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,979: INFO: model_training: Rank 0, Epoch 2922, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,980: INFO: model_training: Rank 0, Epoch 2922, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,981: INFO: model_training: Rank 0, Epoch 2922, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,982: INFO: model_training: Rank 0, Epoch 2922, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,983: INFO: model_training: Rank 0, Epoch 2923, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,985: INFO: model_training: Rank 0, Epoch 2923, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,986: INFO: model_training: Rank 0, Epoch 2923, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,987: INFO: model_training: Rank 0, Epoch 2923, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,989: INFO: model_training: Rank 0, Epoch 2923, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,990: INFO: model_training: Rank 0, Epoch 2924, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,991: INFO: model_training: Rank 0, Epoch 2924, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,992: INFO: model_training: Rank 0, Epoch 2924, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,994: INFO: model_training: Rank 0, Epoch 2924, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:44,995: INFO: model_training: Rank 0, Epoch 2924, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:44,996: INFO: model_training: Rank 0, Epoch 2925, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:44,997: INFO: model_training: Rank 0, Epoch 2925, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:44,998: INFO: model_training: Rank 0, Epoch 2925, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:44,999: INFO: model_training: Rank 0, Epoch 2925, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,001: INFO: model_training: Rank 0, Epoch 2925, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,002: INFO: model_training: Rank 0, Epoch 2926, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,003: INFO: model_training: Rank 0, Epoch 2926, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,004: INFO: model_training: Rank 0, Epoch 2926, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,005: INFO: model_training: Rank 0, Epoch 2926, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,006: INFO: model_training: Rank 0, Epoch 2926, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,007: INFO: model_training: Rank 0, Epoch 2927, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,009: INFO: model_training: Rank 0, Epoch 2927, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,010: INFO: model_training: Rank 0, Epoch 2927, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,011: INFO: model_training: Rank 0, Epoch 2927, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,012: INFO: model_training: Rank 0, Epoch 2927, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,013: INFO: model_training: Rank 0, Epoch 2928, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,015: INFO: model_training: Rank 0, Epoch 2928, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,017: INFO: model_training: Rank 0, Epoch 2928, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,018: INFO: model_training: Rank 0, Epoch 2928, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,019: INFO: model_training: Rank 0, Epoch 2928, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,020: INFO: model_training: Rank 0, Epoch 2929, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,021: INFO: model_training: Rank 0, Epoch 2929, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,022: INFO: model_training: Rank 0, Epoch 2929, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,024: INFO: model_training: Rank 0, Epoch 2929, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,025: INFO: model_training: Rank 0, Epoch 2929, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,026: INFO: model_training: Rank 0, Epoch 2930, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,027: INFO: model_training: Rank 0, Epoch 2930, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,028: INFO: model_training: Rank 0, Epoch 2930, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,029: INFO: model_training: Rank 0, Epoch 2930, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,031: INFO: model_training: Rank 0, Epoch 2930, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,032: INFO: model_training: Rank 0, Epoch 2931, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,033: INFO: model_training: Rank 0, Epoch 2931, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,034: INFO: model_training: Rank 0, Epoch 2931, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,036: INFO: model_training: Rank 0, Epoch 2931, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,037: INFO: model_training: Rank 0, Epoch 2931, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,038: INFO: model_training: Rank 0, Epoch 2932, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,039: INFO: model_training: Rank 0, Epoch 2932, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,040: INFO: model_training: Rank 0, Epoch 2932, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,042: INFO: model_training: Rank 0, Epoch 2932, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,043: INFO: model_training: Rank 0, Epoch 2932, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,044: INFO: model_training: Rank 0, Epoch 2933, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,045: INFO: model_training: Rank 0, Epoch 2933, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,047: INFO: model_training: Rank 0, Epoch 2933, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,048: INFO: model_training: Rank 0, Epoch 2933, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,049: INFO: model_training: Rank 0, Epoch 2933, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,050: INFO: model_training: Rank 0, Epoch 2934, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,051: INFO: model_training: Rank 0, Epoch 2934, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,052: INFO: model_training: Rank 0, Epoch 2934, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,054: INFO: model_training: Rank 0, Epoch 2934, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,054: INFO: model_training: Rank 0, Epoch 2934, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,056: INFO: model_training: Rank 0, Epoch 2935, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,057: INFO: model_training: Rank 0, Epoch 2935, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,059: INFO: model_training: Rank 0, Epoch 2935, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,060: INFO: model_training: Rank 0, Epoch 2935, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,061: INFO: model_training: Rank 0, Epoch 2935, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,062: INFO: model_training: Rank 0, Epoch 2936, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,063: INFO: model_training: Rank 0, Epoch 2936, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,064: INFO: model_training: Rank 0, Epoch 2936, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,065: INFO: model_training: Rank 0, Epoch 2936, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,067: INFO: model_training: Rank 0, Epoch 2936, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,068: INFO: model_training: Rank 0, Epoch 2937, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,070: INFO: model_training: Rank 0, Epoch 2937, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,071: INFO: model_training: Rank 0, Epoch 2937, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,072: INFO: model_training: Rank 0, Epoch 2937, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,073: INFO: model_training: Rank 0, Epoch 2937, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,074: INFO: model_training: Rank 0, Epoch 2938, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,075: INFO: model_training: Rank 0, Epoch 2938, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,076: INFO: model_training: Rank 0, Epoch 2938, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,077: INFO: model_training: Rank 0, Epoch 2938, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,079: INFO: model_training: Rank 0, Epoch 2938, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,080: INFO: model_training: Rank 0, Epoch 2939, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,081: INFO: model_training: Rank 0, Epoch 2939, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,082: INFO: model_training: Rank 0, Epoch 2939, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,084: INFO: model_training: Rank 0, Epoch 2939, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,085: INFO: model_training: Rank 0, Epoch 2939, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,086: INFO: model_training: Rank 0, Epoch 2940, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,087: INFO: model_training: Rank 0, Epoch 2940, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,089: INFO: model_training: Rank 0, Epoch 2940, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,090: INFO: model_training: Rank 0, Epoch 2940, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,091: INFO: model_training: Rank 0, Epoch 2940, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,092: INFO: model_training: Rank 0, Epoch 2941, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,094: INFO: model_training: Rank 0, Epoch 2941, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,095: INFO: model_training: Rank 0, Epoch 2941, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,096: INFO: model_training: Rank 0, Epoch 2941, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,097: INFO: model_training: Rank 0, Epoch 2941, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,099: INFO: model_training: Rank 0, Epoch 2942, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,100: INFO: model_training: Rank 0, Epoch 2942, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,101: INFO: model_training: Rank 0, Epoch 2942, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,102: INFO: model_training: Rank 0, Epoch 2942, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,103: INFO: model_training: Rank 0, Epoch 2942, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,104: INFO: model_training: Rank 0, Epoch 2943, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,105: INFO: model_training: Rank 0, Epoch 2943, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,107: INFO: model_training: Rank 0, Epoch 2943, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,108: INFO: model_training: Rank 0, Epoch 2943, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,109: INFO: model_training: Rank 0, Epoch 2943, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,110: INFO: model_training: Rank 0, Epoch 2944, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,111: INFO: model_training: Rank 0, Epoch 2944, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,113: INFO: model_training: Rank 0, Epoch 2944, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,114: INFO: model_training: Rank 0, Epoch 2944, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,115: INFO: model_training: Rank 0, Epoch 2944, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,116: INFO: model_training: Rank 0, Epoch 2945, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,117: INFO: model_training: Rank 0, Epoch 2945, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,119: INFO: model_training: Rank 0, Epoch 2945, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,120: INFO: model_training: Rank 0, Epoch 2945, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,121: INFO: model_training: Rank 0, Epoch 2945, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,122: INFO: model_training: Rank 0, Epoch 2946, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,123: INFO: model_training: Rank 0, Epoch 2946, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,125: INFO: model_training: Rank 0, Epoch 2946, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,126: INFO: model_training: Rank 0, Epoch 2946, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,128: INFO: model_training: Rank 0, Epoch 2946, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,129: INFO: model_training: Rank 0, Epoch 2947, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,131: INFO: model_training: Rank 0, Epoch 2947, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,132: INFO: model_training: Rank 0, Epoch 2947, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,133: INFO: model_training: Rank 0, Epoch 2947, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,134: INFO: model_training: Rank 0, Epoch 2947, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,135: INFO: model_training: Rank 0, Epoch 2948, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,136: INFO: model_training: Rank 0, Epoch 2948, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,138: INFO: model_training: Rank 0, Epoch 2948, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,139: INFO: model_training: Rank 0, Epoch 2948, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,141: INFO: model_training: Rank 0, Epoch 2948, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,142: INFO: model_training: Rank 0, Epoch 2949, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,143: INFO: model_training: Rank 0, Epoch 2949, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,144: INFO: model_training: Rank 0, Epoch 2949, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,146: INFO: model_training: Rank 0, Epoch 2949, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,147: INFO: model_training: Rank 0, Epoch 2949, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,148: INFO: model_training: Rank 0, Epoch 2950, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,149: INFO: model_training: Rank 0, Epoch 2950, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,150: INFO: model_training: Rank 0, Epoch 2950, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,152: INFO: model_training: Rank 0, Epoch 2950, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,153: INFO: model_training: Rank 0, Epoch 2950, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,154: INFO: model_training: Rank 0, Epoch 2951, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,155: INFO: model_training: Rank 0, Epoch 2951, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,156: INFO: model_training: Rank 0, Epoch 2951, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,158: INFO: model_training: Rank 0, Epoch 2951, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,160: INFO: model_training: Rank 0, Epoch 2951, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,161: INFO: model_training: Rank 0, Epoch 2952, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,162: INFO: model_training: Rank 0, Epoch 2952, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,163: INFO: model_training: Rank 0, Epoch 2952, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,164: INFO: model_training: Rank 0, Epoch 2952, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,165: INFO: model_training: Rank 0, Epoch 2952, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,167: INFO: model_training: Rank 0, Epoch 2953, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,168: INFO: model_training: Rank 0, Epoch 2953, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,169: INFO: model_training: Rank 0, Epoch 2953, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,170: INFO: model_training: Rank 0, Epoch 2953, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,172: INFO: model_training: Rank 0, Epoch 2953, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,173: INFO: model_training: Rank 0, Epoch 2954, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,175: INFO: model_training: Rank 0, Epoch 2954, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,176: INFO: model_training: Rank 0, Epoch 2954, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,177: INFO: model_training: Rank 0, Epoch 2954, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,178: INFO: model_training: Rank 0, Epoch 2954, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,180: INFO: model_training: Rank 0, Epoch 2955, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,181: INFO: model_training: Rank 0, Epoch 2955, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,182: INFO: model_training: Rank 0, Epoch 2955, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,183: INFO: model_training: Rank 0, Epoch 2955, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,184: INFO: model_training: Rank 0, Epoch 2955, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,185: INFO: model_training: Rank 0, Epoch 2956, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,187: INFO: model_training: Rank 0, Epoch 2956, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,188: INFO: model_training: Rank 0, Epoch 2956, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,189: INFO: model_training: Rank 0, Epoch 2956, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,190: INFO: model_training: Rank 0, Epoch 2956, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,192: INFO: model_training: Rank 0, Epoch 2957, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,193: INFO: model_training: Rank 0, Epoch 2957, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,194: INFO: model_training: Rank 0, Epoch 2957, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,195: INFO: model_training: Rank 0, Epoch 2957, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,196: INFO: model_training: Rank 0, Epoch 2957, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,197: INFO: model_training: Rank 0, Epoch 2958, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,199: INFO: model_training: Rank 0, Epoch 2958, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,200: INFO: model_training: Rank 0, Epoch 2958, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,201: INFO: model_training: Rank 0, Epoch 2958, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,202: INFO: model_training: Rank 0, Epoch 2958, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,203: INFO: model_training: Rank 0, Epoch 2959, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,204: INFO: model_training: Rank 0, Epoch 2959, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,206: INFO: model_training: Rank 0, Epoch 2959, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,207: INFO: model_training: Rank 0, Epoch 2959, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,208: INFO: model_training: Rank 0, Epoch 2959, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,210: INFO: model_training: Rank 0, Epoch 2960, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,211: INFO: model_training: Rank 0, Epoch 2960, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,213: INFO: model_training: Rank 0, Epoch 2960, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,214: INFO: model_training: Rank 0, Epoch 2960, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,215: INFO: model_training: Rank 0, Epoch 2960, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,216: INFO: model_training: Rank 0, Epoch 2961, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,217: INFO: model_training: Rank 0, Epoch 2961, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,219: INFO: model_training: Rank 0, Epoch 2961, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,220: INFO: model_training: Rank 0, Epoch 2961, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,221: INFO: model_training: Rank 0, Epoch 2961, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,222: INFO: model_training: Rank 0, Epoch 2962, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,223: INFO: model_training: Rank 0, Epoch 2962, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,224: INFO: model_training: Rank 0, Epoch 2962, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,226: INFO: model_training: Rank 0, Epoch 2962, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,228: INFO: model_training: Rank 0, Epoch 2962, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,229: INFO: model_training: Rank 0, Epoch 2963, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,230: INFO: model_training: Rank 0, Epoch 2963, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,232: INFO: model_training: Rank 0, Epoch 2963, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,233: INFO: model_training: Rank 0, Epoch 2963, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,234: INFO: model_training: Rank 0, Epoch 2963, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,236: INFO: model_training: Rank 0, Epoch 2964, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,236: INFO: model_training: Rank 0, Epoch 2964, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,238: INFO: model_training: Rank 0, Epoch 2964, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,240: INFO: model_training: Rank 0, Epoch 2964, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,242: INFO: model_training: Rank 0, Epoch 2964, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,243: INFO: model_training: Rank 0, Epoch 2965, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,245: INFO: model_training: Rank 0, Epoch 2965, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,246: INFO: model_training: Rank 0, Epoch 2965, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,247: INFO: model_training: Rank 0, Epoch 2965, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,249: INFO: model_training: Rank 0, Epoch 2965, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,250: INFO: model_training: Rank 0, Epoch 2966, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,252: INFO: model_training: Rank 0, Epoch 2966, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,253: INFO: model_training: Rank 0, Epoch 2966, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,254: INFO: model_training: Rank 0, Epoch 2966, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,256: INFO: model_training: Rank 0, Epoch 2966, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,258: INFO: model_training: Rank 0, Epoch 2967, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,260: INFO: model_training: Rank 0, Epoch 2967, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,261: INFO: model_training: Rank 0, Epoch 2967, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,263: INFO: model_training: Rank 0, Epoch 2967, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,264: INFO: model_training: Rank 0, Epoch 2967, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,266: INFO: model_training: Rank 0, Epoch 2968, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,267: INFO: model_training: Rank 0, Epoch 2968, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,269: INFO: model_training: Rank 0, Epoch 2968, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,270: INFO: model_training: Rank 0, Epoch 2968, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,272: INFO: model_training: Rank 0, Epoch 2968, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,274: INFO: model_training: Rank 0, Epoch 2969, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,276: INFO: model_training: Rank 0, Epoch 2969, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,278: INFO: model_training: Rank 0, Epoch 2969, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,279: INFO: model_training: Rank 0, Epoch 2969, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,281: INFO: model_training: Rank 0, Epoch 2969, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,282: INFO: model_training: Rank 0, Epoch 2970, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,284: INFO: model_training: Rank 0, Epoch 2970, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,285: INFO: model_training: Rank 0, Epoch 2970, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,287: INFO: model_training: Rank 0, Epoch 2970, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,288: INFO: model_training: Rank 0, Epoch 2970, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,289: INFO: model_training: Rank 0, Epoch 2971, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,292: INFO: model_training: Rank 0, Epoch 2971, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,293: INFO: model_training: Rank 0, Epoch 2971, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,295: INFO: model_training: Rank 0, Epoch 2971, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,296: INFO: model_training: Rank 0, Epoch 2971, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,297: INFO: model_training: Rank 0, Epoch 2972, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,299: INFO: model_training: Rank 0, Epoch 2972, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,301: INFO: model_training: Rank 0, Epoch 2972, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,303: INFO: model_training: Rank 0, Epoch 2972, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,304: INFO: model_training: Rank 0, Epoch 2972, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,305: INFO: model_training: Rank 0, Epoch 2973, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,307: INFO: model_training: Rank 0, Epoch 2973, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,310: INFO: model_training: Rank 0, Epoch 2973, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,311: INFO: model_training: Rank 0, Epoch 2973, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,312: INFO: model_training: Rank 0, Epoch 2973, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,314: INFO: model_training: Rank 0, Epoch 2974, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,316: INFO: model_training: Rank 0, Epoch 2974, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,317: INFO: model_training: Rank 0, Epoch 2974, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,319: INFO: model_training: Rank 0, Epoch 2974, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,320: INFO: model_training: Rank 0, Epoch 2974, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,321: INFO: model_training: Rank 0, Epoch 2975, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,323: INFO: model_training: Rank 0, Epoch 2975, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,325: INFO: model_training: Rank 0, Epoch 2975, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,327: INFO: model_training: Rank 0, Epoch 2975, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,328: INFO: model_training: Rank 0, Epoch 2975, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,330: INFO: model_training: Rank 0, Epoch 2976, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,332: INFO: model_training: Rank 0, Epoch 2976, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,333: INFO: model_training: Rank 0, Epoch 2976, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,335: INFO: model_training: Rank 0, Epoch 2976, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,336: INFO: model_training: Rank 0, Epoch 2976, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,337: INFO: model_training: Rank 0, Epoch 2977, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,339: INFO: model_training: Rank 0, Epoch 2977, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,342: INFO: model_training: Rank 0, Epoch 2977, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,343: INFO: model_training: Rank 0, Epoch 2977, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,344: INFO: model_training: Rank 0, Epoch 2977, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,345: INFO: model_training: Rank 0, Epoch 2978, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,347: INFO: model_training: Rank 0, Epoch 2978, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,349: INFO: model_training: Rank 0, Epoch 2978, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,351: INFO: model_training: Rank 0, Epoch 2978, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,352: INFO: model_training: Rank 0, Epoch 2978, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,353: INFO: model_training: Rank 0, Epoch 2979, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,354: INFO: model_training: Rank 0, Epoch 2979, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,356: INFO: model_training: Rank 0, Epoch 2979, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,358: INFO: model_training: Rank 0, Epoch 2979, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,360: INFO: model_training: Rank 0, Epoch 2979, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,361: INFO: model_training: Rank 0, Epoch 2980, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,363: INFO: model_training: Rank 0, Epoch 2980, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,364: INFO: model_training: Rank 0, Epoch 2980, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,366: INFO: model_training: Rank 0, Epoch 2980, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,368: INFO: model_training: Rank 0, Epoch 2980, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,370: INFO: model_training: Rank 0, Epoch 2981, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,371: INFO: model_training: Rank 0, Epoch 2981, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,372: INFO: model_training: Rank 0, Epoch 2981, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,375: INFO: model_training: Rank 0, Epoch 2981, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,377: INFO: model_training: Rank 0, Epoch 2981, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,378: INFO: model_training: Rank 0, Epoch 2982, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,379: INFO: model_training: Rank 0, Epoch 2982, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,380: INFO: model_training: Rank 0, Epoch 2982, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,382: INFO: model_training: Rank 0, Epoch 2982, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,384: INFO: model_training: Rank 0, Epoch 2982, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,385: INFO: model_training: Rank 0, Epoch 2983, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,387: INFO: model_training: Rank 0, Epoch 2983, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,388: INFO: model_training: Rank 0, Epoch 2983, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,390: INFO: model_training: Rank 0, Epoch 2983, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,392: INFO: model_training: Rank 0, Epoch 2983, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,394: INFO: model_training: Rank 0, Epoch 2984, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,395: INFO: model_training: Rank 0, Epoch 2984, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,396: INFO: model_training: Rank 0, Epoch 2984, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,398: INFO: model_training: Rank 0, Epoch 2984, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,400: INFO: model_training: Rank 0, Epoch 2984, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,402: INFO: model_training: Rank 0, Epoch 2985, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,403: INFO: model_training: Rank 0, Epoch 2985, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,405: INFO: model_training: Rank 0, Epoch 2985, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,407: INFO: model_training: Rank 0, Epoch 2985, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,409: INFO: model_training: Rank 0, Epoch 2985, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,411: INFO: model_training: Rank 0, Epoch 2986, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,412: INFO: model_training: Rank 0, Epoch 2986, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,414: INFO: model_training: Rank 0, Epoch 2986, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,416: INFO: model_training: Rank 0, Epoch 2986, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,417: INFO: model_training: Rank 0, Epoch 2986, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,419: INFO: model_training: Rank 0, Epoch 2987, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,420: INFO: model_training: Rank 0, Epoch 2987, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,422: INFO: model_training: Rank 0, Epoch 2987, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,425: INFO: model_training: Rank 0, Epoch 2987, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,426: INFO: model_training: Rank 0, Epoch 2987, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,428: INFO: model_training: Rank 0, Epoch 2988, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,429: INFO: model_training: Rank 0, Epoch 2988, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,430: INFO: model_training: Rank 0, Epoch 2988, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,432: INFO: model_training: Rank 0, Epoch 2988, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,434: INFO: model_training: Rank 0, Epoch 2988, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,435: INFO: model_training: Rank 0, Epoch 2989, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,436: INFO: model_training: Rank 0, Epoch 2989, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,438: INFO: model_training: Rank 0, Epoch 2989, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,440: INFO: model_training: Rank 0, Epoch 2989, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,442: INFO: model_training: Rank 0, Epoch 2989, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,444: INFO: model_training: Rank 0, Epoch 2990, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,445: INFO: model_training: Rank 0, Epoch 2990, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,446: INFO: model_training: Rank 0, Epoch 2990, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,448: INFO: model_training: Rank 0, Epoch 2990, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,450: INFO: model_training: Rank 0, Epoch 2990, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,452: INFO: model_training: Rank 0, Epoch 2991, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,453: INFO: model_training: Rank 0, Epoch 2991, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,454: INFO: model_training: Rank 0, Epoch 2991, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,455: INFO: model_training: Rank 0, Epoch 2991, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,457: INFO: model_training: Rank 0, Epoch 2991, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,460: INFO: model_training: Rank 0, Epoch 2992, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,462: INFO: model_training: Rank 0, Epoch 2992, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,463: INFO: model_training: Rank 0, Epoch 2992, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,464: INFO: model_training: Rank 0, Epoch 2992, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,466: INFO: model_training: Rank 0, Epoch 2992, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,467: INFO: model_training: Rank 0, Epoch 2993, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,469: INFO: model_training: Rank 0, Epoch 2993, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,470: INFO: model_training: Rank 0, Epoch 2993, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,471: INFO: model_training: Rank 0, Epoch 2993, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,473: INFO: model_training: Rank 0, Epoch 2993, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,475: INFO: model_training: Rank 0, Epoch 2994, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,477: INFO: model_training: Rank 0, Epoch 2994, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,479: INFO: model_training: Rank 0, Epoch 2994, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,480: INFO: model_training: Rank 0, Epoch 2994, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,481: INFO: model_training: Rank 0, Epoch 2994, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,483: INFO: model_training: Rank 0, Epoch 2995, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,484: INFO: model_training: Rank 0, Epoch 2995, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,486: INFO: model_training: Rank 0, Epoch 2995, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,487: INFO: model_training: Rank 0, Epoch 2995, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,489: INFO: model_training: Rank 0, Epoch 2995, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,491: INFO: model_training: Rank 0, Epoch 2996, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,493: INFO: model_training: Rank 0, Epoch 2996, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,495: INFO: model_training: Rank 0, Epoch 2996, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,497: INFO: model_training: Rank 0, Epoch 2996, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,500: INFO: model_training: Rank 0, Epoch 2996, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,501: INFO: model_training: Rank 0, Epoch 2997, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,503: INFO: model_training: Rank 0, Epoch 2997, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,504: INFO: model_training: Rank 0, Epoch 2997, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,505: INFO: model_training: Rank 0, Epoch 2997, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,507: INFO: model_training: Rank 0, Epoch 2997, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,509: INFO: model_training: Rank 0, Epoch 2998, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,511: INFO: model_training: Rank 0, Epoch 2998, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,512: INFO: model_training: Rank 0, Epoch 2998, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,514: INFO: model_training: Rank 0, Epoch 2998, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,516: INFO: model_training: Rank 0, Epoch 2998, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,518: INFO: model_training: Rank 0, Epoch 2999, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,520: INFO: model_training: Rank 0, Epoch 2999, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,521: INFO: model_training: Rank 0, Epoch 2999, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,522: INFO: model_training: Rank 0, Epoch 2999, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,525: INFO: model_training: Rank 0, Epoch 2999, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,527: INFO: model_training: Rank 0, Epoch 3000, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,528: INFO: model_training: Rank 0, Epoch 3000, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,530: INFO: model_training: Rank 0, Epoch 3000, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,531: INFO: model_training: Rank 0, Epoch 3000, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,533: INFO: model_training: Rank 0, Epoch 3000, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,535: INFO: model_training: Rank 0, Epoch 3001, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,537: INFO: model_training: Rank 0, Epoch 3001, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,538: INFO: model_training: Rank 0, Epoch 3001, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,539: INFO: model_training: Rank 0, Epoch 3001, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,542: INFO: model_training: Rank 0, Epoch 3001, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,543: INFO: model_training: Rank 0, Epoch 3002, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,545: INFO: model_training: Rank 0, Epoch 3002, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,546: INFO: model_training: Rank 0, Epoch 3002, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,548: INFO: model_training: Rank 0, Epoch 3002, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,550: INFO: model_training: Rank 0, Epoch 3002, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,551: INFO: model_training: Rank 0, Epoch 3003, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,552: INFO: model_training: Rank 0, Epoch 3003, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,554: INFO: model_training: Rank 0, Epoch 3003, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,556: INFO: model_training: Rank 0, Epoch 3003, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,558: INFO: model_training: Rank 0, Epoch 3003, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,560: INFO: model_training: Rank 0, Epoch 3004, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,561: INFO: model_training: Rank 0, Epoch 3004, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,563: INFO: model_training: Rank 0, Epoch 3004, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,564: INFO: model_training: Rank 0, Epoch 3004, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,567: INFO: model_training: Rank 0, Epoch 3004, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,569: INFO: model_training: Rank 0, Epoch 3005, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,570: INFO: model_training: Rank 0, Epoch 3005, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,571: INFO: model_training: Rank 0, Epoch 3005, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,573: INFO: model_training: Rank 0, Epoch 3005, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,575: INFO: model_training: Rank 0, Epoch 3005, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,577: INFO: model_training: Rank 0, Epoch 3006, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,579: INFO: model_training: Rank 0, Epoch 3006, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,580: INFO: model_training: Rank 0, Epoch 3006, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,582: INFO: model_training: Rank 0, Epoch 3006, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,584: INFO: model_training: Rank 0, Epoch 3006, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,585: INFO: model_training: Rank 0, Epoch 3007, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,586: INFO: model_training: Rank 0, Epoch 3007, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,587: INFO: model_training: Rank 0, Epoch 3007, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,589: INFO: model_training: Rank 0, Epoch 3007, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,592: INFO: model_training: Rank 0, Epoch 3007, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,593: INFO: model_training: Rank 0, Epoch 3008, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,595: INFO: model_training: Rank 0, Epoch 3008, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,596: INFO: model_training: Rank 0, Epoch 3008, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,597: INFO: model_training: Rank 0, Epoch 3008, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,600: INFO: model_training: Rank 0, Epoch 3008, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,601: INFO: model_training: Rank 0, Epoch 3009, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,603: INFO: model_training: Rank 0, Epoch 3009, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,604: INFO: model_training: Rank 0, Epoch 3009, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,605: INFO: model_training: Rank 0, Epoch 3009, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,607: INFO: model_training: Rank 0, Epoch 3009, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,609: INFO: model_training: Rank 0, Epoch 3010, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,611: INFO: model_training: Rank 0, Epoch 3010, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,612: INFO: model_training: Rank 0, Epoch 3010, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,614: INFO: model_training: Rank 0, Epoch 3010, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,615: INFO: model_training: Rank 0, Epoch 3010, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,617: INFO: model_training: Rank 0, Epoch 3011, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,619: INFO: model_training: Rank 0, Epoch 3011, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,620: INFO: model_training: Rank 0, Epoch 3011, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,621: INFO: model_training: Rank 0, Epoch 3011, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,623: INFO: model_training: Rank 0, Epoch 3011, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,625: INFO: model_training: Rank 0, Epoch 3012, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,627: INFO: model_training: Rank 0, Epoch 3012, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,629: INFO: model_training: Rank 0, Epoch 3012, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,631: INFO: model_training: Rank 0, Epoch 3012, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,632: INFO: model_training: Rank 0, Epoch 3012, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,634: INFO: model_training: Rank 0, Epoch 3013, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,635: INFO: model_training: Rank 0, Epoch 3013, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,637: INFO: model_training: Rank 0, Epoch 3013, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,638: INFO: model_training: Rank 0, Epoch 3013, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,640: INFO: model_training: Rank 0, Epoch 3013, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,642: INFO: model_training: Rank 0, Epoch 3014, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,644: INFO: model_training: Rank 0, Epoch 3014, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,645: INFO: model_training: Rank 0, Epoch 3014, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,646: INFO: model_training: Rank 0, Epoch 3014, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,649: INFO: model_training: Rank 0, Epoch 3014, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,650: INFO: model_training: Rank 0, Epoch 3015, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,652: INFO: model_training: Rank 0, Epoch 3015, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,653: INFO: model_training: Rank 0, Epoch 3015, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,655: INFO: model_training: Rank 0, Epoch 3015, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,656: INFO: model_training: Rank 0, Epoch 3015, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,658: INFO: model_training: Rank 0, Epoch 3016, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,660: INFO: model_training: Rank 0, Epoch 3016, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,661: INFO: model_training: Rank 0, Epoch 3016, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,663: INFO: model_training: Rank 0, Epoch 3016, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,665: INFO: model_training: Rank 0, Epoch 3016, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,667: INFO: model_training: Rank 0, Epoch 3017, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,669: INFO: model_training: Rank 0, Epoch 3017, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,670: INFO: model_training: Rank 0, Epoch 3017, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,672: INFO: model_training: Rank 0, Epoch 3017, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,674: INFO: model_training: Rank 0, Epoch 3017, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,676: INFO: model_training: Rank 0, Epoch 3018, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,677: INFO: model_training: Rank 0, Epoch 3018, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,678: INFO: model_training: Rank 0, Epoch 3018, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,679: INFO: model_training: Rank 0, Epoch 3018, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,681: INFO: model_training: Rank 0, Epoch 3018, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,683: INFO: model_training: Rank 0, Epoch 3019, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,684: INFO: model_training: Rank 0, Epoch 3019, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,685: INFO: model_training: Rank 0, Epoch 3019, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,687: INFO: model_training: Rank 0, Epoch 3019, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,689: INFO: model_training: Rank 0, Epoch 3019, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,690: INFO: model_training: Rank 0, Epoch 3020, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,692: INFO: model_training: Rank 0, Epoch 3020, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,694: INFO: model_training: Rank 0, Epoch 3020, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,695: INFO: model_training: Rank 0, Epoch 3020, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,696: INFO: model_training: Rank 0, Epoch 3020, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,698: INFO: model_training: Rank 0, Epoch 3021, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,700: INFO: model_training: Rank 0, Epoch 3021, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,701: INFO: model_training: Rank 0, Epoch 3021, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,702: INFO: model_training: Rank 0, Epoch 3021, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,704: INFO: model_training: Rank 0, Epoch 3021, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,706: INFO: model_training: Rank 0, Epoch 3022, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,707: INFO: model_training: Rank 0, Epoch 3022, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,709: INFO: model_training: Rank 0, Epoch 3022, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,711: INFO: model_training: Rank 0, Epoch 3022, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,712: INFO: model_training: Rank 0, Epoch 3022, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,714: INFO: model_training: Rank 0, Epoch 3023, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,716: INFO: model_training: Rank 0, Epoch 3023, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,717: INFO: model_training: Rank 0, Epoch 3023, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,718: INFO: model_training: Rank 0, Epoch 3023, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,720: INFO: model_training: Rank 0, Epoch 3023, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,721: INFO: model_training: Rank 0, Epoch 3024, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,723: INFO: model_training: Rank 0, Epoch 3024, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,725: INFO: model_training: Rank 0, Epoch 3024, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,727: INFO: model_training: Rank 0, Epoch 3024, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,728: INFO: model_training: Rank 0, Epoch 3024, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,729: INFO: model_training: Rank 0, Epoch 3025, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,731: INFO: model_training: Rank 0, Epoch 3025, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,733: INFO: model_training: Rank 0, Epoch 3025, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,734: INFO: model_training: Rank 0, Epoch 3025, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,735: INFO: model_training: Rank 0, Epoch 3025, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,737: INFO: model_training: Rank 0, Epoch 3026, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,738: INFO: model_training: Rank 0, Epoch 3026, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,740: INFO: model_training: Rank 0, Epoch 3026, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,742: INFO: model_training: Rank 0, Epoch 3026, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,744: INFO: model_training: Rank 0, Epoch 3026, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,745: INFO: model_training: Rank 0, Epoch 3027, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,747: INFO: model_training: Rank 0, Epoch 3027, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,749: INFO: model_training: Rank 0, Epoch 3027, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,750: INFO: model_training: Rank 0, Epoch 3027, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,752: INFO: model_training: Rank 0, Epoch 3027, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,753: INFO: model_training: Rank 0, Epoch 3028, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,755: INFO: model_training: Rank 0, Epoch 3028, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,756: INFO: model_training: Rank 0, Epoch 3028, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,759: INFO: model_training: Rank 0, Epoch 3028, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,760: INFO: model_training: Rank 0, Epoch 3028, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,761: INFO: model_training: Rank 0, Epoch 3029, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,764: INFO: model_training: Rank 0, Epoch 3029, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,765: INFO: model_training: Rank 0, Epoch 3029, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,767: INFO: model_training: Rank 0, Epoch 3029, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,768: INFO: model_training: Rank 0, Epoch 3029, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,769: INFO: model_training: Rank 0, Epoch 3030, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,771: INFO: model_training: Rank 0, Epoch 3030, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,772: INFO: model_training: Rank 0, Epoch 3030, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,774: INFO: model_training: Rank 0, Epoch 3030, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,776: INFO: model_training: Rank 0, Epoch 3030, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,778: INFO: model_training: Rank 0, Epoch 3031, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,779: INFO: model_training: Rank 0, Epoch 3031, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,781: INFO: model_training: Rank 0, Epoch 3031, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,783: INFO: model_training: Rank 0, Epoch 3031, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,784: INFO: model_training: Rank 0, Epoch 3031, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,786: INFO: model_training: Rank 0, Epoch 3032, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,787: INFO: model_training: Rank 0, Epoch 3032, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,789: INFO: model_training: Rank 0, Epoch 3032, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,791: INFO: model_training: Rank 0, Epoch 3032, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,792: INFO: model_training: Rank 0, Epoch 3032, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,793: INFO: model_training: Rank 0, Epoch 3033, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,795: INFO: model_training: Rank 0, Epoch 3033, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,796: INFO: model_training: Rank 0, Epoch 3033, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,798: INFO: model_training: Rank 0, Epoch 3033, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,800: INFO: model_training: Rank 0, Epoch 3033, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,802: INFO: model_training: Rank 0, Epoch 3034, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,803: INFO: model_training: Rank 0, Epoch 3034, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,805: INFO: model_training: Rank 0, Epoch 3034, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,806: INFO: model_training: Rank 0, Epoch 3034, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,808: INFO: model_training: Rank 0, Epoch 3034, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,810: INFO: model_training: Rank 0, Epoch 3035, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,811: INFO: model_training: Rank 0, Epoch 3035, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,813: INFO: model_training: Rank 0, Epoch 3035, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,814: INFO: model_training: Rank 0, Epoch 3035, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,816: INFO: model_training: Rank 0, Epoch 3035, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,817: INFO: model_training: Rank 0, Epoch 3036, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,818: INFO: model_training: Rank 0, Epoch 3036, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,820: INFO: model_training: Rank 0, Epoch 3036, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,822: INFO: model_training: Rank 0, Epoch 3036, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,824: INFO: model_training: Rank 0, Epoch 3036, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,826: INFO: model_training: Rank 0, Epoch 3037, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,827: INFO: model_training: Rank 0, Epoch 3037, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,829: INFO: model_training: Rank 0, Epoch 3037, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,830: INFO: model_training: Rank 0, Epoch 3037, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,831: INFO: model_training: Rank 0, Epoch 3037, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,833: INFO: model_training: Rank 0, Epoch 3038, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,835: INFO: model_training: Rank 0, Epoch 3038, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,836: INFO: model_training: Rank 0, Epoch 3038, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,838: INFO: model_training: Rank 0, Epoch 3038, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,840: INFO: model_training: Rank 0, Epoch 3038, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,841: INFO: model_training: Rank 0, Epoch 3039, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,843: INFO: model_training: Rank 0, Epoch 3039, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,845: INFO: model_training: Rank 0, Epoch 3039, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,847: INFO: model_training: Rank 0, Epoch 3039, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,848: INFO: model_training: Rank 0, Epoch 3039, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,850: INFO: model_training: Rank 0, Epoch 3040, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,852: INFO: model_training: Rank 0, Epoch 3040, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,853: INFO: model_training: Rank 0, Epoch 3040, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,854: INFO: model_training: Rank 0, Epoch 3040, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,856: INFO: model_training: Rank 0, Epoch 3040, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,859: INFO: model_training: Rank 0, Epoch 3041, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,860: INFO: model_training: Rank 0, Epoch 3041, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,862: INFO: model_training: Rank 0, Epoch 3041, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,863: INFO: model_training: Rank 0, Epoch 3041, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,865: INFO: model_training: Rank 0, Epoch 3041, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,867: INFO: model_training: Rank 0, Epoch 3042, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,868: INFO: model_training: Rank 0, Epoch 3042, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,869: INFO: model_training: Rank 0, Epoch 3042, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,870: INFO: model_training: Rank 0, Epoch 3042, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,872: INFO: model_training: Rank 0, Epoch 3042, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,874: INFO: model_training: Rank 0, Epoch 3043, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,875: INFO: model_training: Rank 0, Epoch 3043, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,876: INFO: model_training: Rank 0, Epoch 3043, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,877: INFO: model_training: Rank 0, Epoch 3043, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,879: INFO: model_training: Rank 0, Epoch 3043, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,880: INFO: model_training: Rank 0, Epoch 3044, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,881: INFO: model_training: Rank 0, Epoch 3044, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,883: INFO: model_training: Rank 0, Epoch 3044, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,884: INFO: model_training: Rank 0, Epoch 3044, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,885: INFO: model_training: Rank 0, Epoch 3044, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,887: INFO: model_training: Rank 0, Epoch 3045, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,888: INFO: model_training: Rank 0, Epoch 3045, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,890: INFO: model_training: Rank 0, Epoch 3045, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,891: INFO: model_training: Rank 0, Epoch 3045, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,893: INFO: model_training: Rank 0, Epoch 3045, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,894: INFO: model_training: Rank 0, Epoch 3046, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,895: INFO: model_training: Rank 0, Epoch 3046, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,896: INFO: model_training: Rank 0, Epoch 3046, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,898: INFO: model_training: Rank 0, Epoch 3046, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,899: INFO: model_training: Rank 0, Epoch 3046, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,900: INFO: model_training: Rank 0, Epoch 3047, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,902: INFO: model_training: Rank 0, Epoch 3047, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,903: INFO: model_training: Rank 0, Epoch 3047, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,904: INFO: model_training: Rank 0, Epoch 3047, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,906: INFO: model_training: Rank 0, Epoch 3047, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,907: INFO: model_training: Rank 0, Epoch 3048, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,908: INFO: model_training: Rank 0, Epoch 3048, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,910: INFO: model_training: Rank 0, Epoch 3048, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,911: INFO: model_training: Rank 0, Epoch 3048, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,912: INFO: model_training: Rank 0, Epoch 3048, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,914: INFO: model_training: Rank 0, Epoch 3049, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,915: INFO: model_training: Rank 0, Epoch 3049, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,916: INFO: model_training: Rank 0, Epoch 3049, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,917: INFO: model_training: Rank 0, Epoch 3049, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,918: INFO: model_training: Rank 0, Epoch 3049, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,920: INFO: model_training: Rank 0, Epoch 3050, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,921: INFO: model_training: Rank 0, Epoch 3050, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,923: INFO: model_training: Rank 0, Epoch 3050, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,924: INFO: model_training: Rank 0, Epoch 3050, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,925: INFO: model_training: Rank 0, Epoch 3050, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,926: INFO: model_training: Rank 0, Epoch 3051, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,927: INFO: model_training: Rank 0, Epoch 3051, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,929: INFO: model_training: Rank 0, Epoch 3051, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,930: INFO: model_training: Rank 0, Epoch 3051, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,931: INFO: model_training: Rank 0, Epoch 3051, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,932: INFO: model_training: Rank 0, Epoch 3052, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,933: INFO: model_training: Rank 0, Epoch 3052, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,935: INFO: model_training: Rank 0, Epoch 3052, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,936: INFO: model_training: Rank 0, Epoch 3052, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,937: INFO: model_training: Rank 0, Epoch 3052, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,939: INFO: model_training: Rank 0, Epoch 3053, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,940: INFO: model_training: Rank 0, Epoch 3053, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,942: INFO: model_training: Rank 0, Epoch 3053, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,943: INFO: model_training: Rank 0, Epoch 3053, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,944: INFO: model_training: Rank 0, Epoch 3053, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,945: INFO: model_training: Rank 0, Epoch 3054, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,946: INFO: model_training: Rank 0, Epoch 3054, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,947: INFO: model_training: Rank 0, Epoch 3054, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,948: INFO: model_training: Rank 0, Epoch 3054, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,950: INFO: model_training: Rank 0, Epoch 3054, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,951: INFO: model_training: Rank 0, Epoch 3055, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,953: INFO: model_training: Rank 0, Epoch 3055, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,954: INFO: model_training: Rank 0, Epoch 3055, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,954: INFO: model_training: Rank 0, Epoch 3055, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,956: INFO: model_training: Rank 0, Epoch 3055, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,957: INFO: model_training: Rank 0, Epoch 3056, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,959: INFO: model_training: Rank 0, Epoch 3056, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,960: INFO: model_training: Rank 0, Epoch 3056, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,962: INFO: model_training: Rank 0, Epoch 3056, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,964: INFO: model_training: Rank 0, Epoch 3056, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,966: INFO: model_training: Rank 0, Epoch 3057, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,967: INFO: model_training: Rank 0, Epoch 3057, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,969: INFO: model_training: Rank 0, Epoch 3057, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,970: INFO: model_training: Rank 0, Epoch 3057, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,972: INFO: model_training: Rank 0, Epoch 3057, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,973: INFO: model_training: Rank 0, Epoch 3058, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,974: INFO: model_training: Rank 0, Epoch 3058, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,975: INFO: model_training: Rank 0, Epoch 3058, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,977: INFO: model_training: Rank 0, Epoch 3058, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,978: INFO: model_training: Rank 0, Epoch 3058, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,980: INFO: model_training: Rank 0, Epoch 3059, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,981: INFO: model_training: Rank 0, Epoch 3059, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,982: INFO: model_training: Rank 0, Epoch 3059, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,984: INFO: model_training: Rank 0, Epoch 3059, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,986: INFO: model_training: Rank 0, Epoch 3059, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,987: INFO: model_training: Rank 0, Epoch 3060, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,988: INFO: model_training: Rank 0, Epoch 3060, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,989: INFO: model_training: Rank 0, Epoch 3060, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,990: INFO: model_training: Rank 0, Epoch 3060, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,992: INFO: model_training: Rank 0, Epoch 3060, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,993: INFO: model_training: Rank 0, Epoch 3061, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:45,994: INFO: model_training: Rank 0, Epoch 3061, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:45,995: INFO: model_training: Rank 0, Epoch 3061, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:45,996: INFO: model_training: Rank 0, Epoch 3061, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:45,997: INFO: model_training: Rank 0, Epoch 3061, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:45,998: INFO: model_training: Rank 0, Epoch 3062, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,000: INFO: model_training: Rank 0, Epoch 3062, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,001: INFO: model_training: Rank 0, Epoch 3062, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,002: INFO: model_training: Rank 0, Epoch 3062, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,003: INFO: model_training: Rank 0, Epoch 3062, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,004: INFO: model_training: Rank 0, Epoch 3063, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,005: INFO: model_training: Rank 0, Epoch 3063, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,007: INFO: model_training: Rank 0, Epoch 3063, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,008: INFO: model_training: Rank 0, Epoch 3063, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,009: INFO: model_training: Rank 0, Epoch 3063, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,011: INFO: model_training: Rank 0, Epoch 3064, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,012: INFO: model_training: Rank 0, Epoch 3064, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,014: INFO: model_training: Rank 0, Epoch 3064, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,015: INFO: model_training: Rank 0, Epoch 3064, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,016: INFO: model_training: Rank 0, Epoch 3064, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,017: INFO: model_training: Rank 0, Epoch 3065, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,018: INFO: model_training: Rank 0, Epoch 3065, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,019: INFO: model_training: Rank 0, Epoch 3065, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,021: INFO: model_training: Rank 0, Epoch 3065, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,022: INFO: model_training: Rank 0, Epoch 3065, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,023: INFO: model_training: Rank 0, Epoch 3066, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,025: INFO: model_training: Rank 0, Epoch 3066, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,026: INFO: model_training: Rank 0, Epoch 3066, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,027: INFO: model_training: Rank 0, Epoch 3066, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,030: INFO: model_training: Rank 0, Epoch 3066, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,031: INFO: model_training: Rank 0, Epoch 3067, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,033: INFO: model_training: Rank 0, Epoch 3067, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,035: INFO: model_training: Rank 0, Epoch 3067, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,036: INFO: model_training: Rank 0, Epoch 3067, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,037: INFO: model_training: Rank 0, Epoch 3067, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,039: INFO: model_training: Rank 0, Epoch 3068, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,041: INFO: model_training: Rank 0, Epoch 3068, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,042: INFO: model_training: Rank 0, Epoch 3068, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,045: INFO: model_training: Rank 0, Epoch 3068, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,046: INFO: model_training: Rank 0, Epoch 3068, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,048: INFO: model_training: Rank 0, Epoch 3069, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,050: INFO: model_training: Rank 0, Epoch 3069, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,051: INFO: model_training: Rank 0, Epoch 3069, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,053: INFO: model_training: Rank 0, Epoch 3069, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,054: INFO: model_training: Rank 0, Epoch 3069, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,056: INFO: model_training: Rank 0, Epoch 3070, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,057: INFO: model_training: Rank 0, Epoch 3070, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,059: INFO: model_training: Rank 0, Epoch 3070, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,060: INFO: model_training: Rank 0, Epoch 3070, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,062: INFO: model_training: Rank 0, Epoch 3070, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,064: INFO: model_training: Rank 0, Epoch 3071, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,065: INFO: model_training: Rank 0, Epoch 3071, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,067: INFO: model_training: Rank 0, Epoch 3071, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,068: INFO: model_training: Rank 0, Epoch 3071, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,069: INFO: model_training: Rank 0, Epoch 3071, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,071: INFO: model_training: Rank 0, Epoch 3072, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,072: INFO: model_training: Rank 0, Epoch 3072, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,074: INFO: model_training: Rank 0, Epoch 3072, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,075: INFO: model_training: Rank 0, Epoch 3072, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,077: INFO: model_training: Rank 0, Epoch 3072, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,079: INFO: model_training: Rank 0, Epoch 3073, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,080: INFO: model_training: Rank 0, Epoch 3073, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,082: INFO: model_training: Rank 0, Epoch 3073, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,083: INFO: model_training: Rank 0, Epoch 3073, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,085: INFO: model_training: Rank 0, Epoch 3073, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,086: INFO: model_training: Rank 0, Epoch 3074, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,089: INFO: model_training: Rank 0, Epoch 3074, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,091: INFO: model_training: Rank 0, Epoch 3074, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,093: INFO: model_training: Rank 0, Epoch 3074, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,095: INFO: model_training: Rank 0, Epoch 3074, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,097: INFO: model_training: Rank 0, Epoch 3075, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,100: INFO: model_training: Rank 0, Epoch 3075, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,102: INFO: model_training: Rank 0, Epoch 3075, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,103: INFO: model_training: Rank 0, Epoch 3075, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,105: INFO: model_training: Rank 0, Epoch 3075, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,107: INFO: model_training: Rank 0, Epoch 3076, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,109: INFO: model_training: Rank 0, Epoch 3076, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,111: INFO: model_training: Rank 0, Epoch 3076, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,113: INFO: model_training: Rank 0, Epoch 3076, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,114: INFO: model_training: Rank 0, Epoch 3076, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,116: INFO: model_training: Rank 0, Epoch 3077, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,117: INFO: model_training: Rank 0, Epoch 3077, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,119: INFO: model_training: Rank 0, Epoch 3077, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,121: INFO: model_training: Rank 0, Epoch 3077, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,122: INFO: model_training: Rank 0, Epoch 3077, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,123: INFO: model_training: Rank 0, Epoch 3078, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,125: INFO: model_training: Rank 0, Epoch 3078, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,126: INFO: model_training: Rank 0, Epoch 3078, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,128: INFO: model_training: Rank 0, Epoch 3078, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,129: INFO: model_training: Rank 0, Epoch 3078, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,131: INFO: model_training: Rank 0, Epoch 3079, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,132: INFO: model_training: Rank 0, Epoch 3079, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,133: INFO: model_training: Rank 0, Epoch 3079, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,135: INFO: model_training: Rank 0, Epoch 3079, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,136: INFO: model_training: Rank 0, Epoch 3079, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,138: INFO: model_training: Rank 0, Epoch 3080, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,139: INFO: model_training: Rank 0, Epoch 3080, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,140: INFO: model_training: Rank 0, Epoch 3080, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,142: INFO: model_training: Rank 0, Epoch 3080, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,144: INFO: model_training: Rank 0, Epoch 3080, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,145: INFO: model_training: Rank 0, Epoch 3081, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,147: INFO: model_training: Rank 0, Epoch 3081, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,148: INFO: model_training: Rank 0, Epoch 3081, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,149: INFO: model_training: Rank 0, Epoch 3081, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,151: INFO: model_training: Rank 0, Epoch 3081, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,152: INFO: model_training: Rank 0, Epoch 3082, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,154: INFO: model_training: Rank 0, Epoch 3082, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,155: INFO: model_training: Rank 0, Epoch 3082, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,156: INFO: model_training: Rank 0, Epoch 3082, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,158: INFO: model_training: Rank 0, Epoch 3082, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,159: INFO: model_training: Rank 0, Epoch 3083, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,160: INFO: model_training: Rank 0, Epoch 3083, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,162: INFO: model_training: Rank 0, Epoch 3083, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,163: INFO: model_training: Rank 0, Epoch 3083, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,165: INFO: model_training: Rank 0, Epoch 3083, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,166: INFO: model_training: Rank 0, Epoch 3084, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,167: INFO: model_training: Rank 0, Epoch 3084, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,169: INFO: model_training: Rank 0, Epoch 3084, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,170: INFO: model_training: Rank 0, Epoch 3084, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,172: INFO: model_training: Rank 0, Epoch 3084, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,173: INFO: model_training: Rank 0, Epoch 3085, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,174: INFO: model_training: Rank 0, Epoch 3085, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,175: INFO: model_training: Rank 0, Epoch 3085, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,177: INFO: model_training: Rank 0, Epoch 3085, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,178: INFO: model_training: Rank 0, Epoch 3085, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,180: INFO: model_training: Rank 0, Epoch 3086, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,181: INFO: model_training: Rank 0, Epoch 3086, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,183: INFO: model_training: Rank 0, Epoch 3086, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,184: INFO: model_training: Rank 0, Epoch 3086, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,186: INFO: model_training: Rank 0, Epoch 3086, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,187: INFO: model_training: Rank 0, Epoch 3087, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,188: INFO: model_training: Rank 0, Epoch 3087, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,190: INFO: model_training: Rank 0, Epoch 3087, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,191: INFO: model_training: Rank 0, Epoch 3087, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,193: INFO: model_training: Rank 0, Epoch 3087, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,195: INFO: model_training: Rank 0, Epoch 3088, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,196: INFO: model_training: Rank 0, Epoch 3088, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,197: INFO: model_training: Rank 0, Epoch 3088, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,198: INFO: model_training: Rank 0, Epoch 3088, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,200: INFO: model_training: Rank 0, Epoch 3088, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,202: INFO: model_training: Rank 0, Epoch 3089, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,203: INFO: model_training: Rank 0, Epoch 3089, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,204: INFO: model_training: Rank 0, Epoch 3089, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,205: INFO: model_training: Rank 0, Epoch 3089, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,207: INFO: model_training: Rank 0, Epoch 3089, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,209: INFO: model_training: Rank 0, Epoch 3090, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,210: INFO: model_training: Rank 0, Epoch 3090, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,212: INFO: model_training: Rank 0, Epoch 3090, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,214: INFO: model_training: Rank 0, Epoch 3090, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,215: INFO: model_training: Rank 0, Epoch 3090, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,217: INFO: model_training: Rank 0, Epoch 3091, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,218: INFO: model_training: Rank 0, Epoch 3091, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,220: INFO: model_training: Rank 0, Epoch 3091, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,221: INFO: model_training: Rank 0, Epoch 3091, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,223: INFO: model_training: Rank 0, Epoch 3091, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,225: INFO: model_training: Rank 0, Epoch 3092, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,226: INFO: model_training: Rank 0, Epoch 3092, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,229: INFO: model_training: Rank 0, Epoch 3092, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,230: INFO: model_training: Rank 0, Epoch 3092, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,231: INFO: model_training: Rank 0, Epoch 3092, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,233: INFO: model_training: Rank 0, Epoch 3093, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,234: INFO: model_training: Rank 0, Epoch 3093, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,235: INFO: model_training: Rank 0, Epoch 3093, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,237: INFO: model_training: Rank 0, Epoch 3093, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,239: INFO: model_training: Rank 0, Epoch 3093, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,240: INFO: model_training: Rank 0, Epoch 3094, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,242: INFO: model_training: Rank 0, Epoch 3094, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,244: INFO: model_training: Rank 0, Epoch 3094, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,246: INFO: model_training: Rank 0, Epoch 3094, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,247: INFO: model_training: Rank 0, Epoch 3094, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,248: INFO: model_training: Rank 0, Epoch 3095, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,250: INFO: model_training: Rank 0, Epoch 3095, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,251: INFO: model_training: Rank 0, Epoch 3095, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,252: INFO: model_training: Rank 0, Epoch 3095, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,254: INFO: model_training: Rank 0, Epoch 3095, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,255: INFO: model_training: Rank 0, Epoch 3096, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,257: INFO: model_training: Rank 0, Epoch 3096, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,259: INFO: model_training: Rank 0, Epoch 3096, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,260: INFO: model_training: Rank 0, Epoch 3096, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,262: INFO: model_training: Rank 0, Epoch 3096, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,263: INFO: model_training: Rank 0, Epoch 3097, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,265: INFO: model_training: Rank 0, Epoch 3097, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,266: INFO: model_training: Rank 0, Epoch 3097, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,267: INFO: model_training: Rank 0, Epoch 3097, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,269: INFO: model_training: Rank 0, Epoch 3097, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,271: INFO: model_training: Rank 0, Epoch 3098, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,272: INFO: model_training: Rank 0, Epoch 3098, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,273: INFO: model_training: Rank 0, Epoch 3098, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,276: INFO: model_training: Rank 0, Epoch 3098, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,277: INFO: model_training: Rank 0, Epoch 3098, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,279: INFO: model_training: Rank 0, Epoch 3099, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,280: INFO: model_training: Rank 0, Epoch 3099, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,282: INFO: model_training: Rank 0, Epoch 3099, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,283: INFO: model_training: Rank 0, Epoch 3099, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,284: INFO: model_training: Rank 0, Epoch 3099, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,286: INFO: model_training: Rank 0, Epoch 3100, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,287: INFO: model_training: Rank 0, Epoch 3100, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,289: INFO: model_training: Rank 0, Epoch 3100, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,290: INFO: model_training: Rank 0, Epoch 3100, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,292: INFO: model_training: Rank 0, Epoch 3100, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,295: INFO: model_training: Rank 0, Epoch 3101, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,297: INFO: model_training: Rank 0, Epoch 3101, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,298: INFO: model_training: Rank 0, Epoch 3101, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,299: INFO: model_training: Rank 0, Epoch 3101, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,300: INFO: model_training: Rank 0, Epoch 3101, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,302: INFO: model_training: Rank 0, Epoch 3102, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,304: INFO: model_training: Rank 0, Epoch 3102, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,305: INFO: model_training: Rank 0, Epoch 3102, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,307: INFO: model_training: Rank 0, Epoch 3102, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,309: INFO: model_training: Rank 0, Epoch 3102, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,312: INFO: model_training: Rank 0, Epoch 3103, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,314: INFO: model_training: Rank 0, Epoch 3103, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,315: INFO: model_training: Rank 0, Epoch 3103, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,317: INFO: model_training: Rank 0, Epoch 3103, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,318: INFO: model_training: Rank 0, Epoch 3103, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,320: INFO: model_training: Rank 0, Epoch 3104, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,321: INFO: model_training: Rank 0, Epoch 3104, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,322: INFO: model_training: Rank 0, Epoch 3104, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,323: INFO: model_training: Rank 0, Epoch 3104, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,324: INFO: model_training: Rank 0, Epoch 3104, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,326: INFO: model_training: Rank 0, Epoch 3105, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,328: INFO: model_training: Rank 0, Epoch 3105, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,330: INFO: model_training: Rank 0, Epoch 3105, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,331: INFO: model_training: Rank 0, Epoch 3105, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,332: INFO: model_training: Rank 0, Epoch 3105, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,334: INFO: model_training: Rank 0, Epoch 3106, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,336: INFO: model_training: Rank 0, Epoch 3106, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,337: INFO: model_training: Rank 0, Epoch 3106, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,338: INFO: model_training: Rank 0, Epoch 3106, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,340: INFO: model_training: Rank 0, Epoch 3106, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,342: INFO: model_training: Rank 0, Epoch 3107, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,343: INFO: model_training: Rank 0, Epoch 3107, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,345: INFO: model_training: Rank 0, Epoch 3107, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,347: INFO: model_training: Rank 0, Epoch 3107, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,348: INFO: model_training: Rank 0, Epoch 3107, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,349: INFO: model_training: Rank 0, Epoch 3108, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,351: INFO: model_training: Rank 0, Epoch 3108, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,353: INFO: model_training: Rank 0, Epoch 3108, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,354: INFO: model_training: Rank 0, Epoch 3108, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,356: INFO: model_training: Rank 0, Epoch 3108, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,357: INFO: model_training: Rank 0, Epoch 3109, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,359: INFO: model_training: Rank 0, Epoch 3109, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,361: INFO: model_training: Rank 0, Epoch 3109, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,362: INFO: model_training: Rank 0, Epoch 3109, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,363: INFO: model_training: Rank 0, Epoch 3109, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,365: INFO: model_training: Rank 0, Epoch 3110, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,366: INFO: model_training: Rank 0, Epoch 3110, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,368: INFO: model_training: Rank 0, Epoch 3110, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,369: INFO: model_training: Rank 0, Epoch 3110, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,371: INFO: model_training: Rank 0, Epoch 3110, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,373: INFO: model_training: Rank 0, Epoch 3111, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,374: INFO: model_training: Rank 0, Epoch 3111, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,376: INFO: model_training: Rank 0, Epoch 3111, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,377: INFO: model_training: Rank 0, Epoch 3111, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,378: INFO: model_training: Rank 0, Epoch 3111, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,379: INFO: model_training: Rank 0, Epoch 3112, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,381: INFO: model_training: Rank 0, Epoch 3112, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,382: INFO: model_training: Rank 0, Epoch 3112, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,384: INFO: model_training: Rank 0, Epoch 3112, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,385: INFO: model_training: Rank 0, Epoch 3112, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,386: INFO: model_training: Rank 0, Epoch 3113, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,388: INFO: model_training: Rank 0, Epoch 3113, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,390: INFO: model_training: Rank 0, Epoch 3113, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,392: INFO: model_training: Rank 0, Epoch 3113, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,394: INFO: model_training: Rank 0, Epoch 3113, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,395: INFO: model_training: Rank 0, Epoch 3114, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,397: INFO: model_training: Rank 0, Epoch 3114, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,398: INFO: model_training: Rank 0, Epoch 3114, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,399: INFO: model_training: Rank 0, Epoch 3114, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,400: INFO: model_training: Rank 0, Epoch 3114, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,401: INFO: model_training: Rank 0, Epoch 3115, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,403: INFO: model_training: Rank 0, Epoch 3115, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,404: INFO: model_training: Rank 0, Epoch 3115, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,405: INFO: model_training: Rank 0, Epoch 3115, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,407: INFO: model_training: Rank 0, Epoch 3115, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,408: INFO: model_training: Rank 0, Epoch 3116, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,410: INFO: model_training: Rank 0, Epoch 3116, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,411: INFO: model_training: Rank 0, Epoch 3116, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,412: INFO: model_training: Rank 0, Epoch 3116, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,413: INFO: model_training: Rank 0, Epoch 3116, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,415: INFO: model_training: Rank 0, Epoch 3117, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,416: INFO: model_training: Rank 0, Epoch 3117, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,417: INFO: model_training: Rank 0, Epoch 3117, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,418: INFO: model_training: Rank 0, Epoch 3117, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,419: INFO: model_training: Rank 0, Epoch 3117, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,420: INFO: model_training: Rank 0, Epoch 3118, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,422: INFO: model_training: Rank 0, Epoch 3118, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,423: INFO: model_training: Rank 0, Epoch 3118, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,424: INFO: model_training: Rank 0, Epoch 3118, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,426: INFO: model_training: Rank 0, Epoch 3118, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,427: INFO: model_training: Rank 0, Epoch 3119, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,429: INFO: model_training: Rank 0, Epoch 3119, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,430: INFO: model_training: Rank 0, Epoch 3119, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,431: INFO: model_training: Rank 0, Epoch 3119, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,433: INFO: model_training: Rank 0, Epoch 3119, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,434: INFO: model_training: Rank 0, Epoch 3120, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,436: INFO: model_training: Rank 0, Epoch 3120, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,437: INFO: model_training: Rank 0, Epoch 3120, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,438: INFO: model_training: Rank 0, Epoch 3120, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,440: INFO: model_training: Rank 0, Epoch 3120, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,441: INFO: model_training: Rank 0, Epoch 3121, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,442: INFO: model_training: Rank 0, Epoch 3121, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,444: INFO: model_training: Rank 0, Epoch 3121, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,445: INFO: model_training: Rank 0, Epoch 3121, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,446: INFO: model_training: Rank 0, Epoch 3121, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,447: INFO: model_training: Rank 0, Epoch 3122, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,448: INFO: model_training: Rank 0, Epoch 3122, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,450: INFO: model_training: Rank 0, Epoch 3122, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,451: INFO: model_training: Rank 0, Epoch 3122, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,452: INFO: model_training: Rank 0, Epoch 3122, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,454: INFO: model_training: Rank 0, Epoch 3123, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,455: INFO: model_training: Rank 0, Epoch 3123, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,457: INFO: model_training: Rank 0, Epoch 3123, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,459: INFO: model_training: Rank 0, Epoch 3123, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,460: INFO: model_training: Rank 0, Epoch 3123, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,462: INFO: model_training: Rank 0, Epoch 3124, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,463: INFO: model_training: Rank 0, Epoch 3124, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,464: INFO: model_training: Rank 0, Epoch 3124, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,466: INFO: model_training: Rank 0, Epoch 3124, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,467: INFO: model_training: Rank 0, Epoch 3124, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,468: INFO: model_training: Rank 0, Epoch 3125, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,470: INFO: model_training: Rank 0, Epoch 3125, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,471: INFO: model_training: Rank 0, Epoch 3125, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,472: INFO: model_training: Rank 0, Epoch 3125, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,474: INFO: model_training: Rank 0, Epoch 3125, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,475: INFO: model_training: Rank 0, Epoch 3126, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,476: INFO: model_training: Rank 0, Epoch 3126, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,477: INFO: model_training: Rank 0, Epoch 3126, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,479: INFO: model_training: Rank 0, Epoch 3126, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,480: INFO: model_training: Rank 0, Epoch 3126, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,481: INFO: model_training: Rank 0, Epoch 3127, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,482: INFO: model_training: Rank 0, Epoch 3127, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,484: INFO: model_training: Rank 0, Epoch 3127, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,485: INFO: model_training: Rank 0, Epoch 3127, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,486: INFO: model_training: Rank 0, Epoch 3127, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,487: INFO: model_training: Rank 0, Epoch 3128, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,489: INFO: model_training: Rank 0, Epoch 3128, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,490: INFO: model_training: Rank 0, Epoch 3128, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,492: INFO: model_training: Rank 0, Epoch 3128, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,493: INFO: model_training: Rank 0, Epoch 3128, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,494: INFO: model_training: Rank 0, Epoch 3129, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,495: INFO: model_training: Rank 0, Epoch 3129, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,497: INFO: model_training: Rank 0, Epoch 3129, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,498: INFO: model_training: Rank 0, Epoch 3129, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,499: INFO: model_training: Rank 0, Epoch 3129, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,500: INFO: model_training: Rank 0, Epoch 3130, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,502: INFO: model_training: Rank 0, Epoch 3130, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,503: INFO: model_training: Rank 0, Epoch 3130, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,505: INFO: model_training: Rank 0, Epoch 3130, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,506: INFO: model_training: Rank 0, Epoch 3130, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,507: INFO: model_training: Rank 0, Epoch 3131, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,508: INFO: model_training: Rank 0, Epoch 3131, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,509: INFO: model_training: Rank 0, Epoch 3131, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,510: INFO: model_training: Rank 0, Epoch 3131, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,512: INFO: model_training: Rank 0, Epoch 3131, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,513: INFO: model_training: Rank 0, Epoch 3132, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,514: INFO: model_training: Rank 0, Epoch 3132, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,515: INFO: model_training: Rank 0, Epoch 3132, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,516: INFO: model_training: Rank 0, Epoch 3132, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,518: INFO: model_training: Rank 0, Epoch 3132, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,519: INFO: model_training: Rank 0, Epoch 3133, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,521: INFO: model_training: Rank 0, Epoch 3133, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,522: INFO: model_training: Rank 0, Epoch 3133, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,523: INFO: model_training: Rank 0, Epoch 3133, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,524: INFO: model_training: Rank 0, Epoch 3133, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,525: INFO: model_training: Rank 0, Epoch 3134, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,526: INFO: model_training: Rank 0, Epoch 3134, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,527: INFO: model_training: Rank 0, Epoch 3134, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,528: INFO: model_training: Rank 0, Epoch 3134, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,529: INFO: model_training: Rank 0, Epoch 3134, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,531: INFO: model_training: Rank 0, Epoch 3135, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,533: INFO: model_training: Rank 0, Epoch 3135, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,534: INFO: model_training: Rank 0, Epoch 3135, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,535: INFO: model_training: Rank 0, Epoch 3135, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,536: INFO: model_training: Rank 0, Epoch 3135, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,537: INFO: model_training: Rank 0, Epoch 3136, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,538: INFO: model_training: Rank 0, Epoch 3136, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,539: INFO: model_training: Rank 0, Epoch 3136, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,540: INFO: model_training: Rank 0, Epoch 3136, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,542: INFO: model_training: Rank 0, Epoch 3136, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,543: INFO: model_training: Rank 0, Epoch 3137, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,544: INFO: model_training: Rank 0, Epoch 3137, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,545: INFO: model_training: Rank 0, Epoch 3137, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,546: INFO: model_training: Rank 0, Epoch 3137, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,548: INFO: model_training: Rank 0, Epoch 3137, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,549: INFO: model_training: Rank 0, Epoch 3138, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,550: INFO: model_training: Rank 0, Epoch 3138, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,552: INFO: model_training: Rank 0, Epoch 3138, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,553: INFO: model_training: Rank 0, Epoch 3138, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,554: INFO: model_training: Rank 0, Epoch 3138, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,555: INFO: model_training: Rank 0, Epoch 3139, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,557: INFO: model_training: Rank 0, Epoch 3139, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,558: INFO: model_training: Rank 0, Epoch 3139, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,559: INFO: model_training: Rank 0, Epoch 3139, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,561: INFO: model_training: Rank 0, Epoch 3139, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,562: INFO: model_training: Rank 0, Epoch 3140, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,563: INFO: model_training: Rank 0, Epoch 3140, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,564: INFO: model_training: Rank 0, Epoch 3140, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,565: INFO: model_training: Rank 0, Epoch 3140, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,566: INFO: model_training: Rank 0, Epoch 3140, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,567: INFO: model_training: Rank 0, Epoch 3141, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,568: INFO: model_training: Rank 0, Epoch 3141, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,569: INFO: model_training: Rank 0, Epoch 3141, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,571: INFO: model_training: Rank 0, Epoch 3141, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,572: INFO: model_training: Rank 0, Epoch 3141, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,573: INFO: model_training: Rank 0, Epoch 3142, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,575: INFO: model_training: Rank 0, Epoch 3142, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,576: INFO: model_training: Rank 0, Epoch 3142, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,578: INFO: model_training: Rank 0, Epoch 3142, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,579: INFO: model_training: Rank 0, Epoch 3142, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,580: INFO: model_training: Rank 0, Epoch 3143, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,581: INFO: model_training: Rank 0, Epoch 3143, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,583: INFO: model_training: Rank 0, Epoch 3143, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,584: INFO: model_training: Rank 0, Epoch 3143, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,585: INFO: model_training: Rank 0, Epoch 3143, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,586: INFO: model_training: Rank 0, Epoch 3144, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,587: INFO: model_training: Rank 0, Epoch 3144, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,588: INFO: model_training: Rank 0, Epoch 3144, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,590: INFO: model_training: Rank 0, Epoch 3144, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,591: INFO: model_training: Rank 0, Epoch 3144, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,593: INFO: model_training: Rank 0, Epoch 3145, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,594: INFO: model_training: Rank 0, Epoch 3145, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,595: INFO: model_training: Rank 0, Epoch 3145, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,596: INFO: model_training: Rank 0, Epoch 3145, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,597: INFO: model_training: Rank 0, Epoch 3145, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,599: INFO: model_training: Rank 0, Epoch 3146, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,600: INFO: model_training: Rank 0, Epoch 3146, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,601: INFO: model_training: Rank 0, Epoch 3146, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,602: INFO: model_training: Rank 0, Epoch 3146, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,603: INFO: model_training: Rank 0, Epoch 3146, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,605: INFO: model_training: Rank 0, Epoch 3147, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,606: INFO: model_training: Rank 0, Epoch 3147, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,607: INFO: model_training: Rank 0, Epoch 3147, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,609: INFO: model_training: Rank 0, Epoch 3147, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,610: INFO: model_training: Rank 0, Epoch 3147, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,611: INFO: model_training: Rank 0, Epoch 3148, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,613: INFO: model_training: Rank 0, Epoch 3148, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,614: INFO: model_training: Rank 0, Epoch 3148, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,615: INFO: model_training: Rank 0, Epoch 3148, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,616: INFO: model_training: Rank 0, Epoch 3148, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,618: INFO: model_training: Rank 0, Epoch 3149, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,619: INFO: model_training: Rank 0, Epoch 3149, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,620: INFO: model_training: Rank 0, Epoch 3149, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,621: INFO: model_training: Rank 0, Epoch 3149, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,623: INFO: model_training: Rank 0, Epoch 3149, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,624: INFO: model_training: Rank 0, Epoch 3150, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,626: INFO: model_training: Rank 0, Epoch 3150, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,627: INFO: model_training: Rank 0, Epoch 3150, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,628: INFO: model_training: Rank 0, Epoch 3150, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,629: INFO: model_training: Rank 0, Epoch 3150, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,630: INFO: model_training: Rank 0, Epoch 3151, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,631: INFO: model_training: Rank 0, Epoch 3151, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,633: INFO: model_training: Rank 0, Epoch 3151, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,634: INFO: model_training: Rank 0, Epoch 3151, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,636: INFO: model_training: Rank 0, Epoch 3151, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,637: INFO: model_training: Rank 0, Epoch 3152, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,638: INFO: model_training: Rank 0, Epoch 3152, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,640: INFO: model_training: Rank 0, Epoch 3152, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,641: INFO: model_training: Rank 0, Epoch 3152, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,643: INFO: model_training: Rank 0, Epoch 3152, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,644: INFO: model_training: Rank 0, Epoch 3153, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,645: INFO: model_training: Rank 0, Epoch 3153, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,646: INFO: model_training: Rank 0, Epoch 3153, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,647: INFO: model_training: Rank 0, Epoch 3153, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,648: INFO: model_training: Rank 0, Epoch 3153, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,650: INFO: model_training: Rank 0, Epoch 3154, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,651: INFO: model_training: Rank 0, Epoch 3154, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,652: INFO: model_training: Rank 0, Epoch 3154, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,654: INFO: model_training: Rank 0, Epoch 3154, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,655: INFO: model_training: Rank 0, Epoch 3154, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,656: INFO: model_training: Rank 0, Epoch 3155, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,657: INFO: model_training: Rank 0, Epoch 3155, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,659: INFO: model_training: Rank 0, Epoch 3155, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,660: INFO: model_training: Rank 0, Epoch 3155, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,661: INFO: model_training: Rank 0, Epoch 3155, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,663: INFO: model_training: Rank 0, Epoch 3156, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,664: INFO: model_training: Rank 0, Epoch 3156, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,665: INFO: model_training: Rank 0, Epoch 3156, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,666: INFO: model_training: Rank 0, Epoch 3156, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,667: INFO: model_training: Rank 0, Epoch 3156, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,668: INFO: model_training: Rank 0, Epoch 3157, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,670: INFO: model_training: Rank 0, Epoch 3157, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,671: INFO: model_training: Rank 0, Epoch 3157, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,673: INFO: model_training: Rank 0, Epoch 3157, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,674: INFO: model_training: Rank 0, Epoch 3157, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,675: INFO: model_training: Rank 0, Epoch 3158, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,676: INFO: model_training: Rank 0, Epoch 3158, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,677: INFO: model_training: Rank 0, Epoch 3158, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,678: INFO: model_training: Rank 0, Epoch 3158, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,680: INFO: model_training: Rank 0, Epoch 3158, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,681: INFO: model_training: Rank 0, Epoch 3159, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,682: INFO: model_training: Rank 0, Epoch 3159, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,683: INFO: model_training: Rank 0, Epoch 3159, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,684: INFO: model_training: Rank 0, Epoch 3159, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,686: INFO: model_training: Rank 0, Epoch 3159, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,687: INFO: model_training: Rank 0, Epoch 3160, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,689: INFO: model_training: Rank 0, Epoch 3160, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,690: INFO: model_training: Rank 0, Epoch 3160, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,691: INFO: model_training: Rank 0, Epoch 3160, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,693: INFO: model_training: Rank 0, Epoch 3160, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,694: INFO: model_training: Rank 0, Epoch 3161, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,695: INFO: model_training: Rank 0, Epoch 3161, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,697: INFO: model_training: Rank 0, Epoch 3161, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,698: INFO: model_training: Rank 0, Epoch 3161, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,700: INFO: model_training: Rank 0, Epoch 3161, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,701: INFO: model_training: Rank 0, Epoch 3162, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,703: INFO: model_training: Rank 0, Epoch 3162, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,704: INFO: model_training: Rank 0, Epoch 3162, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,705: INFO: model_training: Rank 0, Epoch 3162, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,706: INFO: model_training: Rank 0, Epoch 3162, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,708: INFO: model_training: Rank 0, Epoch 3163, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,709: INFO: model_training: Rank 0, Epoch 3163, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,710: INFO: model_training: Rank 0, Epoch 3163, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,712: INFO: model_training: Rank 0, Epoch 3163, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,714: INFO: model_training: Rank 0, Epoch 3163, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,715: INFO: model_training: Rank 0, Epoch 3164, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,716: INFO: model_training: Rank 0, Epoch 3164, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,717: INFO: model_training: Rank 0, Epoch 3164, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,719: INFO: model_training: Rank 0, Epoch 3164, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,720: INFO: model_training: Rank 0, Epoch 3164, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,721: INFO: model_training: Rank 0, Epoch 3165, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,723: INFO: model_training: Rank 0, Epoch 3165, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,724: INFO: model_training: Rank 0, Epoch 3165, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,725: INFO: model_training: Rank 0, Epoch 3165, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,726: INFO: model_training: Rank 0, Epoch 3165, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,728: INFO: model_training: Rank 0, Epoch 3166, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,729: INFO: model_training: Rank 0, Epoch 3166, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,730: INFO: model_training: Rank 0, Epoch 3166, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,731: INFO: model_training: Rank 0, Epoch 3166, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,733: INFO: model_training: Rank 0, Epoch 3166, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,734: INFO: model_training: Rank 0, Epoch 3167, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,736: INFO: model_training: Rank 0, Epoch 3167, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,737: INFO: model_training: Rank 0, Epoch 3167, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,738: INFO: model_training: Rank 0, Epoch 3167, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,740: INFO: model_training: Rank 0, Epoch 3167, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,741: INFO: model_training: Rank 0, Epoch 3168, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,742: INFO: model_training: Rank 0, Epoch 3168, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,743: INFO: model_training: Rank 0, Epoch 3168, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,744: INFO: model_training: Rank 0, Epoch 3168, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,745: INFO: model_training: Rank 0, Epoch 3168, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,747: INFO: model_training: Rank 0, Epoch 3169, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,748: INFO: model_training: Rank 0, Epoch 3169, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,749: INFO: model_training: Rank 0, Epoch 3169, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,751: INFO: model_training: Rank 0, Epoch 3169, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,752: INFO: model_training: Rank 0, Epoch 3169, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,754: INFO: model_training: Rank 0, Epoch 3170, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,755: INFO: model_training: Rank 0, Epoch 3170, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,756: INFO: model_training: Rank 0, Epoch 3170, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,757: INFO: model_training: Rank 0, Epoch 3170, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,759: INFO: model_training: Rank 0, Epoch 3170, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,760: INFO: model_training: Rank 0, Epoch 3171, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,761: INFO: model_training: Rank 0, Epoch 3171, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,762: INFO: model_training: Rank 0, Epoch 3171, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,763: INFO: model_training: Rank 0, Epoch 3171, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,765: INFO: model_training: Rank 0, Epoch 3171, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,767: INFO: model_training: Rank 0, Epoch 3172, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,768: INFO: model_training: Rank 0, Epoch 3172, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,769: INFO: model_training: Rank 0, Epoch 3172, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,771: INFO: model_training: Rank 0, Epoch 3172, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,772: INFO: model_training: Rank 0, Epoch 3172, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,773: INFO: model_training: Rank 0, Epoch 3173, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,774: INFO: model_training: Rank 0, Epoch 3173, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,775: INFO: model_training: Rank 0, Epoch 3173, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,776: INFO: model_training: Rank 0, Epoch 3173, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,777: INFO: model_training: Rank 0, Epoch 3173, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,779: INFO: model_training: Rank 0, Epoch 3174, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,781: INFO: model_training: Rank 0, Epoch 3174, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,782: INFO: model_training: Rank 0, Epoch 3174, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,783: INFO: model_training: Rank 0, Epoch 3174, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,784: INFO: model_training: Rank 0, Epoch 3174, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,785: INFO: model_training: Rank 0, Epoch 3175, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,787: INFO: model_training: Rank 0, Epoch 3175, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,788: INFO: model_training: Rank 0, Epoch 3175, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,789: INFO: model_training: Rank 0, Epoch 3175, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,791: INFO: model_training: Rank 0, Epoch 3175, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,792: INFO: model_training: Rank 0, Epoch 3176, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,793: INFO: model_training: Rank 0, Epoch 3176, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,795: INFO: model_training: Rank 0, Epoch 3176, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,796: INFO: model_training: Rank 0, Epoch 3176, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,797: INFO: model_training: Rank 0, Epoch 3176, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,799: INFO: model_training: Rank 0, Epoch 3177, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,800: INFO: model_training: Rank 0, Epoch 3177, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,801: INFO: model_training: Rank 0, Epoch 3177, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,802: INFO: model_training: Rank 0, Epoch 3177, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,804: INFO: model_training: Rank 0, Epoch 3177, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,805: INFO: model_training: Rank 0, Epoch 3178, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,806: INFO: model_training: Rank 0, Epoch 3178, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,808: INFO: model_training: Rank 0, Epoch 3178, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,809: INFO: model_training: Rank 0, Epoch 3178, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,811: INFO: model_training: Rank 0, Epoch 3178, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,812: INFO: model_training: Rank 0, Epoch 3179, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,814: INFO: model_training: Rank 0, Epoch 3179, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,815: INFO: model_training: Rank 0, Epoch 3179, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,816: INFO: model_training: Rank 0, Epoch 3179, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,817: INFO: model_training: Rank 0, Epoch 3179, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,819: INFO: model_training: Rank 0, Epoch 3180, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,820: INFO: model_training: Rank 0, Epoch 3180, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,822: INFO: model_training: Rank 0, Epoch 3180, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,823: INFO: model_training: Rank 0, Epoch 3180, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,824: INFO: model_training: Rank 0, Epoch 3180, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,826: INFO: model_training: Rank 0, Epoch 3181, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,827: INFO: model_training: Rank 0, Epoch 3181, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,828: INFO: model_training: Rank 0, Epoch 3181, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,829: INFO: model_training: Rank 0, Epoch 3181, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,830: INFO: model_training: Rank 0, Epoch 3181, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,831: INFO: model_training: Rank 0, Epoch 3182, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,832: INFO: model_training: Rank 0, Epoch 3182, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,834: INFO: model_training: Rank 0, Epoch 3182, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,835: INFO: model_training: Rank 0, Epoch 3182, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,837: INFO: model_training: Rank 0, Epoch 3182, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,838: INFO: model_training: Rank 0, Epoch 3183, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,839: INFO: model_training: Rank 0, Epoch 3183, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,840: INFO: model_training: Rank 0, Epoch 3183, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,841: INFO: model_training: Rank 0, Epoch 3183, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,842: INFO: model_training: Rank 0, Epoch 3183, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,844: INFO: model_training: Rank 0, Epoch 3184, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,845: INFO: model_training: Rank 0, Epoch 3184, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,846: INFO: model_training: Rank 0, Epoch 3184, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,847: INFO: model_training: Rank 0, Epoch 3184, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,848: INFO: model_training: Rank 0, Epoch 3184, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,850: INFO: model_training: Rank 0, Epoch 3185, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,851: INFO: model_training: Rank 0, Epoch 3185, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,852: INFO: model_training: Rank 0, Epoch 3185, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,853: INFO: model_training: Rank 0, Epoch 3185, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,854: INFO: model_training: Rank 0, Epoch 3185, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,855: INFO: model_training: Rank 0, Epoch 3186, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,857: INFO: model_training: Rank 0, Epoch 3186, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,858: INFO: model_training: Rank 0, Epoch 3186, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,859: INFO: model_training: Rank 0, Epoch 3186, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,860: INFO: model_training: Rank 0, Epoch 3186, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,861: INFO: model_training: Rank 0, Epoch 3187, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,863: INFO: model_training: Rank 0, Epoch 3187, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,864: INFO: model_training: Rank 0, Epoch 3187, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,865: INFO: model_training: Rank 0, Epoch 3187, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,866: INFO: model_training: Rank 0, Epoch 3187, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,867: INFO: model_training: Rank 0, Epoch 3188, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,868: INFO: model_training: Rank 0, Epoch 3188, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,870: INFO: model_training: Rank 0, Epoch 3188, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,871: INFO: model_training: Rank 0, Epoch 3188, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,872: INFO: model_training: Rank 0, Epoch 3188, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,873: INFO: model_training: Rank 0, Epoch 3189, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,874: INFO: model_training: Rank 0, Epoch 3189, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,876: INFO: model_training: Rank 0, Epoch 3189, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,877: INFO: model_training: Rank 0, Epoch 3189, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,878: INFO: model_training: Rank 0, Epoch 3189, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,879: INFO: model_training: Rank 0, Epoch 3190, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,881: INFO: model_training: Rank 0, Epoch 3190, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,882: INFO: model_training: Rank 0, Epoch 3190, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,883: INFO: model_training: Rank 0, Epoch 3190, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,884: INFO: model_training: Rank 0, Epoch 3190, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,885: INFO: model_training: Rank 0, Epoch 3191, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,886: INFO: model_training: Rank 0, Epoch 3191, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,887: INFO: model_training: Rank 0, Epoch 3191, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,890: INFO: model_training: Rank 0, Epoch 3191, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,891: INFO: model_training: Rank 0, Epoch 3191, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,892: INFO: model_training: Rank 0, Epoch 3192, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,893: INFO: model_training: Rank 0, Epoch 3192, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,894: INFO: model_training: Rank 0, Epoch 3192, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,895: INFO: model_training: Rank 0, Epoch 3192, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,897: INFO: model_training: Rank 0, Epoch 3192, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,898: INFO: model_training: Rank 0, Epoch 3193, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,899: INFO: model_training: Rank 0, Epoch 3193, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,900: INFO: model_training: Rank 0, Epoch 3193, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,902: INFO: model_training: Rank 0, Epoch 3193, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,903: INFO: model_training: Rank 0, Epoch 3193, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,904: INFO: model_training: Rank 0, Epoch 3194, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,906: INFO: model_training: Rank 0, Epoch 3194, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,907: INFO: model_training: Rank 0, Epoch 3194, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,908: INFO: model_training: Rank 0, Epoch 3194, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,910: INFO: model_training: Rank 0, Epoch 3194, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,911: INFO: model_training: Rank 0, Epoch 3195, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,912: INFO: model_training: Rank 0, Epoch 3195, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,913: INFO: model_training: Rank 0, Epoch 3195, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,914: INFO: model_training: Rank 0, Epoch 3195, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,915: INFO: model_training: Rank 0, Epoch 3195, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,916: INFO: model_training: Rank 0, Epoch 3196, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,918: INFO: model_training: Rank 0, Epoch 3196, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,920: INFO: model_training: Rank 0, Epoch 3196, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,921: INFO: model_training: Rank 0, Epoch 3196, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,922: INFO: model_training: Rank 0, Epoch 3196, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,923: INFO: model_training: Rank 0, Epoch 3197, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,925: INFO: model_training: Rank 0, Epoch 3197, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,926: INFO: model_training: Rank 0, Epoch 3197, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,927: INFO: model_training: Rank 0, Epoch 3197, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,928: INFO: model_training: Rank 0, Epoch 3197, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,930: INFO: model_training: Rank 0, Epoch 3198, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,931: INFO: model_training: Rank 0, Epoch 3198, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,932: INFO: model_training: Rank 0, Epoch 3198, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,933: INFO: model_training: Rank 0, Epoch 3198, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,934: INFO: model_training: Rank 0, Epoch 3198, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,936: INFO: model_training: Rank 0, Epoch 3199, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,937: INFO: model_training: Rank 0, Epoch 3199, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,939: INFO: model_training: Rank 0, Epoch 3199, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,940: INFO: model_training: Rank 0, Epoch 3199, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,941: INFO: model_training: Rank 0, Epoch 3199, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,942: INFO: model_training: Rank 0, Epoch 3200, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,943: INFO: model_training: Rank 0, Epoch 3200, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,945: INFO: model_training: Rank 0, Epoch 3200, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,946: INFO: model_training: Rank 0, Epoch 3200, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,947: INFO: model_training: Rank 0, Epoch 3200, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,948: INFO: model_training: Rank 0, Epoch 3201, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,950: INFO: model_training: Rank 0, Epoch 3201, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,953: INFO: model_training: Rank 0, Epoch 3201, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,954: INFO: model_training: Rank 0, Epoch 3201, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,956: INFO: model_training: Rank 0, Epoch 3201, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,957: INFO: model_training: Rank 0, Epoch 3202, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,959: INFO: model_training: Rank 0, Epoch 3202, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,960: INFO: model_training: Rank 0, Epoch 3202, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,962: INFO: model_training: Rank 0, Epoch 3202, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,963: INFO: model_training: Rank 0, Epoch 3202, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,964: INFO: model_training: Rank 0, Epoch 3203, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,966: INFO: model_training: Rank 0, Epoch 3203, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,967: INFO: model_training: Rank 0, Epoch 3203, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,970: INFO: model_training: Rank 0, Epoch 3203, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,971: INFO: model_training: Rank 0, Epoch 3203, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,973: INFO: model_training: Rank 0, Epoch 3204, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,974: INFO: model_training: Rank 0, Epoch 3204, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,975: INFO: model_training: Rank 0, Epoch 3204, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,976: INFO: model_training: Rank 0, Epoch 3204, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,978: INFO: model_training: Rank 0, Epoch 3204, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,979: INFO: model_training: Rank 0, Epoch 3205, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,981: INFO: model_training: Rank 0, Epoch 3205, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,982: INFO: model_training: Rank 0, Epoch 3205, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,983: INFO: model_training: Rank 0, Epoch 3205, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,985: INFO: model_training: Rank 0, Epoch 3205, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,986: INFO: model_training: Rank 0, Epoch 3206, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,988: INFO: model_training: Rank 0, Epoch 3206, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,989: INFO: model_training: Rank 0, Epoch 3206, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,990: INFO: model_training: Rank 0, Epoch 3206, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,992: INFO: model_training: Rank 0, Epoch 3206, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:46,993: INFO: model_training: Rank 0, Epoch 3207, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:46,994: INFO: model_training: Rank 0, Epoch 3207, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:46,996: INFO: model_training: Rank 0, Epoch 3207, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:46,997: INFO: model_training: Rank 0, Epoch 3207, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:46,998: INFO: model_training: Rank 0, Epoch 3207, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,000: INFO: model_training: Rank 0, Epoch 3208, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,002: INFO: model_training: Rank 0, Epoch 3208, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,003: INFO: model_training: Rank 0, Epoch 3208, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,004: INFO: model_training: Rank 0, Epoch 3208, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,006: INFO: model_training: Rank 0, Epoch 3208, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,007: INFO: model_training: Rank 0, Epoch 3209, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,009: INFO: model_training: Rank 0, Epoch 3209, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,010: INFO: model_training: Rank 0, Epoch 3209, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,012: INFO: model_training: Rank 0, Epoch 3209, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,013: INFO: model_training: Rank 0, Epoch 3209, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,015: INFO: model_training: Rank 0, Epoch 3210, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,016: INFO: model_training: Rank 0, Epoch 3210, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,018: INFO: model_training: Rank 0, Epoch 3210, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,020: INFO: model_training: Rank 0, Epoch 3210, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,021: INFO: model_training: Rank 0, Epoch 3210, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,022: INFO: model_training: Rank 0, Epoch 3211, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,024: INFO: model_training: Rank 0, Epoch 3211, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,025: INFO: model_training: Rank 0, Epoch 3211, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,027: INFO: model_training: Rank 0, Epoch 3211, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,028: INFO: model_training: Rank 0, Epoch 3211, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,029: INFO: model_training: Rank 0, Epoch 3212, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,031: INFO: model_training: Rank 0, Epoch 3212, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,032: INFO: model_training: Rank 0, Epoch 3212, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,034: INFO: model_training: Rank 0, Epoch 3212, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,036: INFO: model_training: Rank 0, Epoch 3212, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,037: INFO: model_training: Rank 0, Epoch 3213, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,039: INFO: model_training: Rank 0, Epoch 3213, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,040: INFO: model_training: Rank 0, Epoch 3213, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,042: INFO: model_training: Rank 0, Epoch 3213, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,043: INFO: model_training: Rank 0, Epoch 3213, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,045: INFO: model_training: Rank 0, Epoch 3214, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,046: INFO: model_training: Rank 0, Epoch 3214, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,047: INFO: model_training: Rank 0, Epoch 3214, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,049: INFO: model_training: Rank 0, Epoch 3214, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,050: INFO: model_training: Rank 0, Epoch 3214, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,052: INFO: model_training: Rank 0, Epoch 3215, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,053: INFO: model_training: Rank 0, Epoch 3215, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,055: INFO: model_training: Rank 0, Epoch 3215, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,056: INFO: model_training: Rank 0, Epoch 3215, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,058: INFO: model_training: Rank 0, Epoch 3215, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,059: INFO: model_training: Rank 0, Epoch 3216, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,061: INFO: model_training: Rank 0, Epoch 3216, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,062: INFO: model_training: Rank 0, Epoch 3216, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,063: INFO: model_training: Rank 0, Epoch 3216, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,065: INFO: model_training: Rank 0, Epoch 3216, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,066: INFO: model_training: Rank 0, Epoch 3217, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,067: INFO: model_training: Rank 0, Epoch 3217, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,069: INFO: model_training: Rank 0, Epoch 3217, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,071: INFO: model_training: Rank 0, Epoch 3217, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,073: INFO: model_training: Rank 0, Epoch 3217, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,075: INFO: model_training: Rank 0, Epoch 3218, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,076: INFO: model_training: Rank 0, Epoch 3218, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,077: INFO: model_training: Rank 0, Epoch 3218, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,079: INFO: model_training: Rank 0, Epoch 3218, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,080: INFO: model_training: Rank 0, Epoch 3218, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,081: INFO: model_training: Rank 0, Epoch 3219, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,083: INFO: model_training: Rank 0, Epoch 3219, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,084: INFO: model_training: Rank 0, Epoch 3219, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,085: INFO: model_training: Rank 0, Epoch 3219, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,087: INFO: model_training: Rank 0, Epoch 3219, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,088: INFO: model_training: Rank 0, Epoch 3220, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,090: INFO: model_training: Rank 0, Epoch 3220, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,092: INFO: model_training: Rank 0, Epoch 3220, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,093: INFO: model_training: Rank 0, Epoch 3220, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,095: INFO: model_training: Rank 0, Epoch 3220, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,097: INFO: model_training: Rank 0, Epoch 3221, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,098: INFO: model_training: Rank 0, Epoch 3221, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,100: INFO: model_training: Rank 0, Epoch 3221, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,105: INFO: model_training: Rank 0, Epoch 3221, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,107: INFO: model_training: Rank 0, Epoch 3221, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,109: INFO: model_training: Rank 0, Epoch 3222, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,110: INFO: model_training: Rank 0, Epoch 3222, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,113: INFO: model_training: Rank 0, Epoch 3222, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,115: INFO: model_training: Rank 0, Epoch 3222, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,117: INFO: model_training: Rank 0, Epoch 3222, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,119: INFO: model_training: Rank 0, Epoch 3223, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,120: INFO: model_training: Rank 0, Epoch 3223, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,121: INFO: model_training: Rank 0, Epoch 3223, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,122: INFO: model_training: Rank 0, Epoch 3223, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,124: INFO: model_training: Rank 0, Epoch 3223, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,125: INFO: model_training: Rank 0, Epoch 3224, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,126: INFO: model_training: Rank 0, Epoch 3224, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,127: INFO: model_training: Rank 0, Epoch 3224, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,129: INFO: model_training: Rank 0, Epoch 3224, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,130: INFO: model_training: Rank 0, Epoch 3224, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,131: INFO: model_training: Rank 0, Epoch 3225, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,132: INFO: model_training: Rank 0, Epoch 3225, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,134: INFO: model_training: Rank 0, Epoch 3225, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,135: INFO: model_training: Rank 0, Epoch 3225, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,136: INFO: model_training: Rank 0, Epoch 3225, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,137: INFO: model_training: Rank 0, Epoch 3226, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,139: INFO: model_training: Rank 0, Epoch 3226, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,140: INFO: model_training: Rank 0, Epoch 3226, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,141: INFO: model_training: Rank 0, Epoch 3226, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,142: INFO: model_training: Rank 0, Epoch 3226, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,144: INFO: model_training: Rank 0, Epoch 3227, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,145: INFO: model_training: Rank 0, Epoch 3227, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,146: INFO: model_training: Rank 0, Epoch 3227, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,147: INFO: model_training: Rank 0, Epoch 3227, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,149: INFO: model_training: Rank 0, Epoch 3227, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,150: INFO: model_training: Rank 0, Epoch 3228, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,151: INFO: model_training: Rank 0, Epoch 3228, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,152: INFO: model_training: Rank 0, Epoch 3228, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,153: INFO: model_training: Rank 0, Epoch 3228, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,155: INFO: model_training: Rank 0, Epoch 3228, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,156: INFO: model_training: Rank 0, Epoch 3229, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,157: INFO: model_training: Rank 0, Epoch 3229, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,159: INFO: model_training: Rank 0, Epoch 3229, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,161: INFO: model_training: Rank 0, Epoch 3229, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,162: INFO: model_training: Rank 0, Epoch 3229, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,163: INFO: model_training: Rank 0, Epoch 3230, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,164: INFO: model_training: Rank 0, Epoch 3230, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,166: INFO: model_training: Rank 0, Epoch 3230, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,167: INFO: model_training: Rank 0, Epoch 3230, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,168: INFO: model_training: Rank 0, Epoch 3230, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,169: INFO: model_training: Rank 0, Epoch 3231, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,171: INFO: model_training: Rank 0, Epoch 3231, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,172: INFO: model_training: Rank 0, Epoch 3231, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,173: INFO: model_training: Rank 0, Epoch 3231, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,175: INFO: model_training: Rank 0, Epoch 3231, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,176: INFO: model_training: Rank 0, Epoch 3232, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,177: INFO: model_training: Rank 0, Epoch 3232, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,178: INFO: model_training: Rank 0, Epoch 3232, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,179: INFO: model_training: Rank 0, Epoch 3232, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,181: INFO: model_training: Rank 0, Epoch 3232, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,182: INFO: model_training: Rank 0, Epoch 3233, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,183: INFO: model_training: Rank 0, Epoch 3233, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,184: INFO: model_training: Rank 0, Epoch 3233, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,186: INFO: model_training: Rank 0, Epoch 3233, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,187: INFO: model_training: Rank 0, Epoch 3233, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,189: INFO: model_training: Rank 0, Epoch 3234, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,190: INFO: model_training: Rank 0, Epoch 3234, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,191: INFO: model_training: Rank 0, Epoch 3234, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,192: INFO: model_training: Rank 0, Epoch 3234, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,194: INFO: model_training: Rank 0, Epoch 3234, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,195: INFO: model_training: Rank 0, Epoch 3235, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,196: INFO: model_training: Rank 0, Epoch 3235, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,197: INFO: model_training: Rank 0, Epoch 3235, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,198: INFO: model_training: Rank 0, Epoch 3235, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,200: INFO: model_training: Rank 0, Epoch 3235, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,201: INFO: model_training: Rank 0, Epoch 3236, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,202: INFO: model_training: Rank 0, Epoch 3236, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,204: INFO: model_training: Rank 0, Epoch 3236, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,205: INFO: model_training: Rank 0, Epoch 3236, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,206: INFO: model_training: Rank 0, Epoch 3236, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,207: INFO: model_training: Rank 0, Epoch 3237, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,209: INFO: model_training: Rank 0, Epoch 3237, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,210: INFO: model_training: Rank 0, Epoch 3237, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,211: INFO: model_training: Rank 0, Epoch 3237, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,213: INFO: model_training: Rank 0, Epoch 3237, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,214: INFO: model_training: Rank 0, Epoch 3238, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,215: INFO: model_training: Rank 0, Epoch 3238, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,217: INFO: model_training: Rank 0, Epoch 3238, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,218: INFO: model_training: Rank 0, Epoch 3238, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,220: INFO: model_training: Rank 0, Epoch 3238, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,221: INFO: model_training: Rank 0, Epoch 3239, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,222: INFO: model_training: Rank 0, Epoch 3239, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,223: INFO: model_training: Rank 0, Epoch 3239, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,225: INFO: model_training: Rank 0, Epoch 3239, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,226: INFO: model_training: Rank 0, Epoch 3239, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,227: INFO: model_training: Rank 0, Epoch 3240, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,229: INFO: model_training: Rank 0, Epoch 3240, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,230: INFO: model_training: Rank 0, Epoch 3240, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,232: INFO: model_training: Rank 0, Epoch 3240, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,233: INFO: model_training: Rank 0, Epoch 3240, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,234: INFO: model_training: Rank 0, Epoch 3241, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,236: INFO: model_training: Rank 0, Epoch 3241, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,237: INFO: model_training: Rank 0, Epoch 3241, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,238: INFO: model_training: Rank 0, Epoch 3241, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,239: INFO: model_training: Rank 0, Epoch 3241, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,240: INFO: model_training: Rank 0, Epoch 3242, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,241: INFO: model_training: Rank 0, Epoch 3242, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,243: INFO: model_training: Rank 0, Epoch 3242, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,244: INFO: model_training: Rank 0, Epoch 3242, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,245: INFO: model_training: Rank 0, Epoch 3242, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,247: INFO: model_training: Rank 0, Epoch 3243, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,248: INFO: model_training: Rank 0, Epoch 3243, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,249: INFO: model_training: Rank 0, Epoch 3243, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,251: INFO: model_training: Rank 0, Epoch 3243, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,252: INFO: model_training: Rank 0, Epoch 3243, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,253: INFO: model_training: Rank 0, Epoch 3244, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,254: INFO: model_training: Rank 0, Epoch 3244, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,256: INFO: model_training: Rank 0, Epoch 3244, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,257: INFO: model_training: Rank 0, Epoch 3244, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,258: INFO: model_training: Rank 0, Epoch 3244, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,259: INFO: model_training: Rank 0, Epoch 3245, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,260: INFO: model_training: Rank 0, Epoch 3245, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,262: INFO: model_training: Rank 0, Epoch 3245, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,263: INFO: model_training: Rank 0, Epoch 3245, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,264: INFO: model_training: Rank 0, Epoch 3245, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,265: INFO: model_training: Rank 0, Epoch 3246, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,267: INFO: model_training: Rank 0, Epoch 3246, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,268: INFO: model_training: Rank 0, Epoch 3246, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,269: INFO: model_training: Rank 0, Epoch 3246, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,270: INFO: model_training: Rank 0, Epoch 3246, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,272: INFO: model_training: Rank 0, Epoch 3247, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,273: INFO: model_training: Rank 0, Epoch 3247, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,274: INFO: model_training: Rank 0, Epoch 3247, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,275: INFO: model_training: Rank 0, Epoch 3247, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,276: INFO: model_training: Rank 0, Epoch 3247, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,278: INFO: model_training: Rank 0, Epoch 3248, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,279: INFO: model_training: Rank 0, Epoch 3248, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,280: INFO: model_training: Rank 0, Epoch 3248, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,282: INFO: model_training: Rank 0, Epoch 3248, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,283: INFO: model_training: Rank 0, Epoch 3248, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,284: INFO: model_training: Rank 0, Epoch 3249, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,286: INFO: model_training: Rank 0, Epoch 3249, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,287: INFO: model_training: Rank 0, Epoch 3249, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,288: INFO: model_training: Rank 0, Epoch 3249, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,289: INFO: model_training: Rank 0, Epoch 3249, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,291: INFO: model_training: Rank 0, Epoch 3250, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,292: INFO: model_training: Rank 0, Epoch 3250, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,293: INFO: model_training: Rank 0, Epoch 3250, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,295: INFO: model_training: Rank 0, Epoch 3250, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,296: INFO: model_training: Rank 0, Epoch 3250, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,297: INFO: model_training: Rank 0, Epoch 3251, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,299: INFO: model_training: Rank 0, Epoch 3251, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,300: INFO: model_training: Rank 0, Epoch 3251, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,301: INFO: model_training: Rank 0, Epoch 3251, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,302: INFO: model_training: Rank 0, Epoch 3251, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,304: INFO: model_training: Rank 0, Epoch 3252, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,305: INFO: model_training: Rank 0, Epoch 3252, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,306: INFO: model_training: Rank 0, Epoch 3252, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,307: INFO: model_training: Rank 0, Epoch 3252, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,308: INFO: model_training: Rank 0, Epoch 3252, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,310: INFO: model_training: Rank 0, Epoch 3253, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,312: INFO: model_training: Rank 0, Epoch 3253, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,313: INFO: model_training: Rank 0, Epoch 3253, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,314: INFO: model_training: Rank 0, Epoch 3253, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,315: INFO: model_training: Rank 0, Epoch 3253, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,316: INFO: model_training: Rank 0, Epoch 3254, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,318: INFO: model_training: Rank 0, Epoch 3254, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,319: INFO: model_training: Rank 0, Epoch 3254, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,320: INFO: model_training: Rank 0, Epoch 3254, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,321: INFO: model_training: Rank 0, Epoch 3254, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,322: INFO: model_training: Rank 0, Epoch 3255, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,324: INFO: model_training: Rank 0, Epoch 3255, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,325: INFO: model_training: Rank 0, Epoch 3255, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,326: INFO: model_training: Rank 0, Epoch 3255, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,328: INFO: model_training: Rank 0, Epoch 3255, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,329: INFO: model_training: Rank 0, Epoch 3256, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,331: INFO: model_training: Rank 0, Epoch 3256, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,332: INFO: model_training: Rank 0, Epoch 3256, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,333: INFO: model_training: Rank 0, Epoch 3256, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,334: INFO: model_training: Rank 0, Epoch 3256, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,336: INFO: model_training: Rank 0, Epoch 3257, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,337: INFO: model_training: Rank 0, Epoch 3257, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,338: INFO: model_training: Rank 0, Epoch 3257, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,340: INFO: model_training: Rank 0, Epoch 3257, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,341: INFO: model_training: Rank 0, Epoch 3257, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,342: INFO: model_training: Rank 0, Epoch 3258, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,343: INFO: model_training: Rank 0, Epoch 3258, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,344: INFO: model_training: Rank 0, Epoch 3258, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,345: INFO: model_training: Rank 0, Epoch 3258, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,346: INFO: model_training: Rank 0, Epoch 3258, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,347: INFO: model_training: Rank 0, Epoch 3259, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,349: INFO: model_training: Rank 0, Epoch 3259, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,350: INFO: model_training: Rank 0, Epoch 3259, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,352: INFO: model_training: Rank 0, Epoch 3259, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,353: INFO: model_training: Rank 0, Epoch 3259, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,354: INFO: model_training: Rank 0, Epoch 3260, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,355: INFO: model_training: Rank 0, Epoch 3260, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,357: INFO: model_training: Rank 0, Epoch 3260, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,358: INFO: model_training: Rank 0, Epoch 3260, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,359: INFO: model_training: Rank 0, Epoch 3260, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,360: INFO: model_training: Rank 0, Epoch 3261, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,362: INFO: model_training: Rank 0, Epoch 3261, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,364: INFO: model_training: Rank 0, Epoch 3261, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,365: INFO: model_training: Rank 0, Epoch 3261, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,367: INFO: model_training: Rank 0, Epoch 3261, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,368: INFO: model_training: Rank 0, Epoch 3262, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,370: INFO: model_training: Rank 0, Epoch 3262, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,371: INFO: model_training: Rank 0, Epoch 3262, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,373: INFO: model_training: Rank 0, Epoch 3262, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,374: INFO: model_training: Rank 0, Epoch 3262, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,376: INFO: model_training: Rank 0, Epoch 3263, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,377: INFO: model_training: Rank 0, Epoch 3263, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,379: INFO: model_training: Rank 0, Epoch 3263, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,380: INFO: model_training: Rank 0, Epoch 3263, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,382: INFO: model_training: Rank 0, Epoch 3263, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,383: INFO: model_training: Rank 0, Epoch 3264, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,384: INFO: model_training: Rank 0, Epoch 3264, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,385: INFO: model_training: Rank 0, Epoch 3264, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,387: INFO: model_training: Rank 0, Epoch 3264, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,389: INFO: model_training: Rank 0, Epoch 3264, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,390: INFO: model_training: Rank 0, Epoch 3265, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,392: INFO: model_training: Rank 0, Epoch 3265, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,394: INFO: model_training: Rank 0, Epoch 3265, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,395: INFO: model_training: Rank 0, Epoch 3265, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,396: INFO: model_training: Rank 0, Epoch 3265, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,397: INFO: model_training: Rank 0, Epoch 3266, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,399: INFO: model_training: Rank 0, Epoch 3266, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,400: INFO: model_training: Rank 0, Epoch 3266, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,401: INFO: model_training: Rank 0, Epoch 3266, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,403: INFO: model_training: Rank 0, Epoch 3266, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,405: INFO: model_training: Rank 0, Epoch 3267, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,407: INFO: model_training: Rank 0, Epoch 3267, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,409: INFO: model_training: Rank 0, Epoch 3267, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,410: INFO: model_training: Rank 0, Epoch 3267, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,411: INFO: model_training: Rank 0, Epoch 3267, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,413: INFO: model_training: Rank 0, Epoch 3268, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,414: INFO: model_training: Rank 0, Epoch 3268, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,415: INFO: model_training: Rank 0, Epoch 3268, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,416: INFO: model_training: Rank 0, Epoch 3268, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,418: INFO: model_training: Rank 0, Epoch 3268, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,420: INFO: model_training: Rank 0, Epoch 3269, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,422: INFO: model_training: Rank 0, Epoch 3269, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,423: INFO: model_training: Rank 0, Epoch 3269, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,425: INFO: model_training: Rank 0, Epoch 3269, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,427: INFO: model_training: Rank 0, Epoch 3269, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,429: INFO: model_training: Rank 0, Epoch 3270, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,431: INFO: model_training: Rank 0, Epoch 3270, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,432: INFO: model_training: Rank 0, Epoch 3270, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,433: INFO: model_training: Rank 0, Epoch 3270, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,434: INFO: model_training: Rank 0, Epoch 3270, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,437: INFO: model_training: Rank 0, Epoch 3271, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,439: INFO: model_training: Rank 0, Epoch 3271, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,441: INFO: model_training: Rank 0, Epoch 3271, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,443: INFO: model_training: Rank 0, Epoch 3271, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,445: INFO: model_training: Rank 0, Epoch 3271, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,446: INFO: model_training: Rank 0, Epoch 3272, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,448: INFO: model_training: Rank 0, Epoch 3272, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,449: INFO: model_training: Rank 0, Epoch 3272, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,451: INFO: model_training: Rank 0, Epoch 3272, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,453: INFO: model_training: Rank 0, Epoch 3272, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,455: INFO: model_training: Rank 0, Epoch 3273, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,457: INFO: model_training: Rank 0, Epoch 3273, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,459: INFO: model_training: Rank 0, Epoch 3273, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,460: INFO: model_training: Rank 0, Epoch 3273, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,462: INFO: model_training: Rank 0, Epoch 3273, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,464: INFO: model_training: Rank 0, Epoch 3274, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,465: INFO: model_training: Rank 0, Epoch 3274, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,467: INFO: model_training: Rank 0, Epoch 3274, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,469: INFO: model_training: Rank 0, Epoch 3274, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,471: INFO: model_training: Rank 0, Epoch 3274, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,472: INFO: model_training: Rank 0, Epoch 3275, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,473: INFO: model_training: Rank 0, Epoch 3275, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,475: INFO: model_training: Rank 0, Epoch 3275, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,476: INFO: model_training: Rank 0, Epoch 3275, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,477: INFO: model_training: Rank 0, Epoch 3275, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,479: INFO: model_training: Rank 0, Epoch 3276, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,481: INFO: model_training: Rank 0, Epoch 3276, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,482: INFO: model_training: Rank 0, Epoch 3276, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,484: INFO: model_training: Rank 0, Epoch 3276, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,485: INFO: model_training: Rank 0, Epoch 3276, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,487: INFO: model_training: Rank 0, Epoch 3277, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,488: INFO: model_training: Rank 0, Epoch 3277, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,489: INFO: model_training: Rank 0, Epoch 3277, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,491: INFO: model_training: Rank 0, Epoch 3277, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,492: INFO: model_training: Rank 0, Epoch 3277, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,494: INFO: model_training: Rank 0, Epoch 3278, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,495: INFO: model_training: Rank 0, Epoch 3278, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,497: INFO: model_training: Rank 0, Epoch 3278, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,499: INFO: model_training: Rank 0, Epoch 3278, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,501: INFO: model_training: Rank 0, Epoch 3278, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,502: INFO: model_training: Rank 0, Epoch 3279, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,505: INFO: model_training: Rank 0, Epoch 3279, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,507: INFO: model_training: Rank 0, Epoch 3279, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,509: INFO: model_training: Rank 0, Epoch 3279, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,512: INFO: model_training: Rank 0, Epoch 3279, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,515: INFO: model_training: Rank 0, Epoch 3280, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,517: INFO: model_training: Rank 0, Epoch 3280, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,520: INFO: model_training: Rank 0, Epoch 3280, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,522: INFO: model_training: Rank 0, Epoch 3280, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,523: INFO: model_training: Rank 0, Epoch 3280, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,525: INFO: model_training: Rank 0, Epoch 3281, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,526: INFO: model_training: Rank 0, Epoch 3281, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,527: INFO: model_training: Rank 0, Epoch 3281, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,529: INFO: model_training: Rank 0, Epoch 3281, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,530: INFO: model_training: Rank 0, Epoch 3281, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,531: INFO: model_training: Rank 0, Epoch 3282, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,533: INFO: model_training: Rank 0, Epoch 3282, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,534: INFO: model_training: Rank 0, Epoch 3282, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,535: INFO: model_training: Rank 0, Epoch 3282, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,536: INFO: model_training: Rank 0, Epoch 3282, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,537: INFO: model_training: Rank 0, Epoch 3283, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,538: INFO: model_training: Rank 0, Epoch 3283, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,540: INFO: model_training: Rank 0, Epoch 3283, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,541: INFO: model_training: Rank 0, Epoch 3283, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,543: INFO: model_training: Rank 0, Epoch 3283, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,544: INFO: model_training: Rank 0, Epoch 3284, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,545: INFO: model_training: Rank 0, Epoch 3284, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,547: INFO: model_training: Rank 0, Epoch 3284, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,548: INFO: model_training: Rank 0, Epoch 3284, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,550: INFO: model_training: Rank 0, Epoch 3284, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,551: INFO: model_training: Rank 0, Epoch 3285, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,552: INFO: model_training: Rank 0, Epoch 3285, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,554: INFO: model_training: Rank 0, Epoch 3285, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,555: INFO: model_training: Rank 0, Epoch 3285, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,556: INFO: model_training: Rank 0, Epoch 3285, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,558: INFO: model_training: Rank 0, Epoch 3286, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,559: INFO: model_training: Rank 0, Epoch 3286, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,560: INFO: model_training: Rank 0, Epoch 3286, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,561: INFO: model_training: Rank 0, Epoch 3286, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,562: INFO: model_training: Rank 0, Epoch 3286, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,564: INFO: model_training: Rank 0, Epoch 3287, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,565: INFO: model_training: Rank 0, Epoch 3287, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,567: INFO: model_training: Rank 0, Epoch 3287, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,568: INFO: model_training: Rank 0, Epoch 3287, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,569: INFO: model_training: Rank 0, Epoch 3287, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,570: INFO: model_training: Rank 0, Epoch 3288, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,572: INFO: model_training: Rank 0, Epoch 3288, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,573: INFO: model_training: Rank 0, Epoch 3288, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,574: INFO: model_training: Rank 0, Epoch 3288, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,575: INFO: model_training: Rank 0, Epoch 3288, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,577: INFO: model_training: Rank 0, Epoch 3289, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,578: INFO: model_training: Rank 0, Epoch 3289, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,579: INFO: model_training: Rank 0, Epoch 3289, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,581: INFO: model_training: Rank 0, Epoch 3289, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,582: INFO: model_training: Rank 0, Epoch 3289, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,583: INFO: model_training: Rank 0, Epoch 3290, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,584: INFO: model_training: Rank 0, Epoch 3290, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,585: INFO: model_training: Rank 0, Epoch 3290, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,586: INFO: model_training: Rank 0, Epoch 3290, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,588: INFO: model_training: Rank 0, Epoch 3290, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,590: INFO: model_training: Rank 0, Epoch 3291, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,591: INFO: model_training: Rank 0, Epoch 3291, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,592: INFO: model_training: Rank 0, Epoch 3291, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,593: INFO: model_training: Rank 0, Epoch 3291, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,594: INFO: model_training: Rank 0, Epoch 3291, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,596: INFO: model_training: Rank 0, Epoch 3292, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,597: INFO: model_training: Rank 0, Epoch 3292, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,598: INFO: model_training: Rank 0, Epoch 3292, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,600: INFO: model_training: Rank 0, Epoch 3292, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,601: INFO: model_training: Rank 0, Epoch 3292, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,602: INFO: model_training: Rank 0, Epoch 3293, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,604: INFO: model_training: Rank 0, Epoch 3293, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,605: INFO: model_training: Rank 0, Epoch 3293, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,606: INFO: model_training: Rank 0, Epoch 3293, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,607: INFO: model_training: Rank 0, Epoch 3293, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,609: INFO: model_training: Rank 0, Epoch 3294, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,611: INFO: model_training: Rank 0, Epoch 3294, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,612: INFO: model_training: Rank 0, Epoch 3294, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,613: INFO: model_training: Rank 0, Epoch 3294, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,615: INFO: model_training: Rank 0, Epoch 3294, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,616: INFO: model_training: Rank 0, Epoch 3295, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,618: INFO: model_training: Rank 0, Epoch 3295, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,619: INFO: model_training: Rank 0, Epoch 3295, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,620: INFO: model_training: Rank 0, Epoch 3295, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,622: INFO: model_training: Rank 0, Epoch 3295, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,623: INFO: model_training: Rank 0, Epoch 3296, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,624: INFO: model_training: Rank 0, Epoch 3296, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,626: INFO: model_training: Rank 0, Epoch 3296, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,627: INFO: model_training: Rank 0, Epoch 3296, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,628: INFO: model_training: Rank 0, Epoch 3296, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,629: INFO: model_training: Rank 0, Epoch 3297, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,631: INFO: model_training: Rank 0, Epoch 3297, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,632: INFO: model_training: Rank 0, Epoch 3297, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,633: INFO: model_training: Rank 0, Epoch 3297, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,634: INFO: model_training: Rank 0, Epoch 3297, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,636: INFO: model_training: Rank 0, Epoch 3298, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,637: INFO: model_training: Rank 0, Epoch 3298, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,639: INFO: model_training: Rank 0, Epoch 3298, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,640: INFO: model_training: Rank 0, Epoch 3298, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,641: INFO: model_training: Rank 0, Epoch 3298, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,643: INFO: model_training: Rank 0, Epoch 3299, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,644: INFO: model_training: Rank 0, Epoch 3299, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,645: INFO: model_training: Rank 0, Epoch 3299, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,646: INFO: model_training: Rank 0, Epoch 3299, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,648: INFO: model_training: Rank 0, Epoch 3299, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,649: INFO: model_training: Rank 0, Epoch 3300, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,651: INFO: model_training: Rank 0, Epoch 3300, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,652: INFO: model_training: Rank 0, Epoch 3300, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,653: INFO: model_training: Rank 0, Epoch 3300, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,654: INFO: model_training: Rank 0, Epoch 3300, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,656: INFO: model_training: Rank 0, Epoch 3301, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,657: INFO: model_training: Rank 0, Epoch 3301, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,658: INFO: model_training: Rank 0, Epoch 3301, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,659: INFO: model_training: Rank 0, Epoch 3301, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,661: INFO: model_training: Rank 0, Epoch 3301, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,662: INFO: model_training: Rank 0, Epoch 3302, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,663: INFO: model_training: Rank 0, Epoch 3302, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,665: INFO: model_training: Rank 0, Epoch 3302, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,666: INFO: model_training: Rank 0, Epoch 3302, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,667: INFO: model_training: Rank 0, Epoch 3302, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,668: INFO: model_training: Rank 0, Epoch 3303, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,670: INFO: model_training: Rank 0, Epoch 3303, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,671: INFO: model_training: Rank 0, Epoch 3303, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,672: INFO: model_training: Rank 0, Epoch 3303, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,673: INFO: model_training: Rank 0, Epoch 3303, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,674: INFO: model_training: Rank 0, Epoch 3304, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,676: INFO: model_training: Rank 0, Epoch 3304, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,678: INFO: model_training: Rank 0, Epoch 3304, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,679: INFO: model_training: Rank 0, Epoch 3304, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,681: INFO: model_training: Rank 0, Epoch 3304, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,682: INFO: model_training: Rank 0, Epoch 3305, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,683: INFO: model_training: Rank 0, Epoch 3305, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,684: INFO: model_training: Rank 0, Epoch 3305, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,686: INFO: model_training: Rank 0, Epoch 3305, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,687: INFO: model_training: Rank 0, Epoch 3305, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,689: INFO: model_training: Rank 0, Epoch 3306, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,690: INFO: model_training: Rank 0, Epoch 3306, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,691: INFO: model_training: Rank 0, Epoch 3306, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,692: INFO: model_training: Rank 0, Epoch 3306, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,694: INFO: model_training: Rank 0, Epoch 3306, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,695: INFO: model_training: Rank 0, Epoch 3307, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,696: INFO: model_training: Rank 0, Epoch 3307, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,697: INFO: model_training: Rank 0, Epoch 3307, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,699: INFO: model_training: Rank 0, Epoch 3307, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,700: INFO: model_training: Rank 0, Epoch 3307, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,702: INFO: model_training: Rank 0, Epoch 3308, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,703: INFO: model_training: Rank 0, Epoch 3308, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,704: INFO: model_training: Rank 0, Epoch 3308, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,705: INFO: model_training: Rank 0, Epoch 3308, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,706: INFO: model_training: Rank 0, Epoch 3308, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,707: INFO: model_training: Rank 0, Epoch 3309, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,709: INFO: model_training: Rank 0, Epoch 3309, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,710: INFO: model_training: Rank 0, Epoch 3309, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,711: INFO: model_training: Rank 0, Epoch 3309, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,713: INFO: model_training: Rank 0, Epoch 3309, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,714: INFO: model_training: Rank 0, Epoch 3310, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,716: INFO: model_training: Rank 0, Epoch 3310, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,717: INFO: model_training: Rank 0, Epoch 3310, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,718: INFO: model_training: Rank 0, Epoch 3310, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,719: INFO: model_training: Rank 0, Epoch 3310, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,720: INFO: model_training: Rank 0, Epoch 3311, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,722: INFO: model_training: Rank 0, Epoch 3311, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,723: INFO: model_training: Rank 0, Epoch 3311, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,725: INFO: model_training: Rank 0, Epoch 3311, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,727: INFO: model_training: Rank 0, Epoch 3311, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,728: INFO: model_training: Rank 0, Epoch 3312, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,729: INFO: model_training: Rank 0, Epoch 3312, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,730: INFO: model_training: Rank 0, Epoch 3312, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,732: INFO: model_training: Rank 0, Epoch 3312, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,733: INFO: model_training: Rank 0, Epoch 3312, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,736: INFO: model_training: Rank 0, Epoch 3313, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,738: INFO: model_training: Rank 0, Epoch 3313, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,740: INFO: model_training: Rank 0, Epoch 3313, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,742: INFO: model_training: Rank 0, Epoch 3313, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,743: INFO: model_training: Rank 0, Epoch 3313, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,744: INFO: model_training: Rank 0, Epoch 3314, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,746: INFO: model_training: Rank 0, Epoch 3314, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,747: INFO: model_training: Rank 0, Epoch 3314, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,749: INFO: model_training: Rank 0, Epoch 3314, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,751: INFO: model_training: Rank 0, Epoch 3314, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,753: INFO: model_training: Rank 0, Epoch 3315, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,754: INFO: model_training: Rank 0, Epoch 3315, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,756: INFO: model_training: Rank 0, Epoch 3315, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,758: INFO: model_training: Rank 0, Epoch 3315, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,759: INFO: model_training: Rank 0, Epoch 3315, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,761: INFO: model_training: Rank 0, Epoch 3316, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,764: INFO: model_training: Rank 0, Epoch 3316, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,765: INFO: model_training: Rank 0, Epoch 3316, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,767: INFO: model_training: Rank 0, Epoch 3316, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,768: INFO: model_training: Rank 0, Epoch 3316, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,770: INFO: model_training: Rank 0, Epoch 3317, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,771: INFO: model_training: Rank 0, Epoch 3317, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,772: INFO: model_training: Rank 0, Epoch 3317, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,774: INFO: model_training: Rank 0, Epoch 3317, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,776: INFO: model_training: Rank 0, Epoch 3317, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,777: INFO: model_training: Rank 0, Epoch 3318, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,780: INFO: model_training: Rank 0, Epoch 3318, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,782: INFO: model_training: Rank 0, Epoch 3318, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,783: INFO: model_training: Rank 0, Epoch 3318, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,784: INFO: model_training: Rank 0, Epoch 3318, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,786: INFO: model_training: Rank 0, Epoch 3319, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,787: INFO: model_training: Rank 0, Epoch 3319, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,789: INFO: model_training: Rank 0, Epoch 3319, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,791: INFO: model_training: Rank 0, Epoch 3319, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,792: INFO: model_training: Rank 0, Epoch 3319, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,793: INFO: model_training: Rank 0, Epoch 3320, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,795: INFO: model_training: Rank 0, Epoch 3320, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,797: INFO: model_training: Rank 0, Epoch 3320, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,799: INFO: model_training: Rank 0, Epoch 3320, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,801: INFO: model_training: Rank 0, Epoch 3320, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,803: INFO: model_training: Rank 0, Epoch 3321, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,805: INFO: model_training: Rank 0, Epoch 3321, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,806: INFO: model_training: Rank 0, Epoch 3321, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,808: INFO: model_training: Rank 0, Epoch 3321, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,809: INFO: model_training: Rank 0, Epoch 3321, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,811: INFO: model_training: Rank 0, Epoch 3322, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,814: INFO: model_training: Rank 0, Epoch 3322, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,816: INFO: model_training: Rank 0, Epoch 3322, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,817: INFO: model_training: Rank 0, Epoch 3322, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,819: INFO: model_training: Rank 0, Epoch 3322, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,821: INFO: model_training: Rank 0, Epoch 3323, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,822: INFO: model_training: Rank 0, Epoch 3323, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,824: INFO: model_training: Rank 0, Epoch 3323, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,825: INFO: model_training: Rank 0, Epoch 3323, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,826: INFO: model_training: Rank 0, Epoch 3323, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,827: INFO: model_training: Rank 0, Epoch 3324, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,829: INFO: model_training: Rank 0, Epoch 3324, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,831: INFO: model_training: Rank 0, Epoch 3324, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,833: INFO: model_training: Rank 0, Epoch 3324, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,834: INFO: model_training: Rank 0, Epoch 3324, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,837: INFO: model_training: Rank 0, Epoch 3325, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,838: INFO: model_training: Rank 0, Epoch 3325, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,839: INFO: model_training: Rank 0, Epoch 3325, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,841: INFO: model_training: Rank 0, Epoch 3325, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,842: INFO: model_training: Rank 0, Epoch 3325, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,843: INFO: model_training: Rank 0, Epoch 3326, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,845: INFO: model_training: Rank 0, Epoch 3326, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,847: INFO: model_training: Rank 0, Epoch 3326, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,849: INFO: model_training: Rank 0, Epoch 3326, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,851: INFO: model_training: Rank 0, Epoch 3326, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,852: INFO: model_training: Rank 0, Epoch 3327, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,854: INFO: model_training: Rank 0, Epoch 3327, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,855: INFO: model_training: Rank 0, Epoch 3327, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,856: INFO: model_training: Rank 0, Epoch 3327, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,857: INFO: model_training: Rank 0, Epoch 3327, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,859: INFO: model_training: Rank 0, Epoch 3328, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,860: INFO: model_training: Rank 0, Epoch 3328, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,863: INFO: model_training: Rank 0, Epoch 3328, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,864: INFO: model_training: Rank 0, Epoch 3328, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,867: INFO: model_training: Rank 0, Epoch 3328, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,868: INFO: model_training: Rank 0, Epoch 3329, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,870: INFO: model_training: Rank 0, Epoch 3329, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,871: INFO: model_training: Rank 0, Epoch 3329, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,872: INFO: model_training: Rank 0, Epoch 3329, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,873: INFO: model_training: Rank 0, Epoch 3329, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,875: INFO: model_training: Rank 0, Epoch 3330, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,876: INFO: model_training: Rank 0, Epoch 3330, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,878: INFO: model_training: Rank 0, Epoch 3330, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,880: INFO: model_training: Rank 0, Epoch 3330, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,882: INFO: model_training: Rank 0, Epoch 3330, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,883: INFO: model_training: Rank 0, Epoch 3331, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,885: INFO: model_training: Rank 0, Epoch 3331, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,886: INFO: model_training: Rank 0, Epoch 3331, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,887: INFO: model_training: Rank 0, Epoch 3331, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,889: INFO: model_training: Rank 0, Epoch 3331, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,890: INFO: model_training: Rank 0, Epoch 3332, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,891: INFO: model_training: Rank 0, Epoch 3332, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,892: INFO: model_training: Rank 0, Epoch 3332, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,894: INFO: model_training: Rank 0, Epoch 3332, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,896: INFO: model_training: Rank 0, Epoch 3332, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,898: INFO: model_training: Rank 0, Epoch 3333, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,900: INFO: model_training: Rank 0, Epoch 3333, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,901: INFO: model_training: Rank 0, Epoch 3333, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,903: INFO: model_training: Rank 0, Epoch 3333, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,904: INFO: model_training: Rank 0, Epoch 3333, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,905: INFO: model_training: Rank 0, Epoch 3334, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,906: INFO: model_training: Rank 0, Epoch 3334, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,908: INFO: model_training: Rank 0, Epoch 3334, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,909: INFO: model_training: Rank 0, Epoch 3334, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,911: INFO: model_training: Rank 0, Epoch 3334, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,913: INFO: model_training: Rank 0, Epoch 3335, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,915: INFO: model_training: Rank 0, Epoch 3335, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,916: INFO: model_training: Rank 0, Epoch 3335, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,918: INFO: model_training: Rank 0, Epoch 3335, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,920: INFO: model_training: Rank 0, Epoch 3335, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,922: INFO: model_training: Rank 0, Epoch 3336, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,923: INFO: model_training: Rank 0, Epoch 3336, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,925: INFO: model_training: Rank 0, Epoch 3336, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,927: INFO: model_training: Rank 0, Epoch 3336, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,928: INFO: model_training: Rank 0, Epoch 3336, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,931: INFO: model_training: Rank 0, Epoch 3337, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,932: INFO: model_training: Rank 0, Epoch 3337, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,934: INFO: model_training: Rank 0, Epoch 3337, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,935: INFO: model_training: Rank 0, Epoch 3337, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,937: INFO: model_training: Rank 0, Epoch 3337, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,938: INFO: model_training: Rank 0, Epoch 3338, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,939: INFO: model_training: Rank 0, Epoch 3338, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,941: INFO: model_training: Rank 0, Epoch 3338, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,942: INFO: model_training: Rank 0, Epoch 3338, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,943: INFO: model_training: Rank 0, Epoch 3338, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,945: INFO: model_training: Rank 0, Epoch 3339, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,947: INFO: model_training: Rank 0, Epoch 3339, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,949: INFO: model_training: Rank 0, Epoch 3339, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,950: INFO: model_training: Rank 0, Epoch 3339, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,952: INFO: model_training: Rank 0, Epoch 3339, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,953: INFO: model_training: Rank 0, Epoch 3340, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,955: INFO: model_training: Rank 0, Epoch 3340, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,957: INFO: model_training: Rank 0, Epoch 3340, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,958: INFO: model_training: Rank 0, Epoch 3340, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,959: INFO: model_training: Rank 0, Epoch 3340, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,961: INFO: model_training: Rank 0, Epoch 3341, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,962: INFO: model_training: Rank 0, Epoch 3341, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,965: INFO: model_training: Rank 0, Epoch 3341, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,966: INFO: model_training: Rank 0, Epoch 3341, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,968: INFO: model_training: Rank 0, Epoch 3341, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,970: INFO: model_training: Rank 0, Epoch 3342, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,972: INFO: model_training: Rank 0, Epoch 3342, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,973: INFO: model_training: Rank 0, Epoch 3342, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,974: INFO: model_training: Rank 0, Epoch 3342, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,975: INFO: model_training: Rank 0, Epoch 3342, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,976: INFO: model_training: Rank 0, Epoch 3343, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,978: INFO: model_training: Rank 0, Epoch 3343, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,980: INFO: model_training: Rank 0, Epoch 3343, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,982: INFO: model_training: Rank 0, Epoch 3343, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,983: INFO: model_training: Rank 0, Epoch 3343, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,985: INFO: model_training: Rank 0, Epoch 3344, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,987: INFO: model_training: Rank 0, Epoch 3344, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,988: INFO: model_training: Rank 0, Epoch 3344, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,990: INFO: model_training: Rank 0, Epoch 3344, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,991: INFO: model_training: Rank 0, Epoch 3344, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:47,992: INFO: model_training: Rank 0, Epoch 3345, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:47,993: INFO: model_training: Rank 0, Epoch 3345, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:47,995: INFO: model_training: Rank 0, Epoch 3345, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:47,997: INFO: model_training: Rank 0, Epoch 3345, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:47,999: INFO: model_training: Rank 0, Epoch 3345, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,001: INFO: model_training: Rank 0, Epoch 3346, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,003: INFO: model_training: Rank 0, Epoch 3346, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,004: INFO: model_training: Rank 0, Epoch 3346, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,005: INFO: model_training: Rank 0, Epoch 3346, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,007: INFO: model_training: Rank 0, Epoch 3346, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,008: INFO: model_training: Rank 0, Epoch 3347, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,010: INFO: model_training: Rank 0, Epoch 3347, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,012: INFO: model_training: Rank 0, Epoch 3347, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,014: INFO: model_training: Rank 0, Epoch 3347, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,016: INFO: model_training: Rank 0, Epoch 3347, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,017: INFO: model_training: Rank 0, Epoch 3348, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,019: INFO: model_training: Rank 0, Epoch 3348, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,020: INFO: model_training: Rank 0, Epoch 3348, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,021: INFO: model_training: Rank 0, Epoch 3348, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,022: INFO: model_training: Rank 0, Epoch 3348, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,024: INFO: model_training: Rank 0, Epoch 3349, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,026: INFO: model_training: Rank 0, Epoch 3349, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,027: INFO: model_training: Rank 0, Epoch 3349, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,030: INFO: model_training: Rank 0, Epoch 3349, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,031: INFO: model_training: Rank 0, Epoch 3349, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,033: INFO: model_training: Rank 0, Epoch 3350, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,035: INFO: model_training: Rank 0, Epoch 3350, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,036: INFO: model_training: Rank 0, Epoch 3350, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,038: INFO: model_training: Rank 0, Epoch 3350, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,039: INFO: model_training: Rank 0, Epoch 3350, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,041: INFO: model_training: Rank 0, Epoch 3351, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,042: INFO: model_training: Rank 0, Epoch 3351, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,043: INFO: model_training: Rank 0, Epoch 3351, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,046: INFO: model_training: Rank 0, Epoch 3351, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,048: INFO: model_training: Rank 0, Epoch 3351, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,050: INFO: model_training: Rank 0, Epoch 3352, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,051: INFO: model_training: Rank 0, Epoch 3352, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,053: INFO: model_training: Rank 0, Epoch 3352, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,054: INFO: model_training: Rank 0, Epoch 3352, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,055: INFO: model_training: Rank 0, Epoch 3352, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,056: INFO: model_training: Rank 0, Epoch 3353, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,058: INFO: model_training: Rank 0, Epoch 3353, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,059: INFO: model_training: Rank 0, Epoch 3353, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,060: INFO: model_training: Rank 0, Epoch 3353, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,062: INFO: model_training: Rank 0, Epoch 3353, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,064: INFO: model_training: Rank 0, Epoch 3354, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,067: INFO: model_training: Rank 0, Epoch 3354, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,069: INFO: model_training: Rank 0, Epoch 3354, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,070: INFO: model_training: Rank 0, Epoch 3354, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,072: INFO: model_training: Rank 0, Epoch 3354, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,073: INFO: model_training: Rank 0, Epoch 3355, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,074: INFO: model_training: Rank 0, Epoch 3355, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,075: INFO: model_training: Rank 0, Epoch 3355, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,076: INFO: model_training: Rank 0, Epoch 3355, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,078: INFO: model_training: Rank 0, Epoch 3355, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,081: INFO: model_training: Rank 0, Epoch 3356, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,083: INFO: model_training: Rank 0, Epoch 3356, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,084: INFO: model_training: Rank 0, Epoch 3356, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,086: INFO: model_training: Rank 0, Epoch 3356, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,087: INFO: model_training: Rank 0, Epoch 3356, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,088: INFO: model_training: Rank 0, Epoch 3357, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,089: INFO: model_training: Rank 0, Epoch 3357, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,091: INFO: model_training: Rank 0, Epoch 3357, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,093: INFO: model_training: Rank 0, Epoch 3357, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,095: INFO: model_training: Rank 0, Epoch 3357, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,098: INFO: model_training: Rank 0, Epoch 3358, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,100: INFO: model_training: Rank 0, Epoch 3358, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,101: INFO: model_training: Rank 0, Epoch 3358, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,102: INFO: model_training: Rank 0, Epoch 3358, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,104: INFO: model_training: Rank 0, Epoch 3358, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,105: INFO: model_training: Rank 0, Epoch 3359, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,106: INFO: model_training: Rank 0, Epoch 3359, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,108: INFO: model_training: Rank 0, Epoch 3359, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,109: INFO: model_training: Rank 0, Epoch 3359, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,111: INFO: model_training: Rank 0, Epoch 3359, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,113: INFO: model_training: Rank 0, Epoch 3360, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,115: INFO: model_training: Rank 0, Epoch 3360, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,117: INFO: model_training: Rank 0, Epoch 3360, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,118: INFO: model_training: Rank 0, Epoch 3360, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,119: INFO: model_training: Rank 0, Epoch 3360, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,121: INFO: model_training: Rank 0, Epoch 3361, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,122: INFO: model_training: Rank 0, Epoch 3361, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,123: INFO: model_training: Rank 0, Epoch 3361, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,124: INFO: model_training: Rank 0, Epoch 3361, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,126: INFO: model_training: Rank 0, Epoch 3361, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,127: INFO: model_training: Rank 0, Epoch 3362, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,130: INFO: model_training: Rank 0, Epoch 3362, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,132: INFO: model_training: Rank 0, Epoch 3362, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,133: INFO: model_training: Rank 0, Epoch 3362, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,134: INFO: model_training: Rank 0, Epoch 3362, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,135: INFO: model_training: Rank 0, Epoch 3363, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,137: INFO: model_training: Rank 0, Epoch 3363, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,138: INFO: model_training: Rank 0, Epoch 3363, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,140: INFO: model_training: Rank 0, Epoch 3363, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,141: INFO: model_training: Rank 0, Epoch 3363, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,143: INFO: model_training: Rank 0, Epoch 3364, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,144: INFO: model_training: Rank 0, Epoch 3364, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,146: INFO: model_training: Rank 0, Epoch 3364, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,147: INFO: model_training: Rank 0, Epoch 3364, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,149: INFO: model_training: Rank 0, Epoch 3364, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,150: INFO: model_training: Rank 0, Epoch 3365, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,152: INFO: model_training: Rank 0, Epoch 3365, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,153: INFO: model_training: Rank 0, Epoch 3365, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,155: INFO: model_training: Rank 0, Epoch 3365, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,156: INFO: model_training: Rank 0, Epoch 3365, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,158: INFO: model_training: Rank 0, Epoch 3366, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,159: INFO: model_training: Rank 0, Epoch 3366, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,160: INFO: model_training: Rank 0, Epoch 3366, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,162: INFO: model_training: Rank 0, Epoch 3366, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,163: INFO: model_training: Rank 0, Epoch 3366, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,165: INFO: model_training: Rank 0, Epoch 3367, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,167: INFO: model_training: Rank 0, Epoch 3367, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,168: INFO: model_training: Rank 0, Epoch 3367, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,170: INFO: model_training: Rank 0, Epoch 3367, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,171: INFO: model_training: Rank 0, Epoch 3367, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,172: INFO: model_training: Rank 0, Epoch 3368, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,173: INFO: model_training: Rank 0, Epoch 3368, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,174: INFO: model_training: Rank 0, Epoch 3368, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,176: INFO: model_training: Rank 0, Epoch 3368, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,177: INFO: model_training: Rank 0, Epoch 3368, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,179: INFO: model_training: Rank 0, Epoch 3369, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,181: INFO: model_training: Rank 0, Epoch 3369, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,182: INFO: model_training: Rank 0, Epoch 3369, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,183: INFO: model_training: Rank 0, Epoch 3369, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,185: INFO: model_training: Rank 0, Epoch 3369, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,186: INFO: model_training: Rank 0, Epoch 3370, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,187: INFO: model_training: Rank 0, Epoch 3370, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,189: INFO: model_training: Rank 0, Epoch 3370, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,191: INFO: model_training: Rank 0, Epoch 3370, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,192: INFO: model_training: Rank 0, Epoch 3370, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,193: INFO: model_training: Rank 0, Epoch 3371, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,195: INFO: model_training: Rank 0, Epoch 3371, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,197: INFO: model_training: Rank 0, Epoch 3371, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,198: INFO: model_training: Rank 0, Epoch 3371, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,200: INFO: model_training: Rank 0, Epoch 3371, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,201: INFO: model_training: Rank 0, Epoch 3372, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,203: INFO: model_training: Rank 0, Epoch 3372, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,204: INFO: model_training: Rank 0, Epoch 3372, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,205: INFO: model_training: Rank 0, Epoch 3372, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,206: INFO: model_training: Rank 0, Epoch 3372, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,207: INFO: model_training: Rank 0, Epoch 3373, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,209: INFO: model_training: Rank 0, Epoch 3373, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,211: INFO: model_training: Rank 0, Epoch 3373, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,213: INFO: model_training: Rank 0, Epoch 3373, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,214: INFO: model_training: Rank 0, Epoch 3373, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,215: INFO: model_training: Rank 0, Epoch 3374, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,217: INFO: model_training: Rank 0, Epoch 3374, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,218: INFO: model_training: Rank 0, Epoch 3374, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,219: INFO: model_training: Rank 0, Epoch 3374, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,221: INFO: model_training: Rank 0, Epoch 3374, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,222: INFO: model_training: Rank 0, Epoch 3375, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,224: INFO: model_training: Rank 0, Epoch 3375, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,225: INFO: model_training: Rank 0, Epoch 3375, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,226: INFO: model_training: Rank 0, Epoch 3375, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,228: INFO: model_training: Rank 0, Epoch 3375, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,230: INFO: model_training: Rank 0, Epoch 3376, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,231: INFO: model_training: Rank 0, Epoch 3376, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,233: INFO: model_training: Rank 0, Epoch 3376, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,234: INFO: model_training: Rank 0, Epoch 3376, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,236: INFO: model_training: Rank 0, Epoch 3376, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,237: INFO: model_training: Rank 0, Epoch 3377, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,239: INFO: model_training: Rank 0, Epoch 3377, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,240: INFO: model_training: Rank 0, Epoch 3377, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,241: INFO: model_training: Rank 0, Epoch 3377, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,242: INFO: model_training: Rank 0, Epoch 3377, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,244: INFO: model_training: Rank 0, Epoch 3378, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,246: INFO: model_training: Rank 0, Epoch 3378, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,248: INFO: model_training: Rank 0, Epoch 3378, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,251: INFO: model_training: Rank 0, Epoch 3378, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,252: INFO: model_training: Rank 0, Epoch 3378, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,253: INFO: model_training: Rank 0, Epoch 3379, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,255: INFO: model_training: Rank 0, Epoch 3379, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,256: INFO: model_training: Rank 0, Epoch 3379, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,258: INFO: model_training: Rank 0, Epoch 3379, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,259: INFO: model_training: Rank 0, Epoch 3379, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,260: INFO: model_training: Rank 0, Epoch 3380, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,262: INFO: model_training: Rank 0, Epoch 3380, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,264: INFO: model_training: Rank 0, Epoch 3380, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,266: INFO: model_training: Rank 0, Epoch 3380, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,267: INFO: model_training: Rank 0, Epoch 3380, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,268: INFO: model_training: Rank 0, Epoch 3381, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,270: INFO: model_training: Rank 0, Epoch 3381, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,271: INFO: model_training: Rank 0, Epoch 3381, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,273: INFO: model_training: Rank 0, Epoch 3381, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,274: INFO: model_training: Rank 0, Epoch 3381, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,275: INFO: model_training: Rank 0, Epoch 3382, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,277: INFO: model_training: Rank 0, Epoch 3382, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,279: INFO: model_training: Rank 0, Epoch 3382, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,281: INFO: model_training: Rank 0, Epoch 3382, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,283: INFO: model_training: Rank 0, Epoch 3382, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,284: INFO: model_training: Rank 0, Epoch 3383, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,285: INFO: model_training: Rank 0, Epoch 3383, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,286: INFO: model_training: Rank 0, Epoch 3383, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,287: INFO: model_training: Rank 0, Epoch 3383, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,289: INFO: model_training: Rank 0, Epoch 3383, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,291: INFO: model_training: Rank 0, Epoch 3384, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,292: INFO: model_training: Rank 0, Epoch 3384, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,293: INFO: model_training: Rank 0, Epoch 3384, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,294: INFO: model_training: Rank 0, Epoch 3384, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,297: INFO: model_training: Rank 0, Epoch 3384, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,298: INFO: model_training: Rank 0, Epoch 3385, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,299: INFO: model_training: Rank 0, Epoch 3385, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,300: INFO: model_training: Rank 0, Epoch 3385, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,302: INFO: model_training: Rank 0, Epoch 3385, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,304: INFO: model_training: Rank 0, Epoch 3385, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,305: INFO: model_training: Rank 0, Epoch 3386, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,307: INFO: model_training: Rank 0, Epoch 3386, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,308: INFO: model_training: Rank 0, Epoch 3386, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,309: INFO: model_training: Rank 0, Epoch 3386, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,311: INFO: model_training: Rank 0, Epoch 3386, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,312: INFO: model_training: Rank 0, Epoch 3387, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,314: INFO: model_training: Rank 0, Epoch 3387, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,315: INFO: model_training: Rank 0, Epoch 3387, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,317: INFO: model_training: Rank 0, Epoch 3387, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,318: INFO: model_training: Rank 0, Epoch 3387, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,319: INFO: model_training: Rank 0, Epoch 3388, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,320: INFO: model_training: Rank 0, Epoch 3388, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,322: INFO: model_training: Rank 0, Epoch 3388, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,323: INFO: model_training: Rank 0, Epoch 3388, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,324: INFO: model_training: Rank 0, Epoch 3388, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,325: INFO: model_training: Rank 0, Epoch 3389, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,327: INFO: model_training: Rank 0, Epoch 3389, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,329: INFO: model_training: Rank 0, Epoch 3389, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,331: INFO: model_training: Rank 0, Epoch 3389, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,332: INFO: model_training: Rank 0, Epoch 3389, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,333: INFO: model_training: Rank 0, Epoch 3390, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,334: INFO: model_training: Rank 0, Epoch 3390, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,335: INFO: model_training: Rank 0, Epoch 3390, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,337: INFO: model_training: Rank 0, Epoch 3390, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,338: INFO: model_training: Rank 0, Epoch 3390, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,340: INFO: model_training: Rank 0, Epoch 3391, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,341: INFO: model_training: Rank 0, Epoch 3391, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,342: INFO: model_training: Rank 0, Epoch 3391, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,344: INFO: model_training: Rank 0, Epoch 3391, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,345: INFO: model_training: Rank 0, Epoch 3391, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,346: INFO: model_training: Rank 0, Epoch 3392, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,347: INFO: model_training: Rank 0, Epoch 3392, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,349: INFO: model_training: Rank 0, Epoch 3392, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,350: INFO: model_training: Rank 0, Epoch 3392, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,352: INFO: model_training: Rank 0, Epoch 3392, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,353: INFO: model_training: Rank 0, Epoch 3393, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,354: INFO: model_training: Rank 0, Epoch 3393, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,355: INFO: model_training: Rank 0, Epoch 3393, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,356: INFO: model_training: Rank 0, Epoch 3393, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,358: INFO: model_training: Rank 0, Epoch 3393, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,359: INFO: model_training: Rank 0, Epoch 3394, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,360: INFO: model_training: Rank 0, Epoch 3394, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,362: INFO: model_training: Rank 0, Epoch 3394, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,364: INFO: model_training: Rank 0, Epoch 3394, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,365: INFO: model_training: Rank 0, Epoch 3394, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,366: INFO: model_training: Rank 0, Epoch 3395, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,367: INFO: model_training: Rank 0, Epoch 3395, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,369: INFO: model_training: Rank 0, Epoch 3395, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,370: INFO: model_training: Rank 0, Epoch 3395, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,371: INFO: model_training: Rank 0, Epoch 3395, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,372: INFO: model_training: Rank 0, Epoch 3396, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,374: INFO: model_training: Rank 0, Epoch 3396, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,375: INFO: model_training: Rank 0, Epoch 3396, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,376: INFO: model_training: Rank 0, Epoch 3396, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,377: INFO: model_training: Rank 0, Epoch 3396, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,379: INFO: model_training: Rank 0, Epoch 3397, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,380: INFO: model_training: Rank 0, Epoch 3397, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,381: INFO: model_training: Rank 0, Epoch 3397, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,382: INFO: model_training: Rank 0, Epoch 3397, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,384: INFO: model_training: Rank 0, Epoch 3397, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,385: INFO: model_training: Rank 0, Epoch 3398, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,386: INFO: model_training: Rank 0, Epoch 3398, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,388: INFO: model_training: Rank 0, Epoch 3398, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,389: INFO: model_training: Rank 0, Epoch 3398, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,390: INFO: model_training: Rank 0, Epoch 3398, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,391: INFO: model_training: Rank 0, Epoch 3399, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,393: INFO: model_training: Rank 0, Epoch 3399, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,394: INFO: model_training: Rank 0, Epoch 3399, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,395: INFO: model_training: Rank 0, Epoch 3399, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,396: INFO: model_training: Rank 0, Epoch 3399, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,397: INFO: model_training: Rank 0, Epoch 3400, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,399: INFO: model_training: Rank 0, Epoch 3400, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,400: INFO: model_training: Rank 0, Epoch 3400, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,401: INFO: model_training: Rank 0, Epoch 3400, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,402: INFO: model_training: Rank 0, Epoch 3400, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,404: INFO: model_training: Rank 0, Epoch 3401, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,405: INFO: model_training: Rank 0, Epoch 3401, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,406: INFO: model_training: Rank 0, Epoch 3401, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,407: INFO: model_training: Rank 0, Epoch 3401, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,409: INFO: model_training: Rank 0, Epoch 3401, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,410: INFO: model_training: Rank 0, Epoch 3402, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,411: INFO: model_training: Rank 0, Epoch 3402, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,413: INFO: model_training: Rank 0, Epoch 3402, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,414: INFO: model_training: Rank 0, Epoch 3402, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,415: INFO: model_training: Rank 0, Epoch 3402, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,416: INFO: model_training: Rank 0, Epoch 3403, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,417: INFO: model_training: Rank 0, Epoch 3403, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,419: INFO: model_training: Rank 0, Epoch 3403, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,420: INFO: model_training: Rank 0, Epoch 3403, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,422: INFO: model_training: Rank 0, Epoch 3403, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,423: INFO: model_training: Rank 0, Epoch 3404, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,424: INFO: model_training: Rank 0, Epoch 3404, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,425: INFO: model_training: Rank 0, Epoch 3404, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,426: INFO: model_training: Rank 0, Epoch 3404, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,428: INFO: model_training: Rank 0, Epoch 3404, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,429: INFO: model_training: Rank 0, Epoch 3405, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,430: INFO: model_training: Rank 0, Epoch 3405, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,431: INFO: model_training: Rank 0, Epoch 3405, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,432: INFO: model_training: Rank 0, Epoch 3405, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,434: INFO: model_training: Rank 0, Epoch 3405, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,435: INFO: model_training: Rank 0, Epoch 3406, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,436: INFO: model_training: Rank 0, Epoch 3406, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,437: INFO: model_training: Rank 0, Epoch 3406, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,439: INFO: model_training: Rank 0, Epoch 3406, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,440: INFO: model_training: Rank 0, Epoch 3406, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,441: INFO: model_training: Rank 0, Epoch 3407, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,442: INFO: model_training: Rank 0, Epoch 3407, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,444: INFO: model_training: Rank 0, Epoch 3407, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,445: INFO: model_training: Rank 0, Epoch 3407, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,446: INFO: model_training: Rank 0, Epoch 3407, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,448: INFO: model_training: Rank 0, Epoch 3408, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,449: INFO: model_training: Rank 0, Epoch 3408, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,451: INFO: model_training: Rank 0, Epoch 3408, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,452: INFO: model_training: Rank 0, Epoch 3408, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,453: INFO: model_training: Rank 0, Epoch 3408, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,454: INFO: model_training: Rank 0, Epoch 3409, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,456: INFO: model_training: Rank 0, Epoch 3409, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,457: INFO: model_training: Rank 0, Epoch 3409, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,458: INFO: model_training: Rank 0, Epoch 3409, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,459: INFO: model_training: Rank 0, Epoch 3409, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,461: INFO: model_training: Rank 0, Epoch 3410, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,462: INFO: model_training: Rank 0, Epoch 3410, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,463: INFO: model_training: Rank 0, Epoch 3410, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,464: INFO: model_training: Rank 0, Epoch 3410, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,466: INFO: model_training: Rank 0, Epoch 3410, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,467: INFO: model_training: Rank 0, Epoch 3411, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,468: INFO: model_training: Rank 0, Epoch 3411, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,469: INFO: model_training: Rank 0, Epoch 3411, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,470: INFO: model_training: Rank 0, Epoch 3411, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,472: INFO: model_training: Rank 0, Epoch 3411, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,473: INFO: model_training: Rank 0, Epoch 3412, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,474: INFO: model_training: Rank 0, Epoch 3412, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,476: INFO: model_training: Rank 0, Epoch 3412, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,477: INFO: model_training: Rank 0, Epoch 3412, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,478: INFO: model_training: Rank 0, Epoch 3412, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,479: INFO: model_training: Rank 0, Epoch 3413, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,480: INFO: model_training: Rank 0, Epoch 3413, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,482: INFO: model_training: Rank 0, Epoch 3413, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,483: INFO: model_training: Rank 0, Epoch 3413, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,484: INFO: model_training: Rank 0, Epoch 3413, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,486: INFO: model_training: Rank 0, Epoch 3414, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,487: INFO: model_training: Rank 0, Epoch 3414, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,488: INFO: model_training: Rank 0, Epoch 3414, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,489: INFO: model_training: Rank 0, Epoch 3414, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,490: INFO: model_training: Rank 0, Epoch 3414, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,492: INFO: model_training: Rank 0, Epoch 3415, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,493: INFO: model_training: Rank 0, Epoch 3415, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,494: INFO: model_training: Rank 0, Epoch 3415, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,495: INFO: model_training: Rank 0, Epoch 3415, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,497: INFO: model_training: Rank 0, Epoch 3415, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,498: INFO: model_training: Rank 0, Epoch 3416, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,499: INFO: model_training: Rank 0, Epoch 3416, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,501: INFO: model_training: Rank 0, Epoch 3416, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,502: INFO: model_training: Rank 0, Epoch 3416, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,503: INFO: model_training: Rank 0, Epoch 3416, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,504: INFO: model_training: Rank 0, Epoch 3417, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,505: INFO: model_training: Rank 0, Epoch 3417, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,506: INFO: model_training: Rank 0, Epoch 3417, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,508: INFO: model_training: Rank 0, Epoch 3417, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,510: INFO: model_training: Rank 0, Epoch 3417, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,511: INFO: model_training: Rank 0, Epoch 3418, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,512: INFO: model_training: Rank 0, Epoch 3418, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,513: INFO: model_training: Rank 0, Epoch 3418, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,514: INFO: model_training: Rank 0, Epoch 3418, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,515: INFO: model_training: Rank 0, Epoch 3418, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,516: INFO: model_training: Rank 0, Epoch 3419, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,518: INFO: model_training: Rank 0, Epoch 3419, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,519: INFO: model_training: Rank 0, Epoch 3419, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,520: INFO: model_training: Rank 0, Epoch 3419, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,522: INFO: model_training: Rank 0, Epoch 3419, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,523: INFO: model_training: Rank 0, Epoch 3420, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,524: INFO: model_training: Rank 0, Epoch 3420, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,526: INFO: model_training: Rank 0, Epoch 3420, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,527: INFO: model_training: Rank 0, Epoch 3420, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,528: INFO: model_training: Rank 0, Epoch 3420, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,529: INFO: model_training: Rank 0, Epoch 3421, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,530: INFO: model_training: Rank 0, Epoch 3421, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,531: INFO: model_training: Rank 0, Epoch 3421, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,533: INFO: model_training: Rank 0, Epoch 3421, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,534: INFO: model_training: Rank 0, Epoch 3421, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,536: INFO: model_training: Rank 0, Epoch 3422, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,537: INFO: model_training: Rank 0, Epoch 3422, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,538: INFO: model_training: Rank 0, Epoch 3422, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,539: INFO: model_training: Rank 0, Epoch 3422, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,541: INFO: model_training: Rank 0, Epoch 3422, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,542: INFO: model_training: Rank 0, Epoch 3423, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,543: INFO: model_training: Rank 0, Epoch 3423, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,544: INFO: model_training: Rank 0, Epoch 3423, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,545: INFO: model_training: Rank 0, Epoch 3423, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,547: INFO: model_training: Rank 0, Epoch 3423, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,548: INFO: model_training: Rank 0, Epoch 3424, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,549: INFO: model_training: Rank 0, Epoch 3424, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,550: INFO: model_training: Rank 0, Epoch 3424, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,551: INFO: model_training: Rank 0, Epoch 3424, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,552: INFO: model_training: Rank 0, Epoch 3424, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,554: INFO: model_training: Rank 0, Epoch 3425, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,555: INFO: model_training: Rank 0, Epoch 3425, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,556: INFO: model_training: Rank 0, Epoch 3425, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,558: INFO: model_training: Rank 0, Epoch 3425, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,559: INFO: model_training: Rank 0, Epoch 3425, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,560: INFO: model_training: Rank 0, Epoch 3426, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,562: INFO: model_training: Rank 0, Epoch 3426, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,563: INFO: model_training: Rank 0, Epoch 3426, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,564: INFO: model_training: Rank 0, Epoch 3426, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,565: INFO: model_training: Rank 0, Epoch 3426, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,566: INFO: model_training: Rank 0, Epoch 3427, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,567: INFO: model_training: Rank 0, Epoch 3427, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,569: INFO: model_training: Rank 0, Epoch 3427, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,571: INFO: model_training: Rank 0, Epoch 3427, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,572: INFO: model_training: Rank 0, Epoch 3427, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,573: INFO: model_training: Rank 0, Epoch 3428, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,574: INFO: model_training: Rank 0, Epoch 3428, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,575: INFO: model_training: Rank 0, Epoch 3428, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,577: INFO: model_training: Rank 0, Epoch 3428, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,578: INFO: model_training: Rank 0, Epoch 3428, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,579: INFO: model_training: Rank 0, Epoch 3429, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,580: INFO: model_training: Rank 0, Epoch 3429, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,582: INFO: model_training: Rank 0, Epoch 3429, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,583: INFO: model_training: Rank 0, Epoch 3429, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,584: INFO: model_training: Rank 0, Epoch 3429, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,585: INFO: model_training: Rank 0, Epoch 3430, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,587: INFO: model_training: Rank 0, Epoch 3430, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,588: INFO: model_training: Rank 0, Epoch 3430, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,589: INFO: model_training: Rank 0, Epoch 3430, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,590: INFO: model_training: Rank 0, Epoch 3430, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,592: INFO: model_training: Rank 0, Epoch 3431, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,593: INFO: model_training: Rank 0, Epoch 3431, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,595: INFO: model_training: Rank 0, Epoch 3431, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,596: INFO: model_training: Rank 0, Epoch 3431, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,597: INFO: model_training: Rank 0, Epoch 3431, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,598: INFO: model_training: Rank 0, Epoch 3432, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,599: INFO: model_training: Rank 0, Epoch 3432, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,601: INFO: model_training: Rank 0, Epoch 3432, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,602: INFO: model_training: Rank 0, Epoch 3432, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,603: INFO: model_training: Rank 0, Epoch 3432, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,604: INFO: model_training: Rank 0, Epoch 3433, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,605: INFO: model_training: Rank 0, Epoch 3433, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,607: INFO: model_training: Rank 0, Epoch 3433, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,608: INFO: model_training: Rank 0, Epoch 3433, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,609: INFO: model_training: Rank 0, Epoch 3433, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,610: INFO: model_training: Rank 0, Epoch 3434, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,611: INFO: model_training: Rank 0, Epoch 3434, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,613: INFO: model_training: Rank 0, Epoch 3434, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,614: INFO: model_training: Rank 0, Epoch 3434, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,615: INFO: model_training: Rank 0, Epoch 3434, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,616: INFO: model_training: Rank 0, Epoch 3435, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,618: INFO: model_training: Rank 0, Epoch 3435, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,619: INFO: model_training: Rank 0, Epoch 3435, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,621: INFO: model_training: Rank 0, Epoch 3435, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,622: INFO: model_training: Rank 0, Epoch 3435, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,623: INFO: model_training: Rank 0, Epoch 3436, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,624: INFO: model_training: Rank 0, Epoch 3436, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,625: INFO: model_training: Rank 0, Epoch 3436, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,626: INFO: model_training: Rank 0, Epoch 3436, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,627: INFO: model_training: Rank 0, Epoch 3436, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,629: INFO: model_training: Rank 0, Epoch 3437, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,630: INFO: model_training: Rank 0, Epoch 3437, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,632: INFO: model_training: Rank 0, Epoch 3437, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,633: INFO: model_training: Rank 0, Epoch 3437, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,634: INFO: model_training: Rank 0, Epoch 3437, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,635: INFO: model_training: Rank 0, Epoch 3438, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,637: INFO: model_training: Rank 0, Epoch 3438, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,638: INFO: model_training: Rank 0, Epoch 3438, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,639: INFO: model_training: Rank 0, Epoch 3438, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,641: INFO: model_training: Rank 0, Epoch 3438, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,642: INFO: model_training: Rank 0, Epoch 3439, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,643: INFO: model_training: Rank 0, Epoch 3439, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,645: INFO: model_training: Rank 0, Epoch 3439, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,646: INFO: model_training: Rank 0, Epoch 3439, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,647: INFO: model_training: Rank 0, Epoch 3439, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,648: INFO: model_training: Rank 0, Epoch 3440, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,649: INFO: model_training: Rank 0, Epoch 3440, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,651: INFO: model_training: Rank 0, Epoch 3440, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,652: INFO: model_training: Rank 0, Epoch 3440, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,653: INFO: model_training: Rank 0, Epoch 3440, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,655: INFO: model_training: Rank 0, Epoch 3441, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,656: INFO: model_training: Rank 0, Epoch 3441, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,658: INFO: model_training: Rank 0, Epoch 3441, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,659: INFO: model_training: Rank 0, Epoch 3441, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,660: INFO: model_training: Rank 0, Epoch 3441, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,661: INFO: model_training: Rank 0, Epoch 3442, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,663: INFO: model_training: Rank 0, Epoch 3442, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,664: INFO: model_training: Rank 0, Epoch 3442, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,665: INFO: model_training: Rank 0, Epoch 3442, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,666: INFO: model_training: Rank 0, Epoch 3442, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,667: INFO: model_training: Rank 0, Epoch 3443, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,669: INFO: model_training: Rank 0, Epoch 3443, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,670: INFO: model_training: Rank 0, Epoch 3443, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,672: INFO: model_training: Rank 0, Epoch 3443, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,673: INFO: model_training: Rank 0, Epoch 3443, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,674: INFO: model_training: Rank 0, Epoch 3444, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,675: INFO: model_training: Rank 0, Epoch 3444, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,677: INFO: model_training: Rank 0, Epoch 3444, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,678: INFO: model_training: Rank 0, Epoch 3444, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,679: INFO: model_training: Rank 0, Epoch 3444, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,681: INFO: model_training: Rank 0, Epoch 3445, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,682: INFO: model_training: Rank 0, Epoch 3445, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,684: INFO: model_training: Rank 0, Epoch 3445, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,685: INFO: model_training: Rank 0, Epoch 3445, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,686: INFO: model_training: Rank 0, Epoch 3445, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,687: INFO: model_training: Rank 0, Epoch 3446, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,688: INFO: model_training: Rank 0, Epoch 3446, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,690: INFO: model_training: Rank 0, Epoch 3446, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,691: INFO: model_training: Rank 0, Epoch 3446, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,692: INFO: model_training: Rank 0, Epoch 3446, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,693: INFO: model_training: Rank 0, Epoch 3447, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,695: INFO: model_training: Rank 0, Epoch 3447, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,696: INFO: model_training: Rank 0, Epoch 3447, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,698: INFO: model_training: Rank 0, Epoch 3447, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,699: INFO: model_training: Rank 0, Epoch 3447, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,700: INFO: model_training: Rank 0, Epoch 3448, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,701: INFO: model_training: Rank 0, Epoch 3448, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,703: INFO: model_training: Rank 0, Epoch 3448, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,704: INFO: model_training: Rank 0, Epoch 3448, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,705: INFO: model_training: Rank 0, Epoch 3448, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,707: INFO: model_training: Rank 0, Epoch 3449, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,708: INFO: model_training: Rank 0, Epoch 3449, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,709: INFO: model_training: Rank 0, Epoch 3449, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,710: INFO: model_training: Rank 0, Epoch 3449, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,712: INFO: model_training: Rank 0, Epoch 3449, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,713: INFO: model_training: Rank 0, Epoch 3450, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,714: INFO: model_training: Rank 0, Epoch 3450, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,716: INFO: model_training: Rank 0, Epoch 3450, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,717: INFO: model_training: Rank 0, Epoch 3450, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,718: INFO: model_training: Rank 0, Epoch 3450, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,720: INFO: model_training: Rank 0, Epoch 3451, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,721: INFO: model_training: Rank 0, Epoch 3451, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,722: INFO: model_training: Rank 0, Epoch 3451, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,724: INFO: model_training: Rank 0, Epoch 3451, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,725: INFO: model_training: Rank 0, Epoch 3451, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,726: INFO: model_training: Rank 0, Epoch 3452, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,727: INFO: model_training: Rank 0, Epoch 3452, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,728: INFO: model_training: Rank 0, Epoch 3452, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,730: INFO: model_training: Rank 0, Epoch 3452, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,731: INFO: model_training: Rank 0, Epoch 3452, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,733: INFO: model_training: Rank 0, Epoch 3453, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,734: INFO: model_training: Rank 0, Epoch 3453, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,735: INFO: model_training: Rank 0, Epoch 3453, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,736: INFO: model_training: Rank 0, Epoch 3453, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,737: INFO: model_training: Rank 0, Epoch 3453, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,739: INFO: model_training: Rank 0, Epoch 3454, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,740: INFO: model_training: Rank 0, Epoch 3454, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,741: INFO: model_training: Rank 0, Epoch 3454, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,743: INFO: model_training: Rank 0, Epoch 3454, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,744: INFO: model_training: Rank 0, Epoch 3454, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,746: INFO: model_training: Rank 0, Epoch 3455, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,747: INFO: model_training: Rank 0, Epoch 3455, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,748: INFO: model_training: Rank 0, Epoch 3455, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,749: INFO: model_training: Rank 0, Epoch 3455, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,751: INFO: model_training: Rank 0, Epoch 3455, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,752: INFO: model_training: Rank 0, Epoch 3456, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,753: INFO: model_training: Rank 0, Epoch 3456, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,754: INFO: model_training: Rank 0, Epoch 3456, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,755: INFO: model_training: Rank 0, Epoch 3456, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,757: INFO: model_training: Rank 0, Epoch 3456, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,758: INFO: model_training: Rank 0, Epoch 3457, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,759: INFO: model_training: Rank 0, Epoch 3457, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,760: INFO: model_training: Rank 0, Epoch 3457, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,762: INFO: model_training: Rank 0, Epoch 3457, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,763: INFO: model_training: Rank 0, Epoch 3457, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,764: INFO: model_training: Rank 0, Epoch 3458, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,765: INFO: model_training: Rank 0, Epoch 3458, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,766: INFO: model_training: Rank 0, Epoch 3458, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,768: INFO: model_training: Rank 0, Epoch 3458, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,770: INFO: model_training: Rank 0, Epoch 3458, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,771: INFO: model_training: Rank 0, Epoch 3459, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,772: INFO: model_training: Rank 0, Epoch 3459, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,773: INFO: model_training: Rank 0, Epoch 3459, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,775: INFO: model_training: Rank 0, Epoch 3459, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,776: INFO: model_training: Rank 0, Epoch 3459, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,777: INFO: model_training: Rank 0, Epoch 3460, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,778: INFO: model_training: Rank 0, Epoch 3460, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,779: INFO: model_training: Rank 0, Epoch 3460, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,781: INFO: model_training: Rank 0, Epoch 3460, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,783: INFO: model_training: Rank 0, Epoch 3460, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,784: INFO: model_training: Rank 0, Epoch 3461, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,785: INFO: model_training: Rank 0, Epoch 3461, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,786: INFO: model_training: Rank 0, Epoch 3461, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,787: INFO: model_training: Rank 0, Epoch 3461, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,789: INFO: model_training: Rank 0, Epoch 3461, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,790: INFO: model_training: Rank 0, Epoch 3462, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,791: INFO: model_training: Rank 0, Epoch 3462, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,792: INFO: model_training: Rank 0, Epoch 3462, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,794: INFO: model_training: Rank 0, Epoch 3462, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,795: INFO: model_training: Rank 0, Epoch 3462, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,796: INFO: model_training: Rank 0, Epoch 3463, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,797: INFO: model_training: Rank 0, Epoch 3463, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,798: INFO: model_training: Rank 0, Epoch 3463, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,800: INFO: model_training: Rank 0, Epoch 3463, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,801: INFO: model_training: Rank 0, Epoch 3463, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,802: INFO: model_training: Rank 0, Epoch 3464, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,803: INFO: model_training: Rank 0, Epoch 3464, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,804: INFO: model_training: Rank 0, Epoch 3464, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,805: INFO: model_training: Rank 0, Epoch 3464, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,807: INFO: model_training: Rank 0, Epoch 3464, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,808: INFO: model_training: Rank 0, Epoch 3465, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,809: INFO: model_training: Rank 0, Epoch 3465, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,811: INFO: model_training: Rank 0, Epoch 3465, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,812: INFO: model_training: Rank 0, Epoch 3465, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,813: INFO: model_training: Rank 0, Epoch 3465, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,814: INFO: model_training: Rank 0, Epoch 3466, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,815: INFO: model_training: Rank 0, Epoch 3466, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,816: INFO: model_training: Rank 0, Epoch 3466, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,817: INFO: model_training: Rank 0, Epoch 3466, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,819: INFO: model_training: Rank 0, Epoch 3466, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,821: INFO: model_training: Rank 0, Epoch 3467, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,822: INFO: model_training: Rank 0, Epoch 3467, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,823: INFO: model_training: Rank 0, Epoch 3467, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,824: INFO: model_training: Rank 0, Epoch 3467, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,826: INFO: model_training: Rank 0, Epoch 3467, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,827: INFO: model_training: Rank 0, Epoch 3468, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,828: INFO: model_training: Rank 0, Epoch 3468, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,829: INFO: model_training: Rank 0, Epoch 3468, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,831: INFO: model_training: Rank 0, Epoch 3468, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,832: INFO: model_training: Rank 0, Epoch 3468, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,834: INFO: model_training: Rank 0, Epoch 3469, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,835: INFO: model_training: Rank 0, Epoch 3469, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,836: INFO: model_training: Rank 0, Epoch 3469, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,837: INFO: model_training: Rank 0, Epoch 3469, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,839: INFO: model_training: Rank 0, Epoch 3469, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,840: INFO: model_training: Rank 0, Epoch 3470, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,841: INFO: model_training: Rank 0, Epoch 3470, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,843: INFO: model_training: Rank 0, Epoch 3470, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,844: INFO: model_training: Rank 0, Epoch 3470, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,846: INFO: model_training: Rank 0, Epoch 3470, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,849: INFO: model_training: Rank 0, Epoch 3471, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,850: INFO: model_training: Rank 0, Epoch 3471, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,852: INFO: model_training: Rank 0, Epoch 3471, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,853: INFO: model_training: Rank 0, Epoch 3471, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,854: INFO: model_training: Rank 0, Epoch 3471, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,855: INFO: model_training: Rank 0, Epoch 3472, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,857: INFO: model_training: Rank 0, Epoch 3472, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,858: INFO: model_training: Rank 0, Epoch 3472, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,859: INFO: model_training: Rank 0, Epoch 3472, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,861: INFO: model_training: Rank 0, Epoch 3472, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,862: INFO: model_training: Rank 0, Epoch 3473, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,863: INFO: model_training: Rank 0, Epoch 3473, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,864: INFO: model_training: Rank 0, Epoch 3473, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,866: INFO: model_training: Rank 0, Epoch 3473, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,867: INFO: model_training: Rank 0, Epoch 3473, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,868: INFO: model_training: Rank 0, Epoch 3474, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,869: INFO: model_training: Rank 0, Epoch 3474, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,870: INFO: model_training: Rank 0, Epoch 3474, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,872: INFO: model_training: Rank 0, Epoch 3474, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,873: INFO: model_training: Rank 0, Epoch 3474, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,875: INFO: model_training: Rank 0, Epoch 3475, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,876: INFO: model_training: Rank 0, Epoch 3475, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,877: INFO: model_training: Rank 0, Epoch 3475, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,878: INFO: model_training: Rank 0, Epoch 3475, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,880: INFO: model_training: Rank 0, Epoch 3475, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,881: INFO: model_training: Rank 0, Epoch 3476, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,882: INFO: model_training: Rank 0, Epoch 3476, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,884: INFO: model_training: Rank 0, Epoch 3476, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,885: INFO: model_training: Rank 0, Epoch 3476, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,886: INFO: model_training: Rank 0, Epoch 3476, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,888: INFO: model_training: Rank 0, Epoch 3477, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,889: INFO: model_training: Rank 0, Epoch 3477, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,890: INFO: model_training: Rank 0, Epoch 3477, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,892: INFO: model_training: Rank 0, Epoch 3477, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,893: INFO: model_training: Rank 0, Epoch 3477, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,894: INFO: model_training: Rank 0, Epoch 3478, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,896: INFO: model_training: Rank 0, Epoch 3478, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,898: INFO: model_training: Rank 0, Epoch 3478, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,899: INFO: model_training: Rank 0, Epoch 3478, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,900: INFO: model_training: Rank 0, Epoch 3478, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,901: INFO: model_training: Rank 0, Epoch 3479, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,902: INFO: model_training: Rank 0, Epoch 3479, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,904: INFO: model_training: Rank 0, Epoch 3479, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,905: INFO: model_training: Rank 0, Epoch 3479, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,906: INFO: model_training: Rank 0, Epoch 3479, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,908: INFO: model_training: Rank 0, Epoch 3480, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,909: INFO: model_training: Rank 0, Epoch 3480, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,911: INFO: model_training: Rank 0, Epoch 3480, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,912: INFO: model_training: Rank 0, Epoch 3480, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,913: INFO: model_training: Rank 0, Epoch 3480, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,915: INFO: model_training: Rank 0, Epoch 3481, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,916: INFO: model_training: Rank 0, Epoch 3481, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,917: INFO: model_training: Rank 0, Epoch 3481, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,918: INFO: model_training: Rank 0, Epoch 3481, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,920: INFO: model_training: Rank 0, Epoch 3481, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,921: INFO: model_training: Rank 0, Epoch 3482, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,922: INFO: model_training: Rank 0, Epoch 3482, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,924: INFO: model_training: Rank 0, Epoch 3482, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,925: INFO: model_training: Rank 0, Epoch 3482, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,926: INFO: model_training: Rank 0, Epoch 3482, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,927: INFO: model_training: Rank 0, Epoch 3483, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,928: INFO: model_training: Rank 0, Epoch 3483, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,929: INFO: model_training: Rank 0, Epoch 3483, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,930: INFO: model_training: Rank 0, Epoch 3483, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,932: INFO: model_training: Rank 0, Epoch 3483, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,933: INFO: model_training: Rank 0, Epoch 3484, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,935: INFO: model_training: Rank 0, Epoch 3484, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,936: INFO: model_training: Rank 0, Epoch 3484, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,937: INFO: model_training: Rank 0, Epoch 3484, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,938: INFO: model_training: Rank 0, Epoch 3484, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,939: INFO: model_training: Rank 0, Epoch 3485, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,940: INFO: model_training: Rank 0, Epoch 3485, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,942: INFO: model_training: Rank 0, Epoch 3485, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,943: INFO: model_training: Rank 0, Epoch 3485, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,944: INFO: model_training: Rank 0, Epoch 3485, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,945: INFO: model_training: Rank 0, Epoch 3486, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,947: INFO: model_training: Rank 0, Epoch 3486, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,949: INFO: model_training: Rank 0, Epoch 3486, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,950: INFO: model_training: Rank 0, Epoch 3486, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,951: INFO: model_training: Rank 0, Epoch 3486, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,952: INFO: model_training: Rank 0, Epoch 3487, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,953: INFO: model_training: Rank 0, Epoch 3487, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,955: INFO: model_training: Rank 0, Epoch 3487, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,956: INFO: model_training: Rank 0, Epoch 3487, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,957: INFO: model_training: Rank 0, Epoch 3487, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,959: INFO: model_training: Rank 0, Epoch 3488, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,961: INFO: model_training: Rank 0, Epoch 3488, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,962: INFO: model_training: Rank 0, Epoch 3488, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,963: INFO: model_training: Rank 0, Epoch 3488, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,964: INFO: model_training: Rank 0, Epoch 3488, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,966: INFO: model_training: Rank 0, Epoch 3489, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,967: INFO: model_training: Rank 0, Epoch 3489, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,968: INFO: model_training: Rank 0, Epoch 3489, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,969: INFO: model_training: Rank 0, Epoch 3489, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,971: INFO: model_training: Rank 0, Epoch 3489, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,972: INFO: model_training: Rank 0, Epoch 3490, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,974: INFO: model_training: Rank 0, Epoch 3490, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,975: INFO: model_training: Rank 0, Epoch 3490, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,976: INFO: model_training: Rank 0, Epoch 3490, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,977: INFO: model_training: Rank 0, Epoch 3490, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,979: INFO: model_training: Rank 0, Epoch 3491, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,980: INFO: model_training: Rank 0, Epoch 3491, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,981: INFO: model_training: Rank 0, Epoch 3491, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,983: INFO: model_training: Rank 0, Epoch 3491, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,984: INFO: model_training: Rank 0, Epoch 3491, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,985: INFO: model_training: Rank 0, Epoch 3492, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,987: INFO: model_training: Rank 0, Epoch 3492, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,988: INFO: model_training: Rank 0, Epoch 3492, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,989: INFO: model_training: Rank 0, Epoch 3492, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,991: INFO: model_training: Rank 0, Epoch 3492, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,992: INFO: model_training: Rank 0, Epoch 3493, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,993: INFO: model_training: Rank 0, Epoch 3493, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:48,994: INFO: model_training: Rank 0, Epoch 3493, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:48,996: INFO: model_training: Rank 0, Epoch 3493, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:48,997: INFO: model_training: Rank 0, Epoch 3493, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:48,998: INFO: model_training: Rank 0, Epoch 3494, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:48,999: INFO: model_training: Rank 0, Epoch 3494, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,001: INFO: model_training: Rank 0, Epoch 3494, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,002: INFO: model_training: Rank 0, Epoch 3494, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,003: INFO: model_training: Rank 0, Epoch 3494, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,004: INFO: model_training: Rank 0, Epoch 3495, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,006: INFO: model_training: Rank 0, Epoch 3495, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,007: INFO: model_training: Rank 0, Epoch 3495, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,008: INFO: model_training: Rank 0, Epoch 3495, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,009: INFO: model_training: Rank 0, Epoch 3495, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,011: INFO: model_training: Rank 0, Epoch 3496, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,012: INFO: model_training: Rank 0, Epoch 3496, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,013: INFO: model_training: Rank 0, Epoch 3496, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,014: INFO: model_training: Rank 0, Epoch 3496, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,016: INFO: model_training: Rank 0, Epoch 3496, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,017: INFO: model_training: Rank 0, Epoch 3497, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,018: INFO: model_training: Rank 0, Epoch 3497, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,020: INFO: model_training: Rank 0, Epoch 3497, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,021: INFO: model_training: Rank 0, Epoch 3497, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,022: INFO: model_training: Rank 0, Epoch 3497, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,023: INFO: model_training: Rank 0, Epoch 3498, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,025: INFO: model_training: Rank 0, Epoch 3498, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,026: INFO: model_training: Rank 0, Epoch 3498, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,027: INFO: model_training: Rank 0, Epoch 3498, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,029: INFO: model_training: Rank 0, Epoch 3498, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,030: INFO: model_training: Rank 0, Epoch 3499, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,032: INFO: model_training: Rank 0, Epoch 3499, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,033: INFO: model_training: Rank 0, Epoch 3499, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,034: INFO: model_training: Rank 0, Epoch 3499, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,035: INFO: model_training: Rank 0, Epoch 3499, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,037: INFO: model_training: Rank 0, Epoch 3500, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,038: INFO: model_training: Rank 0, Epoch 3500, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,039: INFO: model_training: Rank 0, Epoch 3500, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,040: INFO: model_training: Rank 0, Epoch 3500, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,042: INFO: model_training: Rank 0, Epoch 3500, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,043: INFO: model_training: Rank 0, Epoch 3501, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,044: INFO: model_training: Rank 0, Epoch 3501, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,045: INFO: model_training: Rank 0, Epoch 3501, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,047: INFO: model_training: Rank 0, Epoch 3501, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,048: INFO: model_training: Rank 0, Epoch 3501, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,049: INFO: model_training: Rank 0, Epoch 3502, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,050: INFO: model_training: Rank 0, Epoch 3502, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,052: INFO: model_training: Rank 0, Epoch 3502, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,053: INFO: model_training: Rank 0, Epoch 3502, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,054: INFO: model_training: Rank 0, Epoch 3502, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,056: INFO: model_training: Rank 0, Epoch 3503, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,057: INFO: model_training: Rank 0, Epoch 3503, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,058: INFO: model_training: Rank 0, Epoch 3503, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,059: INFO: model_training: Rank 0, Epoch 3503, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,060: INFO: model_training: Rank 0, Epoch 3503, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,062: INFO: model_training: Rank 0, Epoch 3504, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,063: INFO: model_training: Rank 0, Epoch 3504, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,065: INFO: model_training: Rank 0, Epoch 3504, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,066: INFO: model_training: Rank 0, Epoch 3504, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,067: INFO: model_training: Rank 0, Epoch 3504, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,068: INFO: model_training: Rank 0, Epoch 3505, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,070: INFO: model_training: Rank 0, Epoch 3505, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,071: INFO: model_training: Rank 0, Epoch 3505, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,072: INFO: model_training: Rank 0, Epoch 3505, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,074: INFO: model_training: Rank 0, Epoch 3505, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,075: INFO: model_training: Rank 0, Epoch 3506, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,077: INFO: model_training: Rank 0, Epoch 3506, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,078: INFO: model_training: Rank 0, Epoch 3506, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,079: INFO: model_training: Rank 0, Epoch 3506, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,080: INFO: model_training: Rank 0, Epoch 3506, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,081: INFO: model_training: Rank 0, Epoch 3507, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,082: INFO: model_training: Rank 0, Epoch 3507, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,083: INFO: model_training: Rank 0, Epoch 3507, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,084: INFO: model_training: Rank 0, Epoch 3507, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,086: INFO: model_training: Rank 0, Epoch 3507, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,087: INFO: model_training: Rank 0, Epoch 3508, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,089: INFO: model_training: Rank 0, Epoch 3508, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,090: INFO: model_training: Rank 0, Epoch 3508, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,091: INFO: model_training: Rank 0, Epoch 3508, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,092: INFO: model_training: Rank 0, Epoch 3508, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,093: INFO: model_training: Rank 0, Epoch 3509, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,094: INFO: model_training: Rank 0, Epoch 3509, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,096: INFO: model_training: Rank 0, Epoch 3509, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,097: INFO: model_training: Rank 0, Epoch 3509, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,098: INFO: model_training: Rank 0, Epoch 3509, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,100: INFO: model_training: Rank 0, Epoch 3510, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,101: INFO: model_training: Rank 0, Epoch 3510, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,103: INFO: model_training: Rank 0, Epoch 3510, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,104: INFO: model_training: Rank 0, Epoch 3510, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,105: INFO: model_training: Rank 0, Epoch 3510, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,106: INFO: model_training: Rank 0, Epoch 3511, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,107: INFO: model_training: Rank 0, Epoch 3511, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,109: INFO: model_training: Rank 0, Epoch 3511, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,110: INFO: model_training: Rank 0, Epoch 3511, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,111: INFO: model_training: Rank 0, Epoch 3511, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,113: INFO: model_training: Rank 0, Epoch 3512, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,114: INFO: model_training: Rank 0, Epoch 3512, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,115: INFO: model_training: Rank 0, Epoch 3512, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,116: INFO: model_training: Rank 0, Epoch 3512, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,117: INFO: model_training: Rank 0, Epoch 3512, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,118: INFO: model_training: Rank 0, Epoch 3513, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,120: INFO: model_training: Rank 0, Epoch 3513, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,121: INFO: model_training: Rank 0, Epoch 3513, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,122: INFO: model_training: Rank 0, Epoch 3513, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,124: INFO: model_training: Rank 0, Epoch 3513, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,125: INFO: model_training: Rank 0, Epoch 3514, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,126: INFO: model_training: Rank 0, Epoch 3514, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,127: INFO: model_training: Rank 0, Epoch 3514, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,128: INFO: model_training: Rank 0, Epoch 3514, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,129: INFO: model_training: Rank 0, Epoch 3514, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,130: INFO: model_training: Rank 0, Epoch 3515, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,132: INFO: model_training: Rank 0, Epoch 3515, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,133: INFO: model_training: Rank 0, Epoch 3515, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,135: INFO: model_training: Rank 0, Epoch 3515, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,136: INFO: model_training: Rank 0, Epoch 3515, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,137: INFO: model_training: Rank 0, Epoch 3516, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,139: INFO: model_training: Rank 0, Epoch 3516, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,140: INFO: model_training: Rank 0, Epoch 3516, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,141: INFO: model_training: Rank 0, Epoch 3516, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,142: INFO: model_training: Rank 0, Epoch 3516, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,144: INFO: model_training: Rank 0, Epoch 3517, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,145: INFO: model_training: Rank 0, Epoch 3517, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,147: INFO: model_training: Rank 0, Epoch 3517, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,148: INFO: model_training: Rank 0, Epoch 3517, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,149: INFO: model_training: Rank 0, Epoch 3517, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,150: INFO: model_training: Rank 0, Epoch 3518, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,151: INFO: model_training: Rank 0, Epoch 3518, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,152: INFO: model_training: Rank 0, Epoch 3518, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,154: INFO: model_training: Rank 0, Epoch 3518, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,155: INFO: model_training: Rank 0, Epoch 3518, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,156: INFO: model_training: Rank 0, Epoch 3519, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,157: INFO: model_training: Rank 0, Epoch 3519, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,159: INFO: model_training: Rank 0, Epoch 3519, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,160: INFO: model_training: Rank 0, Epoch 3519, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,162: INFO: model_training: Rank 0, Epoch 3519, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,163: INFO: model_training: Rank 0, Epoch 3520, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,164: INFO: model_training: Rank 0, Epoch 3520, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,165: INFO: model_training: Rank 0, Epoch 3520, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,166: INFO: model_training: Rank 0, Epoch 3520, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,167: INFO: model_training: Rank 0, Epoch 3520, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,169: INFO: model_training: Rank 0, Epoch 3521, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,170: INFO: model_training: Rank 0, Epoch 3521, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,172: INFO: model_training: Rank 0, Epoch 3521, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,173: INFO: model_training: Rank 0, Epoch 3521, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,174: INFO: model_training: Rank 0, Epoch 3521, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,175: INFO: model_training: Rank 0, Epoch 3522, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,176: INFO: model_training: Rank 0, Epoch 3522, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,177: INFO: model_training: Rank 0, Epoch 3522, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,179: INFO: model_training: Rank 0, Epoch 3522, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,180: INFO: model_training: Rank 0, Epoch 3522, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,181: INFO: model_training: Rank 0, Epoch 3523, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,183: INFO: model_training: Rank 0, Epoch 3523, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,184: INFO: model_training: Rank 0, Epoch 3523, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,185: INFO: model_training: Rank 0, Epoch 3523, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,187: INFO: model_training: Rank 0, Epoch 3523, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,188: INFO: model_training: Rank 0, Epoch 3524, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,189: INFO: model_training: Rank 0, Epoch 3524, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,190: INFO: model_training: Rank 0, Epoch 3524, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,191: INFO: model_training: Rank 0, Epoch 3524, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,193: INFO: model_training: Rank 0, Epoch 3524, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,194: INFO: model_training: Rank 0, Epoch 3525, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,195: INFO: model_training: Rank 0, Epoch 3525, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,197: INFO: model_training: Rank 0, Epoch 3525, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,198: INFO: model_training: Rank 0, Epoch 3525, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,199: INFO: model_training: Rank 0, Epoch 3525, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,200: INFO: model_training: Rank 0, Epoch 3526, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,202: INFO: model_training: Rank 0, Epoch 3526, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,203: INFO: model_training: Rank 0, Epoch 3526, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,205: INFO: model_training: Rank 0, Epoch 3526, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,206: INFO: model_training: Rank 0, Epoch 3526, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,207: INFO: model_training: Rank 0, Epoch 3527, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,209: INFO: model_training: Rank 0, Epoch 3527, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,211: INFO: model_training: Rank 0, Epoch 3527, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,212: INFO: model_training: Rank 0, Epoch 3527, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,213: INFO: model_training: Rank 0, Epoch 3527, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,214: INFO: model_training: Rank 0, Epoch 3528, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,216: INFO: model_training: Rank 0, Epoch 3528, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,217: INFO: model_training: Rank 0, Epoch 3528, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,218: INFO: model_training: Rank 0, Epoch 3528, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,219: INFO: model_training: Rank 0, Epoch 3528, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,221: INFO: model_training: Rank 0, Epoch 3529, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,222: INFO: model_training: Rank 0, Epoch 3529, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,223: INFO: model_training: Rank 0, Epoch 3529, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,225: INFO: model_training: Rank 0, Epoch 3529, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,226: INFO: model_training: Rank 0, Epoch 3529, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,227: INFO: model_training: Rank 0, Epoch 3530, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,229: INFO: model_training: Rank 0, Epoch 3530, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,230: INFO: model_training: Rank 0, Epoch 3530, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,231: INFO: model_training: Rank 0, Epoch 3530, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,232: INFO: model_training: Rank 0, Epoch 3530, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,234: INFO: model_training: Rank 0, Epoch 3531, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,235: INFO: model_training: Rank 0, Epoch 3531, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,237: INFO: model_training: Rank 0, Epoch 3531, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,238: INFO: model_training: Rank 0, Epoch 3531, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,239: INFO: model_training: Rank 0, Epoch 3531, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,240: INFO: model_training: Rank 0, Epoch 3532, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,242: INFO: model_training: Rank 0, Epoch 3532, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,243: INFO: model_training: Rank 0, Epoch 3532, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,244: INFO: model_training: Rank 0, Epoch 3532, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,245: INFO: model_training: Rank 0, Epoch 3532, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,247: INFO: model_training: Rank 0, Epoch 3533, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,249: INFO: model_training: Rank 0, Epoch 3533, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,250: INFO: model_training: Rank 0, Epoch 3533, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,251: INFO: model_training: Rank 0, Epoch 3533, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,252: INFO: model_training: Rank 0, Epoch 3533, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,254: INFO: model_training: Rank 0, Epoch 3534, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,255: INFO: model_training: Rank 0, Epoch 3534, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,256: INFO: model_training: Rank 0, Epoch 3534, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,257: INFO: model_training: Rank 0, Epoch 3534, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,259: INFO: model_training: Rank 0, Epoch 3534, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,260: INFO: model_training: Rank 0, Epoch 3535, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,261: INFO: model_training: Rank 0, Epoch 3535, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,263: INFO: model_training: Rank 0, Epoch 3535, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,264: INFO: model_training: Rank 0, Epoch 3535, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,265: INFO: model_training: Rank 0, Epoch 3535, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,266: INFO: model_training: Rank 0, Epoch 3536, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,267: INFO: model_training: Rank 0, Epoch 3536, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,269: INFO: model_training: Rank 0, Epoch 3536, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,270: INFO: model_training: Rank 0, Epoch 3536, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,271: INFO: model_training: Rank 0, Epoch 3536, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,273: INFO: model_training: Rank 0, Epoch 3537, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,274: INFO: model_training: Rank 0, Epoch 3537, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,276: INFO: model_training: Rank 0, Epoch 3537, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,277: INFO: model_training: Rank 0, Epoch 3537, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,279: INFO: model_training: Rank 0, Epoch 3537, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,280: INFO: model_training: Rank 0, Epoch 3538, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,281: INFO: model_training: Rank 0, Epoch 3538, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,282: INFO: model_training: Rank 0, Epoch 3538, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,284: INFO: model_training: Rank 0, Epoch 3538, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,285: INFO: model_training: Rank 0, Epoch 3538, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,287: INFO: model_training: Rank 0, Epoch 3539, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,288: INFO: model_training: Rank 0, Epoch 3539, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,289: INFO: model_training: Rank 0, Epoch 3539, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,290: INFO: model_training: Rank 0, Epoch 3539, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,292: INFO: model_training: Rank 0, Epoch 3539, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,293: INFO: model_training: Rank 0, Epoch 3540, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,294: INFO: model_training: Rank 0, Epoch 3540, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,296: INFO: model_training: Rank 0, Epoch 3540, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,297: INFO: model_training: Rank 0, Epoch 3540, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,299: INFO: model_training: Rank 0, Epoch 3540, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,300: INFO: model_training: Rank 0, Epoch 3541, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,301: INFO: model_training: Rank 0, Epoch 3541, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,302: INFO: model_training: Rank 0, Epoch 3541, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,304: INFO: model_training: Rank 0, Epoch 3541, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,305: INFO: model_training: Rank 0, Epoch 3541, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,306: INFO: model_training: Rank 0, Epoch 3542, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,307: INFO: model_training: Rank 0, Epoch 3542, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,309: INFO: model_training: Rank 0, Epoch 3542, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,310: INFO: model_training: Rank 0, Epoch 3542, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,311: INFO: model_training: Rank 0, Epoch 3542, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,313: INFO: model_training: Rank 0, Epoch 3543, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,314: INFO: model_training: Rank 0, Epoch 3543, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,315: INFO: model_training: Rank 0, Epoch 3543, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,317: INFO: model_training: Rank 0, Epoch 3543, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,318: INFO: model_training: Rank 0, Epoch 3543, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,319: INFO: model_training: Rank 0, Epoch 3544, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,320: INFO: model_training: Rank 0, Epoch 3544, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,322: INFO: model_training: Rank 0, Epoch 3544, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,324: INFO: model_training: Rank 0, Epoch 3544, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,325: INFO: model_training: Rank 0, Epoch 3544, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,326: INFO: model_training: Rank 0, Epoch 3545, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,327: INFO: model_training: Rank 0, Epoch 3545, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,329: INFO: model_training: Rank 0, Epoch 3545, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,330: INFO: model_training: Rank 0, Epoch 3545, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,331: INFO: model_training: Rank 0, Epoch 3545, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,332: INFO: model_training: Rank 0, Epoch 3546, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,334: INFO: model_training: Rank 0, Epoch 3546, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,335: INFO: model_training: Rank 0, Epoch 3546, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,336: INFO: model_training: Rank 0, Epoch 3546, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,338: INFO: model_training: Rank 0, Epoch 3546, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,339: INFO: model_training: Rank 0, Epoch 3547, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,340: INFO: model_training: Rank 0, Epoch 3547, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,341: INFO: model_training: Rank 0, Epoch 3547, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,343: INFO: model_training: Rank 0, Epoch 3547, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,344: INFO: model_training: Rank 0, Epoch 3547, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,346: INFO: model_training: Rank 0, Epoch 3548, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,347: INFO: model_training: Rank 0, Epoch 3548, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,349: INFO: model_training: Rank 0, Epoch 3548, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,350: INFO: model_training: Rank 0, Epoch 3548, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,351: INFO: model_training: Rank 0, Epoch 3548, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,353: INFO: model_training: Rank 0, Epoch 3549, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,354: INFO: model_training: Rank 0, Epoch 3549, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,355: INFO: model_training: Rank 0, Epoch 3549, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,356: INFO: model_training: Rank 0, Epoch 3549, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,358: INFO: model_training: Rank 0, Epoch 3549, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,359: INFO: model_training: Rank 0, Epoch 3550, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,361: INFO: model_training: Rank 0, Epoch 3550, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,362: INFO: model_training: Rank 0, Epoch 3550, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,363: INFO: model_training: Rank 0, Epoch 3550, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,365: INFO: model_training: Rank 0, Epoch 3550, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,366: INFO: model_training: Rank 0, Epoch 3551, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,367: INFO: model_training: Rank 0, Epoch 3551, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,368: INFO: model_training: Rank 0, Epoch 3551, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,370: INFO: model_training: Rank 0, Epoch 3551, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,372: INFO: model_training: Rank 0, Epoch 3551, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,373: INFO: model_training: Rank 0, Epoch 3552, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,374: INFO: model_training: Rank 0, Epoch 3552, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,375: INFO: model_training: Rank 0, Epoch 3552, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,377: INFO: model_training: Rank 0, Epoch 3552, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,378: INFO: model_training: Rank 0, Epoch 3552, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,379: INFO: model_training: Rank 0, Epoch 3553, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,380: INFO: model_training: Rank 0, Epoch 3553, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,382: INFO: model_training: Rank 0, Epoch 3553, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,383: INFO: model_training: Rank 0, Epoch 3553, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,384: INFO: model_training: Rank 0, Epoch 3553, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,386: INFO: model_training: Rank 0, Epoch 3554, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,387: INFO: model_training: Rank 0, Epoch 3554, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,388: INFO: model_training: Rank 0, Epoch 3554, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,389: INFO: model_training: Rank 0, Epoch 3554, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,390: INFO: model_training: Rank 0, Epoch 3554, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,392: INFO: model_training: Rank 0, Epoch 3555, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,394: INFO: model_training: Rank 0, Epoch 3555, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,395: INFO: model_training: Rank 0, Epoch 3555, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,396: INFO: model_training: Rank 0, Epoch 3555, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,397: INFO: model_training: Rank 0, Epoch 3555, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,398: INFO: model_training: Rank 0, Epoch 3556, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,400: INFO: model_training: Rank 0, Epoch 3556, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,401: INFO: model_training: Rank 0, Epoch 3556, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,402: INFO: model_training: Rank 0, Epoch 3556, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,404: INFO: model_training: Rank 0, Epoch 3556, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,405: INFO: model_training: Rank 0, Epoch 3557, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,406: INFO: model_training: Rank 0, Epoch 3557, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,408: INFO: model_training: Rank 0, Epoch 3557, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,409: INFO: model_training: Rank 0, Epoch 3557, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,410: INFO: model_training: Rank 0, Epoch 3557, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,411: INFO: model_training: Rank 0, Epoch 3558, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,413: INFO: model_training: Rank 0, Epoch 3558, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,414: INFO: model_training: Rank 0, Epoch 3558, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,416: INFO: model_training: Rank 0, Epoch 3558, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,417: INFO: model_training: Rank 0, Epoch 3558, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,418: INFO: model_training: Rank 0, Epoch 3559, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,420: INFO: model_training: Rank 0, Epoch 3559, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,421: INFO: model_training: Rank 0, Epoch 3559, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,422: INFO: model_training: Rank 0, Epoch 3559, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,423: INFO: model_training: Rank 0, Epoch 3559, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,425: INFO: model_training: Rank 0, Epoch 3560, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,426: INFO: model_training: Rank 0, Epoch 3560, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,428: INFO: model_training: Rank 0, Epoch 3560, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,429: INFO: model_training: Rank 0, Epoch 3560, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,431: INFO: model_training: Rank 0, Epoch 3560, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,433: INFO: model_training: Rank 0, Epoch 3561, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,435: INFO: model_training: Rank 0, Epoch 3561, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,437: INFO: model_training: Rank 0, Epoch 3561, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,438: INFO: model_training: Rank 0, Epoch 3561, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,440: INFO: model_training: Rank 0, Epoch 3561, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,442: INFO: model_training: Rank 0, Epoch 3562, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,444: INFO: model_training: Rank 0, Epoch 3562, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,445: INFO: model_training: Rank 0, Epoch 3562, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,446: INFO: model_training: Rank 0, Epoch 3562, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,448: INFO: model_training: Rank 0, Epoch 3562, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,450: INFO: model_training: Rank 0, Epoch 3563, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,451: INFO: model_training: Rank 0, Epoch 3563, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,452: INFO: model_training: Rank 0, Epoch 3563, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,453: INFO: model_training: Rank 0, Epoch 3563, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,455: INFO: model_training: Rank 0, Epoch 3563, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,456: INFO: model_training: Rank 0, Epoch 3564, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,458: INFO: model_training: Rank 0, Epoch 3564, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,459: INFO: model_training: Rank 0, Epoch 3564, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,460: INFO: model_training: Rank 0, Epoch 3564, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,462: INFO: model_training: Rank 0, Epoch 3564, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,463: INFO: model_training: Rank 0, Epoch 3565, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,464: INFO: model_training: Rank 0, Epoch 3565, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,465: INFO: model_training: Rank 0, Epoch 3565, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,467: INFO: model_training: Rank 0, Epoch 3565, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,468: INFO: model_training: Rank 0, Epoch 3565, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,470: INFO: model_training: Rank 0, Epoch 3566, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,471: INFO: model_training: Rank 0, Epoch 3566, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,472: INFO: model_training: Rank 0, Epoch 3566, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,474: INFO: model_training: Rank 0, Epoch 3566, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,475: INFO: model_training: Rank 0, Epoch 3566, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,476: INFO: model_training: Rank 0, Epoch 3567, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,478: INFO: model_training: Rank 0, Epoch 3567, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,479: INFO: model_training: Rank 0, Epoch 3567, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,480: INFO: model_training: Rank 0, Epoch 3567, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,482: INFO: model_training: Rank 0, Epoch 3567, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,483: INFO: model_training: Rank 0, Epoch 3568, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,485: INFO: model_training: Rank 0, Epoch 3568, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,486: INFO: model_training: Rank 0, Epoch 3568, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,487: INFO: model_training: Rank 0, Epoch 3568, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,489: INFO: model_training: Rank 0, Epoch 3568, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,490: INFO: model_training: Rank 0, Epoch 3569, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,491: INFO: model_training: Rank 0, Epoch 3569, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,492: INFO: model_training: Rank 0, Epoch 3569, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,494: INFO: model_training: Rank 0, Epoch 3569, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,496: INFO: model_training: Rank 0, Epoch 3569, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,497: INFO: model_training: Rank 0, Epoch 3570, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,498: INFO: model_training: Rank 0, Epoch 3570, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,500: INFO: model_training: Rank 0, Epoch 3570, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,501: INFO: model_training: Rank 0, Epoch 3570, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,502: INFO: model_training: Rank 0, Epoch 3570, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,503: INFO: model_training: Rank 0, Epoch 3571, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,505: INFO: model_training: Rank 0, Epoch 3571, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,506: INFO: model_training: Rank 0, Epoch 3571, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,508: INFO: model_training: Rank 0, Epoch 3571, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,510: INFO: model_training: Rank 0, Epoch 3571, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,511: INFO: model_training: Rank 0, Epoch 3572, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,512: INFO: model_training: Rank 0, Epoch 3572, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,514: INFO: model_training: Rank 0, Epoch 3572, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,515: INFO: model_training: Rank 0, Epoch 3572, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,516: INFO: model_training: Rank 0, Epoch 3572, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,517: INFO: model_training: Rank 0, Epoch 3573, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,519: INFO: model_training: Rank 0, Epoch 3573, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,520: INFO: model_training: Rank 0, Epoch 3573, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,522: INFO: model_training: Rank 0, Epoch 3573, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,523: INFO: model_training: Rank 0, Epoch 3573, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,524: INFO: model_training: Rank 0, Epoch 3574, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,525: INFO: model_training: Rank 0, Epoch 3574, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,527: INFO: model_training: Rank 0, Epoch 3574, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,528: INFO: model_training: Rank 0, Epoch 3574, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,529: INFO: model_training: Rank 0, Epoch 3574, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,530: INFO: model_training: Rank 0, Epoch 3575, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,532: INFO: model_training: Rank 0, Epoch 3575, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,534: INFO: model_training: Rank 0, Epoch 3575, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,535: INFO: model_training: Rank 0, Epoch 3575, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,537: INFO: model_training: Rank 0, Epoch 3575, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,538: INFO: model_training: Rank 0, Epoch 3576, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,539: INFO: model_training: Rank 0, Epoch 3576, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,541: INFO: model_training: Rank 0, Epoch 3576, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,542: INFO: model_training: Rank 0, Epoch 3576, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,543: INFO: model_training: Rank 0, Epoch 3576, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,544: INFO: model_training: Rank 0, Epoch 3577, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,546: INFO: model_training: Rank 0, Epoch 3577, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,547: INFO: model_training: Rank 0, Epoch 3577, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,548: INFO: model_training: Rank 0, Epoch 3577, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,549: INFO: model_training: Rank 0, Epoch 3577, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,551: INFO: model_training: Rank 0, Epoch 3578, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,552: INFO: model_training: Rank 0, Epoch 3578, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,553: INFO: model_training: Rank 0, Epoch 3578, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,554: INFO: model_training: Rank 0, Epoch 3578, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,555: INFO: model_training: Rank 0, Epoch 3578, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,556: INFO: model_training: Rank 0, Epoch 3579, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,557: INFO: model_training: Rank 0, Epoch 3579, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,559: INFO: model_training: Rank 0, Epoch 3579, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,560: INFO: model_training: Rank 0, Epoch 3579, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,562: INFO: model_training: Rank 0, Epoch 3579, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,563: INFO: model_training: Rank 0, Epoch 3580, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,564: INFO: model_training: Rank 0, Epoch 3580, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,565: INFO: model_training: Rank 0, Epoch 3580, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,567: INFO: model_training: Rank 0, Epoch 3580, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,568: INFO: model_training: Rank 0, Epoch 3580, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,569: INFO: model_training: Rank 0, Epoch 3581, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,570: INFO: model_training: Rank 0, Epoch 3581, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,572: INFO: model_training: Rank 0, Epoch 3581, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,573: INFO: model_training: Rank 0, Epoch 3581, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,574: INFO: model_training: Rank 0, Epoch 3581, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,576: INFO: model_training: Rank 0, Epoch 3582, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,577: INFO: model_training: Rank 0, Epoch 3582, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,578: INFO: model_training: Rank 0, Epoch 3582, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,579: INFO: model_training: Rank 0, Epoch 3582, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,580: INFO: model_training: Rank 0, Epoch 3582, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,582: INFO: model_training: Rank 0, Epoch 3583, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,583: INFO: model_training: Rank 0, Epoch 3583, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,585: INFO: model_training: Rank 0, Epoch 3583, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,586: INFO: model_training: Rank 0, Epoch 3583, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,587: INFO: model_training: Rank 0, Epoch 3583, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,589: INFO: model_training: Rank 0, Epoch 3584, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,590: INFO: model_training: Rank 0, Epoch 3584, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,591: INFO: model_training: Rank 0, Epoch 3584, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,592: INFO: model_training: Rank 0, Epoch 3584, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,593: INFO: model_training: Rank 0, Epoch 3584, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,595: INFO: model_training: Rank 0, Epoch 3585, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,596: INFO: model_training: Rank 0, Epoch 3585, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,598: INFO: model_training: Rank 0, Epoch 3585, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,599: INFO: model_training: Rank 0, Epoch 3585, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,601: INFO: model_training: Rank 0, Epoch 3585, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,602: INFO: model_training: Rank 0, Epoch 3586, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,603: INFO: model_training: Rank 0, Epoch 3586, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,605: INFO: model_training: Rank 0, Epoch 3586, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,606: INFO: model_training: Rank 0, Epoch 3586, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,607: INFO: model_training: Rank 0, Epoch 3586, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,608: INFO: model_training: Rank 0, Epoch 3587, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,610: INFO: model_training: Rank 0, Epoch 3587, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,611: INFO: model_training: Rank 0, Epoch 3587, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,612: INFO: model_training: Rank 0, Epoch 3587, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,613: INFO: model_training: Rank 0, Epoch 3587, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,614: INFO: model_training: Rank 0, Epoch 3588, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,616: INFO: model_training: Rank 0, Epoch 3588, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,617: INFO: model_training: Rank 0, Epoch 3588, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,618: INFO: model_training: Rank 0, Epoch 3588, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,620: INFO: model_training: Rank 0, Epoch 3588, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,621: INFO: model_training: Rank 0, Epoch 3589, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,623: INFO: model_training: Rank 0, Epoch 3589, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,624: INFO: model_training: Rank 0, Epoch 3589, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,625: INFO: model_training: Rank 0, Epoch 3589, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,626: INFO: model_training: Rank 0, Epoch 3589, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,627: INFO: model_training: Rank 0, Epoch 3590, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,629: INFO: model_training: Rank 0, Epoch 3590, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,630: INFO: model_training: Rank 0, Epoch 3590, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,631: INFO: model_training: Rank 0, Epoch 3590, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,633: INFO: model_training: Rank 0, Epoch 3590, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,634: INFO: model_training: Rank 0, Epoch 3591, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,635: INFO: model_training: Rank 0, Epoch 3591, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,637: INFO: model_training: Rank 0, Epoch 3591, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,638: INFO: model_training: Rank 0, Epoch 3591, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,639: INFO: model_training: Rank 0, Epoch 3591, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,640: INFO: model_training: Rank 0, Epoch 3592, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,641: INFO: model_training: Rank 0, Epoch 3592, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,643: INFO: model_training: Rank 0, Epoch 3592, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,644: INFO: model_training: Rank 0, Epoch 3592, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,646: INFO: model_training: Rank 0, Epoch 3592, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,647: INFO: model_training: Rank 0, Epoch 3593, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,649: INFO: model_training: Rank 0, Epoch 3593, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,650: INFO: model_training: Rank 0, Epoch 3593, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,651: INFO: model_training: Rank 0, Epoch 3593, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,652: INFO: model_training: Rank 0, Epoch 3593, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,654: INFO: model_training: Rank 0, Epoch 3594, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,655: INFO: model_training: Rank 0, Epoch 3594, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,656: INFO: model_training: Rank 0, Epoch 3594, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,657: INFO: model_training: Rank 0, Epoch 3594, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,659: INFO: model_training: Rank 0, Epoch 3594, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,661: INFO: model_training: Rank 0, Epoch 3595, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,662: INFO: model_training: Rank 0, Epoch 3595, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,663: INFO: model_training: Rank 0, Epoch 3595, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,664: INFO: model_training: Rank 0, Epoch 3595, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,666: INFO: model_training: Rank 0, Epoch 3595, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,667: INFO: model_training: Rank 0, Epoch 3596, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,668: INFO: model_training: Rank 0, Epoch 3596, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,669: INFO: model_training: Rank 0, Epoch 3596, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,671: INFO: model_training: Rank 0, Epoch 3596, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,672: INFO: model_training: Rank 0, Epoch 3596, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,673: INFO: model_training: Rank 0, Epoch 3597, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,674: INFO: model_training: Rank 0, Epoch 3597, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,676: INFO: model_training: Rank 0, Epoch 3597, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,677: INFO: model_training: Rank 0, Epoch 3597, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,678: INFO: model_training: Rank 0, Epoch 3597, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,679: INFO: model_training: Rank 0, Epoch 3598, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,681: INFO: model_training: Rank 0, Epoch 3598, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,682: INFO: model_training: Rank 0, Epoch 3598, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,683: INFO: model_training: Rank 0, Epoch 3598, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,685: INFO: model_training: Rank 0, Epoch 3598, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,686: INFO: model_training: Rank 0, Epoch 3599, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,687: INFO: model_training: Rank 0, Epoch 3599, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,688: INFO: model_training: Rank 0, Epoch 3599, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,689: INFO: model_training: Rank 0, Epoch 3599, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,691: INFO: model_training: Rank 0, Epoch 3599, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,692: INFO: model_training: Rank 0, Epoch 3600, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,693: INFO: model_training: Rank 0, Epoch 3600, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,694: INFO: model_training: Rank 0, Epoch 3600, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,696: INFO: model_training: Rank 0, Epoch 3600, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,697: INFO: model_training: Rank 0, Epoch 3600, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,698: INFO: model_training: Rank 0, Epoch 3601, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,699: INFO: model_training: Rank 0, Epoch 3601, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,701: INFO: model_training: Rank 0, Epoch 3601, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,702: INFO: model_training: Rank 0, Epoch 3601, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,703: INFO: model_training: Rank 0, Epoch 3601, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,704: INFO: model_training: Rank 0, Epoch 3602, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,705: INFO: model_training: Rank 0, Epoch 3602, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,707: INFO: model_training: Rank 0, Epoch 3602, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,708: INFO: model_training: Rank 0, Epoch 3602, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,710: INFO: model_training: Rank 0, Epoch 3602, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,711: INFO: model_training: Rank 0, Epoch 3603, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,713: INFO: model_training: Rank 0, Epoch 3603, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,714: INFO: model_training: Rank 0, Epoch 3603, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,716: INFO: model_training: Rank 0, Epoch 3603, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,717: INFO: model_training: Rank 0, Epoch 3603, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,719: INFO: model_training: Rank 0, Epoch 3604, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,720: INFO: model_training: Rank 0, Epoch 3604, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,722: INFO: model_training: Rank 0, Epoch 3604, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,723: INFO: model_training: Rank 0, Epoch 3604, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,724: INFO: model_training: Rank 0, Epoch 3604, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,725: INFO: model_training: Rank 0, Epoch 3605, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,727: INFO: model_training: Rank 0, Epoch 3605, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,728: INFO: model_training: Rank 0, Epoch 3605, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,729: INFO: model_training: Rank 0, Epoch 3605, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,731: INFO: model_training: Rank 0, Epoch 3605, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,732: INFO: model_training: Rank 0, Epoch 3606, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,734: INFO: model_training: Rank 0, Epoch 3606, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,735: INFO: model_training: Rank 0, Epoch 3606, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,736: INFO: model_training: Rank 0, Epoch 3606, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,737: INFO: model_training: Rank 0, Epoch 3606, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,739: INFO: model_training: Rank 0, Epoch 3607, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,740: INFO: model_training: Rank 0, Epoch 3607, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,741: INFO: model_training: Rank 0, Epoch 3607, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,742: INFO: model_training: Rank 0, Epoch 3607, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,743: INFO: model_training: Rank 0, Epoch 3607, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,745: INFO: model_training: Rank 0, Epoch 3608, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,746: INFO: model_training: Rank 0, Epoch 3608, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,747: INFO: model_training: Rank 0, Epoch 3608, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,749: INFO: model_training: Rank 0, Epoch 3608, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,750: INFO: model_training: Rank 0, Epoch 3608, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,751: INFO: model_training: Rank 0, Epoch 3609, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,753: INFO: model_training: Rank 0, Epoch 3609, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,754: INFO: model_training: Rank 0, Epoch 3609, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,755: INFO: model_training: Rank 0, Epoch 3609, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,756: INFO: model_training: Rank 0, Epoch 3609, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,758: INFO: model_training: Rank 0, Epoch 3610, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,760: INFO: model_training: Rank 0, Epoch 3610, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,761: INFO: model_training: Rank 0, Epoch 3610, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,762: INFO: model_training: Rank 0, Epoch 3610, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,763: INFO: model_training: Rank 0, Epoch 3610, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,765: INFO: model_training: Rank 0, Epoch 3611, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,766: INFO: model_training: Rank 0, Epoch 3611, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,767: INFO: model_training: Rank 0, Epoch 3611, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,769: INFO: model_training: Rank 0, Epoch 3611, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,770: INFO: model_training: Rank 0, Epoch 3611, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,772: INFO: model_training: Rank 0, Epoch 3612, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,773: INFO: model_training: Rank 0, Epoch 3612, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,774: INFO: model_training: Rank 0, Epoch 3612, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,775: INFO: model_training: Rank 0, Epoch 3612, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,776: INFO: model_training: Rank 0, Epoch 3612, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,778: INFO: model_training: Rank 0, Epoch 3613, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,779: INFO: model_training: Rank 0, Epoch 3613, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,780: INFO: model_training: Rank 0, Epoch 3613, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,781: INFO: model_training: Rank 0, Epoch 3613, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,783: INFO: model_training: Rank 0, Epoch 3613, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,784: INFO: model_training: Rank 0, Epoch 3614, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,786: INFO: model_training: Rank 0, Epoch 3614, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,787: INFO: model_training: Rank 0, Epoch 3614, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,788: INFO: model_training: Rank 0, Epoch 3614, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,790: INFO: model_training: Rank 0, Epoch 3614, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,791: INFO: model_training: Rank 0, Epoch 3615, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,792: INFO: model_training: Rank 0, Epoch 3615, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,794: INFO: model_training: Rank 0, Epoch 3615, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,795: INFO: model_training: Rank 0, Epoch 3615, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,796: INFO: model_training: Rank 0, Epoch 3615, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,797: INFO: model_training: Rank 0, Epoch 3616, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,799: INFO: model_training: Rank 0, Epoch 3616, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,800: INFO: model_training: Rank 0, Epoch 3616, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,801: INFO: model_training: Rank 0, Epoch 3616, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,802: INFO: model_training: Rank 0, Epoch 3616, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,803: INFO: model_training: Rank 0, Epoch 3617, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,806: INFO: model_training: Rank 0, Epoch 3617, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,807: INFO: model_training: Rank 0, Epoch 3617, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,808: INFO: model_training: Rank 0, Epoch 3617, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,809: INFO: model_training: Rank 0, Epoch 3617, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,811: INFO: model_training: Rank 0, Epoch 3618, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,812: INFO: model_training: Rank 0, Epoch 3618, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,813: INFO: model_training: Rank 0, Epoch 3618, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,814: INFO: model_training: Rank 0, Epoch 3618, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,815: INFO: model_training: Rank 0, Epoch 3618, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,816: INFO: model_training: Rank 0, Epoch 3619, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,817: INFO: model_training: Rank 0, Epoch 3619, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,818: INFO: model_training: Rank 0, Epoch 3619, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,819: INFO: model_training: Rank 0, Epoch 3619, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,820: INFO: model_training: Rank 0, Epoch 3619, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,821: INFO: model_training: Rank 0, Epoch 3620, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,822: INFO: model_training: Rank 0, Epoch 3620, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,823: INFO: model_training: Rank 0, Epoch 3620, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,824: INFO: model_training: Rank 0, Epoch 3620, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,825: INFO: model_training: Rank 0, Epoch 3620, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,827: INFO: model_training: Rank 0, Epoch 3621, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,828: INFO: model_training: Rank 0, Epoch 3621, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,829: INFO: model_training: Rank 0, Epoch 3621, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,830: INFO: model_training: Rank 0, Epoch 3621, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,831: INFO: model_training: Rank 0, Epoch 3621, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,832: INFO: model_training: Rank 0, Epoch 3622, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,833: INFO: model_training: Rank 0, Epoch 3622, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,834: INFO: model_training: Rank 0, Epoch 3622, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,835: INFO: model_training: Rank 0, Epoch 3622, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,836: INFO: model_training: Rank 0, Epoch 3622, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,838: INFO: model_training: Rank 0, Epoch 3623, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,839: INFO: model_training: Rank 0, Epoch 3623, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,840: INFO: model_training: Rank 0, Epoch 3623, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,841: INFO: model_training: Rank 0, Epoch 3623, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,842: INFO: model_training: Rank 0, Epoch 3623, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,843: INFO: model_training: Rank 0, Epoch 3624, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,844: INFO: model_training: Rank 0, Epoch 3624, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,845: INFO: model_training: Rank 0, Epoch 3624, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,847: INFO: model_training: Rank 0, Epoch 3624, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,848: INFO: model_training: Rank 0, Epoch 3624, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,849: INFO: model_training: Rank 0, Epoch 3625, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,850: INFO: model_training: Rank 0, Epoch 3625, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,851: INFO: model_training: Rank 0, Epoch 3625, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,852: INFO: model_training: Rank 0, Epoch 3625, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,853: INFO: model_training: Rank 0, Epoch 3625, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,855: INFO: model_training: Rank 0, Epoch 3626, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,856: INFO: model_training: Rank 0, Epoch 3626, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,857: INFO: model_training: Rank 0, Epoch 3626, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,858: INFO: model_training: Rank 0, Epoch 3626, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,859: INFO: model_training: Rank 0, Epoch 3626, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,860: INFO: model_training: Rank 0, Epoch 3627, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,861: INFO: model_training: Rank 0, Epoch 3627, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,863: INFO: model_training: Rank 0, Epoch 3627, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,864: INFO: model_training: Rank 0, Epoch 3627, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,865: INFO: model_training: Rank 0, Epoch 3627, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,866: INFO: model_training: Rank 0, Epoch 3628, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,867: INFO: model_training: Rank 0, Epoch 3628, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,868: INFO: model_training: Rank 0, Epoch 3628, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,869: INFO: model_training: Rank 0, Epoch 3628, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,870: INFO: model_training: Rank 0, Epoch 3628, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,871: INFO: model_training: Rank 0, Epoch 3629, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,872: INFO: model_training: Rank 0, Epoch 3629, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,873: INFO: model_training: Rank 0, Epoch 3629, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,874: INFO: model_training: Rank 0, Epoch 3629, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,875: INFO: model_training: Rank 0, Epoch 3629, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,876: INFO: model_training: Rank 0, Epoch 3630, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,877: INFO: model_training: Rank 0, Epoch 3630, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,878: INFO: model_training: Rank 0, Epoch 3630, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,879: INFO: model_training: Rank 0, Epoch 3630, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,880: INFO: model_training: Rank 0, Epoch 3630, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,882: INFO: model_training: Rank 0, Epoch 3631, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,883: INFO: model_training: Rank 0, Epoch 3631, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,884: INFO: model_training: Rank 0, Epoch 3631, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,885: INFO: model_training: Rank 0, Epoch 3631, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,886: INFO: model_training: Rank 0, Epoch 3631, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,887: INFO: model_training: Rank 0, Epoch 3632, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,888: INFO: model_training: Rank 0, Epoch 3632, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,889: INFO: model_training: Rank 0, Epoch 3632, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,890: INFO: model_training: Rank 0, Epoch 3632, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,891: INFO: model_training: Rank 0, Epoch 3632, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,892: INFO: model_training: Rank 0, Epoch 3633, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,893: INFO: model_training: Rank 0, Epoch 3633, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,895: INFO: model_training: Rank 0, Epoch 3633, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,896: INFO: model_training: Rank 0, Epoch 3633, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,897: INFO: model_training: Rank 0, Epoch 3633, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,899: INFO: model_training: Rank 0, Epoch 3634, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,900: INFO: model_training: Rank 0, Epoch 3634, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,901: INFO: model_training: Rank 0, Epoch 3634, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,902: INFO: model_training: Rank 0, Epoch 3634, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,903: INFO: model_training: Rank 0, Epoch 3634, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,904: INFO: model_training: Rank 0, Epoch 3635, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,905: INFO: model_training: Rank 0, Epoch 3635, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,906: INFO: model_training: Rank 0, Epoch 3635, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,908: INFO: model_training: Rank 0, Epoch 3635, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,909: INFO: model_training: Rank 0, Epoch 3635, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,910: INFO: model_training: Rank 0, Epoch 3636, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,911: INFO: model_training: Rank 0, Epoch 3636, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,912: INFO: model_training: Rank 0, Epoch 3636, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,913: INFO: model_training: Rank 0, Epoch 3636, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,914: INFO: model_training: Rank 0, Epoch 3636, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,915: INFO: model_training: Rank 0, Epoch 3637, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,916: INFO: model_training: Rank 0, Epoch 3637, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,917: INFO: model_training: Rank 0, Epoch 3637, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,918: INFO: model_training: Rank 0, Epoch 3637, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,920: INFO: model_training: Rank 0, Epoch 3637, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,921: INFO: model_training: Rank 0, Epoch 3638, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,922: INFO: model_training: Rank 0, Epoch 3638, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,923: INFO: model_training: Rank 0, Epoch 3638, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,924: INFO: model_training: Rank 0, Epoch 3638, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,925: INFO: model_training: Rank 0, Epoch 3638, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,926: INFO: model_training: Rank 0, Epoch 3639, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,927: INFO: model_training: Rank 0, Epoch 3639, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,928: INFO: model_training: Rank 0, Epoch 3639, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,929: INFO: model_training: Rank 0, Epoch 3639, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,930: INFO: model_training: Rank 0, Epoch 3639, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,932: INFO: model_training: Rank 0, Epoch 3640, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,933: INFO: model_training: Rank 0, Epoch 3640, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,934: INFO: model_training: Rank 0, Epoch 3640, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,935: INFO: model_training: Rank 0, Epoch 3640, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,936: INFO: model_training: Rank 0, Epoch 3640, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,937: INFO: model_training: Rank 0, Epoch 3641, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,938: INFO: model_training: Rank 0, Epoch 3641, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,939: INFO: model_training: Rank 0, Epoch 3641, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,940: INFO: model_training: Rank 0, Epoch 3641, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,941: INFO: model_training: Rank 0, Epoch 3641, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,942: INFO: model_training: Rank 0, Epoch 3642, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,943: INFO: model_training: Rank 0, Epoch 3642, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,944: INFO: model_training: Rank 0, Epoch 3642, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,945: INFO: model_training: Rank 0, Epoch 3642, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,946: INFO: model_training: Rank 0, Epoch 3642, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,947: INFO: model_training: Rank 0, Epoch 3643, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,948: INFO: model_training: Rank 0, Epoch 3643, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,949: INFO: model_training: Rank 0, Epoch 3643, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,950: INFO: model_training: Rank 0, Epoch 3643, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,951: INFO: model_training: Rank 0, Epoch 3643, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,952: INFO: model_training: Rank 0, Epoch 3644, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,953: INFO: model_training: Rank 0, Epoch 3644, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,954: INFO: model_training: Rank 0, Epoch 3644, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,955: INFO: model_training: Rank 0, Epoch 3644, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,957: INFO: model_training: Rank 0, Epoch 3644, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,958: INFO: model_training: Rank 0, Epoch 3645, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,959: INFO: model_training: Rank 0, Epoch 3645, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,960: INFO: model_training: Rank 0, Epoch 3645, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,961: INFO: model_training: Rank 0, Epoch 3645, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,962: INFO: model_training: Rank 0, Epoch 3645, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,964: INFO: model_training: Rank 0, Epoch 3646, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,965: INFO: model_training: Rank 0, Epoch 3646, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,966: INFO: model_training: Rank 0, Epoch 3646, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,967: INFO: model_training: Rank 0, Epoch 3646, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,968: INFO: model_training: Rank 0, Epoch 3646, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,969: INFO: model_training: Rank 0, Epoch 3647, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,970: INFO: model_training: Rank 0, Epoch 3647, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,972: INFO: model_training: Rank 0, Epoch 3647, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,973: INFO: model_training: Rank 0, Epoch 3647, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,974: INFO: model_training: Rank 0, Epoch 3647, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,975: INFO: model_training: Rank 0, Epoch 3648, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,976: INFO: model_training: Rank 0, Epoch 3648, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,977: INFO: model_training: Rank 0, Epoch 3648, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,978: INFO: model_training: Rank 0, Epoch 3648, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,979: INFO: model_training: Rank 0, Epoch 3648, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,980: INFO: model_training: Rank 0, Epoch 3649, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,981: INFO: model_training: Rank 0, Epoch 3649, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,982: INFO: model_training: Rank 0, Epoch 3649, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,983: INFO: model_training: Rank 0, Epoch 3649, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,984: INFO: model_training: Rank 0, Epoch 3649, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,985: INFO: model_training: Rank 0, Epoch 3650, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,987: INFO: model_training: Rank 0, Epoch 3650, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,988: INFO: model_training: Rank 0, Epoch 3650, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,989: INFO: model_training: Rank 0, Epoch 3650, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,990: INFO: model_training: Rank 0, Epoch 3650, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,991: INFO: model_training: Rank 0, Epoch 3651, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,992: INFO: model_training: Rank 0, Epoch 3651, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,994: INFO: model_training: Rank 0, Epoch 3651, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:49,995: INFO: model_training: Rank 0, Epoch 3651, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:49,996: INFO: model_training: Rank 0, Epoch 3651, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:49,997: INFO: model_training: Rank 0, Epoch 3652, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:49,998: INFO: model_training: Rank 0, Epoch 3652, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:49,998: INFO: model_training: Rank 0, Epoch 3652, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,000: INFO: model_training: Rank 0, Epoch 3652, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,001: INFO: model_training: Rank 0, Epoch 3652, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,002: INFO: model_training: Rank 0, Epoch 3653, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,003: INFO: model_training: Rank 0, Epoch 3653, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,004: INFO: model_training: Rank 0, Epoch 3653, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,005: INFO: model_training: Rank 0, Epoch 3653, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,006: INFO: model_training: Rank 0, Epoch 3653, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,007: INFO: model_training: Rank 0, Epoch 3654, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,009: INFO: model_training: Rank 0, Epoch 3654, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,010: INFO: model_training: Rank 0, Epoch 3654, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,011: INFO: model_training: Rank 0, Epoch 3654, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,012: INFO: model_training: Rank 0, Epoch 3654, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,013: INFO: model_training: Rank 0, Epoch 3655, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,014: INFO: model_training: Rank 0, Epoch 3655, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,015: INFO: model_training: Rank 0, Epoch 3655, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,016: INFO: model_training: Rank 0, Epoch 3655, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,017: INFO: model_training: Rank 0, Epoch 3655, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,019: INFO: model_training: Rank 0, Epoch 3656, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,020: INFO: model_training: Rank 0, Epoch 3656, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,021: INFO: model_training: Rank 0, Epoch 3656, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,021: INFO: model_training: Rank 0, Epoch 3656, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,022: INFO: model_training: Rank 0, Epoch 3656, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,024: INFO: model_training: Rank 0, Epoch 3657, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,025: INFO: model_training: Rank 0, Epoch 3657, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,026: INFO: model_training: Rank 0, Epoch 3657, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,027: INFO: model_training: Rank 0, Epoch 3657, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,028: INFO: model_training: Rank 0, Epoch 3657, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,029: INFO: model_training: Rank 0, Epoch 3658, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,030: INFO: model_training: Rank 0, Epoch 3658, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,031: INFO: model_training: Rank 0, Epoch 3658, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,032: INFO: model_training: Rank 0, Epoch 3658, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,034: INFO: model_training: Rank 0, Epoch 3658, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,035: INFO: model_training: Rank 0, Epoch 3659, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,036: INFO: model_training: Rank 0, Epoch 3659, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,037: INFO: model_training: Rank 0, Epoch 3659, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,038: INFO: model_training: Rank 0, Epoch 3659, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,039: INFO: model_training: Rank 0, Epoch 3659, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,040: INFO: model_training: Rank 0, Epoch 3660, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,041: INFO: model_training: Rank 0, Epoch 3660, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,042: INFO: model_training: Rank 0, Epoch 3660, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,043: INFO: model_training: Rank 0, Epoch 3660, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,044: INFO: model_training: Rank 0, Epoch 3660, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,045: INFO: model_training: Rank 0, Epoch 3661, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,046: INFO: model_training: Rank 0, Epoch 3661, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,048: INFO: model_training: Rank 0, Epoch 3661, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,049: INFO: model_training: Rank 0, Epoch 3661, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,050: INFO: model_training: Rank 0, Epoch 3661, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,051: INFO: model_training: Rank 0, Epoch 3662, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,052: INFO: model_training: Rank 0, Epoch 3662, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,053: INFO: model_training: Rank 0, Epoch 3662, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,054: INFO: model_training: Rank 0, Epoch 3662, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,054: INFO: model_training: Rank 0, Epoch 3662, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,056: INFO: model_training: Rank 0, Epoch 3663, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,057: INFO: model_training: Rank 0, Epoch 3663, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,058: INFO: model_training: Rank 0, Epoch 3663, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,059: INFO: model_training: Rank 0, Epoch 3663, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,060: INFO: model_training: Rank 0, Epoch 3663, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,061: INFO: model_training: Rank 0, Epoch 3664, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,062: INFO: model_training: Rank 0, Epoch 3664, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,063: INFO: model_training: Rank 0, Epoch 3664, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,064: INFO: model_training: Rank 0, Epoch 3664, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,065: INFO: model_training: Rank 0, Epoch 3664, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,066: INFO: model_training: Rank 0, Epoch 3665, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,067: INFO: model_training: Rank 0, Epoch 3665, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,068: INFO: model_training: Rank 0, Epoch 3665, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,069: INFO: model_training: Rank 0, Epoch 3665, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,070: INFO: model_training: Rank 0, Epoch 3665, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,071: INFO: model_training: Rank 0, Epoch 3666, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,072: INFO: model_training: Rank 0, Epoch 3666, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,074: INFO: model_training: Rank 0, Epoch 3666, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,075: INFO: model_training: Rank 0, Epoch 3666, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,076: INFO: model_training: Rank 0, Epoch 3666, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,077: INFO: model_training: Rank 0, Epoch 3667, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,078: INFO: model_training: Rank 0, Epoch 3667, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,080: INFO: model_training: Rank 0, Epoch 3667, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,081: INFO: model_training: Rank 0, Epoch 3667, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,083: INFO: model_training: Rank 0, Epoch 3667, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,084: INFO: model_training: Rank 0, Epoch 3668, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,086: INFO: model_training: Rank 0, Epoch 3668, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,087: INFO: model_training: Rank 0, Epoch 3668, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,088: INFO: model_training: Rank 0, Epoch 3668, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,090: INFO: model_training: Rank 0, Epoch 3668, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,092: INFO: model_training: Rank 0, Epoch 3669, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,093: INFO: model_training: Rank 0, Epoch 3669, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,094: INFO: model_training: Rank 0, Epoch 3669, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,096: INFO: model_training: Rank 0, Epoch 3669, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,098: INFO: model_training: Rank 0, Epoch 3669, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,099: INFO: model_training: Rank 0, Epoch 3670, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,100: INFO: model_training: Rank 0, Epoch 3670, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,102: INFO: model_training: Rank 0, Epoch 3670, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,103: INFO: model_training: Rank 0, Epoch 3670, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,104: INFO: model_training: Rank 0, Epoch 3670, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,105: INFO: model_training: Rank 0, Epoch 3671, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,106: INFO: model_training: Rank 0, Epoch 3671, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,108: INFO: model_training: Rank 0, Epoch 3671, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,109: INFO: model_training: Rank 0, Epoch 3671, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,110: INFO: model_training: Rank 0, Epoch 3671, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,111: INFO: model_training: Rank 0, Epoch 3672, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,113: INFO: model_training: Rank 0, Epoch 3672, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,114: INFO: model_training: Rank 0, Epoch 3672, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,115: INFO: model_training: Rank 0, Epoch 3672, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,117: INFO: model_training: Rank 0, Epoch 3672, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,118: INFO: model_training: Rank 0, Epoch 3673, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,119: INFO: model_training: Rank 0, Epoch 3673, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,120: INFO: model_training: Rank 0, Epoch 3673, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,121: INFO: model_training: Rank 0, Epoch 3673, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,122: INFO: model_training: Rank 0, Epoch 3673, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,123: INFO: model_training: Rank 0, Epoch 3674, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,124: INFO: model_training: Rank 0, Epoch 3674, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,125: INFO: model_training: Rank 0, Epoch 3674, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,126: INFO: model_training: Rank 0, Epoch 3674, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,128: INFO: model_training: Rank 0, Epoch 3674, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,129: INFO: model_training: Rank 0, Epoch 3675, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,130: INFO: model_training: Rank 0, Epoch 3675, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,131: INFO: model_training: Rank 0, Epoch 3675, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,132: INFO: model_training: Rank 0, Epoch 3675, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,133: INFO: model_training: Rank 0, Epoch 3675, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,135: INFO: model_training: Rank 0, Epoch 3676, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,136: INFO: model_training: Rank 0, Epoch 3676, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,137: INFO: model_training: Rank 0, Epoch 3676, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,138: INFO: model_training: Rank 0, Epoch 3676, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,140: INFO: model_training: Rank 0, Epoch 3676, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,141: INFO: model_training: Rank 0, Epoch 3677, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,142: INFO: model_training: Rank 0, Epoch 3677, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,143: INFO: model_training: Rank 0, Epoch 3677, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,144: INFO: model_training: Rank 0, Epoch 3677, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,145: INFO: model_training: Rank 0, Epoch 3677, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,147: INFO: model_training: Rank 0, Epoch 3678, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,148: INFO: model_training: Rank 0, Epoch 3678, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,149: INFO: model_training: Rank 0, Epoch 3678, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,150: INFO: model_training: Rank 0, Epoch 3678, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,151: INFO: model_training: Rank 0, Epoch 3678, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,153: INFO: model_training: Rank 0, Epoch 3679, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,154: INFO: model_training: Rank 0, Epoch 3679, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,155: INFO: model_training: Rank 0, Epoch 3679, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,157: INFO: model_training: Rank 0, Epoch 3679, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,159: INFO: model_training: Rank 0, Epoch 3679, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,161: INFO: model_training: Rank 0, Epoch 3680, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,162: INFO: model_training: Rank 0, Epoch 3680, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,164: INFO: model_training: Rank 0, Epoch 3680, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,166: INFO: model_training: Rank 0, Epoch 3680, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,167: INFO: model_training: Rank 0, Epoch 3680, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,168: INFO: model_training: Rank 0, Epoch 3681, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,169: INFO: model_training: Rank 0, Epoch 3681, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,170: INFO: model_training: Rank 0, Epoch 3681, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,172: INFO: model_training: Rank 0, Epoch 3681, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,173: INFO: model_training: Rank 0, Epoch 3681, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,175: INFO: model_training: Rank 0, Epoch 3682, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,176: INFO: model_training: Rank 0, Epoch 3682, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,177: INFO: model_training: Rank 0, Epoch 3682, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,179: INFO: model_training: Rank 0, Epoch 3682, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,180: INFO: model_training: Rank 0, Epoch 3682, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,182: INFO: model_training: Rank 0, Epoch 3683, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,183: INFO: model_training: Rank 0, Epoch 3683, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,184: INFO: model_training: Rank 0, Epoch 3683, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,186: INFO: model_training: Rank 0, Epoch 3683, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,187: INFO: model_training: Rank 0, Epoch 3683, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,188: INFO: model_training: Rank 0, Epoch 3684, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,190: INFO: model_training: Rank 0, Epoch 3684, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,191: INFO: model_training: Rank 0, Epoch 3684, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,192: INFO: model_training: Rank 0, Epoch 3684, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,193: INFO: model_training: Rank 0, Epoch 3684, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,195: INFO: model_training: Rank 0, Epoch 3685, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,196: INFO: model_training: Rank 0, Epoch 3685, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,198: INFO: model_training: Rank 0, Epoch 3685, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,199: INFO: model_training: Rank 0, Epoch 3685, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,200: INFO: model_training: Rank 0, Epoch 3685, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,202: INFO: model_training: Rank 0, Epoch 3686, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,203: INFO: model_training: Rank 0, Epoch 3686, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,204: INFO: model_training: Rank 0, Epoch 3686, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,206: INFO: model_training: Rank 0, Epoch 3686, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,207: INFO: model_training: Rank 0, Epoch 3686, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,208: INFO: model_training: Rank 0, Epoch 3687, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,210: INFO: model_training: Rank 0, Epoch 3687, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,212: INFO: model_training: Rank 0, Epoch 3687, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,214: INFO: model_training: Rank 0, Epoch 3687, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,215: INFO: model_training: Rank 0, Epoch 3687, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,216: INFO: model_training: Rank 0, Epoch 3688, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,217: INFO: model_training: Rank 0, Epoch 3688, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,218: INFO: model_training: Rank 0, Epoch 3688, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,220: INFO: model_training: Rank 0, Epoch 3688, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,222: INFO: model_training: Rank 0, Epoch 3688, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,223: INFO: model_training: Rank 0, Epoch 3689, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,224: INFO: model_training: Rank 0, Epoch 3689, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,225: INFO: model_training: Rank 0, Epoch 3689, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,227: INFO: model_training: Rank 0, Epoch 3689, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,229: INFO: model_training: Rank 0, Epoch 3689, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,230: INFO: model_training: Rank 0, Epoch 3690, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,231: INFO: model_training: Rank 0, Epoch 3690, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,232: INFO: model_training: Rank 0, Epoch 3690, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,234: INFO: model_training: Rank 0, Epoch 3690, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,235: INFO: model_training: Rank 0, Epoch 3690, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,236: INFO: model_training: Rank 0, Epoch 3691, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,237: INFO: model_training: Rank 0, Epoch 3691, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,239: INFO: model_training: Rank 0, Epoch 3691, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,240: INFO: model_training: Rank 0, Epoch 3691, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,242: INFO: model_training: Rank 0, Epoch 3691, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,243: INFO: model_training: Rank 0, Epoch 3692, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,245: INFO: model_training: Rank 0, Epoch 3692, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,246: INFO: model_training: Rank 0, Epoch 3692, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,247: INFO: model_training: Rank 0, Epoch 3692, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,249: INFO: model_training: Rank 0, Epoch 3692, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,250: INFO: model_training: Rank 0, Epoch 3693, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,252: INFO: model_training: Rank 0, Epoch 3693, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,253: INFO: model_training: Rank 0, Epoch 3693, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,254: INFO: model_training: Rank 0, Epoch 3693, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,257: INFO: model_training: Rank 0, Epoch 3693, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,258: INFO: model_training: Rank 0, Epoch 3694, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,259: INFO: model_training: Rank 0, Epoch 3694, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,260: INFO: model_training: Rank 0, Epoch 3694, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,261: INFO: model_training: Rank 0, Epoch 3694, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,263: INFO: model_training: Rank 0, Epoch 3694, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,264: INFO: model_training: Rank 0, Epoch 3695, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,265: INFO: model_training: Rank 0, Epoch 3695, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,267: INFO: model_training: Rank 0, Epoch 3695, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,268: INFO: model_training: Rank 0, Epoch 3695, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,269: INFO: model_training: Rank 0, Epoch 3695, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,271: INFO: model_training: Rank 0, Epoch 3696, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,272: INFO: model_training: Rank 0, Epoch 3696, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,274: INFO: model_training: Rank 0, Epoch 3696, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,275: INFO: model_training: Rank 0, Epoch 3696, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,276: INFO: model_training: Rank 0, Epoch 3696, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,278: INFO: model_training: Rank 0, Epoch 3697, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,279: INFO: model_training: Rank 0, Epoch 3697, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,281: INFO: model_training: Rank 0, Epoch 3697, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,282: INFO: model_training: Rank 0, Epoch 3697, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,283: INFO: model_training: Rank 0, Epoch 3697, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,285: INFO: model_training: Rank 0, Epoch 3698, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,286: INFO: model_training: Rank 0, Epoch 3698, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,287: INFO: model_training: Rank 0, Epoch 3698, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,289: INFO: model_training: Rank 0, Epoch 3698, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,291: INFO: model_training: Rank 0, Epoch 3698, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,292: INFO: model_training: Rank 0, Epoch 3699, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,293: INFO: model_training: Rank 0, Epoch 3699, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,294: INFO: model_training: Rank 0, Epoch 3699, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,296: INFO: model_training: Rank 0, Epoch 3699, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,297: INFO: model_training: Rank 0, Epoch 3699, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,299: INFO: model_training: Rank 0, Epoch 3700, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,300: INFO: model_training: Rank 0, Epoch 3700, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,301: INFO: model_training: Rank 0, Epoch 3700, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,303: INFO: model_training: Rank 0, Epoch 3700, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,305: INFO: model_training: Rank 0, Epoch 3700, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,306: INFO: model_training: Rank 0, Epoch 3701, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,307: INFO: model_training: Rank 0, Epoch 3701, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,308: INFO: model_training: Rank 0, Epoch 3701, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,310: INFO: model_training: Rank 0, Epoch 3701, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,311: INFO: model_training: Rank 0, Epoch 3701, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,313: INFO: model_training: Rank 0, Epoch 3702, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,314: INFO: model_training: Rank 0, Epoch 3702, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,316: INFO: model_training: Rank 0, Epoch 3702, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,317: INFO: model_training: Rank 0, Epoch 3702, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,318: INFO: model_training: Rank 0, Epoch 3702, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,320: INFO: model_training: Rank 0, Epoch 3703, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,321: INFO: model_training: Rank 0, Epoch 3703, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,322: INFO: model_training: Rank 0, Epoch 3703, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,324: INFO: model_training: Rank 0, Epoch 3703, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,325: INFO: model_training: Rank 0, Epoch 3703, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,326: INFO: model_training: Rank 0, Epoch 3704, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,329: INFO: model_training: Rank 0, Epoch 3704, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,330: INFO: model_training: Rank 0, Epoch 3704, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,331: INFO: model_training: Rank 0, Epoch 3704, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,332: INFO: model_training: Rank 0, Epoch 3704, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,333: INFO: model_training: Rank 0, Epoch 3705, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,336: INFO: model_training: Rank 0, Epoch 3705, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,337: INFO: model_training: Rank 0, Epoch 3705, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,338: INFO: model_training: Rank 0, Epoch 3705, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,340: INFO: model_training: Rank 0, Epoch 3705, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,341: INFO: model_training: Rank 0, Epoch 3706, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,343: INFO: model_training: Rank 0, Epoch 3706, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,344: INFO: model_training: Rank 0, Epoch 3706, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,346: INFO: model_training: Rank 0, Epoch 3706, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,347: INFO: model_training: Rank 0, Epoch 3706, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,348: INFO: model_training: Rank 0, Epoch 3707, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,349: INFO: model_training: Rank 0, Epoch 3707, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,351: INFO: model_training: Rank 0, Epoch 3707, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,353: INFO: model_training: Rank 0, Epoch 3707, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,354: INFO: model_training: Rank 0, Epoch 3707, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,355: INFO: model_training: Rank 0, Epoch 3708, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,357: INFO: model_training: Rank 0, Epoch 3708, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,358: INFO: model_training: Rank 0, Epoch 3708, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,360: INFO: model_training: Rank 0, Epoch 3708, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,361: INFO: model_training: Rank 0, Epoch 3708, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,363: INFO: model_training: Rank 0, Epoch 3709, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,364: INFO: model_training: Rank 0, Epoch 3709, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,365: INFO: model_training: Rank 0, Epoch 3709, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,367: INFO: model_training: Rank 0, Epoch 3709, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,368: INFO: model_training: Rank 0, Epoch 3709, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,369: INFO: model_training: Rank 0, Epoch 3710, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,370: INFO: model_training: Rank 0, Epoch 3710, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,372: INFO: model_training: Rank 0, Epoch 3710, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,373: INFO: model_training: Rank 0, Epoch 3710, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,374: INFO: model_training: Rank 0, Epoch 3710, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,376: INFO: model_training: Rank 0, Epoch 3711, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,377: INFO: model_training: Rank 0, Epoch 3711, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,378: INFO: model_training: Rank 0, Epoch 3711, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,380: INFO: model_training: Rank 0, Epoch 3711, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,381: INFO: model_training: Rank 0, Epoch 3711, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,382: INFO: model_training: Rank 0, Epoch 3712, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,384: INFO: model_training: Rank 0, Epoch 3712, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,385: INFO: model_training: Rank 0, Epoch 3712, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,386: INFO: model_training: Rank 0, Epoch 3712, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,388: INFO: model_training: Rank 0, Epoch 3712, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,389: INFO: model_training: Rank 0, Epoch 3713, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,390: INFO: model_training: Rank 0, Epoch 3713, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,392: INFO: model_training: Rank 0, Epoch 3713, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,393: INFO: model_training: Rank 0, Epoch 3713, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,394: INFO: model_training: Rank 0, Epoch 3713, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,395: INFO: model_training: Rank 0, Epoch 3714, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,397: INFO: model_training: Rank 0, Epoch 3714, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,398: INFO: model_training: Rank 0, Epoch 3714, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,400: INFO: model_training: Rank 0, Epoch 3714, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,401: INFO: model_training: Rank 0, Epoch 3714, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,402: INFO: model_training: Rank 0, Epoch 3715, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,403: INFO: model_training: Rank 0, Epoch 3715, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,405: INFO: model_training: Rank 0, Epoch 3715, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,406: INFO: model_training: Rank 0, Epoch 3715, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,407: INFO: model_training: Rank 0, Epoch 3715, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,408: INFO: model_training: Rank 0, Epoch 3716, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,410: INFO: model_training: Rank 0, Epoch 3716, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,411: INFO: model_training: Rank 0, Epoch 3716, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,412: INFO: model_training: Rank 0, Epoch 3716, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,413: INFO: model_training: Rank 0, Epoch 3716, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,415: INFO: model_training: Rank 0, Epoch 3717, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,416: INFO: model_training: Rank 0, Epoch 3717, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,417: INFO: model_training: Rank 0, Epoch 3717, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,418: INFO: model_training: Rank 0, Epoch 3717, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,420: INFO: model_training: Rank 0, Epoch 3717, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,422: INFO: model_training: Rank 0, Epoch 3718, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,423: INFO: model_training: Rank 0, Epoch 3718, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,425: INFO: model_training: Rank 0, Epoch 3718, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,426: INFO: model_training: Rank 0, Epoch 3718, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,427: INFO: model_training: Rank 0, Epoch 3718, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,429: INFO: model_training: Rank 0, Epoch 3719, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,431: INFO: model_training: Rank 0, Epoch 3719, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,432: INFO: model_training: Rank 0, Epoch 3719, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,434: INFO: model_training: Rank 0, Epoch 3719, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,436: INFO: model_training: Rank 0, Epoch 3719, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,438: INFO: model_training: Rank 0, Epoch 3720, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,439: INFO: model_training: Rank 0, Epoch 3720, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,441: INFO: model_training: Rank 0, Epoch 3720, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,444: INFO: model_training: Rank 0, Epoch 3720, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,446: INFO: model_training: Rank 0, Epoch 3720, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,447: INFO: model_training: Rank 0, Epoch 3721, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,449: INFO: model_training: Rank 0, Epoch 3721, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,450: INFO: model_training: Rank 0, Epoch 3721, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,451: INFO: model_training: Rank 0, Epoch 3721, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,452: INFO: model_training: Rank 0, Epoch 3721, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,454: INFO: model_training: Rank 0, Epoch 3722, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,456: INFO: model_training: Rank 0, Epoch 3722, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,457: INFO: model_training: Rank 0, Epoch 3722, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,458: INFO: model_training: Rank 0, Epoch 3722, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,460: INFO: model_training: Rank 0, Epoch 3722, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,462: INFO: model_training: Rank 0, Epoch 3723, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,464: INFO: model_training: Rank 0, Epoch 3723, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,465: INFO: model_training: Rank 0, Epoch 3723, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,467: INFO: model_training: Rank 0, Epoch 3723, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,468: INFO: model_training: Rank 0, Epoch 3723, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,469: INFO: model_training: Rank 0, Epoch 3724, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,470: INFO: model_training: Rank 0, Epoch 3724, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,471: INFO: model_training: Rank 0, Epoch 3724, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,472: INFO: model_training: Rank 0, Epoch 3724, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,473: INFO: model_training: Rank 0, Epoch 3724, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,474: INFO: model_training: Rank 0, Epoch 3725, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,475: INFO: model_training: Rank 0, Epoch 3725, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,476: INFO: model_training: Rank 0, Epoch 3725, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,478: INFO: model_training: Rank 0, Epoch 3725, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,479: INFO: model_training: Rank 0, Epoch 3725, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,480: INFO: model_training: Rank 0, Epoch 3726, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,481: INFO: model_training: Rank 0, Epoch 3726, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,482: INFO: model_training: Rank 0, Epoch 3726, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,483: INFO: model_training: Rank 0, Epoch 3726, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,484: INFO: model_training: Rank 0, Epoch 3726, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,485: INFO: model_training: Rank 0, Epoch 3727, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,486: INFO: model_training: Rank 0, Epoch 3727, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,487: INFO: model_training: Rank 0, Epoch 3727, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,488: INFO: model_training: Rank 0, Epoch 3727, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,489: INFO: model_training: Rank 0, Epoch 3727, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,490: INFO: model_training: Rank 0, Epoch 3728, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,491: INFO: model_training: Rank 0, Epoch 3728, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,492: INFO: model_training: Rank 0, Epoch 3728, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,494: INFO: model_training: Rank 0, Epoch 3728, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,495: INFO: model_training: Rank 0, Epoch 3728, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,496: INFO: model_training: Rank 0, Epoch 3729, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,497: INFO: model_training: Rank 0, Epoch 3729, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,498: INFO: model_training: Rank 0, Epoch 3729, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,499: INFO: model_training: Rank 0, Epoch 3729, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,500: INFO: model_training: Rank 0, Epoch 3729, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,501: INFO: model_training: Rank 0, Epoch 3730, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,502: INFO: model_training: Rank 0, Epoch 3730, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,503: INFO: model_training: Rank 0, Epoch 3730, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,504: INFO: model_training: Rank 0, Epoch 3730, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,505: INFO: model_training: Rank 0, Epoch 3730, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,506: INFO: model_training: Rank 0, Epoch 3731, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,507: INFO: model_training: Rank 0, Epoch 3731, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,508: INFO: model_training: Rank 0, Epoch 3731, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,509: INFO: model_training: Rank 0, Epoch 3731, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,510: INFO: model_training: Rank 0, Epoch 3731, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,511: INFO: model_training: Rank 0, Epoch 3732, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,512: INFO: model_training: Rank 0, Epoch 3732, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,513: INFO: model_training: Rank 0, Epoch 3732, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,514: INFO: model_training: Rank 0, Epoch 3732, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,515: INFO: model_training: Rank 0, Epoch 3732, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,516: INFO: model_training: Rank 0, Epoch 3733, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,517: INFO: model_training: Rank 0, Epoch 3733, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,519: INFO: model_training: Rank 0, Epoch 3733, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,520: INFO: model_training: Rank 0, Epoch 3733, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,521: INFO: model_training: Rank 0, Epoch 3733, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,521: INFO: model_training: Rank 0, Epoch 3734, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,523: INFO: model_training: Rank 0, Epoch 3734, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,523: INFO: model_training: Rank 0, Epoch 3734, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,525: INFO: model_training: Rank 0, Epoch 3734, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,526: INFO: model_training: Rank 0, Epoch 3734, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,527: INFO: model_training: Rank 0, Epoch 3735, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,528: INFO: model_training: Rank 0, Epoch 3735, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,529: INFO: model_training: Rank 0, Epoch 3735, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,530: INFO: model_training: Rank 0, Epoch 3735, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,531: INFO: model_training: Rank 0, Epoch 3735, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,532: INFO: model_training: Rank 0, Epoch 3736, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,533: INFO: model_training: Rank 0, Epoch 3736, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,534: INFO: model_training: Rank 0, Epoch 3736, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,535: INFO: model_training: Rank 0, Epoch 3736, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,536: INFO: model_training: Rank 0, Epoch 3736, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,538: INFO: model_training: Rank 0, Epoch 3737, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,539: INFO: model_training: Rank 0, Epoch 3737, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,540: INFO: model_training: Rank 0, Epoch 3737, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,541: INFO: model_training: Rank 0, Epoch 3737, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,542: INFO: model_training: Rank 0, Epoch 3737, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,543: INFO: model_training: Rank 0, Epoch 3738, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,544: INFO: model_training: Rank 0, Epoch 3738, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,545: INFO: model_training: Rank 0, Epoch 3738, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,546: INFO: model_training: Rank 0, Epoch 3738, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,547: INFO: model_training: Rank 0, Epoch 3738, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,548: INFO: model_training: Rank 0, Epoch 3739, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,550: INFO: model_training: Rank 0, Epoch 3739, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,551: INFO: model_training: Rank 0, Epoch 3739, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,552: INFO: model_training: Rank 0, Epoch 3739, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,553: INFO: model_training: Rank 0, Epoch 3739, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,554: INFO: model_training: Rank 0, Epoch 3740, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,555: INFO: model_training: Rank 0, Epoch 3740, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,556: INFO: model_training: Rank 0, Epoch 3740, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,557: INFO: model_training: Rank 0, Epoch 3740, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,558: INFO: model_training: Rank 0, Epoch 3740, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,559: INFO: model_training: Rank 0, Epoch 3741, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,560: INFO: model_training: Rank 0, Epoch 3741, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,561: INFO: model_training: Rank 0, Epoch 3741, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,562: INFO: model_training: Rank 0, Epoch 3741, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,563: INFO: model_training: Rank 0, Epoch 3741, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,564: INFO: model_training: Rank 0, Epoch 3742, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,565: INFO: model_training: Rank 0, Epoch 3742, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,566: INFO: model_training: Rank 0, Epoch 3742, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,568: INFO: model_training: Rank 0, Epoch 3742, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,569: INFO: model_training: Rank 0, Epoch 3742, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,570: INFO: model_training: Rank 0, Epoch 3743, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,571: INFO: model_training: Rank 0, Epoch 3743, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,572: INFO: model_training: Rank 0, Epoch 3743, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,573: INFO: model_training: Rank 0, Epoch 3743, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,574: INFO: model_training: Rank 0, Epoch 3743, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,575: INFO: model_training: Rank 0, Epoch 3744, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,576: INFO: model_training: Rank 0, Epoch 3744, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,577: INFO: model_training: Rank 0, Epoch 3744, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,578: INFO: model_training: Rank 0, Epoch 3744, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,579: INFO: model_training: Rank 0, Epoch 3744, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,580: INFO: model_training: Rank 0, Epoch 3745, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,581: INFO: model_training: Rank 0, Epoch 3745, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,582: INFO: model_training: Rank 0, Epoch 3745, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,583: INFO: model_training: Rank 0, Epoch 3745, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,585: INFO: model_training: Rank 0, Epoch 3745, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,586: INFO: model_training: Rank 0, Epoch 3746, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,587: INFO: model_training: Rank 0, Epoch 3746, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,588: INFO: model_training: Rank 0, Epoch 3746, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,589: INFO: model_training: Rank 0, Epoch 3746, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,591: INFO: model_training: Rank 0, Epoch 3746, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,591: INFO: model_training: Rank 0, Epoch 3747, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,592: INFO: model_training: Rank 0, Epoch 3747, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,594: INFO: model_training: Rank 0, Epoch 3747, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,595: INFO: model_training: Rank 0, Epoch 3747, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,596: INFO: model_training: Rank 0, Epoch 3747, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,597: INFO: model_training: Rank 0, Epoch 3748, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,598: INFO: model_training: Rank 0, Epoch 3748, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,599: INFO: model_training: Rank 0, Epoch 3748, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,600: INFO: model_training: Rank 0, Epoch 3748, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,601: INFO: model_training: Rank 0, Epoch 3748, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,602: INFO: model_training: Rank 0, Epoch 3749, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,603: INFO: model_training: Rank 0, Epoch 3749, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,604: INFO: model_training: Rank 0, Epoch 3749, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,606: INFO: model_training: Rank 0, Epoch 3749, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,607: INFO: model_training: Rank 0, Epoch 3749, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,608: INFO: model_training: Rank 0, Epoch 3750, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,609: INFO: model_training: Rank 0, Epoch 3750, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,610: INFO: model_training: Rank 0, Epoch 3750, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,611: INFO: model_training: Rank 0, Epoch 3750, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,612: INFO: model_training: Rank 0, Epoch 3750, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,613: INFO: model_training: Rank 0, Epoch 3751, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,614: INFO: model_training: Rank 0, Epoch 3751, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,615: INFO: model_training: Rank 0, Epoch 3751, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,616: INFO: model_training: Rank 0, Epoch 3751, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,617: INFO: model_training: Rank 0, Epoch 3751, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,618: INFO: model_training: Rank 0, Epoch 3752, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,619: INFO: model_training: Rank 0, Epoch 3752, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,620: INFO: model_training: Rank 0, Epoch 3752, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,621: INFO: model_training: Rank 0, Epoch 3752, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,622: INFO: model_training: Rank 0, Epoch 3752, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,624: INFO: model_training: Rank 0, Epoch 3753, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,625: INFO: model_training: Rank 0, Epoch 3753, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,626: INFO: model_training: Rank 0, Epoch 3753, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,627: INFO: model_training: Rank 0, Epoch 3753, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,628: INFO: model_training: Rank 0, Epoch 3753, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,629: INFO: model_training: Rank 0, Epoch 3754, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,630: INFO: model_training: Rank 0, Epoch 3754, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,631: INFO: model_training: Rank 0, Epoch 3754, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,633: INFO: model_training: Rank 0, Epoch 3754, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,634: INFO: model_training: Rank 0, Epoch 3754, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,636: INFO: model_training: Rank 0, Epoch 3755, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,637: INFO: model_training: Rank 0, Epoch 3755, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,639: INFO: model_training: Rank 0, Epoch 3755, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,640: INFO: model_training: Rank 0, Epoch 3755, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,641: INFO: model_training: Rank 0, Epoch 3755, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,642: INFO: model_training: Rank 0, Epoch 3756, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,644: INFO: model_training: Rank 0, Epoch 3756, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,645: INFO: model_training: Rank 0, Epoch 3756, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,646: INFO: model_training: Rank 0, Epoch 3756, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,647: INFO: model_training: Rank 0, Epoch 3756, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,649: INFO: model_training: Rank 0, Epoch 3757, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,650: INFO: model_training: Rank 0, Epoch 3757, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,651: INFO: model_training: Rank 0, Epoch 3757, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,653: INFO: model_training: Rank 0, Epoch 3757, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,654: INFO: model_training: Rank 0, Epoch 3757, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,655: INFO: model_training: Rank 0, Epoch 3758, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,657: INFO: model_training: Rank 0, Epoch 3758, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,658: INFO: model_training: Rank 0, Epoch 3758, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,659: INFO: model_training: Rank 0, Epoch 3758, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,661: INFO: model_training: Rank 0, Epoch 3758, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,662: INFO: model_training: Rank 0, Epoch 3759, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,664: INFO: model_training: Rank 0, Epoch 3759, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,665: INFO: model_training: Rank 0, Epoch 3759, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,666: INFO: model_training: Rank 0, Epoch 3759, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,668: INFO: model_training: Rank 0, Epoch 3759, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,669: INFO: model_training: Rank 0, Epoch 3760, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,670: INFO: model_training: Rank 0, Epoch 3760, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,672: INFO: model_training: Rank 0, Epoch 3760, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,673: INFO: model_training: Rank 0, Epoch 3760, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,674: INFO: model_training: Rank 0, Epoch 3760, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,675: INFO: model_training: Rank 0, Epoch 3761, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,677: INFO: model_training: Rank 0, Epoch 3761, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,678: INFO: model_training: Rank 0, Epoch 3761, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,679: INFO: model_training: Rank 0, Epoch 3761, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,681: INFO: model_training: Rank 0, Epoch 3761, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,682: INFO: model_training: Rank 0, Epoch 3762, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,683: INFO: model_training: Rank 0, Epoch 3762, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,684: INFO: model_training: Rank 0, Epoch 3762, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,686: INFO: model_training: Rank 0, Epoch 3762, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,687: INFO: model_training: Rank 0, Epoch 3762, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,688: INFO: model_training: Rank 0, Epoch 3763, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,689: INFO: model_training: Rank 0, Epoch 3763, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,691: INFO: model_training: Rank 0, Epoch 3763, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,692: INFO: model_training: Rank 0, Epoch 3763, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,693: INFO: model_training: Rank 0, Epoch 3763, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,694: INFO: model_training: Rank 0, Epoch 3764, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,696: INFO: model_training: Rank 0, Epoch 3764, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,697: INFO: model_training: Rank 0, Epoch 3764, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,698: INFO: model_training: Rank 0, Epoch 3764, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,700: INFO: model_training: Rank 0, Epoch 3764, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,701: INFO: model_training: Rank 0, Epoch 3765, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,702: INFO: model_training: Rank 0, Epoch 3765, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,704: INFO: model_training: Rank 0, Epoch 3765, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,705: INFO: model_training: Rank 0, Epoch 3765, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,706: INFO: model_training: Rank 0, Epoch 3765, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,708: INFO: model_training: Rank 0, Epoch 3766, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,709: INFO: model_training: Rank 0, Epoch 3766, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,710: INFO: model_training: Rank 0, Epoch 3766, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,712: INFO: model_training: Rank 0, Epoch 3766, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,713: INFO: model_training: Rank 0, Epoch 3766, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,714: INFO: model_training: Rank 0, Epoch 3767, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,715: INFO: model_training: Rank 0, Epoch 3767, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,717: INFO: model_training: Rank 0, Epoch 3767, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,718: INFO: model_training: Rank 0, Epoch 3767, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,719: INFO: model_training: Rank 0, Epoch 3767, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,721: INFO: model_training: Rank 0, Epoch 3768, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,723: INFO: model_training: Rank 0, Epoch 3768, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,724: INFO: model_training: Rank 0, Epoch 3768, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,725: INFO: model_training: Rank 0, Epoch 3768, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,726: INFO: model_training: Rank 0, Epoch 3768, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,728: INFO: model_training: Rank 0, Epoch 3769, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,729: INFO: model_training: Rank 0, Epoch 3769, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,731: INFO: model_training: Rank 0, Epoch 3769, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,732: INFO: model_training: Rank 0, Epoch 3769, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,733: INFO: model_training: Rank 0, Epoch 3769, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,734: INFO: model_training: Rank 0, Epoch 3770, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,736: INFO: model_training: Rank 0, Epoch 3770, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,737: INFO: model_training: Rank 0, Epoch 3770, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,738: INFO: model_training: Rank 0, Epoch 3770, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,739: INFO: model_training: Rank 0, Epoch 3770, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,740: INFO: model_training: Rank 0, Epoch 3771, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,741: INFO: model_training: Rank 0, Epoch 3771, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,743: INFO: model_training: Rank 0, Epoch 3771, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,744: INFO: model_training: Rank 0, Epoch 3771, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,745: INFO: model_training: Rank 0, Epoch 3771, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,747: INFO: model_training: Rank 0, Epoch 3772, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,748: INFO: model_training: Rank 0, Epoch 3772, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,749: INFO: model_training: Rank 0, Epoch 3772, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,750: INFO: model_training: Rank 0, Epoch 3772, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,752: INFO: model_training: Rank 0, Epoch 3772, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,753: INFO: model_training: Rank 0, Epoch 3773, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,755: INFO: model_training: Rank 0, Epoch 3773, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,757: INFO: model_training: Rank 0, Epoch 3773, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,759: INFO: model_training: Rank 0, Epoch 3773, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,761: INFO: model_training: Rank 0, Epoch 3773, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,762: INFO: model_training: Rank 0, Epoch 3774, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,764: INFO: model_training: Rank 0, Epoch 3774, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,766: INFO: model_training: Rank 0, Epoch 3774, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,767: INFO: model_training: Rank 0, Epoch 3774, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,769: INFO: model_training: Rank 0, Epoch 3774, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,771: INFO: model_training: Rank 0, Epoch 3775, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,772: INFO: model_training: Rank 0, Epoch 3775, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,774: INFO: model_training: Rank 0, Epoch 3775, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,775: INFO: model_training: Rank 0, Epoch 3775, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,777: INFO: model_training: Rank 0, Epoch 3775, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,778: INFO: model_training: Rank 0, Epoch 3776, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,779: INFO: model_training: Rank 0, Epoch 3776, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,781: INFO: model_training: Rank 0, Epoch 3776, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,782: INFO: model_training: Rank 0, Epoch 3776, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,783: INFO: model_training: Rank 0, Epoch 3776, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,785: INFO: model_training: Rank 0, Epoch 3777, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,787: INFO: model_training: Rank 0, Epoch 3777, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,789: INFO: model_training: Rank 0, Epoch 3777, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,790: INFO: model_training: Rank 0, Epoch 3777, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,800: INFO: model_training: Rank 0, Epoch 3777, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,803: INFO: model_training: Rank 0, Epoch 3778, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,806: INFO: model_training: Rank 0, Epoch 3778, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,808: INFO: model_training: Rank 0, Epoch 3778, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,811: INFO: model_training: Rank 0, Epoch 3778, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,813: INFO: model_training: Rank 0, Epoch 3778, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,816: INFO: model_training: Rank 0, Epoch 3779, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,818: INFO: model_training: Rank 0, Epoch 3779, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,820: INFO: model_training: Rank 0, Epoch 3779, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,822: INFO: model_training: Rank 0, Epoch 3779, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,824: INFO: model_training: Rank 0, Epoch 3779, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,825: INFO: model_training: Rank 0, Epoch 3780, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,826: INFO: model_training: Rank 0, Epoch 3780, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,828: INFO: model_training: Rank 0, Epoch 3780, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,829: INFO: model_training: Rank 0, Epoch 3780, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,830: INFO: model_training: Rank 0, Epoch 3780, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,832: INFO: model_training: Rank 0, Epoch 3781, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,834: INFO: model_training: Rank 0, Epoch 3781, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,835: INFO: model_training: Rank 0, Epoch 3781, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,837: INFO: model_training: Rank 0, Epoch 3781, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,838: INFO: model_training: Rank 0, Epoch 3781, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,840: INFO: model_training: Rank 0, Epoch 3782, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,842: INFO: model_training: Rank 0, Epoch 3782, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,843: INFO: model_training: Rank 0, Epoch 3782, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,845: INFO: model_training: Rank 0, Epoch 3782, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,846: INFO: model_training: Rank 0, Epoch 3782, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,848: INFO: model_training: Rank 0, Epoch 3783, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,850: INFO: model_training: Rank 0, Epoch 3783, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,851: INFO: model_training: Rank 0, Epoch 3783, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,853: INFO: model_training: Rank 0, Epoch 3783, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,855: INFO: model_training: Rank 0, Epoch 3783, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,857: INFO: model_training: Rank 0, Epoch 3784, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,858: INFO: model_training: Rank 0, Epoch 3784, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,860: INFO: model_training: Rank 0, Epoch 3784, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,862: INFO: model_training: Rank 0, Epoch 3784, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,863: INFO: model_training: Rank 0, Epoch 3784, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,865: INFO: model_training: Rank 0, Epoch 3785, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,867: INFO: model_training: Rank 0, Epoch 3785, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,868: INFO: model_training: Rank 0, Epoch 3785, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,869: INFO: model_training: Rank 0, Epoch 3785, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,871: INFO: model_training: Rank 0, Epoch 3785, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,873: INFO: model_training: Rank 0, Epoch 3786, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,874: INFO: model_training: Rank 0, Epoch 3786, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,876: INFO: model_training: Rank 0, Epoch 3786, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,878: INFO: model_training: Rank 0, Epoch 3786, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,879: INFO: model_training: Rank 0, Epoch 3786, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,881: INFO: model_training: Rank 0, Epoch 3787, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,882: INFO: model_training: Rank 0, Epoch 3787, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,884: INFO: model_training: Rank 0, Epoch 3787, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,885: INFO: model_training: Rank 0, Epoch 3787, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,887: INFO: model_training: Rank 0, Epoch 3787, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,890: INFO: model_training: Rank 0, Epoch 3788, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,892: INFO: model_training: Rank 0, Epoch 3788, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,893: INFO: model_training: Rank 0, Epoch 3788, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,895: INFO: model_training: Rank 0, Epoch 3788, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,896: INFO: model_training: Rank 0, Epoch 3788, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,898: INFO: model_training: Rank 0, Epoch 3789, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,899: INFO: model_training: Rank 0, Epoch 3789, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,900: INFO: model_training: Rank 0, Epoch 3789, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,901: INFO: model_training: Rank 0, Epoch 3789, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,902: INFO: model_training: Rank 0, Epoch 3789, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,904: INFO: model_training: Rank 0, Epoch 3790, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,906: INFO: model_training: Rank 0, Epoch 3790, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,908: INFO: model_training: Rank 0, Epoch 3790, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,909: INFO: model_training: Rank 0, Epoch 3790, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,910: INFO: model_training: Rank 0, Epoch 3790, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,912: INFO: model_training: Rank 0, Epoch 3791, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,913: INFO: model_training: Rank 0, Epoch 3791, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,914: INFO: model_training: Rank 0, Epoch 3791, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,915: INFO: model_training: Rank 0, Epoch 3791, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,916: INFO: model_training: Rank 0, Epoch 3791, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,917: INFO: model_training: Rank 0, Epoch 3792, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,918: INFO: model_training: Rank 0, Epoch 3792, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,920: INFO: model_training: Rank 0, Epoch 3792, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,922: INFO: model_training: Rank 0, Epoch 3792, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,923: INFO: model_training: Rank 0, Epoch 3792, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,925: INFO: model_training: Rank 0, Epoch 3793, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,926: INFO: model_training: Rank 0, Epoch 3793, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,928: INFO: model_training: Rank 0, Epoch 3793, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,929: INFO: model_training: Rank 0, Epoch 3793, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,930: INFO: model_training: Rank 0, Epoch 3793, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,931: INFO: model_training: Rank 0, Epoch 3794, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,932: INFO: model_training: Rank 0, Epoch 3794, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,933: INFO: model_training: Rank 0, Epoch 3794, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,934: INFO: model_training: Rank 0, Epoch 3794, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,935: INFO: model_training: Rank 0, Epoch 3794, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,937: INFO: model_training: Rank 0, Epoch 3795, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,939: INFO: model_training: Rank 0, Epoch 3795, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,940: INFO: model_training: Rank 0, Epoch 3795, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,942: INFO: model_training: Rank 0, Epoch 3795, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,943: INFO: model_training: Rank 0, Epoch 3795, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,945: INFO: model_training: Rank 0, Epoch 3796, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,946: INFO: model_training: Rank 0, Epoch 3796, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,947: INFO: model_training: Rank 0, Epoch 3796, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,949: INFO: model_training: Rank 0, Epoch 3796, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,950: INFO: model_training: Rank 0, Epoch 3796, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,951: INFO: model_training: Rank 0, Epoch 3797, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,952: INFO: model_training: Rank 0, Epoch 3797, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,953: INFO: model_training: Rank 0, Epoch 3797, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,956: INFO: model_training: Rank 0, Epoch 3797, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,957: INFO: model_training: Rank 0, Epoch 3797, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,958: INFO: model_training: Rank 0, Epoch 3798, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,960: INFO: model_training: Rank 0, Epoch 3798, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,961: INFO: model_training: Rank 0, Epoch 3798, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,962: INFO: model_training: Rank 0, Epoch 3798, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,963: INFO: model_training: Rank 0, Epoch 3798, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,965: INFO: model_training: Rank 0, Epoch 3799, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,966: INFO: model_training: Rank 0, Epoch 3799, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,967: INFO: model_training: Rank 0, Epoch 3799, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,968: INFO: model_training: Rank 0, Epoch 3799, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,969: INFO: model_training: Rank 0, Epoch 3799, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,971: INFO: model_training: Rank 0, Epoch 3800, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,973: INFO: model_training: Rank 0, Epoch 3800, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,974: INFO: model_training: Rank 0, Epoch 3800, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,976: INFO: model_training: Rank 0, Epoch 3800, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,977: INFO: model_training: Rank 0, Epoch 3800, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,979: INFO: model_training: Rank 0, Epoch 3801, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,980: INFO: model_training: Rank 0, Epoch 3801, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,981: INFO: model_training: Rank 0, Epoch 3801, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,982: INFO: model_training: Rank 0, Epoch 3801, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,983: INFO: model_training: Rank 0, Epoch 3801, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,984: INFO: model_training: Rank 0, Epoch 3802, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,985: INFO: model_training: Rank 0, Epoch 3802, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,987: INFO: model_training: Rank 0, Epoch 3802, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,989: INFO: model_training: Rank 0, Epoch 3802, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,991: INFO: model_training: Rank 0, Epoch 3802, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,992: INFO: model_training: Rank 0, Epoch 3803, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,993: INFO: model_training: Rank 0, Epoch 3803, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:50,995: INFO: model_training: Rank 0, Epoch 3803, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:50,996: INFO: model_training: Rank 0, Epoch 3803, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:50,997: INFO: model_training: Rank 0, Epoch 3803, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:50,998: INFO: model_training: Rank 0, Epoch 3804, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:50,999: INFO: model_training: Rank 0, Epoch 3804, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,000: INFO: model_training: Rank 0, Epoch 3804, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,001: INFO: model_training: Rank 0, Epoch 3804, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,003: INFO: model_training: Rank 0, Epoch 3804, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,005: INFO: model_training: Rank 0, Epoch 3805, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,007: INFO: model_training: Rank 0, Epoch 3805, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,008: INFO: model_training: Rank 0, Epoch 3805, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,009: INFO: model_training: Rank 0, Epoch 3805, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,010: INFO: model_training: Rank 0, Epoch 3805, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,012: INFO: model_training: Rank 0, Epoch 3806, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,013: INFO: model_training: Rank 0, Epoch 3806, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,014: INFO: model_training: Rank 0, Epoch 3806, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,015: INFO: model_training: Rank 0, Epoch 3806, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,016: INFO: model_training: Rank 0, Epoch 3806, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,017: INFO: model_training: Rank 0, Epoch 3807, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,018: INFO: model_training: Rank 0, Epoch 3807, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,021: INFO: model_training: Rank 0, Epoch 3807, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,022: INFO: model_training: Rank 0, Epoch 3807, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,024: INFO: model_training: Rank 0, Epoch 3807, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,025: INFO: model_training: Rank 0, Epoch 3808, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,026: INFO: model_training: Rank 0, Epoch 3808, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,027: INFO: model_training: Rank 0, Epoch 3808, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,028: INFO: model_training: Rank 0, Epoch 3808, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,029: INFO: model_training: Rank 0, Epoch 3808, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,030: INFO: model_training: Rank 0, Epoch 3809, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,032: INFO: model_training: Rank 0, Epoch 3809, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,032: INFO: model_training: Rank 0, Epoch 3809, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,034: INFO: model_training: Rank 0, Epoch 3809, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,035: INFO: model_training: Rank 0, Epoch 3809, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,036: INFO: model_training: Rank 0, Epoch 3810, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,038: INFO: model_training: Rank 0, Epoch 3810, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,040: INFO: model_training: Rank 0, Epoch 3810, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,041: INFO: model_training: Rank 0, Epoch 3810, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,042: INFO: model_training: Rank 0, Epoch 3810, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,043: INFO: model_training: Rank 0, Epoch 3811, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,045: INFO: model_training: Rank 0, Epoch 3811, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,046: INFO: model_training: Rank 0, Epoch 3811, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,048: INFO: model_training: Rank 0, Epoch 3811, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,049: INFO: model_training: Rank 0, Epoch 3811, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,050: INFO: model_training: Rank 0, Epoch 3812, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,051: INFO: model_training: Rank 0, Epoch 3812, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,053: INFO: model_training: Rank 0, Epoch 3812, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,055: INFO: model_training: Rank 0, Epoch 3812, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,056: INFO: model_training: Rank 0, Epoch 3812, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,057: INFO: model_training: Rank 0, Epoch 3813, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,058: INFO: model_training: Rank 0, Epoch 3813, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,059: INFO: model_training: Rank 0, Epoch 3813, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,061: INFO: model_training: Rank 0, Epoch 3813, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,062: INFO: model_training: Rank 0, Epoch 3813, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,063: INFO: model_training: Rank 0, Epoch 3814, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,064: INFO: model_training: Rank 0, Epoch 3814, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,065: INFO: model_training: Rank 0, Epoch 3814, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,066: INFO: model_training: Rank 0, Epoch 3814, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,067: INFO: model_training: Rank 0, Epoch 3814, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,068: INFO: model_training: Rank 0, Epoch 3815, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,069: INFO: model_training: Rank 0, Epoch 3815, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,070: INFO: model_training: Rank 0, Epoch 3815, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,071: INFO: model_training: Rank 0, Epoch 3815, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,072: INFO: model_training: Rank 0, Epoch 3815, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,073: INFO: model_training: Rank 0, Epoch 3816, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,074: INFO: model_training: Rank 0, Epoch 3816, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,075: INFO: model_training: Rank 0, Epoch 3816, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,076: INFO: model_training: Rank 0, Epoch 3816, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,077: INFO: model_training: Rank 0, Epoch 3816, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,078: INFO: model_training: Rank 0, Epoch 3817, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,080: INFO: model_training: Rank 0, Epoch 3817, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,081: INFO: model_training: Rank 0, Epoch 3817, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,082: INFO: model_training: Rank 0, Epoch 3817, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,083: INFO: model_training: Rank 0, Epoch 3817, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,084: INFO: model_training: Rank 0, Epoch 3818, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,085: INFO: model_training: Rank 0, Epoch 3818, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,086: INFO: model_training: Rank 0, Epoch 3818, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,087: INFO: model_training: Rank 0, Epoch 3818, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,088: INFO: model_training: Rank 0, Epoch 3818, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,089: INFO: model_training: Rank 0, Epoch 3819, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,090: INFO: model_training: Rank 0, Epoch 3819, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,091: INFO: model_training: Rank 0, Epoch 3819, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,092: INFO: model_training: Rank 0, Epoch 3819, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,093: INFO: model_training: Rank 0, Epoch 3819, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,094: INFO: model_training: Rank 0, Epoch 3820, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,096: INFO: model_training: Rank 0, Epoch 3820, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,097: INFO: model_training: Rank 0, Epoch 3820, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,099: INFO: model_training: Rank 0, Epoch 3820, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,100: INFO: model_training: Rank 0, Epoch 3820, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,101: INFO: model_training: Rank 0, Epoch 3821, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,102: INFO: model_training: Rank 0, Epoch 3821, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,103: INFO: model_training: Rank 0, Epoch 3821, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,104: INFO: model_training: Rank 0, Epoch 3821, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,105: INFO: model_training: Rank 0, Epoch 3821, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,106: INFO: model_training: Rank 0, Epoch 3822, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,108: INFO: model_training: Rank 0, Epoch 3822, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,109: INFO: model_training: Rank 0, Epoch 3822, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,110: INFO: model_training: Rank 0, Epoch 3822, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,111: INFO: model_training: Rank 0, Epoch 3822, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,112: INFO: model_training: Rank 0, Epoch 3823, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,114: INFO: model_training: Rank 0, Epoch 3823, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,114: INFO: model_training: Rank 0, Epoch 3823, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,116: INFO: model_training: Rank 0, Epoch 3823, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,117: INFO: model_training: Rank 0, Epoch 3823, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,118: INFO: model_training: Rank 0, Epoch 3824, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,119: INFO: model_training: Rank 0, Epoch 3824, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,120: INFO: model_training: Rank 0, Epoch 3824, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,121: INFO: model_training: Rank 0, Epoch 3824, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,122: INFO: model_training: Rank 0, Epoch 3824, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,123: INFO: model_training: Rank 0, Epoch 3825, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,124: INFO: model_training: Rank 0, Epoch 3825, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,125: INFO: model_training: Rank 0, Epoch 3825, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,126: INFO: model_training: Rank 0, Epoch 3825, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,127: INFO: model_training: Rank 0, Epoch 3825, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,128: INFO: model_training: Rank 0, Epoch 3826, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,129: INFO: model_training: Rank 0, Epoch 3826, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,130: INFO: model_training: Rank 0, Epoch 3826, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,132: INFO: model_training: Rank 0, Epoch 3826, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,133: INFO: model_training: Rank 0, Epoch 3826, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,133: INFO: model_training: Rank 0, Epoch 3827, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,134: INFO: model_training: Rank 0, Epoch 3827, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,135: INFO: model_training: Rank 0, Epoch 3827, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,136: INFO: model_training: Rank 0, Epoch 3827, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,137: INFO: model_training: Rank 0, Epoch 3827, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,138: INFO: model_training: Rank 0, Epoch 3828, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,140: INFO: model_training: Rank 0, Epoch 3828, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,141: INFO: model_training: Rank 0, Epoch 3828, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,142: INFO: model_training: Rank 0, Epoch 3828, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,143: INFO: model_training: Rank 0, Epoch 3828, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,144: INFO: model_training: Rank 0, Epoch 3829, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,145: INFO: model_training: Rank 0, Epoch 3829, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,146: INFO: model_training: Rank 0, Epoch 3829, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,147: INFO: model_training: Rank 0, Epoch 3829, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,148: INFO: model_training: Rank 0, Epoch 3829, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,149: INFO: model_training: Rank 0, Epoch 3830, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,150: INFO: model_training: Rank 0, Epoch 3830, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,151: INFO: model_training: Rank 0, Epoch 3830, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,152: INFO: model_training: Rank 0, Epoch 3830, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,153: INFO: model_training: Rank 0, Epoch 3830, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,154: INFO: model_training: Rank 0, Epoch 3831, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,156: INFO: model_training: Rank 0, Epoch 3831, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,157: INFO: model_training: Rank 0, Epoch 3831, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,158: INFO: model_training: Rank 0, Epoch 3831, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,159: INFO: model_training: Rank 0, Epoch 3831, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,160: INFO: model_training: Rank 0, Epoch 3832, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,161: INFO: model_training: Rank 0, Epoch 3832, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,162: INFO: model_training: Rank 0, Epoch 3832, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,163: INFO: model_training: Rank 0, Epoch 3832, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,164: INFO: model_training: Rank 0, Epoch 3832, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,166: INFO: model_training: Rank 0, Epoch 3833, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,167: INFO: model_training: Rank 0, Epoch 3833, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,168: INFO: model_training: Rank 0, Epoch 3833, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,169: INFO: model_training: Rank 0, Epoch 3833, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,170: INFO: model_training: Rank 0, Epoch 3833, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,171: INFO: model_training: Rank 0, Epoch 3834, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,172: INFO: model_training: Rank 0, Epoch 3834, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,173: INFO: model_training: Rank 0, Epoch 3834, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,174: INFO: model_training: Rank 0, Epoch 3834, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,175: INFO: model_training: Rank 0, Epoch 3834, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,176: INFO: model_training: Rank 0, Epoch 3835, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,178: INFO: model_training: Rank 0, Epoch 3835, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,179: INFO: model_training: Rank 0, Epoch 3835, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,180: INFO: model_training: Rank 0, Epoch 3835, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,181: INFO: model_training: Rank 0, Epoch 3835, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,182: INFO: model_training: Rank 0, Epoch 3836, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,184: INFO: model_training: Rank 0, Epoch 3836, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,185: INFO: model_training: Rank 0, Epoch 3836, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,186: INFO: model_training: Rank 0, Epoch 3836, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,187: INFO: model_training: Rank 0, Epoch 3836, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,188: INFO: model_training: Rank 0, Epoch 3837, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,189: INFO: model_training: Rank 0, Epoch 3837, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,190: INFO: model_training: Rank 0, Epoch 3837, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,191: INFO: model_training: Rank 0, Epoch 3837, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,192: INFO: model_training: Rank 0, Epoch 3837, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,193: INFO: model_training: Rank 0, Epoch 3838, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,194: INFO: model_training: Rank 0, Epoch 3838, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,195: INFO: model_training: Rank 0, Epoch 3838, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,196: INFO: model_training: Rank 0, Epoch 3838, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,197: INFO: model_training: Rank 0, Epoch 3838, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,199: INFO: model_training: Rank 0, Epoch 3839, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,200: INFO: model_training: Rank 0, Epoch 3839, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,201: INFO: model_training: Rank 0, Epoch 3839, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,202: INFO: model_training: Rank 0, Epoch 3839, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,203: INFO: model_training: Rank 0, Epoch 3839, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,204: INFO: model_training: Rank 0, Epoch 3840, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,205: INFO: model_training: Rank 0, Epoch 3840, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,206: INFO: model_training: Rank 0, Epoch 3840, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,207: INFO: model_training: Rank 0, Epoch 3840, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,208: INFO: model_training: Rank 0, Epoch 3840, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,209: INFO: model_training: Rank 0, Epoch 3841, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,210: INFO: model_training: Rank 0, Epoch 3841, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,211: INFO: model_training: Rank 0, Epoch 3841, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,213: INFO: model_training: Rank 0, Epoch 3841, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,214: INFO: model_training: Rank 0, Epoch 3841, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,215: INFO: model_training: Rank 0, Epoch 3842, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,216: INFO: model_training: Rank 0, Epoch 3842, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,217: INFO: model_training: Rank 0, Epoch 3842, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,218: INFO: model_training: Rank 0, Epoch 3842, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,219: INFO: model_training: Rank 0, Epoch 3842, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,220: INFO: model_training: Rank 0, Epoch 3843, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,221: INFO: model_training: Rank 0, Epoch 3843, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,222: INFO: model_training: Rank 0, Epoch 3843, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,223: INFO: model_training: Rank 0, Epoch 3843, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,225: INFO: model_training: Rank 0, Epoch 3843, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,226: INFO: model_training: Rank 0, Epoch 3844, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,227: INFO: model_training: Rank 0, Epoch 3844, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,228: INFO: model_training: Rank 0, Epoch 3844, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,229: INFO: model_training: Rank 0, Epoch 3844, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,230: INFO: model_training: Rank 0, Epoch 3844, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,231: INFO: model_training: Rank 0, Epoch 3845, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,232: INFO: model_training: Rank 0, Epoch 3845, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,233: INFO: model_training: Rank 0, Epoch 3845, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,235: INFO: model_training: Rank 0, Epoch 3845, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,235: INFO: model_training: Rank 0, Epoch 3845, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,236: INFO: model_training: Rank 0, Epoch 3846, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,237: INFO: model_training: Rank 0, Epoch 3846, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,238: INFO: model_training: Rank 0, Epoch 3846, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,240: INFO: model_training: Rank 0, Epoch 3846, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,241: INFO: model_training: Rank 0, Epoch 3846, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,242: INFO: model_training: Rank 0, Epoch 3847, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,243: INFO: model_training: Rank 0, Epoch 3847, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,244: INFO: model_training: Rank 0, Epoch 3847, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,245: INFO: model_training: Rank 0, Epoch 3847, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,246: INFO: model_training: Rank 0, Epoch 3847, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,248: INFO: model_training: Rank 0, Epoch 3848, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,249: INFO: model_training: Rank 0, Epoch 3848, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,251: INFO: model_training: Rank 0, Epoch 3848, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,252: INFO: model_training: Rank 0, Epoch 3848, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,254: INFO: model_training: Rank 0, Epoch 3848, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,255: INFO: model_training: Rank 0, Epoch 3849, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,256: INFO: model_training: Rank 0, Epoch 3849, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,258: INFO: model_training: Rank 0, Epoch 3849, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,259: INFO: model_training: Rank 0, Epoch 3849, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,261: INFO: model_training: Rank 0, Epoch 3849, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,262: INFO: model_training: Rank 0, Epoch 3850, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,263: INFO: model_training: Rank 0, Epoch 3850, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,265: INFO: model_training: Rank 0, Epoch 3850, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,266: INFO: model_training: Rank 0, Epoch 3850, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,267: INFO: model_training: Rank 0, Epoch 3850, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,268: INFO: model_training: Rank 0, Epoch 3851, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,270: INFO: model_training: Rank 0, Epoch 3851, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,271: INFO: model_training: Rank 0, Epoch 3851, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,272: INFO: model_training: Rank 0, Epoch 3851, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,273: INFO: model_training: Rank 0, Epoch 3851, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,275: INFO: model_training: Rank 0, Epoch 3852, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,276: INFO: model_training: Rank 0, Epoch 3852, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,278: INFO: model_training: Rank 0, Epoch 3852, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,280: INFO: model_training: Rank 0, Epoch 3852, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,281: INFO: model_training: Rank 0, Epoch 3852, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,282: INFO: model_training: Rank 0, Epoch 3853, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,283: INFO: model_training: Rank 0, Epoch 3853, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,284: INFO: model_training: Rank 0, Epoch 3853, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,286: INFO: model_training: Rank 0, Epoch 3853, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,287: INFO: model_training: Rank 0, Epoch 3853, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,289: INFO: model_training: Rank 0, Epoch 3854, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,290: INFO: model_training: Rank 0, Epoch 3854, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,292: INFO: model_training: Rank 0, Epoch 3854, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,293: INFO: model_training: Rank 0, Epoch 3854, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,296: INFO: model_training: Rank 0, Epoch 3854, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,298: INFO: model_training: Rank 0, Epoch 3855, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,299: INFO: model_training: Rank 0, Epoch 3855, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,301: INFO: model_training: Rank 0, Epoch 3855, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,302: INFO: model_training: Rank 0, Epoch 3855, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,303: INFO: model_training: Rank 0, Epoch 3855, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,305: INFO: model_training: Rank 0, Epoch 3856, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,307: INFO: model_training: Rank 0, Epoch 3856, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,309: INFO: model_training: Rank 0, Epoch 3856, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,311: INFO: model_training: Rank 0, Epoch 3856, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,312: INFO: model_training: Rank 0, Epoch 3856, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,313: INFO: model_training: Rank 0, Epoch 3857, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,315: INFO: model_training: Rank 0, Epoch 3857, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,316: INFO: model_training: Rank 0, Epoch 3857, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,317: INFO: model_training: Rank 0, Epoch 3857, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,318: INFO: model_training: Rank 0, Epoch 3857, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,319: INFO: model_training: Rank 0, Epoch 3858, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,320: INFO: model_training: Rank 0, Epoch 3858, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,321: INFO: model_training: Rank 0, Epoch 3858, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,323: INFO: model_training: Rank 0, Epoch 3858, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,324: INFO: model_training: Rank 0, Epoch 3858, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,325: INFO: model_training: Rank 0, Epoch 3859, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,326: INFO: model_training: Rank 0, Epoch 3859, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,327: INFO: model_training: Rank 0, Epoch 3859, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,329: INFO: model_training: Rank 0, Epoch 3859, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,330: INFO: model_training: Rank 0, Epoch 3859, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,331: INFO: model_training: Rank 0, Epoch 3860, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,332: INFO: model_training: Rank 0, Epoch 3860, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,333: INFO: model_training: Rank 0, Epoch 3860, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,334: INFO: model_training: Rank 0, Epoch 3860, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,335: INFO: model_training: Rank 0, Epoch 3860, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,337: INFO: model_training: Rank 0, Epoch 3861, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,338: INFO: model_training: Rank 0, Epoch 3861, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,339: INFO: model_training: Rank 0, Epoch 3861, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,340: INFO: model_training: Rank 0, Epoch 3861, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,341: INFO: model_training: Rank 0, Epoch 3861, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,342: INFO: model_training: Rank 0, Epoch 3862, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,343: INFO: model_training: Rank 0, Epoch 3862, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,344: INFO: model_training: Rank 0, Epoch 3862, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,345: INFO: model_training: Rank 0, Epoch 3862, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,346: INFO: model_training: Rank 0, Epoch 3862, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,347: INFO: model_training: Rank 0, Epoch 3863, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,349: INFO: model_training: Rank 0, Epoch 3863, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,350: INFO: model_training: Rank 0, Epoch 3863, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,351: INFO: model_training: Rank 0, Epoch 3863, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,352: INFO: model_training: Rank 0, Epoch 3863, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,353: INFO: model_training: Rank 0, Epoch 3864, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,354: INFO: model_training: Rank 0, Epoch 3864, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,355: INFO: model_training: Rank 0, Epoch 3864, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,356: INFO: model_training: Rank 0, Epoch 3864, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,358: INFO: model_training: Rank 0, Epoch 3864, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,359: INFO: model_training: Rank 0, Epoch 3865, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,360: INFO: model_training: Rank 0, Epoch 3865, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,361: INFO: model_training: Rank 0, Epoch 3865, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,362: INFO: model_training: Rank 0, Epoch 3865, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,363: INFO: model_training: Rank 0, Epoch 3865, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,364: INFO: model_training: Rank 0, Epoch 3866, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,365: INFO: model_training: Rank 0, Epoch 3866, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,366: INFO: model_training: Rank 0, Epoch 3866, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,367: INFO: model_training: Rank 0, Epoch 3866, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,369: INFO: model_training: Rank 0, Epoch 3866, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,370: INFO: model_training: Rank 0, Epoch 3867, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,371: INFO: model_training: Rank 0, Epoch 3867, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,372: INFO: model_training: Rank 0, Epoch 3867, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,374: INFO: model_training: Rank 0, Epoch 3867, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,375: INFO: model_training: Rank 0, Epoch 3867, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,376: INFO: model_training: Rank 0, Epoch 3868, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,377: INFO: model_training: Rank 0, Epoch 3868, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,378: INFO: model_training: Rank 0, Epoch 3868, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,379: INFO: model_training: Rank 0, Epoch 3868, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,380: INFO: model_training: Rank 0, Epoch 3868, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,381: INFO: model_training: Rank 0, Epoch 3869, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,382: INFO: model_training: Rank 0, Epoch 3869, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,385: INFO: model_training: Rank 0, Epoch 3869, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,386: INFO: model_training: Rank 0, Epoch 3869, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,388: INFO: model_training: Rank 0, Epoch 3869, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,389: INFO: model_training: Rank 0, Epoch 3870, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,390: INFO: model_training: Rank 0, Epoch 3870, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,391: INFO: model_training: Rank 0, Epoch 3870, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,392: INFO: model_training: Rank 0, Epoch 3870, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,393: INFO: model_training: Rank 0, Epoch 3870, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,394: INFO: model_training: Rank 0, Epoch 3871, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,395: INFO: model_training: Rank 0, Epoch 3871, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,397: INFO: model_training: Rank 0, Epoch 3871, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,398: INFO: model_training: Rank 0, Epoch 3871, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,400: INFO: model_training: Rank 0, Epoch 3871, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,402: INFO: model_training: Rank 0, Epoch 3872, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,403: INFO: model_training: Rank 0, Epoch 3872, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,404: INFO: model_training: Rank 0, Epoch 3872, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,405: INFO: model_training: Rank 0, Epoch 3872, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,406: INFO: model_training: Rank 0, Epoch 3872, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,408: INFO: model_training: Rank 0, Epoch 3873, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,409: INFO: model_training: Rank 0, Epoch 3873, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,410: INFO: model_training: Rank 0, Epoch 3873, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,412: INFO: model_training: Rank 0, Epoch 3873, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,413: INFO: model_training: Rank 0, Epoch 3873, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,414: INFO: model_training: Rank 0, Epoch 3874, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,415: INFO: model_training: Rank 0, Epoch 3874, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,416: INFO: model_training: Rank 0, Epoch 3874, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,417: INFO: model_training: Rank 0, Epoch 3874, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,418: INFO: model_training: Rank 0, Epoch 3874, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,420: INFO: model_training: Rank 0, Epoch 3875, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,422: INFO: model_training: Rank 0, Epoch 3875, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,423: INFO: model_training: Rank 0, Epoch 3875, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,424: INFO: model_training: Rank 0, Epoch 3875, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,426: INFO: model_training: Rank 0, Epoch 3875, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,427: INFO: model_training: Rank 0, Epoch 3876, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,429: INFO: model_training: Rank 0, Epoch 3876, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,430: INFO: model_training: Rank 0, Epoch 3876, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,431: INFO: model_training: Rank 0, Epoch 3876, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,432: INFO: model_training: Rank 0, Epoch 3876, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,433: INFO: model_training: Rank 0, Epoch 3877, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,434: INFO: model_training: Rank 0, Epoch 3877, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,435: INFO: model_training: Rank 0, Epoch 3877, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,436: INFO: model_training: Rank 0, Epoch 3877, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,438: INFO: model_training: Rank 0, Epoch 3877, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,439: INFO: model_training: Rank 0, Epoch 3878, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,440: INFO: model_training: Rank 0, Epoch 3878, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,441: INFO: model_training: Rank 0, Epoch 3878, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,442: INFO: model_training: Rank 0, Epoch 3878, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,443: INFO: model_training: Rank 0, Epoch 3878, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,445: INFO: model_training: Rank 0, Epoch 3879, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,446: INFO: model_training: Rank 0, Epoch 3879, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,447: INFO: model_training: Rank 0, Epoch 3879, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,448: INFO: model_training: Rank 0, Epoch 3879, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,449: INFO: model_training: Rank 0, Epoch 3879, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,450: INFO: model_training: Rank 0, Epoch 3880, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,451: INFO: model_training: Rank 0, Epoch 3880, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,452: INFO: model_training: Rank 0, Epoch 3880, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,453: INFO: model_training: Rank 0, Epoch 3880, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,454: INFO: model_training: Rank 0, Epoch 3880, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,456: INFO: model_training: Rank 0, Epoch 3881, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,457: INFO: model_training: Rank 0, Epoch 3881, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,458: INFO: model_training: Rank 0, Epoch 3881, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,459: INFO: model_training: Rank 0, Epoch 3881, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,460: INFO: model_training: Rank 0, Epoch 3881, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,461: INFO: model_training: Rank 0, Epoch 3882, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,462: INFO: model_training: Rank 0, Epoch 3882, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,463: INFO: model_training: Rank 0, Epoch 3882, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,464: INFO: model_training: Rank 0, Epoch 3882, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,466: INFO: model_training: Rank 0, Epoch 3882, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,467: INFO: model_training: Rank 0, Epoch 3883, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,468: INFO: model_training: Rank 0, Epoch 3883, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,469: INFO: model_training: Rank 0, Epoch 3883, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,470: INFO: model_training: Rank 0, Epoch 3883, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,471: INFO: model_training: Rank 0, Epoch 3883, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,472: INFO: model_training: Rank 0, Epoch 3884, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,473: INFO: model_training: Rank 0, Epoch 3884, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,475: INFO: model_training: Rank 0, Epoch 3884, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,476: INFO: model_training: Rank 0, Epoch 3884, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,477: INFO: model_training: Rank 0, Epoch 3884, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,478: INFO: model_training: Rank 0, Epoch 3885, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,479: INFO: model_training: Rank 0, Epoch 3885, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,481: INFO: model_training: Rank 0, Epoch 3885, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,482: INFO: model_training: Rank 0, Epoch 3885, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,483: INFO: model_training: Rank 0, Epoch 3885, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,484: INFO: model_training: Rank 0, Epoch 3886, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,485: INFO: model_training: Rank 0, Epoch 3886, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,486: INFO: model_training: Rank 0, Epoch 3886, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,488: INFO: model_training: Rank 0, Epoch 3886, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,489: INFO: model_training: Rank 0, Epoch 3886, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,491: INFO: model_training: Rank 0, Epoch 3887, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,491: INFO: model_training: Rank 0, Epoch 3887, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,493: INFO: model_training: Rank 0, Epoch 3887, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,494: INFO: model_training: Rank 0, Epoch 3887, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,495: INFO: model_training: Rank 0, Epoch 3887, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,496: INFO: model_training: Rank 0, Epoch 3888, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,497: INFO: model_training: Rank 0, Epoch 3888, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,497: INFO: model_training: Rank 0, Epoch 3888, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,498: INFO: model_training: Rank 0, Epoch 3888, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,499: INFO: model_training: Rank 0, Epoch 3888, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,500: INFO: model_training: Rank 0, Epoch 3889, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,502: INFO: model_training: Rank 0, Epoch 3889, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,503: INFO: model_training: Rank 0, Epoch 3889, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,504: INFO: model_training: Rank 0, Epoch 3889, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,505: INFO: model_training: Rank 0, Epoch 3889, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,506: INFO: model_training: Rank 0, Epoch 3890, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,507: INFO: model_training: Rank 0, Epoch 3890, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,508: INFO: model_training: Rank 0, Epoch 3890, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,510: INFO: model_training: Rank 0, Epoch 3890, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,511: INFO: model_training: Rank 0, Epoch 3890, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,512: INFO: model_training: Rank 0, Epoch 3891, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,513: INFO: model_training: Rank 0, Epoch 3891, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,515: INFO: model_training: Rank 0, Epoch 3891, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,516: INFO: model_training: Rank 0, Epoch 3891, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,517: INFO: model_training: Rank 0, Epoch 3891, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,518: INFO: model_training: Rank 0, Epoch 3892, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,519: INFO: model_training: Rank 0, Epoch 3892, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,521: INFO: model_training: Rank 0, Epoch 3892, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,522: INFO: model_training: Rank 0, Epoch 3892, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,523: INFO: model_training: Rank 0, Epoch 3892, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,525: INFO: model_training: Rank 0, Epoch 3893, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,526: INFO: model_training: Rank 0, Epoch 3893, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,527: INFO: model_training: Rank 0, Epoch 3893, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,528: INFO: model_training: Rank 0, Epoch 3893, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,529: INFO: model_training: Rank 0, Epoch 3893, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,531: INFO: model_training: Rank 0, Epoch 3894, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,533: INFO: model_training: Rank 0, Epoch 3894, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,534: INFO: model_training: Rank 0, Epoch 3894, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,535: INFO: model_training: Rank 0, Epoch 3894, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,537: INFO: model_training: Rank 0, Epoch 3894, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,538: INFO: model_training: Rank 0, Epoch 3895, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,539: INFO: model_training: Rank 0, Epoch 3895, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,540: INFO: model_training: Rank 0, Epoch 3895, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,542: INFO: model_training: Rank 0, Epoch 3895, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,543: INFO: model_training: Rank 0, Epoch 3895, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,545: INFO: model_training: Rank 0, Epoch 3896, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,546: INFO: model_training: Rank 0, Epoch 3896, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,547: INFO: model_training: Rank 0, Epoch 3896, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,548: INFO: model_training: Rank 0, Epoch 3896, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,550: INFO: model_training: Rank 0, Epoch 3896, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,551: INFO: model_training: Rank 0, Epoch 3897, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,553: INFO: model_training: Rank 0, Epoch 3897, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,554: INFO: model_training: Rank 0, Epoch 3897, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,555: INFO: model_training: Rank 0, Epoch 3897, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,556: INFO: model_training: Rank 0, Epoch 3897, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,557: INFO: model_training: Rank 0, Epoch 3898, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,558: INFO: model_training: Rank 0, Epoch 3898, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,559: INFO: model_training: Rank 0, Epoch 3898, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,560: INFO: model_training: Rank 0, Epoch 3898, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,561: INFO: model_training: Rank 0, Epoch 3898, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,562: INFO: model_training: Rank 0, Epoch 3899, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,563: INFO: model_training: Rank 0, Epoch 3899, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,564: INFO: model_training: Rank 0, Epoch 3899, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,565: INFO: model_training: Rank 0, Epoch 3899, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,566: INFO: model_training: Rank 0, Epoch 3899, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,567: INFO: model_training: Rank 0, Epoch 3900, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,569: INFO: model_training: Rank 0, Epoch 3900, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,570: INFO: model_training: Rank 0, Epoch 3900, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,571: INFO: model_training: Rank 0, Epoch 3900, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,572: INFO: model_training: Rank 0, Epoch 3900, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,573: INFO: model_training: Rank 0, Epoch 3901, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,574: INFO: model_training: Rank 0, Epoch 3901, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,575: INFO: model_training: Rank 0, Epoch 3901, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,576: INFO: model_training: Rank 0, Epoch 3901, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,577: INFO: model_training: Rank 0, Epoch 3901, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,578: INFO: model_training: Rank 0, Epoch 3902, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,579: INFO: model_training: Rank 0, Epoch 3902, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,580: INFO: model_training: Rank 0, Epoch 3902, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,581: INFO: model_training: Rank 0, Epoch 3902, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,582: INFO: model_training: Rank 0, Epoch 3902, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,583: INFO: model_training: Rank 0, Epoch 3903, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,585: INFO: model_training: Rank 0, Epoch 3903, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,586: INFO: model_training: Rank 0, Epoch 3903, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,587: INFO: model_training: Rank 0, Epoch 3903, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,588: INFO: model_training: Rank 0, Epoch 3903, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,589: INFO: model_training: Rank 0, Epoch 3904, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,590: INFO: model_training: Rank 0, Epoch 3904, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,591: INFO: model_training: Rank 0, Epoch 3904, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,592: INFO: model_training: Rank 0, Epoch 3904, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,593: INFO: model_training: Rank 0, Epoch 3904, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,594: INFO: model_training: Rank 0, Epoch 3905, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,595: INFO: model_training: Rank 0, Epoch 3905, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,596: INFO: model_training: Rank 0, Epoch 3905, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,597: INFO: model_training: Rank 0, Epoch 3905, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,598: INFO: model_training: Rank 0, Epoch 3905, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,599: INFO: model_training: Rank 0, Epoch 3906, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,600: INFO: model_training: Rank 0, Epoch 3906, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,601: INFO: model_training: Rank 0, Epoch 3906, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,602: INFO: model_training: Rank 0, Epoch 3906, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,604: INFO: model_training: Rank 0, Epoch 3906, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,605: INFO: model_training: Rank 0, Epoch 3907, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,606: INFO: model_training: Rank 0, Epoch 3907, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,607: INFO: model_training: Rank 0, Epoch 3907, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,608: INFO: model_training: Rank 0, Epoch 3907, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,608: INFO: model_training: Rank 0, Epoch 3907, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,609: INFO: model_training: Rank 0, Epoch 3908, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,610: INFO: model_training: Rank 0, Epoch 3908, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,611: INFO: model_training: Rank 0, Epoch 3908, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,612: INFO: model_training: Rank 0, Epoch 3908, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,614: INFO: model_training: Rank 0, Epoch 3908, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,615: INFO: model_training: Rank 0, Epoch 3909, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,616: INFO: model_training: Rank 0, Epoch 3909, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,617: INFO: model_training: Rank 0, Epoch 3909, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,618: INFO: model_training: Rank 0, Epoch 3909, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,619: INFO: model_training: Rank 0, Epoch 3909, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,620: INFO: model_training: Rank 0, Epoch 3910, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,621: INFO: model_training: Rank 0, Epoch 3910, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,622: INFO: model_training: Rank 0, Epoch 3910, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,623: INFO: model_training: Rank 0, Epoch 3910, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,624: INFO: model_training: Rank 0, Epoch 3910, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,625: INFO: model_training: Rank 0, Epoch 3911, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,626: INFO: model_training: Rank 0, Epoch 3911, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,627: INFO: model_training: Rank 0, Epoch 3911, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,628: INFO: model_training: Rank 0, Epoch 3911, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,629: INFO: model_training: Rank 0, Epoch 3911, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,630: INFO: model_training: Rank 0, Epoch 3912, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,631: INFO: model_training: Rank 0, Epoch 3912, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,632: INFO: model_training: Rank 0, Epoch 3912, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,633: INFO: model_training: Rank 0, Epoch 3912, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,634: INFO: model_training: Rank 0, Epoch 3912, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,635: INFO: model_training: Rank 0, Epoch 3913, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,636: INFO: model_training: Rank 0, Epoch 3913, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,637: INFO: model_training: Rank 0, Epoch 3913, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,638: INFO: model_training: Rank 0, Epoch 3913, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,639: INFO: model_training: Rank 0, Epoch 3913, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,640: INFO: model_training: Rank 0, Epoch 3914, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,641: INFO: model_training: Rank 0, Epoch 3914, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,642: INFO: model_training: Rank 0, Epoch 3914, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,644: INFO: model_training: Rank 0, Epoch 3914, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,645: INFO: model_training: Rank 0, Epoch 3914, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,646: INFO: model_training: Rank 0, Epoch 3915, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,647: INFO: model_training: Rank 0, Epoch 3915, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,648: INFO: model_training: Rank 0, Epoch 3915, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,648: INFO: model_training: Rank 0, Epoch 3915, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,650: INFO: model_training: Rank 0, Epoch 3915, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,651: INFO: model_training: Rank 0, Epoch 3916, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,652: INFO: model_training: Rank 0, Epoch 3916, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,653: INFO: model_training: Rank 0, Epoch 3916, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,654: INFO: model_training: Rank 0, Epoch 3916, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,655: INFO: model_training: Rank 0, Epoch 3916, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,656: INFO: model_training: Rank 0, Epoch 3917, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,657: INFO: model_training: Rank 0, Epoch 3917, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,658: INFO: model_training: Rank 0, Epoch 3917, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,659: INFO: model_training: Rank 0, Epoch 3917, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,660: INFO: model_training: Rank 0, Epoch 3917, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,661: INFO: model_training: Rank 0, Epoch 3918, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,662: INFO: model_training: Rank 0, Epoch 3918, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,663: INFO: model_training: Rank 0, Epoch 3918, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,664: INFO: model_training: Rank 0, Epoch 3918, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,665: INFO: model_training: Rank 0, Epoch 3918, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,667: INFO: model_training: Rank 0, Epoch 3919, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,668: INFO: model_training: Rank 0, Epoch 3919, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,669: INFO: model_training: Rank 0, Epoch 3919, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,670: INFO: model_training: Rank 0, Epoch 3919, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,671: INFO: model_training: Rank 0, Epoch 3919, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,672: INFO: model_training: Rank 0, Epoch 3920, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,673: INFO: model_training: Rank 0, Epoch 3920, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,674: INFO: model_training: Rank 0, Epoch 3920, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,675: INFO: model_training: Rank 0, Epoch 3920, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,677: INFO: model_training: Rank 0, Epoch 3920, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,678: INFO: model_training: Rank 0, Epoch 3921, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,679: INFO: model_training: Rank 0, Epoch 3921, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,680: INFO: model_training: Rank 0, Epoch 3921, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,681: INFO: model_training: Rank 0, Epoch 3921, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,682: INFO: model_training: Rank 0, Epoch 3921, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,683: INFO: model_training: Rank 0, Epoch 3922, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,684: INFO: model_training: Rank 0, Epoch 3922, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,685: INFO: model_training: Rank 0, Epoch 3922, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,686: INFO: model_training: Rank 0, Epoch 3922, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,687: INFO: model_training: Rank 0, Epoch 3922, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,688: INFO: model_training: Rank 0, Epoch 3923, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,689: INFO: model_training: Rank 0, Epoch 3923, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,690: INFO: model_training: Rank 0, Epoch 3923, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,691: INFO: model_training: Rank 0, Epoch 3923, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,693: INFO: model_training: Rank 0, Epoch 3923, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,694: INFO: model_training: Rank 0, Epoch 3924, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,695: INFO: model_training: Rank 0, Epoch 3924, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,696: INFO: model_training: Rank 0, Epoch 3924, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,697: INFO: model_training: Rank 0, Epoch 3924, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,698: INFO: model_training: Rank 0, Epoch 3924, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,699: INFO: model_training: Rank 0, Epoch 3925, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,700: INFO: model_training: Rank 0, Epoch 3925, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,701: INFO: model_training: Rank 0, Epoch 3925, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,702: INFO: model_training: Rank 0, Epoch 3925, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,703: INFO: model_training: Rank 0, Epoch 3925, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,704: INFO: model_training: Rank 0, Epoch 3926, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,705: INFO: model_training: Rank 0, Epoch 3926, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,706: INFO: model_training: Rank 0, Epoch 3926, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,707: INFO: model_training: Rank 0, Epoch 3926, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,708: INFO: model_training: Rank 0, Epoch 3926, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,709: INFO: model_training: Rank 0, Epoch 3927, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,710: INFO: model_training: Rank 0, Epoch 3927, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,711: INFO: model_training: Rank 0, Epoch 3927, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,712: INFO: model_training: Rank 0, Epoch 3927, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,713: INFO: model_training: Rank 0, Epoch 3927, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,714: INFO: model_training: Rank 0, Epoch 3928, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,715: INFO: model_training: Rank 0, Epoch 3928, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,717: INFO: model_training: Rank 0, Epoch 3928, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,718: INFO: model_training: Rank 0, Epoch 3928, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,719: INFO: model_training: Rank 0, Epoch 3928, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,720: INFO: model_training: Rank 0, Epoch 3929, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,721: INFO: model_training: Rank 0, Epoch 3929, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,721: INFO: model_training: Rank 0, Epoch 3929, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,723: INFO: model_training: Rank 0, Epoch 3929, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,724: INFO: model_training: Rank 0, Epoch 3929, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,725: INFO: model_training: Rank 0, Epoch 3930, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,726: INFO: model_training: Rank 0, Epoch 3930, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,727: INFO: model_training: Rank 0, Epoch 3930, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,728: INFO: model_training: Rank 0, Epoch 3930, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,729: INFO: model_training: Rank 0, Epoch 3930, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,730: INFO: model_training: Rank 0, Epoch 3931, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,731: INFO: model_training: Rank 0, Epoch 3931, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,732: INFO: model_training: Rank 0, Epoch 3931, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,733: INFO: model_training: Rank 0, Epoch 3931, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,734: INFO: model_training: Rank 0, Epoch 3931, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,736: INFO: model_training: Rank 0, Epoch 3932, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,736: INFO: model_training: Rank 0, Epoch 3932, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,737: INFO: model_training: Rank 0, Epoch 3932, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,738: INFO: model_training: Rank 0, Epoch 3932, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,740: INFO: model_training: Rank 0, Epoch 3932, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,741: INFO: model_training: Rank 0, Epoch 3933, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,742: INFO: model_training: Rank 0, Epoch 3933, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,743: INFO: model_training: Rank 0, Epoch 3933, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,744: INFO: model_training: Rank 0, Epoch 3933, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,745: INFO: model_training: Rank 0, Epoch 3933, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,746: INFO: model_training: Rank 0, Epoch 3934, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,747: INFO: model_training: Rank 0, Epoch 3934, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,749: INFO: model_training: Rank 0, Epoch 3934, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,750: INFO: model_training: Rank 0, Epoch 3934, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,750: INFO: model_training: Rank 0, Epoch 3934, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,751: INFO: model_training: Rank 0, Epoch 3935, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,752: INFO: model_training: Rank 0, Epoch 3935, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,753: INFO: model_training: Rank 0, Epoch 3935, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,754: INFO: model_training: Rank 0, Epoch 3935, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,756: INFO: model_training: Rank 0, Epoch 3935, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,757: INFO: model_training: Rank 0, Epoch 3936, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,758: INFO: model_training: Rank 0, Epoch 3936, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,759: INFO: model_training: Rank 0, Epoch 3936, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,760: INFO: model_training: Rank 0, Epoch 3936, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,761: INFO: model_training: Rank 0, Epoch 3936, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,763: INFO: model_training: Rank 0, Epoch 3937, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,764: INFO: model_training: Rank 0, Epoch 3937, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,765: INFO: model_training: Rank 0, Epoch 3937, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,765: INFO: model_training: Rank 0, Epoch 3937, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,767: INFO: model_training: Rank 0, Epoch 3937, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,768: INFO: model_training: Rank 0, Epoch 3938, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,769: INFO: model_training: Rank 0, Epoch 3938, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,770: INFO: model_training: Rank 0, Epoch 3938, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,771: INFO: model_training: Rank 0, Epoch 3938, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,772: INFO: model_training: Rank 0, Epoch 3938, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,774: INFO: model_training: Rank 0, Epoch 3939, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,775: INFO: model_training: Rank 0, Epoch 3939, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,776: INFO: model_training: Rank 0, Epoch 3939, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,777: INFO: model_training: Rank 0, Epoch 3939, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,778: INFO: model_training: Rank 0, Epoch 3939, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,779: INFO: model_training: Rank 0, Epoch 3940, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,780: INFO: model_training: Rank 0, Epoch 3940, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,781: INFO: model_training: Rank 0, Epoch 3940, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,782: INFO: model_training: Rank 0, Epoch 3940, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,783: INFO: model_training: Rank 0, Epoch 3940, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,784: INFO: model_training: Rank 0, Epoch 3941, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,785: INFO: model_training: Rank 0, Epoch 3941, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,786: INFO: model_training: Rank 0, Epoch 3941, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,787: INFO: model_training: Rank 0, Epoch 3941, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,788: INFO: model_training: Rank 0, Epoch 3941, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,789: INFO: model_training: Rank 0, Epoch 3942, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,790: INFO: model_training: Rank 0, Epoch 3942, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,791: INFO: model_training: Rank 0, Epoch 3942, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,792: INFO: model_training: Rank 0, Epoch 3942, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,793: INFO: model_training: Rank 0, Epoch 3942, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,794: INFO: model_training: Rank 0, Epoch 3943, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,795: INFO: model_training: Rank 0, Epoch 3943, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,796: INFO: model_training: Rank 0, Epoch 3943, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,797: INFO: model_training: Rank 0, Epoch 3943, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,798: INFO: model_training: Rank 0, Epoch 3943, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,800: INFO: model_training: Rank 0, Epoch 3944, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,801: INFO: model_training: Rank 0, Epoch 3944, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,802: INFO: model_training: Rank 0, Epoch 3944, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,804: INFO: model_training: Rank 0, Epoch 3944, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,804: INFO: model_training: Rank 0, Epoch 3944, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,805: INFO: model_training: Rank 0, Epoch 3945, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,806: INFO: model_training: Rank 0, Epoch 3945, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,807: INFO: model_training: Rank 0, Epoch 3945, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,808: INFO: model_training: Rank 0, Epoch 3945, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,810: INFO: model_training: Rank 0, Epoch 3945, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,811: INFO: model_training: Rank 0, Epoch 3946, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,812: INFO: model_training: Rank 0, Epoch 3946, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,813: INFO: model_training: Rank 0, Epoch 3946, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,814: INFO: model_training: Rank 0, Epoch 3946, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,815: INFO: model_training: Rank 0, Epoch 3946, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,817: INFO: model_training: Rank 0, Epoch 3947, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,818: INFO: model_training: Rank 0, Epoch 3947, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,819: INFO: model_training: Rank 0, Epoch 3947, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,820: INFO: model_training: Rank 0, Epoch 3947, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,821: INFO: model_training: Rank 0, Epoch 3947, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,822: INFO: model_training: Rank 0, Epoch 3948, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,823: INFO: model_training: Rank 0, Epoch 3948, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,824: INFO: model_training: Rank 0, Epoch 3948, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,825: INFO: model_training: Rank 0, Epoch 3948, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,826: INFO: model_training: Rank 0, Epoch 3948, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,827: INFO: model_training: Rank 0, Epoch 3949, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,829: INFO: model_training: Rank 0, Epoch 3949, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,829: INFO: model_training: Rank 0, Epoch 3949, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,830: INFO: model_training: Rank 0, Epoch 3949, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,831: INFO: model_training: Rank 0, Epoch 3949, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,832: INFO: model_training: Rank 0, Epoch 3950, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,833: INFO: model_training: Rank 0, Epoch 3950, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,834: INFO: model_training: Rank 0, Epoch 3950, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,835: INFO: model_training: Rank 0, Epoch 3950, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,836: INFO: model_training: Rank 0, Epoch 3950, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,838: INFO: model_training: Rank 0, Epoch 3951, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,838: INFO: model_training: Rank 0, Epoch 3951, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,840: INFO: model_training: Rank 0, Epoch 3951, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,841: INFO: model_training: Rank 0, Epoch 3951, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,842: INFO: model_training: Rank 0, Epoch 3951, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,843: INFO: model_training: Rank 0, Epoch 3952, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,844: INFO: model_training: Rank 0, Epoch 3952, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,845: INFO: model_training: Rank 0, Epoch 3952, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,846: INFO: model_training: Rank 0, Epoch 3952, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,847: INFO: model_training: Rank 0, Epoch 3952, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,848: INFO: model_training: Rank 0, Epoch 3953, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,849: INFO: model_training: Rank 0, Epoch 3953, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,850: INFO: model_training: Rank 0, Epoch 3953, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,851: INFO: model_training: Rank 0, Epoch 3953, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,852: INFO: model_training: Rank 0, Epoch 3953, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,853: INFO: model_training: Rank 0, Epoch 3954, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,854: INFO: model_training: Rank 0, Epoch 3954, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,855: INFO: model_training: Rank 0, Epoch 3954, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,856: INFO: model_training: Rank 0, Epoch 3954, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,857: INFO: model_training: Rank 0, Epoch 3954, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,859: INFO: model_training: Rank 0, Epoch 3955, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,860: INFO: model_training: Rank 0, Epoch 3955, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,861: INFO: model_training: Rank 0, Epoch 3955, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,862: INFO: model_training: Rank 0, Epoch 3955, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,863: INFO: model_training: Rank 0, Epoch 3955, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,864: INFO: model_training: Rank 0, Epoch 3956, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,866: INFO: model_training: Rank 0, Epoch 3956, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,867: INFO: model_training: Rank 0, Epoch 3956, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,868: INFO: model_training: Rank 0, Epoch 3956, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,869: INFO: model_training: Rank 0, Epoch 3956, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,870: INFO: model_training: Rank 0, Epoch 3957, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,872: INFO: model_training: Rank 0, Epoch 3957, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,873: INFO: model_training: Rank 0, Epoch 3957, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,874: INFO: model_training: Rank 0, Epoch 3957, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,875: INFO: model_training: Rank 0, Epoch 3957, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,877: INFO: model_training: Rank 0, Epoch 3958, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,878: INFO: model_training: Rank 0, Epoch 3958, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,879: INFO: model_training: Rank 0, Epoch 3958, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,880: INFO: model_training: Rank 0, Epoch 3958, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,882: INFO: model_training: Rank 0, Epoch 3958, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,883: INFO: model_training: Rank 0, Epoch 3959, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,884: INFO: model_training: Rank 0, Epoch 3959, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,885: INFO: model_training: Rank 0, Epoch 3959, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,886: INFO: model_training: Rank 0, Epoch 3959, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,887: INFO: model_training: Rank 0, Epoch 3959, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,888: INFO: model_training: Rank 0, Epoch 3960, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,889: INFO: model_training: Rank 0, Epoch 3960, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,890: INFO: model_training: Rank 0, Epoch 3960, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,892: INFO: model_training: Rank 0, Epoch 3960, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,893: INFO: model_training: Rank 0, Epoch 3960, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,894: INFO: model_training: Rank 0, Epoch 3961, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,895: INFO: model_training: Rank 0, Epoch 3961, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,897: INFO: model_training: Rank 0, Epoch 3961, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,898: INFO: model_training: Rank 0, Epoch 3961, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,899: INFO: model_training: Rank 0, Epoch 3961, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,900: INFO: model_training: Rank 0, Epoch 3962, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,902: INFO: model_training: Rank 0, Epoch 3962, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,903: INFO: model_training: Rank 0, Epoch 3962, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,904: INFO: model_training: Rank 0, Epoch 3962, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,905: INFO: model_training: Rank 0, Epoch 3962, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,907: INFO: model_training: Rank 0, Epoch 3963, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,908: INFO: model_training: Rank 0, Epoch 3963, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,909: INFO: model_training: Rank 0, Epoch 3963, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,910: INFO: model_training: Rank 0, Epoch 3963, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,911: INFO: model_training: Rank 0, Epoch 3963, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,912: INFO: model_training: Rank 0, Epoch 3964, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,914: INFO: model_training: Rank 0, Epoch 3964, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,915: INFO: model_training: Rank 0, Epoch 3964, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,916: INFO: model_training: Rank 0, Epoch 3964, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,918: INFO: model_training: Rank 0, Epoch 3964, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,919: INFO: model_training: Rank 0, Epoch 3965, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,920: INFO: model_training: Rank 0, Epoch 3965, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,921: INFO: model_training: Rank 0, Epoch 3965, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,922: INFO: model_training: Rank 0, Epoch 3965, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,923: INFO: model_training: Rank 0, Epoch 3965, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,924: INFO: model_training: Rank 0, Epoch 3966, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,926: INFO: model_training: Rank 0, Epoch 3966, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,927: INFO: model_training: Rank 0, Epoch 3966, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,928: INFO: model_training: Rank 0, Epoch 3966, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,929: INFO: model_training: Rank 0, Epoch 3966, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,930: INFO: model_training: Rank 0, Epoch 3967, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,932: INFO: model_training: Rank 0, Epoch 3967, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,933: INFO: model_training: Rank 0, Epoch 3967, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,934: INFO: model_training: Rank 0, Epoch 3967, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,935: INFO: model_training: Rank 0, Epoch 3967, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,936: INFO: model_training: Rank 0, Epoch 3968, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,937: INFO: model_training: Rank 0, Epoch 3968, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,938: INFO: model_training: Rank 0, Epoch 3968, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,940: INFO: model_training: Rank 0, Epoch 3968, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,941: INFO: model_training: Rank 0, Epoch 3968, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,942: INFO: model_training: Rank 0, Epoch 3969, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,943: INFO: model_training: Rank 0, Epoch 3969, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,944: INFO: model_training: Rank 0, Epoch 3969, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,945: INFO: model_training: Rank 0, Epoch 3969, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,947: INFO: model_training: Rank 0, Epoch 3969, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,948: INFO: model_training: Rank 0, Epoch 3970, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,949: INFO: model_training: Rank 0, Epoch 3970, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,950: INFO: model_training: Rank 0, Epoch 3970, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,951: INFO: model_training: Rank 0, Epoch 3970, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,952: INFO: model_training: Rank 0, Epoch 3970, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,953: INFO: model_training: Rank 0, Epoch 3971, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,954: INFO: model_training: Rank 0, Epoch 3971, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,956: INFO: model_training: Rank 0, Epoch 3971, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,957: INFO: model_training: Rank 0, Epoch 3971, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,958: INFO: model_training: Rank 0, Epoch 3971, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,959: INFO: model_training: Rank 0, Epoch 3972, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,960: INFO: model_training: Rank 0, Epoch 3972, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,961: INFO: model_training: Rank 0, Epoch 3972, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,962: INFO: model_training: Rank 0, Epoch 3972, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,963: INFO: model_training: Rank 0, Epoch 3972, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,965: INFO: model_training: Rank 0, Epoch 3973, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,967: INFO: model_training: Rank 0, Epoch 3973, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,968: INFO: model_training: Rank 0, Epoch 3973, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,969: INFO: model_training: Rank 0, Epoch 3973, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,970: INFO: model_training: Rank 0, Epoch 3973, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,971: INFO: model_training: Rank 0, Epoch 3974, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,972: INFO: model_training: Rank 0, Epoch 3974, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,974: INFO: model_training: Rank 0, Epoch 3974, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,975: INFO: model_training: Rank 0, Epoch 3974, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,976: INFO: model_training: Rank 0, Epoch 3974, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,977: INFO: model_training: Rank 0, Epoch 3975, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,978: INFO: model_training: Rank 0, Epoch 3975, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,979: INFO: model_training: Rank 0, Epoch 3975, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,981: INFO: model_training: Rank 0, Epoch 3975, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,982: INFO: model_training: Rank 0, Epoch 3975, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,983: INFO: model_training: Rank 0, Epoch 3976, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,984: INFO: model_training: Rank 0, Epoch 3976, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,986: INFO: model_training: Rank 0, Epoch 3976, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,987: INFO: model_training: Rank 0, Epoch 3976, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,988: INFO: model_training: Rank 0, Epoch 3976, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,989: INFO: model_training: Rank 0, Epoch 3977, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,990: INFO: model_training: Rank 0, Epoch 3977, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,991: INFO: model_training: Rank 0, Epoch 3977, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,992: INFO: model_training: Rank 0, Epoch 3977, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,993: INFO: model_training: Rank 0, Epoch 3977, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:51,994: INFO: model_training: Rank 0, Epoch 3978, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:51,996: INFO: model_training: Rank 0, Epoch 3978, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:51,997: INFO: model_training: Rank 0, Epoch 3978, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:51,998: INFO: model_training: Rank 0, Epoch 3978, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:51,999: INFO: model_training: Rank 0, Epoch 3978, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,000: INFO: model_training: Rank 0, Epoch 3979, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,001: INFO: model_training: Rank 0, Epoch 3979, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,002: INFO: model_training: Rank 0, Epoch 3979, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,003: INFO: model_training: Rank 0, Epoch 3979, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,005: INFO: model_training: Rank 0, Epoch 3979, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,006: INFO: model_training: Rank 0, Epoch 3980, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,007: INFO: model_training: Rank 0, Epoch 3980, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,008: INFO: model_training: Rank 0, Epoch 3980, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,009: INFO: model_training: Rank 0, Epoch 3980, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,010: INFO: model_training: Rank 0, Epoch 3980, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,011: INFO: model_training: Rank 0, Epoch 3981, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,012: INFO: model_training: Rank 0, Epoch 3981, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,014: INFO: model_training: Rank 0, Epoch 3981, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,016: INFO: model_training: Rank 0, Epoch 3981, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,017: INFO: model_training: Rank 0, Epoch 3981, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,018: INFO: model_training: Rank 0, Epoch 3982, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,019: INFO: model_training: Rank 0, Epoch 3982, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,020: INFO: model_training: Rank 0, Epoch 3982, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,021: INFO: model_training: Rank 0, Epoch 3982, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,023: INFO: model_training: Rank 0, Epoch 3982, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,024: INFO: model_training: Rank 0, Epoch 3983, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,026: INFO: model_training: Rank 0, Epoch 3983, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,027: INFO: model_training: Rank 0, Epoch 3983, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,028: INFO: model_training: Rank 0, Epoch 3983, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,029: INFO: model_training: Rank 0, Epoch 3983, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,030: INFO: model_training: Rank 0, Epoch 3984, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,032: INFO: model_training: Rank 0, Epoch 3984, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,035: INFO: model_training: Rank 0, Epoch 3984, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,037: INFO: model_training: Rank 0, Epoch 3984, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,039: INFO: model_training: Rank 0, Epoch 3984, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,041: INFO: model_training: Rank 0, Epoch 3985, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,042: INFO: model_training: Rank 0, Epoch 3985, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,043: INFO: model_training: Rank 0, Epoch 3985, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,045: INFO: model_training: Rank 0, Epoch 3985, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,046: INFO: model_training: Rank 0, Epoch 3985, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,047: INFO: model_training: Rank 0, Epoch 3986, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,048: INFO: model_training: Rank 0, Epoch 3986, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,050: INFO: model_training: Rank 0, Epoch 3986, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,051: INFO: model_training: Rank 0, Epoch 3986, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,052: INFO: model_training: Rank 0, Epoch 3986, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,053: INFO: model_training: Rank 0, Epoch 3987, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,055: INFO: model_training: Rank 0, Epoch 3987, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,056: INFO: model_training: Rank 0, Epoch 3987, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,057: INFO: model_training: Rank 0, Epoch 3987, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,058: INFO: model_training: Rank 0, Epoch 3987, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,059: INFO: model_training: Rank 0, Epoch 3988, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,060: INFO: model_training: Rank 0, Epoch 3988, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,061: INFO: model_training: Rank 0, Epoch 3988, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,062: INFO: model_training: Rank 0, Epoch 3988, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,064: INFO: model_training: Rank 0, Epoch 3988, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,065: INFO: model_training: Rank 0, Epoch 3989, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,066: INFO: model_training: Rank 0, Epoch 3989, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,067: INFO: model_training: Rank 0, Epoch 3989, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,068: INFO: model_training: Rank 0, Epoch 3989, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,069: INFO: model_training: Rank 0, Epoch 3989, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,071: INFO: model_training: Rank 0, Epoch 3990, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,072: INFO: model_training: Rank 0, Epoch 3990, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,073: INFO: model_training: Rank 0, Epoch 3990, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,074: INFO: model_training: Rank 0, Epoch 3990, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,075: INFO: model_training: Rank 0, Epoch 3990, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,076: INFO: model_training: Rank 0, Epoch 3991, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,077: INFO: model_training: Rank 0, Epoch 3991, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,078: INFO: model_training: Rank 0, Epoch 3991, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,080: INFO: model_training: Rank 0, Epoch 3991, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,082: INFO: model_training: Rank 0, Epoch 3991, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,083: INFO: model_training: Rank 0, Epoch 3992, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,084: INFO: model_training: Rank 0, Epoch 3992, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,085: INFO: model_training: Rank 0, Epoch 3992, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,087: INFO: model_training: Rank 0, Epoch 3992, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,088: INFO: model_training: Rank 0, Epoch 3992, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,089: INFO: model_training: Rank 0, Epoch 3993, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,090: INFO: model_training: Rank 0, Epoch 3993, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,092: INFO: model_training: Rank 0, Epoch 3993, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,093: INFO: model_training: Rank 0, Epoch 3993, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,095: INFO: model_training: Rank 0, Epoch 3993, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,096: INFO: model_training: Rank 0, Epoch 3994, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,097: INFO: model_training: Rank 0, Epoch 3994, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,098: INFO: model_training: Rank 0, Epoch 3994, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,099: INFO: model_training: Rank 0, Epoch 3994, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,100: INFO: model_training: Rank 0, Epoch 3994, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,101: INFO: model_training: Rank 0, Epoch 3995, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,102: INFO: model_training: Rank 0, Epoch 3995, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,104: INFO: model_training: Rank 0, Epoch 3995, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,105: INFO: model_training: Rank 0, Epoch 3995, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,106: INFO: model_training: Rank 0, Epoch 3995, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,107: INFO: model_training: Rank 0, Epoch 3996, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,108: INFO: model_training: Rank 0, Epoch 3996, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,109: INFO: model_training: Rank 0, Epoch 3996, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,110: INFO: model_training: Rank 0, Epoch 3996, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,111: INFO: model_training: Rank 0, Epoch 3996, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,112: INFO: model_training: Rank 0, Epoch 3997, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,113: INFO: model_training: Rank 0, Epoch 3997, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,114: INFO: model_training: Rank 0, Epoch 3997, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,115: INFO: model_training: Rank 0, Epoch 3997, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,116: INFO: model_training: Rank 0, Epoch 3997, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,117: INFO: model_training: Rank 0, Epoch 3998, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,119: INFO: model_training: Rank 0, Epoch 3998, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,120: INFO: model_training: Rank 0, Epoch 3998, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,121: INFO: model_training: Rank 0, Epoch 3998, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,122: INFO: model_training: Rank 0, Epoch 3998, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,123: INFO: model_training: Rank 0, Epoch 3999, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,124: INFO: model_training: Rank 0, Epoch 3999, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,126: INFO: model_training: Rank 0, Epoch 3999, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,127: INFO: model_training: Rank 0, Epoch 3999, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,128: INFO: model_training: Rank 0, Epoch 3999, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,129: INFO: model_training: Rank 0, Epoch 4000, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,130: INFO: model_training: Rank 0, Epoch 4000, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,131: INFO: model_training: Rank 0, Epoch 4000, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,132: INFO: model_training: Rank 0, Epoch 4000, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,133: INFO: model_training: Rank 0, Epoch 4000, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,134: INFO: model_training: Rank 0, Epoch 4001, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,135: INFO: model_training: Rank 0, Epoch 4001, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,136: INFO: model_training: Rank 0, Epoch 4001, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,138: INFO: model_training: Rank 0, Epoch 4001, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,139: INFO: model_training: Rank 0, Epoch 4001, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,140: INFO: model_training: Rank 0, Epoch 4002, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,141: INFO: model_training: Rank 0, Epoch 4002, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,141: INFO: model_training: Rank 0, Epoch 4002, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,143: INFO: model_training: Rank 0, Epoch 4002, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,144: INFO: model_training: Rank 0, Epoch 4002, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,145: INFO: model_training: Rank 0, Epoch 4003, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,146: INFO: model_training: Rank 0, Epoch 4003, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,147: INFO: model_training: Rank 0, Epoch 4003, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,148: INFO: model_training: Rank 0, Epoch 4003, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,150: INFO: model_training: Rank 0, Epoch 4003, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,151: INFO: model_training: Rank 0, Epoch 4004, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,152: INFO: model_training: Rank 0, Epoch 4004, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,153: INFO: model_training: Rank 0, Epoch 4004, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,154: INFO: model_training: Rank 0, Epoch 4004, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,155: INFO: model_training: Rank 0, Epoch 4004, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,156: INFO: model_training: Rank 0, Epoch 4005, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,157: INFO: model_training: Rank 0, Epoch 4005, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,158: INFO: model_training: Rank 0, Epoch 4005, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,159: INFO: model_training: Rank 0, Epoch 4005, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,160: INFO: model_training: Rank 0, Epoch 4005, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,161: INFO: model_training: Rank 0, Epoch 4006, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,162: INFO: model_training: Rank 0, Epoch 4006, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,164: INFO: model_training: Rank 0, Epoch 4006, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,165: INFO: model_training: Rank 0, Epoch 4006, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,166: INFO: model_training: Rank 0, Epoch 4006, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,167: INFO: model_training: Rank 0, Epoch 4007, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,168: INFO: model_training: Rank 0, Epoch 4007, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,169: INFO: model_training: Rank 0, Epoch 4007, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,170: INFO: model_training: Rank 0, Epoch 4007, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,171: INFO: model_training: Rank 0, Epoch 4007, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,172: INFO: model_training: Rank 0, Epoch 4008, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,173: INFO: model_training: Rank 0, Epoch 4008, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,174: INFO: model_training: Rank 0, Epoch 4008, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,175: INFO: model_training: Rank 0, Epoch 4008, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,176: INFO: model_training: Rank 0, Epoch 4008, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,177: INFO: model_training: Rank 0, Epoch 4009, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,179: INFO: model_training: Rank 0, Epoch 4009, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,179: INFO: model_training: Rank 0, Epoch 4009, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,180: INFO: model_training: Rank 0, Epoch 4009, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,181: INFO: model_training: Rank 0, Epoch 4009, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,182: INFO: model_training: Rank 0, Epoch 4010, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,183: INFO: model_training: Rank 0, Epoch 4010, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,184: INFO: model_training: Rank 0, Epoch 4010, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,185: INFO: model_training: Rank 0, Epoch 4010, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,186: INFO: model_training: Rank 0, Epoch 4010, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,187: INFO: model_training: Rank 0, Epoch 4011, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,188: INFO: model_training: Rank 0, Epoch 4011, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,189: INFO: model_training: Rank 0, Epoch 4011, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,190: INFO: model_training: Rank 0, Epoch 4011, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,191: INFO: model_training: Rank 0, Epoch 4011, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,192: INFO: model_training: Rank 0, Epoch 4012, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,193: INFO: model_training: Rank 0, Epoch 4012, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,194: INFO: model_training: Rank 0, Epoch 4012, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,196: INFO: model_training: Rank 0, Epoch 4012, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,197: INFO: model_training: Rank 0, Epoch 4012, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,198: INFO: model_training: Rank 0, Epoch 4013, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,199: INFO: model_training: Rank 0, Epoch 4013, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,200: INFO: model_training: Rank 0, Epoch 4013, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,201: INFO: model_training: Rank 0, Epoch 4013, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,203: INFO: model_training: Rank 0, Epoch 4013, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,205: INFO: model_training: Rank 0, Epoch 4014, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,206: INFO: model_training: Rank 0, Epoch 4014, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,208: INFO: model_training: Rank 0, Epoch 4014, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,210: INFO: model_training: Rank 0, Epoch 4014, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,211: INFO: model_training: Rank 0, Epoch 4014, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,212: INFO: model_training: Rank 0, Epoch 4015, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,213: INFO: model_training: Rank 0, Epoch 4015, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,214: INFO: model_training: Rank 0, Epoch 4015, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,215: INFO: model_training: Rank 0, Epoch 4015, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,216: INFO: model_training: Rank 0, Epoch 4015, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,217: INFO: model_training: Rank 0, Epoch 4016, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,218: INFO: model_training: Rank 0, Epoch 4016, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,219: INFO: model_training: Rank 0, Epoch 4016, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,220: INFO: model_training: Rank 0, Epoch 4016, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,221: INFO: model_training: Rank 0, Epoch 4016, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,222: INFO: model_training: Rank 0, Epoch 4017, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,223: INFO: model_training: Rank 0, Epoch 4017, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,225: INFO: model_training: Rank 0, Epoch 4017, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,226: INFO: model_training: Rank 0, Epoch 4017, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,227: INFO: model_training: Rank 0, Epoch 4017, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,228: INFO: model_training: Rank 0, Epoch 4018, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,229: INFO: model_training: Rank 0, Epoch 4018, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,230: INFO: model_training: Rank 0, Epoch 4018, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,231: INFO: model_training: Rank 0, Epoch 4018, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,232: INFO: model_training: Rank 0, Epoch 4018, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,234: INFO: model_training: Rank 0, Epoch 4019, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,235: INFO: model_training: Rank 0, Epoch 4019, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,236: INFO: model_training: Rank 0, Epoch 4019, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,238: INFO: model_training: Rank 0, Epoch 4019, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,239: INFO: model_training: Rank 0, Epoch 4019, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,240: INFO: model_training: Rank 0, Epoch 4020, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,241: INFO: model_training: Rank 0, Epoch 4020, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,242: INFO: model_training: Rank 0, Epoch 4020, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,243: INFO: model_training: Rank 0, Epoch 4020, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,245: INFO: model_training: Rank 0, Epoch 4020, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,246: INFO: model_training: Rank 0, Epoch 4021, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,247: INFO: model_training: Rank 0, Epoch 4021, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,248: INFO: model_training: Rank 0, Epoch 4021, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,249: INFO: model_training: Rank 0, Epoch 4021, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,250: INFO: model_training: Rank 0, Epoch 4021, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,251: INFO: model_training: Rank 0, Epoch 4022, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,252: INFO: model_training: Rank 0, Epoch 4022, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,253: INFO: model_training: Rank 0, Epoch 4022, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,254: INFO: model_training: Rank 0, Epoch 4022, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,255: INFO: model_training: Rank 0, Epoch 4022, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,256: INFO: model_training: Rank 0, Epoch 4023, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,257: INFO: model_training: Rank 0, Epoch 4023, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,258: INFO: model_training: Rank 0, Epoch 4023, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,259: INFO: model_training: Rank 0, Epoch 4023, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,260: INFO: model_training: Rank 0, Epoch 4023, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,261: INFO: model_training: Rank 0, Epoch 4024, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,263: INFO: model_training: Rank 0, Epoch 4024, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,264: INFO: model_training: Rank 0, Epoch 4024, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,265: INFO: model_training: Rank 0, Epoch 4024, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,266: INFO: model_training: Rank 0, Epoch 4024, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,267: INFO: model_training: Rank 0, Epoch 4025, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,268: INFO: model_training: Rank 0, Epoch 4025, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,269: INFO: model_training: Rank 0, Epoch 4025, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,270: INFO: model_training: Rank 0, Epoch 4025, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,271: INFO: model_training: Rank 0, Epoch 4025, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,272: INFO: model_training: Rank 0, Epoch 4026, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,273: INFO: model_training: Rank 0, Epoch 4026, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,274: INFO: model_training: Rank 0, Epoch 4026, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,276: INFO: model_training: Rank 0, Epoch 4026, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,276: INFO: model_training: Rank 0, Epoch 4026, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,277: INFO: model_training: Rank 0, Epoch 4027, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,278: INFO: model_training: Rank 0, Epoch 4027, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,279: INFO: model_training: Rank 0, Epoch 4027, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,280: INFO: model_training: Rank 0, Epoch 4027, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,281: INFO: model_training: Rank 0, Epoch 4027, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,282: INFO: model_training: Rank 0, Epoch 4028, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,283: INFO: model_training: Rank 0, Epoch 4028, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,284: INFO: model_training: Rank 0, Epoch 4028, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,286: INFO: model_training: Rank 0, Epoch 4028, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,287: INFO: model_training: Rank 0, Epoch 4028, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,288: INFO: model_training: Rank 0, Epoch 4029, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,289: INFO: model_training: Rank 0, Epoch 4029, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,290: INFO: model_training: Rank 0, Epoch 4029, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,291: INFO: model_training: Rank 0, Epoch 4029, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,292: INFO: model_training: Rank 0, Epoch 4029, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,293: INFO: model_training: Rank 0, Epoch 4030, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,294: INFO: model_training: Rank 0, Epoch 4030, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,295: INFO: model_training: Rank 0, Epoch 4030, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,296: INFO: model_training: Rank 0, Epoch 4030, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,297: INFO: model_training: Rank 0, Epoch 4030, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,298: INFO: model_training: Rank 0, Epoch 4031, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,299: INFO: model_training: Rank 0, Epoch 4031, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,300: INFO: model_training: Rank 0, Epoch 4031, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,301: INFO: model_training: Rank 0, Epoch 4031, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,303: INFO: model_training: Rank 0, Epoch 4031, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,304: INFO: model_training: Rank 0, Epoch 4032, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,305: INFO: model_training: Rank 0, Epoch 4032, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,307: INFO: model_training: Rank 0, Epoch 4032, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,307: INFO: model_training: Rank 0, Epoch 4032, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,308: INFO: model_training: Rank 0, Epoch 4032, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,309: INFO: model_training: Rank 0, Epoch 4033, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,311: INFO: model_training: Rank 0, Epoch 4033, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,312: INFO: model_training: Rank 0, Epoch 4033, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,313: INFO: model_training: Rank 0, Epoch 4033, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,314: INFO: model_training: Rank 0, Epoch 4033, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,315: INFO: model_training: Rank 0, Epoch 4034, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,316: INFO: model_training: Rank 0, Epoch 4034, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,317: INFO: model_training: Rank 0, Epoch 4034, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,318: INFO: model_training: Rank 0, Epoch 4034, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,319: INFO: model_training: Rank 0, Epoch 4034, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,320: INFO: model_training: Rank 0, Epoch 4035, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,321: INFO: model_training: Rank 0, Epoch 4035, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,322: INFO: model_training: Rank 0, Epoch 4035, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,323: INFO: model_training: Rank 0, Epoch 4035, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,324: INFO: model_training: Rank 0, Epoch 4035, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,325: INFO: model_training: Rank 0, Epoch 4036, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,326: INFO: model_training: Rank 0, Epoch 4036, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,327: INFO: model_training: Rank 0, Epoch 4036, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,328: INFO: model_training: Rank 0, Epoch 4036, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,329: INFO: model_training: Rank 0, Epoch 4036, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,330: INFO: model_training: Rank 0, Epoch 4037, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,331: INFO: model_training: Rank 0, Epoch 4037, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,332: INFO: model_training: Rank 0, Epoch 4037, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,333: INFO: model_training: Rank 0, Epoch 4037, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,335: INFO: model_training: Rank 0, Epoch 4037, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,336: INFO: model_training: Rank 0, Epoch 4038, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,338: INFO: model_training: Rank 0, Epoch 4038, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,339: INFO: model_training: Rank 0, Epoch 4038, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,340: INFO: model_training: Rank 0, Epoch 4038, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,341: INFO: model_training: Rank 0, Epoch 4038, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,342: INFO: model_training: Rank 0, Epoch 4039, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,343: INFO: model_training: Rank 0, Epoch 4039, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,344: INFO: model_training: Rank 0, Epoch 4039, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,345: INFO: model_training: Rank 0, Epoch 4039, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,346: INFO: model_training: Rank 0, Epoch 4039, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,347: INFO: model_training: Rank 0, Epoch 4040, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,348: INFO: model_training: Rank 0, Epoch 4040, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,349: INFO: model_training: Rank 0, Epoch 4040, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,350: INFO: model_training: Rank 0, Epoch 4040, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,351: INFO: model_training: Rank 0, Epoch 4040, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,352: INFO: model_training: Rank 0, Epoch 4041, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,353: INFO: model_training: Rank 0, Epoch 4041, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,355: INFO: model_training: Rank 0, Epoch 4041, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,356: INFO: model_training: Rank 0, Epoch 4041, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,357: INFO: model_training: Rank 0, Epoch 4041, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,357: INFO: model_training: Rank 0, Epoch 4042, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,359: INFO: model_training: Rank 0, Epoch 4042, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,360: INFO: model_training: Rank 0, Epoch 4042, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,361: INFO: model_training: Rank 0, Epoch 4042, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,362: INFO: model_training: Rank 0, Epoch 4042, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,363: INFO: model_training: Rank 0, Epoch 4043, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,364: INFO: model_training: Rank 0, Epoch 4043, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,365: INFO: model_training: Rank 0, Epoch 4043, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,366: INFO: model_training: Rank 0, Epoch 4043, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,367: INFO: model_training: Rank 0, Epoch 4043, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,368: INFO: model_training: Rank 0, Epoch 4044, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,369: INFO: model_training: Rank 0, Epoch 4044, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,371: INFO: model_training: Rank 0, Epoch 4044, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,372: INFO: model_training: Rank 0, Epoch 4044, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,373: INFO: model_training: Rank 0, Epoch 4044, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,374: INFO: model_training: Rank 0, Epoch 4045, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,375: INFO: model_training: Rank 0, Epoch 4045, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,376: INFO: model_training: Rank 0, Epoch 4045, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,377: INFO: model_training: Rank 0, Epoch 4045, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,378: INFO: model_training: Rank 0, Epoch 4045, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,379: INFO: model_training: Rank 0, Epoch 4046, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,380: INFO: model_training: Rank 0, Epoch 4046, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,381: INFO: model_training: Rank 0, Epoch 4046, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,382: INFO: model_training: Rank 0, Epoch 4046, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,383: INFO: model_training: Rank 0, Epoch 4046, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,384: INFO: model_training: Rank 0, Epoch 4047, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,385: INFO: model_training: Rank 0, Epoch 4047, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,386: INFO: model_training: Rank 0, Epoch 4047, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,387: INFO: model_training: Rank 0, Epoch 4047, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,388: INFO: model_training: Rank 0, Epoch 4047, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,389: INFO: model_training: Rank 0, Epoch 4048, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,390: INFO: model_training: Rank 0, Epoch 4048, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,391: INFO: model_training: Rank 0, Epoch 4048, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,392: INFO: model_training: Rank 0, Epoch 4048, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,393: INFO: model_training: Rank 0, Epoch 4048, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,394: INFO: model_training: Rank 0, Epoch 4049, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,395: INFO: model_training: Rank 0, Epoch 4049, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,396: INFO: model_training: Rank 0, Epoch 4049, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,397: INFO: model_training: Rank 0, Epoch 4049, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,398: INFO: model_training: Rank 0, Epoch 4049, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,399: INFO: model_training: Rank 0, Epoch 4050, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,400: INFO: model_training: Rank 0, Epoch 4050, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,401: INFO: model_training: Rank 0, Epoch 4050, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,402: INFO: model_training: Rank 0, Epoch 4050, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,403: INFO: model_training: Rank 0, Epoch 4050, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,404: INFO: model_training: Rank 0, Epoch 4051, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,406: INFO: model_training: Rank 0, Epoch 4051, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,407: INFO: model_training: Rank 0, Epoch 4051, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,408: INFO: model_training: Rank 0, Epoch 4051, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,409: INFO: model_training: Rank 0, Epoch 4051, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,410: INFO: model_training: Rank 0, Epoch 4052, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,411: INFO: model_training: Rank 0, Epoch 4052, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,412: INFO: model_training: Rank 0, Epoch 4052, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,413: INFO: model_training: Rank 0, Epoch 4052, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,414: INFO: model_training: Rank 0, Epoch 4052, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,415: INFO: model_training: Rank 0, Epoch 4053, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,416: INFO: model_training: Rank 0, Epoch 4053, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,417: INFO: model_training: Rank 0, Epoch 4053, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,418: INFO: model_training: Rank 0, Epoch 4053, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,419: INFO: model_training: Rank 0, Epoch 4053, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,420: INFO: model_training: Rank 0, Epoch 4054, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,422: INFO: model_training: Rank 0, Epoch 4054, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,425: INFO: model_training: Rank 0, Epoch 4054, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,427: INFO: model_training: Rank 0, Epoch 4054, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,428: INFO: model_training: Rank 0, Epoch 4054, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,430: INFO: model_training: Rank 0, Epoch 4055, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,431: INFO: model_training: Rank 0, Epoch 4055, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,433: INFO: model_training: Rank 0, Epoch 4055, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,434: INFO: model_training: Rank 0, Epoch 4055, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,436: INFO: model_training: Rank 0, Epoch 4055, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,437: INFO: model_training: Rank 0, Epoch 4056, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,439: INFO: model_training: Rank 0, Epoch 4056, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,440: INFO: model_training: Rank 0, Epoch 4056, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,442: INFO: model_training: Rank 0, Epoch 4056, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,443: INFO: model_training: Rank 0, Epoch 4056, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,444: INFO: model_training: Rank 0, Epoch 4057, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,445: INFO: model_training: Rank 0, Epoch 4057, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,447: INFO: model_training: Rank 0, Epoch 4057, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,448: INFO: model_training: Rank 0, Epoch 4057, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,450: INFO: model_training: Rank 0, Epoch 4057, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,451: INFO: model_training: Rank 0, Epoch 4058, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,453: INFO: model_training: Rank 0, Epoch 4058, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,454: INFO: model_training: Rank 0, Epoch 4058, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,456: INFO: model_training: Rank 0, Epoch 4058, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,457: INFO: model_training: Rank 0, Epoch 4058, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,459: INFO: model_training: Rank 0, Epoch 4059, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,460: INFO: model_training: Rank 0, Epoch 4059, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,461: INFO: model_training: Rank 0, Epoch 4059, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,462: INFO: model_training: Rank 0, Epoch 4059, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,463: INFO: model_training: Rank 0, Epoch 4059, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,464: INFO: model_training: Rank 0, Epoch 4060, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,465: INFO: model_training: Rank 0, Epoch 4060, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,466: INFO: model_training: Rank 0, Epoch 4060, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,467: INFO: model_training: Rank 0, Epoch 4060, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,468: INFO: model_training: Rank 0, Epoch 4060, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,469: INFO: model_training: Rank 0, Epoch 4061, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,470: INFO: model_training: Rank 0, Epoch 4061, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,471: INFO: model_training: Rank 0, Epoch 4061, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,472: INFO: model_training: Rank 0, Epoch 4061, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,473: INFO: model_training: Rank 0, Epoch 4061, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,474: INFO: model_training: Rank 0, Epoch 4062, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,476: INFO: model_training: Rank 0, Epoch 4062, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,477: INFO: model_training: Rank 0, Epoch 4062, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,478: INFO: model_training: Rank 0, Epoch 4062, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,479: INFO: model_training: Rank 0, Epoch 4062, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,481: INFO: model_training: Rank 0, Epoch 4063, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,482: INFO: model_training: Rank 0, Epoch 4063, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,483: INFO: model_training: Rank 0, Epoch 4063, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,484: INFO: model_training: Rank 0, Epoch 4063, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,485: INFO: model_training: Rank 0, Epoch 4063, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,486: INFO: model_training: Rank 0, Epoch 4064, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,487: INFO: model_training: Rank 0, Epoch 4064, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,488: INFO: model_training: Rank 0, Epoch 4064, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,490: INFO: model_training: Rank 0, Epoch 4064, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,491: INFO: model_training: Rank 0, Epoch 4064, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,492: INFO: model_training: Rank 0, Epoch 4065, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,493: INFO: model_training: Rank 0, Epoch 4065, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,494: INFO: model_training: Rank 0, Epoch 4065, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,495: INFO: model_training: Rank 0, Epoch 4065, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,497: INFO: model_training: Rank 0, Epoch 4065, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,498: INFO: model_training: Rank 0, Epoch 4066, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,499: INFO: model_training: Rank 0, Epoch 4066, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,500: INFO: model_training: Rank 0, Epoch 4066, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,501: INFO: model_training: Rank 0, Epoch 4066, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,502: INFO: model_training: Rank 0, Epoch 4066, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,504: INFO: model_training: Rank 0, Epoch 4067, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,505: INFO: model_training: Rank 0, Epoch 4067, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,506: INFO: model_training: Rank 0, Epoch 4067, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,507: INFO: model_training: Rank 0, Epoch 4067, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,509: INFO: model_training: Rank 0, Epoch 4067, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,510: INFO: model_training: Rank 0, Epoch 4068, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,510: INFO: model_training: Rank 0, Epoch 4068, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,512: INFO: model_training: Rank 0, Epoch 4068, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,513: INFO: model_training: Rank 0, Epoch 4068, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,514: INFO: model_training: Rank 0, Epoch 4068, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,515: INFO: model_training: Rank 0, Epoch 4069, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,516: INFO: model_training: Rank 0, Epoch 4069, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,517: INFO: model_training: Rank 0, Epoch 4069, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,518: INFO: model_training: Rank 0, Epoch 4069, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,519: INFO: model_training: Rank 0, Epoch 4069, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,520: INFO: model_training: Rank 0, Epoch 4070, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,521: INFO: model_training: Rank 0, Epoch 4070, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,522: INFO: model_training: Rank 0, Epoch 4070, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,523: INFO: model_training: Rank 0, Epoch 4070, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,524: INFO: model_training: Rank 0, Epoch 4070, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,525: INFO: model_training: Rank 0, Epoch 4071, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,527: INFO: model_training: Rank 0, Epoch 4071, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,528: INFO: model_training: Rank 0, Epoch 4071, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,529: INFO: model_training: Rank 0, Epoch 4071, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,530: INFO: model_training: Rank 0, Epoch 4071, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,531: INFO: model_training: Rank 0, Epoch 4072, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,532: INFO: model_training: Rank 0, Epoch 4072, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,533: INFO: model_training: Rank 0, Epoch 4072, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,534: INFO: model_training: Rank 0, Epoch 4072, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,535: INFO: model_training: Rank 0, Epoch 4072, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,537: INFO: model_training: Rank 0, Epoch 4073, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,538: INFO: model_training: Rank 0, Epoch 4073, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,539: INFO: model_training: Rank 0, Epoch 4073, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,539: INFO: model_training: Rank 0, Epoch 4073, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,541: INFO: model_training: Rank 0, Epoch 4073, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,542: INFO: model_training: Rank 0, Epoch 4074, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,543: INFO: model_training: Rank 0, Epoch 4074, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,544: INFO: model_training: Rank 0, Epoch 4074, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,545: INFO: model_training: Rank 0, Epoch 4074, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,546: INFO: model_training: Rank 0, Epoch 4074, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,547: INFO: model_training: Rank 0, Epoch 4075, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,548: INFO: model_training: Rank 0, Epoch 4075, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,549: INFO: model_training: Rank 0, Epoch 4075, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,550: INFO: model_training: Rank 0, Epoch 4075, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,551: INFO: model_training: Rank 0, Epoch 4075, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,552: INFO: model_training: Rank 0, Epoch 4076, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,553: INFO: model_training: Rank 0, Epoch 4076, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,555: INFO: model_training: Rank 0, Epoch 4076, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,556: INFO: model_training: Rank 0, Epoch 4076, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,557: INFO: model_training: Rank 0, Epoch 4076, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,558: INFO: model_training: Rank 0, Epoch 4077, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,559: INFO: model_training: Rank 0, Epoch 4077, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,560: INFO: model_training: Rank 0, Epoch 4077, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,561: INFO: model_training: Rank 0, Epoch 4077, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,562: INFO: model_training: Rank 0, Epoch 4077, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,563: INFO: model_training: Rank 0, Epoch 4078, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,565: INFO: model_training: Rank 0, Epoch 4078, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,566: INFO: model_training: Rank 0, Epoch 4078, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,567: INFO: model_training: Rank 0, Epoch 4078, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,568: INFO: model_training: Rank 0, Epoch 4078, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,569: INFO: model_training: Rank 0, Epoch 4079, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,570: INFO: model_training: Rank 0, Epoch 4079, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,571: INFO: model_training: Rank 0, Epoch 4079, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,572: INFO: model_training: Rank 0, Epoch 4079, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,573: INFO: model_training: Rank 0, Epoch 4079, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,574: INFO: model_training: Rank 0, Epoch 4080, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,575: INFO: model_training: Rank 0, Epoch 4080, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,577: INFO: model_training: Rank 0, Epoch 4080, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,577: INFO: model_training: Rank 0, Epoch 4080, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,579: INFO: model_training: Rank 0, Epoch 4080, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,580: INFO: model_training: Rank 0, Epoch 4081, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,580: INFO: model_training: Rank 0, Epoch 4081, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,582: INFO: model_training: Rank 0, Epoch 4081, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,583: INFO: model_training: Rank 0, Epoch 4081, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,584: INFO: model_training: Rank 0, Epoch 4081, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,585: INFO: model_training: Rank 0, Epoch 4082, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,586: INFO: model_training: Rank 0, Epoch 4082, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,587: INFO: model_training: Rank 0, Epoch 4082, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,588: INFO: model_training: Rank 0, Epoch 4082, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,589: INFO: model_training: Rank 0, Epoch 4082, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,590: INFO: model_training: Rank 0, Epoch 4083, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,591: INFO: model_training: Rank 0, Epoch 4083, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,593: INFO: model_training: Rank 0, Epoch 4083, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,593: INFO: model_training: Rank 0, Epoch 4083, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,594: INFO: model_training: Rank 0, Epoch 4083, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,595: INFO: model_training: Rank 0, Epoch 4084, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,597: INFO: model_training: Rank 0, Epoch 4084, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,598: INFO: model_training: Rank 0, Epoch 4084, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,599: INFO: model_training: Rank 0, Epoch 4084, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,600: INFO: model_training: Rank 0, Epoch 4084, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,602: INFO: model_training: Rank 0, Epoch 4085, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,603: INFO: model_training: Rank 0, Epoch 4085, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,604: INFO: model_training: Rank 0, Epoch 4085, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,605: INFO: model_training: Rank 0, Epoch 4085, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,606: INFO: model_training: Rank 0, Epoch 4085, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,607: INFO: model_training: Rank 0, Epoch 4086, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,608: INFO: model_training: Rank 0, Epoch 4086, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,609: INFO: model_training: Rank 0, Epoch 4086, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,610: INFO: model_training: Rank 0, Epoch 4086, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,611: INFO: model_training: Rank 0, Epoch 4086, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,613: INFO: model_training: Rank 0, Epoch 4087, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,614: INFO: model_training: Rank 0, Epoch 4087, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,615: INFO: model_training: Rank 0, Epoch 4087, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,616: INFO: model_training: Rank 0, Epoch 4087, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,617: INFO: model_training: Rank 0, Epoch 4087, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,618: INFO: model_training: Rank 0, Epoch 4088, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,619: INFO: model_training: Rank 0, Epoch 4088, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,620: INFO: model_training: Rank 0, Epoch 4088, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,621: INFO: model_training: Rank 0, Epoch 4088, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,622: INFO: model_training: Rank 0, Epoch 4088, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,623: INFO: model_training: Rank 0, Epoch 4089, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,624: INFO: model_training: Rank 0, Epoch 4089, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,626: INFO: model_training: Rank 0, Epoch 4089, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,627: INFO: model_training: Rank 0, Epoch 4089, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,628: INFO: model_training: Rank 0, Epoch 4089, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,629: INFO: model_training: Rank 0, Epoch 4090, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,630: INFO: model_training: Rank 0, Epoch 4090, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,631: INFO: model_training: Rank 0, Epoch 4090, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,632: INFO: model_training: Rank 0, Epoch 4090, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,634: INFO: model_training: Rank 0, Epoch 4090, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,635: INFO: model_training: Rank 0, Epoch 4091, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,637: INFO: model_training: Rank 0, Epoch 4091, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,638: INFO: model_training: Rank 0, Epoch 4091, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,639: INFO: model_training: Rank 0, Epoch 4091, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,640: INFO: model_training: Rank 0, Epoch 4091, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,641: INFO: model_training: Rank 0, Epoch 4092, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,642: INFO: model_training: Rank 0, Epoch 4092, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,643: INFO: model_training: Rank 0, Epoch 4092, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,644: INFO: model_training: Rank 0, Epoch 4092, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,645: INFO: model_training: Rank 0, Epoch 4092, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,646: INFO: model_training: Rank 0, Epoch 4093, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,648: INFO: model_training: Rank 0, Epoch 4093, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,649: INFO: model_training: Rank 0, Epoch 4093, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,650: INFO: model_training: Rank 0, Epoch 4093, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,651: INFO: model_training: Rank 0, Epoch 4093, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,652: INFO: model_training: Rank 0, Epoch 4094, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,653: INFO: model_training: Rank 0, Epoch 4094, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,654: INFO: model_training: Rank 0, Epoch 4094, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,655: INFO: model_training: Rank 0, Epoch 4094, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,656: INFO: model_training: Rank 0, Epoch 4094, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,657: INFO: model_training: Rank 0, Epoch 4095, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,658: INFO: model_training: Rank 0, Epoch 4095, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,659: INFO: model_training: Rank 0, Epoch 4095, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,660: INFO: model_training: Rank 0, Epoch 4095, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,661: INFO: model_training: Rank 0, Epoch 4095, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,662: INFO: model_training: Rank 0, Epoch 4096, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,663: INFO: model_training: Rank 0, Epoch 4096, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,664: INFO: model_training: Rank 0, Epoch 4096, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,665: INFO: model_training: Rank 0, Epoch 4096, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,666: INFO: model_training: Rank 0, Epoch 4096, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,667: INFO: model_training: Rank 0, Epoch 4097, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,668: INFO: model_training: Rank 0, Epoch 4097, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,669: INFO: model_training: Rank 0, Epoch 4097, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,670: INFO: model_training: Rank 0, Epoch 4097, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,672: INFO: model_training: Rank 0, Epoch 4097, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,673: INFO: model_training: Rank 0, Epoch 4098, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,674: INFO: model_training: Rank 0, Epoch 4098, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,675: INFO: model_training: Rank 0, Epoch 4098, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,676: INFO: model_training: Rank 0, Epoch 4098, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,677: INFO: model_training: Rank 0, Epoch 4098, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,679: INFO: model_training: Rank 0, Epoch 4099, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,680: INFO: model_training: Rank 0, Epoch 4099, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,681: INFO: model_training: Rank 0, Epoch 4099, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,682: INFO: model_training: Rank 0, Epoch 4099, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,683: INFO: model_training: Rank 0, Epoch 4099, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,684: INFO: model_training: Rank 0, Epoch 4100, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,685: INFO: model_training: Rank 0, Epoch 4100, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,687: INFO: model_training: Rank 0, Epoch 4100, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,688: INFO: model_training: Rank 0, Epoch 4100, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,689: INFO: model_training: Rank 0, Epoch 4100, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,690: INFO: model_training: Rank 0, Epoch 4101, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,691: INFO: model_training: Rank 0, Epoch 4101, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,692: INFO: model_training: Rank 0, Epoch 4101, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,693: INFO: model_training: Rank 0, Epoch 4101, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,694: INFO: model_training: Rank 0, Epoch 4101, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,695: INFO: model_training: Rank 0, Epoch 4102, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,696: INFO: model_training: Rank 0, Epoch 4102, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,697: INFO: model_training: Rank 0, Epoch 4102, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,698: INFO: model_training: Rank 0, Epoch 4102, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,699: INFO: model_training: Rank 0, Epoch 4102, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,701: INFO: model_training: Rank 0, Epoch 4103, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,702: INFO: model_training: Rank 0, Epoch 4103, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,703: INFO: model_training: Rank 0, Epoch 4103, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,704: INFO: model_training: Rank 0, Epoch 4103, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,705: INFO: model_training: Rank 0, Epoch 4103, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,707: INFO: model_training: Rank 0, Epoch 4104, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,708: INFO: model_training: Rank 0, Epoch 4104, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,709: INFO: model_training: Rank 0, Epoch 4104, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,710: INFO: model_training: Rank 0, Epoch 4104, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,711: INFO: model_training: Rank 0, Epoch 4104, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,713: INFO: model_training: Rank 0, Epoch 4105, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,714: INFO: model_training: Rank 0, Epoch 4105, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,715: INFO: model_training: Rank 0, Epoch 4105, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,716: INFO: model_training: Rank 0, Epoch 4105, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,717: INFO: model_training: Rank 0, Epoch 4105, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,718: INFO: model_training: Rank 0, Epoch 4106, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,719: INFO: model_training: Rank 0, Epoch 4106, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,720: INFO: model_training: Rank 0, Epoch 4106, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,721: INFO: model_training: Rank 0, Epoch 4106, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,722: INFO: model_training: Rank 0, Epoch 4106, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,723: INFO: model_training: Rank 0, Epoch 4107, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,724: INFO: model_training: Rank 0, Epoch 4107, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,726: INFO: model_training: Rank 0, Epoch 4107, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,726: INFO: model_training: Rank 0, Epoch 4107, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,727: INFO: model_training: Rank 0, Epoch 4107, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,728: INFO: model_training: Rank 0, Epoch 4108, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,729: INFO: model_training: Rank 0, Epoch 4108, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,731: INFO: model_training: Rank 0, Epoch 4108, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,731: INFO: model_training: Rank 0, Epoch 4108, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,732: INFO: model_training: Rank 0, Epoch 4108, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,734: INFO: model_training: Rank 0, Epoch 4109, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,735: INFO: model_training: Rank 0, Epoch 4109, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,736: INFO: model_training: Rank 0, Epoch 4109, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,737: INFO: model_training: Rank 0, Epoch 4109, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,738: INFO: model_training: Rank 0, Epoch 4109, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,739: INFO: model_training: Rank 0, Epoch 4110, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,740: INFO: model_training: Rank 0, Epoch 4110, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,741: INFO: model_training: Rank 0, Epoch 4110, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,742: INFO: model_training: Rank 0, Epoch 4110, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,744: INFO: model_training: Rank 0, Epoch 4110, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,745: INFO: model_training: Rank 0, Epoch 4111, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,746: INFO: model_training: Rank 0, Epoch 4111, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,747: INFO: model_training: Rank 0, Epoch 4111, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,748: INFO: model_training: Rank 0, Epoch 4111, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,749: INFO: model_training: Rank 0, Epoch 4111, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,750: INFO: model_training: Rank 0, Epoch 4112, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,751: INFO: model_training: Rank 0, Epoch 4112, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,752: INFO: model_training: Rank 0, Epoch 4112, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,753: INFO: model_training: Rank 0, Epoch 4112, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,754: INFO: model_training: Rank 0, Epoch 4112, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,756: INFO: model_training: Rank 0, Epoch 4113, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,757: INFO: model_training: Rank 0, Epoch 4113, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,758: INFO: model_training: Rank 0, Epoch 4113, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,759: INFO: model_training: Rank 0, Epoch 4113, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,760: INFO: model_training: Rank 0, Epoch 4113, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,761: INFO: model_training: Rank 0, Epoch 4114, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,762: INFO: model_training: Rank 0, Epoch 4114, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,763: INFO: model_training: Rank 0, Epoch 4114, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,765: INFO: model_training: Rank 0, Epoch 4114, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,766: INFO: model_training: Rank 0, Epoch 4114, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,767: INFO: model_training: Rank 0, Epoch 4115, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,768: INFO: model_training: Rank 0, Epoch 4115, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,769: INFO: model_training: Rank 0, Epoch 4115, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,770: INFO: model_training: Rank 0, Epoch 4115, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,771: INFO: model_training: Rank 0, Epoch 4115, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,772: INFO: model_training: Rank 0, Epoch 4116, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,773: INFO: model_training: Rank 0, Epoch 4116, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,774: INFO: model_training: Rank 0, Epoch 4116, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,775: INFO: model_training: Rank 0, Epoch 4116, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,777: INFO: model_training: Rank 0, Epoch 4116, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,778: INFO: model_training: Rank 0, Epoch 4117, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,780: INFO: model_training: Rank 0, Epoch 4117, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,781: INFO: model_training: Rank 0, Epoch 4117, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,783: INFO: model_training: Rank 0, Epoch 4117, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,784: INFO: model_training: Rank 0, Epoch 4117, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,785: INFO: model_training: Rank 0, Epoch 4118, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,786: INFO: model_training: Rank 0, Epoch 4118, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,788: INFO: model_training: Rank 0, Epoch 4118, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,788: INFO: model_training: Rank 0, Epoch 4118, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,790: INFO: model_training: Rank 0, Epoch 4118, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,791: INFO: model_training: Rank 0, Epoch 4119, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,792: INFO: model_training: Rank 0, Epoch 4119, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,793: INFO: model_training: Rank 0, Epoch 4119, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,794: INFO: model_training: Rank 0, Epoch 4119, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,795: INFO: model_training: Rank 0, Epoch 4119, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,796: INFO: model_training: Rank 0, Epoch 4120, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,797: INFO: model_training: Rank 0, Epoch 4120, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,798: INFO: model_training: Rank 0, Epoch 4120, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,799: INFO: model_training: Rank 0, Epoch 4120, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,800: INFO: model_training: Rank 0, Epoch 4120, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,801: INFO: model_training: Rank 0, Epoch 4121, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,802: INFO: model_training: Rank 0, Epoch 4121, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,803: INFO: model_training: Rank 0, Epoch 4121, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,805: INFO: model_training: Rank 0, Epoch 4121, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,805: INFO: model_training: Rank 0, Epoch 4121, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,806: INFO: model_training: Rank 0, Epoch 4122, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,807: INFO: model_training: Rank 0, Epoch 4122, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,809: INFO: model_training: Rank 0, Epoch 4122, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,810: INFO: model_training: Rank 0, Epoch 4122, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,811: INFO: model_training: Rank 0, Epoch 4122, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,812: INFO: model_training: Rank 0, Epoch 4123, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,813: INFO: model_training: Rank 0, Epoch 4123, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,814: INFO: model_training: Rank 0, Epoch 4123, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,815: INFO: model_training: Rank 0, Epoch 4123, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,816: INFO: model_training: Rank 0, Epoch 4123, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,818: INFO: model_training: Rank 0, Epoch 4124, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,819: INFO: model_training: Rank 0, Epoch 4124, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,820: INFO: model_training: Rank 0, Epoch 4124, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,821: INFO: model_training: Rank 0, Epoch 4124, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,822: INFO: model_training: Rank 0, Epoch 4124, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,823: INFO: model_training: Rank 0, Epoch 4125, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,824: INFO: model_training: Rank 0, Epoch 4125, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,825: INFO: model_training: Rank 0, Epoch 4125, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,826: INFO: model_training: Rank 0, Epoch 4125, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,827: INFO: model_training: Rank 0, Epoch 4125, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,829: INFO: model_training: Rank 0, Epoch 4126, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,830: INFO: model_training: Rank 0, Epoch 4126, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,831: INFO: model_training: Rank 0, Epoch 4126, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,832: INFO: model_training: Rank 0, Epoch 4126, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,833: INFO: model_training: Rank 0, Epoch 4126, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,834: INFO: model_training: Rank 0, Epoch 4127, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,835: INFO: model_training: Rank 0, Epoch 4127, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,836: INFO: model_training: Rank 0, Epoch 4127, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,837: INFO: model_training: Rank 0, Epoch 4127, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,838: INFO: model_training: Rank 0, Epoch 4127, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,839: INFO: model_training: Rank 0, Epoch 4128, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,840: INFO: model_training: Rank 0, Epoch 4128, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,841: INFO: model_training: Rank 0, Epoch 4128, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,842: INFO: model_training: Rank 0, Epoch 4128, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,843: INFO: model_training: Rank 0, Epoch 4128, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,844: INFO: model_training: Rank 0, Epoch 4129, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,845: INFO: model_training: Rank 0, Epoch 4129, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,847: INFO: model_training: Rank 0, Epoch 4129, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,848: INFO: model_training: Rank 0, Epoch 4129, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,849: INFO: model_training: Rank 0, Epoch 4129, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,850: INFO: model_training: Rank 0, Epoch 4130, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,851: INFO: model_training: Rank 0, Epoch 4130, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,852: INFO: model_training: Rank 0, Epoch 4130, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,853: INFO: model_training: Rank 0, Epoch 4130, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,854: INFO: model_training: Rank 0, Epoch 4130, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,855: INFO: model_training: Rank 0, Epoch 4131, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,856: INFO: model_training: Rank 0, Epoch 4131, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,857: INFO: model_training: Rank 0, Epoch 4131, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,858: INFO: model_training: Rank 0, Epoch 4131, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,859: INFO: model_training: Rank 0, Epoch 4131, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,860: INFO: model_training: Rank 0, Epoch 4132, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,861: INFO: model_training: Rank 0, Epoch 4132, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,862: INFO: model_training: Rank 0, Epoch 4132, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,863: INFO: model_training: Rank 0, Epoch 4132, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,865: INFO: model_training: Rank 0, Epoch 4132, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,866: INFO: model_training: Rank 0, Epoch 4133, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,867: INFO: model_training: Rank 0, Epoch 4133, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,867: INFO: model_training: Rank 0, Epoch 4133, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,868: INFO: model_training: Rank 0, Epoch 4133, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,869: INFO: model_training: Rank 0, Epoch 4133, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,871: INFO: model_training: Rank 0, Epoch 4134, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,872: INFO: model_training: Rank 0, Epoch 4134, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,873: INFO: model_training: Rank 0, Epoch 4134, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,874: INFO: model_training: Rank 0, Epoch 4134, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,875: INFO: model_training: Rank 0, Epoch 4134, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,876: INFO: model_training: Rank 0, Epoch 4135, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,877: INFO: model_training: Rank 0, Epoch 4135, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,878: INFO: model_training: Rank 0, Epoch 4135, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,879: INFO: model_training: Rank 0, Epoch 4135, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,880: INFO: model_training: Rank 0, Epoch 4135, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,881: INFO: model_training: Rank 0, Epoch 4136, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,882: INFO: model_training: Rank 0, Epoch 4136, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,884: INFO: model_training: Rank 0, Epoch 4136, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,885: INFO: model_training: Rank 0, Epoch 4136, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,886: INFO: model_training: Rank 0, Epoch 4136, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,887: INFO: model_training: Rank 0, Epoch 4137, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,888: INFO: model_training: Rank 0, Epoch 4137, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,889: INFO: model_training: Rank 0, Epoch 4137, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,890: INFO: model_training: Rank 0, Epoch 4137, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,891: INFO: model_training: Rank 0, Epoch 4137, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,892: INFO: model_training: Rank 0, Epoch 4138, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,893: INFO: model_training: Rank 0, Epoch 4138, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,894: INFO: model_training: Rank 0, Epoch 4138, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,895: INFO: model_training: Rank 0, Epoch 4138, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,896: INFO: model_training: Rank 0, Epoch 4138, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,897: INFO: model_training: Rank 0, Epoch 4139, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,898: INFO: model_training: Rank 0, Epoch 4139, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,899: INFO: model_training: Rank 0, Epoch 4139, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,900: INFO: model_training: Rank 0, Epoch 4139, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,901: INFO: model_training: Rank 0, Epoch 4139, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,902: INFO: model_training: Rank 0, Epoch 4140, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,903: INFO: model_training: Rank 0, Epoch 4140, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,904: INFO: model_training: Rank 0, Epoch 4140, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,905: INFO: model_training: Rank 0, Epoch 4140, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,906: INFO: model_training: Rank 0, Epoch 4140, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,907: INFO: model_training: Rank 0, Epoch 4141, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,908: INFO: model_training: Rank 0, Epoch 4141, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,909: INFO: model_training: Rank 0, Epoch 4141, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,910: INFO: model_training: Rank 0, Epoch 4141, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,911: INFO: model_training: Rank 0, Epoch 4141, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,912: INFO: model_training: Rank 0, Epoch 4142, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,913: INFO: model_training: Rank 0, Epoch 4142, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,914: INFO: model_training: Rank 0, Epoch 4142, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,915: INFO: model_training: Rank 0, Epoch 4142, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,916: INFO: model_training: Rank 0, Epoch 4142, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,917: INFO: model_training: Rank 0, Epoch 4143, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,918: INFO: model_training: Rank 0, Epoch 4143, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,919: INFO: model_training: Rank 0, Epoch 4143, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,920: INFO: model_training: Rank 0, Epoch 4143, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,921: INFO: model_training: Rank 0, Epoch 4143, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,922: INFO: model_training: Rank 0, Epoch 4144, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,923: INFO: model_training: Rank 0, Epoch 4144, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,924: INFO: model_training: Rank 0, Epoch 4144, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,925: INFO: model_training: Rank 0, Epoch 4144, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,926: INFO: model_training: Rank 0, Epoch 4144, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,927: INFO: model_training: Rank 0, Epoch 4145, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,928: INFO: model_training: Rank 0, Epoch 4145, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,929: INFO: model_training: Rank 0, Epoch 4145, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,931: INFO: model_training: Rank 0, Epoch 4145, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,932: INFO: model_training: Rank 0, Epoch 4145, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,932: INFO: model_training: Rank 0, Epoch 4146, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,933: INFO: model_training: Rank 0, Epoch 4146, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,934: INFO: model_training: Rank 0, Epoch 4146, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,935: INFO: model_training: Rank 0, Epoch 4146, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,937: INFO: model_training: Rank 0, Epoch 4146, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,937: INFO: model_training: Rank 0, Epoch 4147, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,939: INFO: model_training: Rank 0, Epoch 4147, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,940: INFO: model_training: Rank 0, Epoch 4147, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,941: INFO: model_training: Rank 0, Epoch 4147, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,942: INFO: model_training: Rank 0, Epoch 4147, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,943: INFO: model_training: Rank 0, Epoch 4148, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,944: INFO: model_training: Rank 0, Epoch 4148, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,945: INFO: model_training: Rank 0, Epoch 4148, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,946: INFO: model_training: Rank 0, Epoch 4148, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,947: INFO: model_training: Rank 0, Epoch 4148, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,948: INFO: model_training: Rank 0, Epoch 4149, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,949: INFO: model_training: Rank 0, Epoch 4149, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,951: INFO: model_training: Rank 0, Epoch 4149, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,952: INFO: model_training: Rank 0, Epoch 4149, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,953: INFO: model_training: Rank 0, Epoch 4149, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,954: INFO: model_training: Rank 0, Epoch 4150, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,955: INFO: model_training: Rank 0, Epoch 4150, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,956: INFO: model_training: Rank 0, Epoch 4150, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,957: INFO: model_training: Rank 0, Epoch 4150, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,958: INFO: model_training: Rank 0, Epoch 4150, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,959: INFO: model_training: Rank 0, Epoch 4151, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,961: INFO: model_training: Rank 0, Epoch 4151, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,962: INFO: model_training: Rank 0, Epoch 4151, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,963: INFO: model_training: Rank 0, Epoch 4151, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,964: INFO: model_training: Rank 0, Epoch 4151, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,965: INFO: model_training: Rank 0, Epoch 4152, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,966: INFO: model_training: Rank 0, Epoch 4152, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,967: INFO: model_training: Rank 0, Epoch 4152, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,968: INFO: model_training: Rank 0, Epoch 4152, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,969: INFO: model_training: Rank 0, Epoch 4152, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,970: INFO: model_training: Rank 0, Epoch 4153, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,971: INFO: model_training: Rank 0, Epoch 4153, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,972: INFO: model_training: Rank 0, Epoch 4153, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,973: INFO: model_training: Rank 0, Epoch 4153, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,974: INFO: model_training: Rank 0, Epoch 4153, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,975: INFO: model_training: Rank 0, Epoch 4154, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,976: INFO: model_training: Rank 0, Epoch 4154, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,977: INFO: model_training: Rank 0, Epoch 4154, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,978: INFO: model_training: Rank 0, Epoch 4154, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,979: INFO: model_training: Rank 0, Epoch 4154, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,981: INFO: model_training: Rank 0, Epoch 4155, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,982: INFO: model_training: Rank 0, Epoch 4155, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,983: INFO: model_training: Rank 0, Epoch 4155, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,984: INFO: model_training: Rank 0, Epoch 4155, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,986: INFO: model_training: Rank 0, Epoch 4155, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,987: INFO: model_training: Rank 0, Epoch 4156, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,988: INFO: model_training: Rank 0, Epoch 4156, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,989: INFO: model_training: Rank 0, Epoch 4156, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,990: INFO: model_training: Rank 0, Epoch 4156, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,991: INFO: model_training: Rank 0, Epoch 4156, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,993: INFO: model_training: Rank 0, Epoch 4157, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,994: INFO: model_training: Rank 0, Epoch 4157, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:52,995: INFO: model_training: Rank 0, Epoch 4157, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:52,996: INFO: model_training: Rank 0, Epoch 4157, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:52,997: INFO: model_training: Rank 0, Epoch 4157, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:52,998: INFO: model_training: Rank 0, Epoch 4158, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:52,999: INFO: model_training: Rank 0, Epoch 4158, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,000: INFO: model_training: Rank 0, Epoch 4158, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,001: INFO: model_training: Rank 0, Epoch 4158, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,002: INFO: model_training: Rank 0, Epoch 4158, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,003: INFO: model_training: Rank 0, Epoch 4159, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,005: INFO: model_training: Rank 0, Epoch 4159, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,006: INFO: model_training: Rank 0, Epoch 4159, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,007: INFO: model_training: Rank 0, Epoch 4159, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,008: INFO: model_training: Rank 0, Epoch 4159, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,009: INFO: model_training: Rank 0, Epoch 4160, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,010: INFO: model_training: Rank 0, Epoch 4160, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,011: INFO: model_training: Rank 0, Epoch 4160, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,012: INFO: model_training: Rank 0, Epoch 4160, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,013: INFO: model_training: Rank 0, Epoch 4160, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,015: INFO: model_training: Rank 0, Epoch 4161, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,016: INFO: model_training: Rank 0, Epoch 4161, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,017: INFO: model_training: Rank 0, Epoch 4161, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,018: INFO: model_training: Rank 0, Epoch 4161, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,019: INFO: model_training: Rank 0, Epoch 4161, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,020: INFO: model_training: Rank 0, Epoch 4162, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,022: INFO: model_training: Rank 0, Epoch 4162, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,023: INFO: model_training: Rank 0, Epoch 4162, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,024: INFO: model_training: Rank 0, Epoch 4162, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,025: INFO: model_training: Rank 0, Epoch 4162, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,026: INFO: model_training: Rank 0, Epoch 4163, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,027: INFO: model_training: Rank 0, Epoch 4163, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,028: INFO: model_training: Rank 0, Epoch 4163, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,029: INFO: model_training: Rank 0, Epoch 4163, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,030: INFO: model_training: Rank 0, Epoch 4163, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,031: INFO: model_training: Rank 0, Epoch 4164, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,032: INFO: model_training: Rank 0, Epoch 4164, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,033: INFO: model_training: Rank 0, Epoch 4164, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,034: INFO: model_training: Rank 0, Epoch 4164, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,035: INFO: model_training: Rank 0, Epoch 4164, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,036: INFO: model_training: Rank 0, Epoch 4165, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,037: INFO: model_training: Rank 0, Epoch 4165, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,038: INFO: model_training: Rank 0, Epoch 4165, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,040: INFO: model_training: Rank 0, Epoch 4165, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,041: INFO: model_training: Rank 0, Epoch 4165, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,042: INFO: model_training: Rank 0, Epoch 4166, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,043: INFO: model_training: Rank 0, Epoch 4166, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,044: INFO: model_training: Rank 0, Epoch 4166, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,045: INFO: model_training: Rank 0, Epoch 4166, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,046: INFO: model_training: Rank 0, Epoch 4166, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,047: INFO: model_training: Rank 0, Epoch 4167, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,048: INFO: model_training: Rank 0, Epoch 4167, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,049: INFO: model_training: Rank 0, Epoch 4167, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,051: INFO: model_training: Rank 0, Epoch 4167, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,052: INFO: model_training: Rank 0, Epoch 4167, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,054: INFO: model_training: Rank 0, Epoch 4168, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,056: INFO: model_training: Rank 0, Epoch 4168, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,058: INFO: model_training: Rank 0, Epoch 4168, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,060: INFO: model_training: Rank 0, Epoch 4168, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,061: INFO: model_training: Rank 0, Epoch 4168, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,062: INFO: model_training: Rank 0, Epoch 4169, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,063: INFO: model_training: Rank 0, Epoch 4169, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,065: INFO: model_training: Rank 0, Epoch 4169, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,066: INFO: model_training: Rank 0, Epoch 4169, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,067: INFO: model_training: Rank 0, Epoch 4169, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,068: INFO: model_training: Rank 0, Epoch 4170, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,069: INFO: model_training: Rank 0, Epoch 4170, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,071: INFO: model_training: Rank 0, Epoch 4170, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,072: INFO: model_training: Rank 0, Epoch 4170, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,073: INFO: model_training: Rank 0, Epoch 4170, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,074: INFO: model_training: Rank 0, Epoch 4171, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,076: INFO: model_training: Rank 0, Epoch 4171, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,077: INFO: model_training: Rank 0, Epoch 4171, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,079: INFO: model_training: Rank 0, Epoch 4171, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,080: INFO: model_training: Rank 0, Epoch 4171, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,081: INFO: model_training: Rank 0, Epoch 4172, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,082: INFO: model_training: Rank 0, Epoch 4172, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,084: INFO: model_training: Rank 0, Epoch 4172, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,085: INFO: model_training: Rank 0, Epoch 4172, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,087: INFO: model_training: Rank 0, Epoch 4172, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,088: INFO: model_training: Rank 0, Epoch 4173, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,089: INFO: model_training: Rank 0, Epoch 4173, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,090: INFO: model_training: Rank 0, Epoch 4173, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,092: INFO: model_training: Rank 0, Epoch 4173, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,095: INFO: model_training: Rank 0, Epoch 4173, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,097: INFO: model_training: Rank 0, Epoch 4174, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,099: INFO: model_training: Rank 0, Epoch 4174, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,101: INFO: model_training: Rank 0, Epoch 4174, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,103: INFO: model_training: Rank 0, Epoch 4174, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,106: INFO: model_training: Rank 0, Epoch 4174, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,107: INFO: model_training: Rank 0, Epoch 4175, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,109: INFO: model_training: Rank 0, Epoch 4175, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,110: INFO: model_training: Rank 0, Epoch 4175, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,112: INFO: model_training: Rank 0, Epoch 4175, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,114: INFO: model_training: Rank 0, Epoch 4175, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,116: INFO: model_training: Rank 0, Epoch 4176, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,117: INFO: model_training: Rank 0, Epoch 4176, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,119: INFO: model_training: Rank 0, Epoch 4176, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,121: INFO: model_training: Rank 0, Epoch 4176, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,123: INFO: model_training: Rank 0, Epoch 4176, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,125: INFO: model_training: Rank 0, Epoch 4177, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,126: INFO: model_training: Rank 0, Epoch 4177, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,127: INFO: model_training: Rank 0, Epoch 4177, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,128: INFO: model_training: Rank 0, Epoch 4177, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,130: INFO: model_training: Rank 0, Epoch 4177, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,131: INFO: model_training: Rank 0, Epoch 4178, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,132: INFO: model_training: Rank 0, Epoch 4178, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,134: INFO: model_training: Rank 0, Epoch 4178, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,136: INFO: model_training: Rank 0, Epoch 4178, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,138: INFO: model_training: Rank 0, Epoch 4178, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,139: INFO: model_training: Rank 0, Epoch 4179, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,140: INFO: model_training: Rank 0, Epoch 4179, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,141: INFO: model_training: Rank 0, Epoch 4179, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,143: INFO: model_training: Rank 0, Epoch 4179, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,144: INFO: model_training: Rank 0, Epoch 4179, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,146: INFO: model_training: Rank 0, Epoch 4180, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,147: INFO: model_training: Rank 0, Epoch 4180, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,148: INFO: model_training: Rank 0, Epoch 4180, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,149: INFO: model_training: Rank 0, Epoch 4180, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,151: INFO: model_training: Rank 0, Epoch 4180, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,153: INFO: model_training: Rank 0, Epoch 4181, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,155: INFO: model_training: Rank 0, Epoch 4181, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,156: INFO: model_training: Rank 0, Epoch 4181, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,158: INFO: model_training: Rank 0, Epoch 4181, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,159: INFO: model_training: Rank 0, Epoch 4181, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,161: INFO: model_training: Rank 0, Epoch 4182, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,163: INFO: model_training: Rank 0, Epoch 4182, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,164: INFO: model_training: Rank 0, Epoch 4182, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,165: INFO: model_training: Rank 0, Epoch 4182, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,167: INFO: model_training: Rank 0, Epoch 4182, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,169: INFO: model_training: Rank 0, Epoch 4183, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,170: INFO: model_training: Rank 0, Epoch 4183, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,171: INFO: model_training: Rank 0, Epoch 4183, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,173: INFO: model_training: Rank 0, Epoch 4183, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,174: INFO: model_training: Rank 0, Epoch 4183, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,175: INFO: model_training: Rank 0, Epoch 4184, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,177: INFO: model_training: Rank 0, Epoch 4184, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,178: INFO: model_training: Rank 0, Epoch 4184, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,179: INFO: model_training: Rank 0, Epoch 4184, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,180: INFO: model_training: Rank 0, Epoch 4184, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,181: INFO: model_training: Rank 0, Epoch 4185, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,182: INFO: model_training: Rank 0, Epoch 4185, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,185: INFO: model_training: Rank 0, Epoch 4185, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,186: INFO: model_training: Rank 0, Epoch 4185, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,188: INFO: model_training: Rank 0, Epoch 4185, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,189: INFO: model_training: Rank 0, Epoch 4186, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,190: INFO: model_training: Rank 0, Epoch 4186, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,191: INFO: model_training: Rank 0, Epoch 4186, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,193: INFO: model_training: Rank 0, Epoch 4186, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,194: INFO: model_training: Rank 0, Epoch 4186, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,195: INFO: model_training: Rank 0, Epoch 4187, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,196: INFO: model_training: Rank 0, Epoch 4187, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,197: INFO: model_training: Rank 0, Epoch 4187, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,199: INFO: model_training: Rank 0, Epoch 4187, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,201: INFO: model_training: Rank 0, Epoch 4187, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,202: INFO: model_training: Rank 0, Epoch 4188, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,204: INFO: model_training: Rank 0, Epoch 4188, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,206: INFO: model_training: Rank 0, Epoch 4188, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,208: INFO: model_training: Rank 0, Epoch 4188, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,209: INFO: model_training: Rank 0, Epoch 4188, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,210: INFO: model_training: Rank 0, Epoch 4189, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,211: INFO: model_training: Rank 0, Epoch 4189, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,212: INFO: model_training: Rank 0, Epoch 4189, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,213: INFO: model_training: Rank 0, Epoch 4189, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,214: INFO: model_training: Rank 0, Epoch 4189, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,216: INFO: model_training: Rank 0, Epoch 4190, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,218: INFO: model_training: Rank 0, Epoch 4190, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,220: INFO: model_training: Rank 0, Epoch 4190, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,221: INFO: model_training: Rank 0, Epoch 4190, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,222: INFO: model_training: Rank 0, Epoch 4190, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,223: INFO: model_training: Rank 0, Epoch 4191, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,224: INFO: model_training: Rank 0, Epoch 4191, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,226: INFO: model_training: Rank 0, Epoch 4191, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,227: INFO: model_training: Rank 0, Epoch 4191, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,228: INFO: model_training: Rank 0, Epoch 4191, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,229: INFO: model_training: Rank 0, Epoch 4192, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,231: INFO: model_training: Rank 0, Epoch 4192, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,233: INFO: model_training: Rank 0, Epoch 4192, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,235: INFO: model_training: Rank 0, Epoch 4192, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,236: INFO: model_training: Rank 0, Epoch 4192, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,237: INFO: model_training: Rank 0, Epoch 4193, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,238: INFO: model_training: Rank 0, Epoch 4193, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,240: INFO: model_training: Rank 0, Epoch 4193, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,241: INFO: model_training: Rank 0, Epoch 4193, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,242: INFO: model_training: Rank 0, Epoch 4193, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,243: INFO: model_training: Rank 0, Epoch 4194, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,244: INFO: model_training: Rank 0, Epoch 4194, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,245: INFO: model_training: Rank 0, Epoch 4194, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,247: INFO: model_training: Rank 0, Epoch 4194, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,248: INFO: model_training: Rank 0, Epoch 4194, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,249: INFO: model_training: Rank 0, Epoch 4195, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,251: INFO: model_training: Rank 0, Epoch 4195, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,253: INFO: model_training: Rank 0, Epoch 4195, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,255: INFO: model_training: Rank 0, Epoch 4195, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,256: INFO: model_training: Rank 0, Epoch 4195, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,257: INFO: model_training: Rank 0, Epoch 4196, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,259: INFO: model_training: Rank 0, Epoch 4196, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,260: INFO: model_training: Rank 0, Epoch 4196, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,261: INFO: model_training: Rank 0, Epoch 4196, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,262: INFO: model_training: Rank 0, Epoch 4196, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,264: INFO: model_training: Rank 0, Epoch 4197, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,265: INFO: model_training: Rank 0, Epoch 4197, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,267: INFO: model_training: Rank 0, Epoch 4197, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,268: INFO: model_training: Rank 0, Epoch 4197, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,269: INFO: model_training: Rank 0, Epoch 4197, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,271: INFO: model_training: Rank 0, Epoch 4198, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,272: INFO: model_training: Rank 0, Epoch 4198, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,274: INFO: model_training: Rank 0, Epoch 4198, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,275: INFO: model_training: Rank 0, Epoch 4198, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,276: INFO: model_training: Rank 0, Epoch 4198, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,277: INFO: model_training: Rank 0, Epoch 4199, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,278: INFO: model_training: Rank 0, Epoch 4199, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,279: INFO: model_training: Rank 0, Epoch 4199, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,280: INFO: model_training: Rank 0, Epoch 4199, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,282: INFO: model_training: Rank 0, Epoch 4199, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,284: INFO: model_training: Rank 0, Epoch 4200, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,286: INFO: model_training: Rank 0, Epoch 4200, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,287: INFO: model_training: Rank 0, Epoch 4200, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,288: INFO: model_training: Rank 0, Epoch 4200, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,289: INFO: model_training: Rank 0, Epoch 4200, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,291: INFO: model_training: Rank 0, Epoch 4201, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,292: INFO: model_training: Rank 0, Epoch 4201, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,293: INFO: model_training: Rank 0, Epoch 4201, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,294: INFO: model_training: Rank 0, Epoch 4201, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,295: INFO: model_training: Rank 0, Epoch 4201, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,296: INFO: model_training: Rank 0, Epoch 4202, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,297: INFO: model_training: Rank 0, Epoch 4202, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,299: INFO: model_training: Rank 0, Epoch 4202, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,301: INFO: model_training: Rank 0, Epoch 4202, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,302: INFO: model_training: Rank 0, Epoch 4202, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,303: INFO: model_training: Rank 0, Epoch 4203, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,305: INFO: model_training: Rank 0, Epoch 4203, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,306: INFO: model_training: Rank 0, Epoch 4203, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,307: INFO: model_training: Rank 0, Epoch 4203, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,308: INFO: model_training: Rank 0, Epoch 4203, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,309: INFO: model_training: Rank 0, Epoch 4204, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,310: INFO: model_training: Rank 0, Epoch 4204, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,311: INFO: model_training: Rank 0, Epoch 4204, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,312: INFO: model_training: Rank 0, Epoch 4204, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,314: INFO: model_training: Rank 0, Epoch 4204, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,315: INFO: model_training: Rank 0, Epoch 4205, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,317: INFO: model_training: Rank 0, Epoch 4205, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,319: INFO: model_training: Rank 0, Epoch 4205, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,320: INFO: model_training: Rank 0, Epoch 4205, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,321: INFO: model_training: Rank 0, Epoch 4205, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,323: INFO: model_training: Rank 0, Epoch 4206, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,324: INFO: model_training: Rank 0, Epoch 4206, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,326: INFO: model_training: Rank 0, Epoch 4206, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,327: INFO: model_training: Rank 0, Epoch 4206, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,328: INFO: model_training: Rank 0, Epoch 4206, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,329: INFO: model_training: Rank 0, Epoch 4207, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,330: INFO: model_training: Rank 0, Epoch 4207, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,331: INFO: model_training: Rank 0, Epoch 4207, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,334: INFO: model_training: Rank 0, Epoch 4207, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,335: INFO: model_training: Rank 0, Epoch 4207, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,337: INFO: model_training: Rank 0, Epoch 4208, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,338: INFO: model_training: Rank 0, Epoch 4208, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,339: INFO: model_training: Rank 0, Epoch 4208, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,341: INFO: model_training: Rank 0, Epoch 4208, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,342: INFO: model_training: Rank 0, Epoch 4208, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,343: INFO: model_training: Rank 0, Epoch 4209, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,344: INFO: model_training: Rank 0, Epoch 4209, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,345: INFO: model_training: Rank 0, Epoch 4209, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,346: INFO: model_training: Rank 0, Epoch 4209, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,347: INFO: model_training: Rank 0, Epoch 4209, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,348: INFO: model_training: Rank 0, Epoch 4210, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,350: INFO: model_training: Rank 0, Epoch 4210, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,352: INFO: model_training: Rank 0, Epoch 4210, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,353: INFO: model_training: Rank 0, Epoch 4210, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,354: INFO: model_training: Rank 0, Epoch 4210, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,356: INFO: model_training: Rank 0, Epoch 4211, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,357: INFO: model_training: Rank 0, Epoch 4211, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,358: INFO: model_training: Rank 0, Epoch 4211, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,359: INFO: model_training: Rank 0, Epoch 4211, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,361: INFO: model_training: Rank 0, Epoch 4211, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,362: INFO: model_training: Rank 0, Epoch 4212, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,363: INFO: model_training: Rank 0, Epoch 4212, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,365: INFO: model_training: Rank 0, Epoch 4212, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,367: INFO: model_training: Rank 0, Epoch 4212, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,369: INFO: model_training: Rank 0, Epoch 4212, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,370: INFO: model_training: Rank 0, Epoch 4213, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,371: INFO: model_training: Rank 0, Epoch 4213, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,373: INFO: model_training: Rank 0, Epoch 4213, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,374: INFO: model_training: Rank 0, Epoch 4213, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,375: INFO: model_training: Rank 0, Epoch 4213, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,376: INFO: model_training: Rank 0, Epoch 4214, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,377: INFO: model_training: Rank 0, Epoch 4214, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,378: INFO: model_training: Rank 0, Epoch 4214, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,379: INFO: model_training: Rank 0, Epoch 4214, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,380: INFO: model_training: Rank 0, Epoch 4214, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,381: INFO: model_training: Rank 0, Epoch 4215, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,383: INFO: model_training: Rank 0, Epoch 4215, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,385: INFO: model_training: Rank 0, Epoch 4215, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,386: INFO: model_training: Rank 0, Epoch 4215, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,388: INFO: model_training: Rank 0, Epoch 4215, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,389: INFO: model_training: Rank 0, Epoch 4216, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,390: INFO: model_training: Rank 0, Epoch 4216, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,391: INFO: model_training: Rank 0, Epoch 4216, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,393: INFO: model_training: Rank 0, Epoch 4216, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,396: INFO: model_training: Rank 0, Epoch 4216, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,397: INFO: model_training: Rank 0, Epoch 4217, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,399: INFO: model_training: Rank 0, Epoch 4217, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,400: INFO: model_training: Rank 0, Epoch 4217, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,401: INFO: model_training: Rank 0, Epoch 4217, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,403: INFO: model_training: Rank 0, Epoch 4217, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,404: INFO: model_training: Rank 0, Epoch 4218, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,405: INFO: model_training: Rank 0, Epoch 4218, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,407: INFO: model_training: Rank 0, Epoch 4218, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,409: INFO: model_training: Rank 0, Epoch 4218, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,410: INFO: model_training: Rank 0, Epoch 4218, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,411: INFO: model_training: Rank 0, Epoch 4219, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,412: INFO: model_training: Rank 0, Epoch 4219, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,414: INFO: model_training: Rank 0, Epoch 4219, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,415: INFO: model_training: Rank 0, Epoch 4219, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,416: INFO: model_training: Rank 0, Epoch 4219, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,418: INFO: model_training: Rank 0, Epoch 4220, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,419: INFO: model_training: Rank 0, Epoch 4220, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,421: INFO: model_training: Rank 0, Epoch 4220, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,422: INFO: model_training: Rank 0, Epoch 4220, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,424: INFO: model_training: Rank 0, Epoch 4220, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,425: INFO: model_training: Rank 0, Epoch 4221, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,427: INFO: model_training: Rank 0, Epoch 4221, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,428: INFO: model_training: Rank 0, Epoch 4221, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,429: INFO: model_training: Rank 0, Epoch 4221, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,431: INFO: model_training: Rank 0, Epoch 4221, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,432: INFO: model_training: Rank 0, Epoch 4222, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,433: INFO: model_training: Rank 0, Epoch 4222, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,435: INFO: model_training: Rank 0, Epoch 4222, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,436: INFO: model_training: Rank 0, Epoch 4222, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,438: INFO: model_training: Rank 0, Epoch 4222, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,439: INFO: model_training: Rank 0, Epoch 4223, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,441: INFO: model_training: Rank 0, Epoch 4223, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,442: INFO: model_training: Rank 0, Epoch 4223, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,444: INFO: model_training: Rank 0, Epoch 4223, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,445: INFO: model_training: Rank 0, Epoch 4223, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,447: INFO: model_training: Rank 0, Epoch 4224, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,449: INFO: model_training: Rank 0, Epoch 4224, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,450: INFO: model_training: Rank 0, Epoch 4224, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,452: INFO: model_training: Rank 0, Epoch 4224, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,454: INFO: model_training: Rank 0, Epoch 4224, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,455: INFO: model_training: Rank 0, Epoch 4225, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,457: INFO: model_training: Rank 0, Epoch 4225, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,458: INFO: model_training: Rank 0, Epoch 4225, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,459: INFO: model_training: Rank 0, Epoch 4225, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,460: INFO: model_training: Rank 0, Epoch 4225, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,462: INFO: model_training: Rank 0, Epoch 4226, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,463: INFO: model_training: Rank 0, Epoch 4226, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,465: INFO: model_training: Rank 0, Epoch 4226, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,466: INFO: model_training: Rank 0, Epoch 4226, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,467: INFO: model_training: Rank 0, Epoch 4226, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,468: INFO: model_training: Rank 0, Epoch 4227, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,469: INFO: model_training: Rank 0, Epoch 4227, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,470: INFO: model_training: Rank 0, Epoch 4227, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,471: INFO: model_training: Rank 0, Epoch 4227, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,473: INFO: model_training: Rank 0, Epoch 4227, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,473: INFO: model_training: Rank 0, Epoch 4228, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,475: INFO: model_training: Rank 0, Epoch 4228, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,476: INFO: model_training: Rank 0, Epoch 4228, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,477: INFO: model_training: Rank 0, Epoch 4228, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,477: INFO: model_training: Rank 0, Epoch 4228, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,479: INFO: model_training: Rank 0, Epoch 4229, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,481: INFO: model_training: Rank 0, Epoch 4229, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,483: INFO: model_training: Rank 0, Epoch 4229, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,485: INFO: model_training: Rank 0, Epoch 4229, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,486: INFO: model_training: Rank 0, Epoch 4229, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,488: INFO: model_training: Rank 0, Epoch 4230, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,489: INFO: model_training: Rank 0, Epoch 4230, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,491: INFO: model_training: Rank 0, Epoch 4230, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,492: INFO: model_training: Rank 0, Epoch 4230, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,493: INFO: model_training: Rank 0, Epoch 4230, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,494: INFO: model_training: Rank 0, Epoch 4231, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,496: INFO: model_training: Rank 0, Epoch 4231, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,497: INFO: model_training: Rank 0, Epoch 4231, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,499: INFO: model_training: Rank 0, Epoch 4231, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,500: INFO: model_training: Rank 0, Epoch 4231, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,502: INFO: model_training: Rank 0, Epoch 4232, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,503: INFO: model_training: Rank 0, Epoch 4232, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,505: INFO: model_training: Rank 0, Epoch 4232, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,506: INFO: model_training: Rank 0, Epoch 4232, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,507: INFO: model_training: Rank 0, Epoch 4232, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,508: INFO: model_training: Rank 0, Epoch 4233, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,510: INFO: model_training: Rank 0, Epoch 4233, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,511: INFO: model_training: Rank 0, Epoch 4233, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,513: INFO: model_training: Rank 0, Epoch 4233, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,514: INFO: model_training: Rank 0, Epoch 4233, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,515: INFO: model_training: Rank 0, Epoch 4234, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,517: INFO: model_training: Rank 0, Epoch 4234, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,519: INFO: model_training: Rank 0, Epoch 4234, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,520: INFO: model_training: Rank 0, Epoch 4234, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,521: INFO: model_training: Rank 0, Epoch 4234, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,523: INFO: model_training: Rank 0, Epoch 4235, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,524: INFO: model_training: Rank 0, Epoch 4235, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,525: INFO: model_training: Rank 0, Epoch 4235, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,526: INFO: model_training: Rank 0, Epoch 4235, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,527: INFO: model_training: Rank 0, Epoch 4235, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,528: INFO: model_training: Rank 0, Epoch 4236, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,530: INFO: model_training: Rank 0, Epoch 4236, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,531: INFO: model_training: Rank 0, Epoch 4236, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,533: INFO: model_training: Rank 0, Epoch 4236, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,534: INFO: model_training: Rank 0, Epoch 4236, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,535: INFO: model_training: Rank 0, Epoch 4237, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,536: INFO: model_training: Rank 0, Epoch 4237, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,538: INFO: model_training: Rank 0, Epoch 4237, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,539: INFO: model_training: Rank 0, Epoch 4237, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,540: INFO: model_training: Rank 0, Epoch 4237, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,541: INFO: model_training: Rank 0, Epoch 4238, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,542: INFO: model_training: Rank 0, Epoch 4238, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,543: INFO: model_training: Rank 0, Epoch 4238, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,545: INFO: model_training: Rank 0, Epoch 4238, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,546: INFO: model_training: Rank 0, Epoch 4238, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,548: INFO: model_training: Rank 0, Epoch 4239, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,549: INFO: model_training: Rank 0, Epoch 4239, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,550: INFO: model_training: Rank 0, Epoch 4239, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,552: INFO: model_training: Rank 0, Epoch 4239, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,553: INFO: model_training: Rank 0, Epoch 4239, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,555: INFO: model_training: Rank 0, Epoch 4240, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,556: INFO: model_training: Rank 0, Epoch 4240, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,558: INFO: model_training: Rank 0, Epoch 4240, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,559: INFO: model_training: Rank 0, Epoch 4240, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,561: INFO: model_training: Rank 0, Epoch 4240, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,562: INFO: model_training: Rank 0, Epoch 4241, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,563: INFO: model_training: Rank 0, Epoch 4241, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,565: INFO: model_training: Rank 0, Epoch 4241, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,568: INFO: model_training: Rank 0, Epoch 4241, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,570: INFO: model_training: Rank 0, Epoch 4241, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,572: INFO: model_training: Rank 0, Epoch 4242, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,573: INFO: model_training: Rank 0, Epoch 4242, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,574: INFO: model_training: Rank 0, Epoch 4242, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,575: INFO: model_training: Rank 0, Epoch 4242, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,576: INFO: model_training: Rank 0, Epoch 4242, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,577: INFO: model_training: Rank 0, Epoch 4243, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,579: INFO: model_training: Rank 0, Epoch 4243, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,580: INFO: model_training: Rank 0, Epoch 4243, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,581: INFO: model_training: Rank 0, Epoch 4243, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,582: INFO: model_training: Rank 0, Epoch 4243, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,583: INFO: model_training: Rank 0, Epoch 4244, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,584: INFO: model_training: Rank 0, Epoch 4244, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,586: INFO: model_training: Rank 0, Epoch 4244, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,587: INFO: model_training: Rank 0, Epoch 4244, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,588: INFO: model_training: Rank 0, Epoch 4244, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,589: INFO: model_training: Rank 0, Epoch 4245, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,589: INFO: model_training: Rank 0, Epoch 4245, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,590: INFO: model_training: Rank 0, Epoch 4245, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,592: INFO: model_training: Rank 0, Epoch 4245, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,593: INFO: model_training: Rank 0, Epoch 4245, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,594: INFO: model_training: Rank 0, Epoch 4246, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,595: INFO: model_training: Rank 0, Epoch 4246, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,596: INFO: model_training: Rank 0, Epoch 4246, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,597: INFO: model_training: Rank 0, Epoch 4246, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,598: INFO: model_training: Rank 0, Epoch 4246, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,599: INFO: model_training: Rank 0, Epoch 4247, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,601: INFO: model_training: Rank 0, Epoch 4247, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,602: INFO: model_training: Rank 0, Epoch 4247, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,603: INFO: model_training: Rank 0, Epoch 4247, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,604: INFO: model_training: Rank 0, Epoch 4247, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,605: INFO: model_training: Rank 0, Epoch 4248, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,606: INFO: model_training: Rank 0, Epoch 4248, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,607: INFO: model_training: Rank 0, Epoch 4248, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,608: INFO: model_training: Rank 0, Epoch 4248, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,609: INFO: model_training: Rank 0, Epoch 4248, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,610: INFO: model_training: Rank 0, Epoch 4249, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,612: INFO: model_training: Rank 0, Epoch 4249, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,612: INFO: model_training: Rank 0, Epoch 4249, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,613: INFO: model_training: Rank 0, Epoch 4249, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,614: INFO: model_training: Rank 0, Epoch 4249, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,616: INFO: model_training: Rank 0, Epoch 4250, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,617: INFO: model_training: Rank 0, Epoch 4250, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,618: INFO: model_training: Rank 0, Epoch 4250, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,619: INFO: model_training: Rank 0, Epoch 4250, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,620: INFO: model_training: Rank 0, Epoch 4250, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,621: INFO: model_training: Rank 0, Epoch 4251, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,623: INFO: model_training: Rank 0, Epoch 4251, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,624: INFO: model_training: Rank 0, Epoch 4251, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,625: INFO: model_training: Rank 0, Epoch 4251, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,627: INFO: model_training: Rank 0, Epoch 4251, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,628: INFO: model_training: Rank 0, Epoch 4252, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,629: INFO: model_training: Rank 0, Epoch 4252, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,631: INFO: model_training: Rank 0, Epoch 4252, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,632: INFO: model_training: Rank 0, Epoch 4252, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,633: INFO: model_training: Rank 0, Epoch 4252, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,634: INFO: model_training: Rank 0, Epoch 4253, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,635: INFO: model_training: Rank 0, Epoch 4253, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,637: INFO: model_training: Rank 0, Epoch 4253, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,638: INFO: model_training: Rank 0, Epoch 4253, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,639: INFO: model_training: Rank 0, Epoch 4253, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,640: INFO: model_training: Rank 0, Epoch 4254, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,641: INFO: model_training: Rank 0, Epoch 4254, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,642: INFO: model_training: Rank 0, Epoch 4254, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,643: INFO: model_training: Rank 0, Epoch 4254, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,645: INFO: model_training: Rank 0, Epoch 4254, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,646: INFO: model_training: Rank 0, Epoch 4255, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,648: INFO: model_training: Rank 0, Epoch 4255, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,649: INFO: model_training: Rank 0, Epoch 4255, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,650: INFO: model_training: Rank 0, Epoch 4255, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,651: INFO: model_training: Rank 0, Epoch 4255, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,652: INFO: model_training: Rank 0, Epoch 4256, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,654: INFO: model_training: Rank 0, Epoch 4256, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,655: INFO: model_training: Rank 0, Epoch 4256, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,656: INFO: model_training: Rank 0, Epoch 4256, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,657: INFO: model_training: Rank 0, Epoch 4256, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,658: INFO: model_training: Rank 0, Epoch 4257, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,659: INFO: model_training: Rank 0, Epoch 4257, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,661: INFO: model_training: Rank 0, Epoch 4257, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,663: INFO: model_training: Rank 0, Epoch 4257, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,665: INFO: model_training: Rank 0, Epoch 4257, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,666: INFO: model_training: Rank 0, Epoch 4258, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,668: INFO: model_training: Rank 0, Epoch 4258, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,669: INFO: model_training: Rank 0, Epoch 4258, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,670: INFO: model_training: Rank 0, Epoch 4258, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,671: INFO: model_training: Rank 0, Epoch 4258, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,672: INFO: model_training: Rank 0, Epoch 4259, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,674: INFO: model_training: Rank 0, Epoch 4259, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,675: INFO: model_training: Rank 0, Epoch 4259, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,676: INFO: model_training: Rank 0, Epoch 4259, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,677: INFO: model_training: Rank 0, Epoch 4259, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,679: INFO: model_training: Rank 0, Epoch 4260, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,681: INFO: model_training: Rank 0, Epoch 4260, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,682: INFO: model_training: Rank 0, Epoch 4260, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,683: INFO: model_training: Rank 0, Epoch 4260, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,685: INFO: model_training: Rank 0, Epoch 4260, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,687: INFO: model_training: Rank 0, Epoch 4261, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,688: INFO: model_training: Rank 0, Epoch 4261, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,689: INFO: model_training: Rank 0, Epoch 4261, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,691: INFO: model_training: Rank 0, Epoch 4261, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,692: INFO: model_training: Rank 0, Epoch 4261, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,693: INFO: model_training: Rank 0, Epoch 4262, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,695: INFO: model_training: Rank 0, Epoch 4262, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,697: INFO: model_training: Rank 0, Epoch 4262, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,698: INFO: model_training: Rank 0, Epoch 4262, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,700: INFO: model_training: Rank 0, Epoch 4262, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,701: INFO: model_training: Rank 0, Epoch 4263, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,702: INFO: model_training: Rank 0, Epoch 4263, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,703: INFO: model_training: Rank 0, Epoch 4263, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,704: INFO: model_training: Rank 0, Epoch 4263, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,706: INFO: model_training: Rank 0, Epoch 4263, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,707: INFO: model_training: Rank 0, Epoch 4264, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,708: INFO: model_training: Rank 0, Epoch 4264, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,709: INFO: model_training: Rank 0, Epoch 4264, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,710: INFO: model_training: Rank 0, Epoch 4264, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,712: INFO: model_training: Rank 0, Epoch 4264, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,714: INFO: model_training: Rank 0, Epoch 4265, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,715: INFO: model_training: Rank 0, Epoch 4265, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,716: INFO: model_training: Rank 0, Epoch 4265, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,718: INFO: model_training: Rank 0, Epoch 4265, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,719: INFO: model_training: Rank 0, Epoch 4265, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,720: INFO: model_training: Rank 0, Epoch 4266, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,721: INFO: model_training: Rank 0, Epoch 4266, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,722: INFO: model_training: Rank 0, Epoch 4266, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,723: INFO: model_training: Rank 0, Epoch 4266, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,725: INFO: model_training: Rank 0, Epoch 4266, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,726: INFO: model_training: Rank 0, Epoch 4267, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,728: INFO: model_training: Rank 0, Epoch 4267, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,730: INFO: model_training: Rank 0, Epoch 4267, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,731: INFO: model_training: Rank 0, Epoch 4267, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,732: INFO: model_training: Rank 0, Epoch 4267, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,734: INFO: model_training: Rank 0, Epoch 4268, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,735: INFO: model_training: Rank 0, Epoch 4268, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,736: INFO: model_training: Rank 0, Epoch 4268, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,737: INFO: model_training: Rank 0, Epoch 4268, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,738: INFO: model_training: Rank 0, Epoch 4268, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,740: INFO: model_training: Rank 0, Epoch 4269, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,741: INFO: model_training: Rank 0, Epoch 4269, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,742: INFO: model_training: Rank 0, Epoch 4269, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,744: INFO: model_training: Rank 0, Epoch 4269, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,746: INFO: model_training: Rank 0, Epoch 4269, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,747: INFO: model_training: Rank 0, Epoch 4270, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,749: INFO: model_training: Rank 0, Epoch 4270, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,750: INFO: model_training: Rank 0, Epoch 4270, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,751: INFO: model_training: Rank 0, Epoch 4270, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,753: INFO: model_training: Rank 0, Epoch 4270, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,754: INFO: model_training: Rank 0, Epoch 4271, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,755: INFO: model_training: Rank 0, Epoch 4271, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,756: INFO: model_training: Rank 0, Epoch 4271, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,757: INFO: model_training: Rank 0, Epoch 4271, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,758: INFO: model_training: Rank 0, Epoch 4271, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,760: INFO: model_training: Rank 0, Epoch 4272, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,762: INFO: model_training: Rank 0, Epoch 4272, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,763: INFO: model_training: Rank 0, Epoch 4272, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,764: INFO: model_training: Rank 0, Epoch 4272, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,765: INFO: model_training: Rank 0, Epoch 4272, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,766: INFO: model_training: Rank 0, Epoch 4273, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,768: INFO: model_training: Rank 0, Epoch 4273, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,769: INFO: model_training: Rank 0, Epoch 4273, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,770: INFO: model_training: Rank 0, Epoch 4273, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,771: INFO: model_training: Rank 0, Epoch 4273, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,772: INFO: model_training: Rank 0, Epoch 4274, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,774: INFO: model_training: Rank 0, Epoch 4274, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,775: INFO: model_training: Rank 0, Epoch 4274, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,777: INFO: model_training: Rank 0, Epoch 4274, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,778: INFO: model_training: Rank 0, Epoch 4274, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,780: INFO: model_training: Rank 0, Epoch 4275, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,781: INFO: model_training: Rank 0, Epoch 4275, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,782: INFO: model_training: Rank 0, Epoch 4275, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,784: INFO: model_training: Rank 0, Epoch 4275, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,785: INFO: model_training: Rank 0, Epoch 4275, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,786: INFO: model_training: Rank 0, Epoch 4276, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,787: INFO: model_training: Rank 0, Epoch 4276, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,788: INFO: model_training: Rank 0, Epoch 4276, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,790: INFO: model_training: Rank 0, Epoch 4276, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,791: INFO: model_training: Rank 0, Epoch 4276, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,793: INFO: model_training: Rank 0, Epoch 4277, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,795: INFO: model_training: Rank 0, Epoch 4277, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,797: INFO: model_training: Rank 0, Epoch 4277, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,798: INFO: model_training: Rank 0, Epoch 4277, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,799: INFO: model_training: Rank 0, Epoch 4277, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,800: INFO: model_training: Rank 0, Epoch 4278, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,802: INFO: model_training: Rank 0, Epoch 4278, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,803: INFO: model_training: Rank 0, Epoch 4278, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,804: INFO: model_training: Rank 0, Epoch 4278, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,805: INFO: model_training: Rank 0, Epoch 4278, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,807: INFO: model_training: Rank 0, Epoch 4279, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,809: INFO: model_training: Rank 0, Epoch 4279, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,810: INFO: model_training: Rank 0, Epoch 4279, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,812: INFO: model_training: Rank 0, Epoch 4279, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,813: INFO: model_training: Rank 0, Epoch 4279, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,814: INFO: model_training: Rank 0, Epoch 4280, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,815: INFO: model_training: Rank 0, Epoch 4280, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,816: INFO: model_training: Rank 0, Epoch 4280, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,817: INFO: model_training: Rank 0, Epoch 4280, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,819: INFO: model_training: Rank 0, Epoch 4280, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,820: INFO: model_training: Rank 0, Epoch 4281, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,821: INFO: model_training: Rank 0, Epoch 4281, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,822: INFO: model_training: Rank 0, Epoch 4281, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,823: INFO: model_training: Rank 0, Epoch 4281, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,825: INFO: model_training: Rank 0, Epoch 4281, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,827: INFO: model_training: Rank 0, Epoch 4282, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,828: INFO: model_training: Rank 0, Epoch 4282, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,830: INFO: model_training: Rank 0, Epoch 4282, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,831: INFO: model_training: Rank 0, Epoch 4282, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,832: INFO: model_training: Rank 0, Epoch 4282, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,833: INFO: model_training: Rank 0, Epoch 4283, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,835: INFO: model_training: Rank 0, Epoch 4283, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,836: INFO: model_training: Rank 0, Epoch 4283, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,837: INFO: model_training: Rank 0, Epoch 4283, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,838: INFO: model_training: Rank 0, Epoch 4283, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,839: INFO: model_training: Rank 0, Epoch 4284, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,841: INFO: model_training: Rank 0, Epoch 4284, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,843: INFO: model_training: Rank 0, Epoch 4284, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,845: INFO: model_training: Rank 0, Epoch 4284, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,846: INFO: model_training: Rank 0, Epoch 4284, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,847: INFO: model_training: Rank 0, Epoch 4285, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,849: INFO: model_training: Rank 0, Epoch 4285, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,850: INFO: model_training: Rank 0, Epoch 4285, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,851: INFO: model_training: Rank 0, Epoch 4285, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,853: INFO: model_training: Rank 0, Epoch 4285, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,854: INFO: model_training: Rank 0, Epoch 4286, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,855: INFO: model_training: Rank 0, Epoch 4286, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,856: INFO: model_training: Rank 0, Epoch 4286, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,858: INFO: model_training: Rank 0, Epoch 4286, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,860: INFO: model_training: Rank 0, Epoch 4286, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,861: INFO: model_training: Rank 0, Epoch 4287, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,863: INFO: model_training: Rank 0, Epoch 4287, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,864: INFO: model_training: Rank 0, Epoch 4287, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,866: INFO: model_training: Rank 0, Epoch 4287, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,867: INFO: model_training: Rank 0, Epoch 4287, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,869: INFO: model_training: Rank 0, Epoch 4288, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,870: INFO: model_training: Rank 0, Epoch 4288, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,871: INFO: model_training: Rank 0, Epoch 4288, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,872: INFO: model_training: Rank 0, Epoch 4288, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,874: INFO: model_training: Rank 0, Epoch 4288, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,875: INFO: model_training: Rank 0, Epoch 4289, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,876: INFO: model_training: Rank 0, Epoch 4289, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,877: INFO: model_training: Rank 0, Epoch 4289, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,879: INFO: model_training: Rank 0, Epoch 4289, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,880: INFO: model_training: Rank 0, Epoch 4289, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,881: INFO: model_training: Rank 0, Epoch 4290, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,882: INFO: model_training: Rank 0, Epoch 4290, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,883: INFO: model_training: Rank 0, Epoch 4290, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,885: INFO: model_training: Rank 0, Epoch 4290, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,886: INFO: model_training: Rank 0, Epoch 4290, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,888: INFO: model_training: Rank 0, Epoch 4291, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,889: INFO: model_training: Rank 0, Epoch 4291, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,891: INFO: model_training: Rank 0, Epoch 4291, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,893: INFO: model_training: Rank 0, Epoch 4291, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,895: INFO: model_training: Rank 0, Epoch 4291, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,896: INFO: model_training: Rank 0, Epoch 4292, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,898: INFO: model_training: Rank 0, Epoch 4292, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,899: INFO: model_training: Rank 0, Epoch 4292, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,901: INFO: model_training: Rank 0, Epoch 4292, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,902: INFO: model_training: Rank 0, Epoch 4292, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,904: INFO: model_training: Rank 0, Epoch 4293, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,905: INFO: model_training: Rank 0, Epoch 4293, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,907: INFO: model_training: Rank 0, Epoch 4293, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,909: INFO: model_training: Rank 0, Epoch 4293, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,910: INFO: model_training: Rank 0, Epoch 4293, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,911: INFO: model_training: Rank 0, Epoch 4294, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,912: INFO: model_training: Rank 0, Epoch 4294, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,914: INFO: model_training: Rank 0, Epoch 4294, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,915: INFO: model_training: Rank 0, Epoch 4294, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,917: INFO: model_training: Rank 0, Epoch 4294, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,919: INFO: model_training: Rank 0, Epoch 4295, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,920: INFO: model_training: Rank 0, Epoch 4295, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,921: INFO: model_training: Rank 0, Epoch 4295, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,923: INFO: model_training: Rank 0, Epoch 4295, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,925: INFO: model_training: Rank 0, Epoch 4295, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,927: INFO: model_training: Rank 0, Epoch 4296, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,929: INFO: model_training: Rank 0, Epoch 4296, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,930: INFO: model_training: Rank 0, Epoch 4296, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,932: INFO: model_training: Rank 0, Epoch 4296, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,934: INFO: model_training: Rank 0, Epoch 4296, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,936: INFO: model_training: Rank 0, Epoch 4297, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,937: INFO: model_training: Rank 0, Epoch 4297, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,938: INFO: model_training: Rank 0, Epoch 4297, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,940: INFO: model_training: Rank 0, Epoch 4297, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,942: INFO: model_training: Rank 0, Epoch 4297, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,943: INFO: model_training: Rank 0, Epoch 4298, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,945: INFO: model_training: Rank 0, Epoch 4298, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,947: INFO: model_training: Rank 0, Epoch 4298, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,948: INFO: model_training: Rank 0, Epoch 4298, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,950: INFO: model_training: Rank 0, Epoch 4298, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,951: INFO: model_training: Rank 0, Epoch 4299, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,953: INFO: model_training: Rank 0, Epoch 4299, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,954: INFO: model_training: Rank 0, Epoch 4299, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,956: INFO: model_training: Rank 0, Epoch 4299, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,958: INFO: model_training: Rank 0, Epoch 4299, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,959: INFO: model_training: Rank 0, Epoch 4300, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,961: INFO: model_training: Rank 0, Epoch 4300, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,963: INFO: model_training: Rank 0, Epoch 4300, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,964: INFO: model_training: Rank 0, Epoch 4300, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,967: INFO: model_training: Rank 0, Epoch 4300, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,969: INFO: model_training: Rank 0, Epoch 4301, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,970: INFO: model_training: Rank 0, Epoch 4301, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,972: INFO: model_training: Rank 0, Epoch 4301, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,974: INFO: model_training: Rank 0, Epoch 4301, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,976: INFO: model_training: Rank 0, Epoch 4301, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,977: INFO: model_training: Rank 0, Epoch 4302, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,979: INFO: model_training: Rank 0, Epoch 4302, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,980: INFO: model_training: Rank 0, Epoch 4302, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,982: INFO: model_training: Rank 0, Epoch 4302, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,983: INFO: model_training: Rank 0, Epoch 4302, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,985: INFO: model_training: Rank 0, Epoch 4303, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,986: INFO: model_training: Rank 0, Epoch 4303, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,987: INFO: model_training: Rank 0, Epoch 4303, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,989: INFO: model_training: Rank 0, Epoch 4303, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:53,991: INFO: model_training: Rank 0, Epoch 4303, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:53,993: INFO: model_training: Rank 0, Epoch 4304, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:53,994: INFO: model_training: Rank 0, Epoch 4304, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:53,996: INFO: model_training: Rank 0, Epoch 4304, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:53,998: INFO: model_training: Rank 0, Epoch 4304, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,000: INFO: model_training: Rank 0, Epoch 4304, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,001: INFO: model_training: Rank 0, Epoch 4305, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,002: INFO: model_training: Rank 0, Epoch 4305, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,004: INFO: model_training: Rank 0, Epoch 4305, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,006: INFO: model_training: Rank 0, Epoch 4305, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,007: INFO: model_training: Rank 0, Epoch 4305, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,009: INFO: model_training: Rank 0, Epoch 4306, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,011: INFO: model_training: Rank 0, Epoch 4306, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,012: INFO: model_training: Rank 0, Epoch 4306, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,014: INFO: model_training: Rank 0, Epoch 4306, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,015: INFO: model_training: Rank 0, Epoch 4306, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,017: INFO: model_training: Rank 0, Epoch 4307, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,018: INFO: model_training: Rank 0, Epoch 4307, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,020: INFO: model_training: Rank 0, Epoch 4307, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,021: INFO: model_training: Rank 0, Epoch 4307, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,023: INFO: model_training: Rank 0, Epoch 4307, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,025: INFO: model_training: Rank 0, Epoch 4308, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,027: INFO: model_training: Rank 0, Epoch 4308, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,028: INFO: model_training: Rank 0, Epoch 4308, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,030: INFO: model_training: Rank 0, Epoch 4308, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,031: INFO: model_training: Rank 0, Epoch 4308, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,033: INFO: model_training: Rank 0, Epoch 4309, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,035: INFO: model_training: Rank 0, Epoch 4309, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,037: INFO: model_training: Rank 0, Epoch 4309, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,039: INFO: model_training: Rank 0, Epoch 4309, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,041: INFO: model_training: Rank 0, Epoch 4309, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,043: INFO: model_training: Rank 0, Epoch 4310, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,046: INFO: model_training: Rank 0, Epoch 4310, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,049: INFO: model_training: Rank 0, Epoch 4310, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,053: INFO: model_training: Rank 0, Epoch 4310, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,056: INFO: model_training: Rank 0, Epoch 4310, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,059: INFO: model_training: Rank 0, Epoch 4311, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,061: INFO: model_training: Rank 0, Epoch 4311, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,063: INFO: model_training: Rank 0, Epoch 4311, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,065: INFO: model_training: Rank 0, Epoch 4311, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,066: INFO: model_training: Rank 0, Epoch 4311, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,068: INFO: model_training: Rank 0, Epoch 4312, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,070: INFO: model_training: Rank 0, Epoch 4312, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,071: INFO: model_training: Rank 0, Epoch 4312, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,072: INFO: model_training: Rank 0, Epoch 4312, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,074: INFO: model_training: Rank 0, Epoch 4312, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,075: INFO: model_training: Rank 0, Epoch 4313, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,076: INFO: model_training: Rank 0, Epoch 4313, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,077: INFO: model_training: Rank 0, Epoch 4313, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,078: INFO: model_training: Rank 0, Epoch 4313, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,080: INFO: model_training: Rank 0, Epoch 4313, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,082: INFO: model_training: Rank 0, Epoch 4314, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,083: INFO: model_training: Rank 0, Epoch 4314, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,084: INFO: model_training: Rank 0, Epoch 4314, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,085: INFO: model_training: Rank 0, Epoch 4314, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,087: INFO: model_training: Rank 0, Epoch 4314, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,088: INFO: model_training: Rank 0, Epoch 4315, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,090: INFO: model_training: Rank 0, Epoch 4315, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,092: INFO: model_training: Rank 0, Epoch 4315, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,093: INFO: model_training: Rank 0, Epoch 4315, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,094: INFO: model_training: Rank 0, Epoch 4315, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,095: INFO: model_training: Rank 0, Epoch 4316, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,097: INFO: model_training: Rank 0, Epoch 4316, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,098: INFO: model_training: Rank 0, Epoch 4316, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,099: INFO: model_training: Rank 0, Epoch 4316, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,100: INFO: model_training: Rank 0, Epoch 4316, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,101: INFO: model_training: Rank 0, Epoch 4317, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,103: INFO: model_training: Rank 0, Epoch 4317, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,104: INFO: model_training: Rank 0, Epoch 4317, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,105: INFO: model_training: Rank 0, Epoch 4317, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,106: INFO: model_training: Rank 0, Epoch 4317, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,107: INFO: model_training: Rank 0, Epoch 4318, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,109: INFO: model_training: Rank 0, Epoch 4318, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,110: INFO: model_training: Rank 0, Epoch 4318, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,111: INFO: model_training: Rank 0, Epoch 4318, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,113: INFO: model_training: Rank 0, Epoch 4318, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,115: INFO: model_training: Rank 0, Epoch 4319, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,116: INFO: model_training: Rank 0, Epoch 4319, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,117: INFO: model_training: Rank 0, Epoch 4319, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,118: INFO: model_training: Rank 0, Epoch 4319, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,120: INFO: model_training: Rank 0, Epoch 4319, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,121: INFO: model_training: Rank 0, Epoch 4320, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,122: INFO: model_training: Rank 0, Epoch 4320, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,123: INFO: model_training: Rank 0, Epoch 4320, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,125: INFO: model_training: Rank 0, Epoch 4320, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,126: INFO: model_training: Rank 0, Epoch 4320, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,127: INFO: model_training: Rank 0, Epoch 4321, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,129: INFO: model_training: Rank 0, Epoch 4321, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,130: INFO: model_training: Rank 0, Epoch 4321, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,132: INFO: model_training: Rank 0, Epoch 4321, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,133: INFO: model_training: Rank 0, Epoch 4321, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,134: INFO: model_training: Rank 0, Epoch 4322, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,135: INFO: model_training: Rank 0, Epoch 4322, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,137: INFO: model_training: Rank 0, Epoch 4322, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,138: INFO: model_training: Rank 0, Epoch 4322, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,139: INFO: model_training: Rank 0, Epoch 4322, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,141: INFO: model_training: Rank 0, Epoch 4323, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,143: INFO: model_training: Rank 0, Epoch 4323, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,144: INFO: model_training: Rank 0, Epoch 4323, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,145: INFO: model_training: Rank 0, Epoch 4323, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,147: INFO: model_training: Rank 0, Epoch 4323, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,148: INFO: model_training: Rank 0, Epoch 4324, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,149: INFO: model_training: Rank 0, Epoch 4324, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,151: INFO: model_training: Rank 0, Epoch 4324, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,152: INFO: model_training: Rank 0, Epoch 4324, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,153: INFO: model_training: Rank 0, Epoch 4324, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,155: INFO: model_training: Rank 0, Epoch 4325, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,156: INFO: model_training: Rank 0, Epoch 4325, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,158: INFO: model_training: Rank 0, Epoch 4325, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,159: INFO: model_training: Rank 0, Epoch 4325, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,160: INFO: model_training: Rank 0, Epoch 4325, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,161: INFO: model_training: Rank 0, Epoch 4326, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,163: INFO: model_training: Rank 0, Epoch 4326, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,164: INFO: model_training: Rank 0, Epoch 4326, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,165: INFO: model_training: Rank 0, Epoch 4326, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,167: INFO: model_training: Rank 0, Epoch 4326, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,168: INFO: model_training: Rank 0, Epoch 4327, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,169: INFO: model_training: Rank 0, Epoch 4327, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,171: INFO: model_training: Rank 0, Epoch 4327, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,173: INFO: model_training: Rank 0, Epoch 4327, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,175: INFO: model_training: Rank 0, Epoch 4327, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,176: INFO: model_training: Rank 0, Epoch 4328, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,178: INFO: model_training: Rank 0, Epoch 4328, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,179: INFO: model_training: Rank 0, Epoch 4328, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,180: INFO: model_training: Rank 0, Epoch 4328, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,182: INFO: model_training: Rank 0, Epoch 4328, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,183: INFO: model_training: Rank 0, Epoch 4329, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,185: INFO: model_training: Rank 0, Epoch 4329, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,186: INFO: model_training: Rank 0, Epoch 4329, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,187: INFO: model_training: Rank 0, Epoch 4329, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,188: INFO: model_training: Rank 0, Epoch 4329, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,190: INFO: model_training: Rank 0, Epoch 4330, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,191: INFO: model_training: Rank 0, Epoch 4330, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,192: INFO: model_training: Rank 0, Epoch 4330, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,194: INFO: model_training: Rank 0, Epoch 4330, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,195: INFO: model_training: Rank 0, Epoch 4330, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,197: INFO: model_training: Rank 0, Epoch 4331, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,198: INFO: model_training: Rank 0, Epoch 4331, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,199: INFO: model_training: Rank 0, Epoch 4331, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,200: INFO: model_training: Rank 0, Epoch 4331, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,201: INFO: model_training: Rank 0, Epoch 4331, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,202: INFO: model_training: Rank 0, Epoch 4332, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,204: INFO: model_training: Rank 0, Epoch 4332, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,206: INFO: model_training: Rank 0, Epoch 4332, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,207: INFO: model_training: Rank 0, Epoch 4332, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,209: INFO: model_training: Rank 0, Epoch 4332, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,210: INFO: model_training: Rank 0, Epoch 4333, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,211: INFO: model_training: Rank 0, Epoch 4333, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,213: INFO: model_training: Rank 0, Epoch 4333, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,214: INFO: model_training: Rank 0, Epoch 4333, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,215: INFO: model_training: Rank 0, Epoch 4333, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,216: INFO: model_training: Rank 0, Epoch 4334, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,218: INFO: model_training: Rank 0, Epoch 4334, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,219: INFO: model_training: Rank 0, Epoch 4334, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,220: INFO: model_training: Rank 0, Epoch 4334, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,221: INFO: model_training: Rank 0, Epoch 4334, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,223: INFO: model_training: Rank 0, Epoch 4335, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,224: INFO: model_training: Rank 0, Epoch 4335, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,225: INFO: model_training: Rank 0, Epoch 4335, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,226: INFO: model_training: Rank 0, Epoch 4335, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,227: INFO: model_training: Rank 0, Epoch 4335, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,229: INFO: model_training: Rank 0, Epoch 4336, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,230: INFO: model_training: Rank 0, Epoch 4336, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,231: INFO: model_training: Rank 0, Epoch 4336, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,232: INFO: model_training: Rank 0, Epoch 4336, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,234: INFO: model_training: Rank 0, Epoch 4336, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,236: INFO: model_training: Rank 0, Epoch 4337, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,237: INFO: model_training: Rank 0, Epoch 4337, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,238: INFO: model_training: Rank 0, Epoch 4337, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,239: INFO: model_training: Rank 0, Epoch 4337, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,240: INFO: model_training: Rank 0, Epoch 4337, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,241: INFO: model_training: Rank 0, Epoch 4338, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,243: INFO: model_training: Rank 0, Epoch 4338, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,244: INFO: model_training: Rank 0, Epoch 4338, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,246: INFO: model_training: Rank 0, Epoch 4338, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,247: INFO: model_training: Rank 0, Epoch 4338, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,248: INFO: model_training: Rank 0, Epoch 4339, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,249: INFO: model_training: Rank 0, Epoch 4339, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,250: INFO: model_training: Rank 0, Epoch 4339, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,251: INFO: model_training: Rank 0, Epoch 4339, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,253: INFO: model_training: Rank 0, Epoch 4339, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,254: INFO: model_training: Rank 0, Epoch 4340, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,255: INFO: model_training: Rank 0, Epoch 4340, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,257: INFO: model_training: Rank 0, Epoch 4340, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,258: INFO: model_training: Rank 0, Epoch 4340, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,259: INFO: model_training: Rank 0, Epoch 4340, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,260: INFO: model_training: Rank 0, Epoch 4341, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,262: INFO: model_training: Rank 0, Epoch 4341, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,263: INFO: model_training: Rank 0, Epoch 4341, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,265: INFO: model_training: Rank 0, Epoch 4341, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,267: INFO: model_training: Rank 0, Epoch 4341, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,268: INFO: model_training: Rank 0, Epoch 4342, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,269: INFO: model_training: Rank 0, Epoch 4342, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,270: INFO: model_training: Rank 0, Epoch 4342, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,271: INFO: model_training: Rank 0, Epoch 4342, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,272: INFO: model_training: Rank 0, Epoch 4342, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,273: INFO: model_training: Rank 0, Epoch 4343, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,275: INFO: model_training: Rank 0, Epoch 4343, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,276: INFO: model_training: Rank 0, Epoch 4343, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,277: INFO: model_training: Rank 0, Epoch 4343, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,278: INFO: model_training: Rank 0, Epoch 4343, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,279: INFO: model_training: Rank 0, Epoch 4344, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,281: INFO: model_training: Rank 0, Epoch 4344, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,282: INFO: model_training: Rank 0, Epoch 4344, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,283: INFO: model_training: Rank 0, Epoch 4344, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,284: INFO: model_training: Rank 0, Epoch 4344, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,285: INFO: model_training: Rank 0, Epoch 4345, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,287: INFO: model_training: Rank 0, Epoch 4345, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,288: INFO: model_training: Rank 0, Epoch 4345, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,289: INFO: model_training: Rank 0, Epoch 4345, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,290: INFO: model_training: Rank 0, Epoch 4345, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,292: INFO: model_training: Rank 0, Epoch 4346, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,293: INFO: model_training: Rank 0, Epoch 4346, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,294: INFO: model_training: Rank 0, Epoch 4346, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,296: INFO: model_training: Rank 0, Epoch 4346, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,297: INFO: model_training: Rank 0, Epoch 4346, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,298: INFO: model_training: Rank 0, Epoch 4347, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,300: INFO: model_training: Rank 0, Epoch 4347, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,301: INFO: model_training: Rank 0, Epoch 4347, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,302: INFO: model_training: Rank 0, Epoch 4347, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,304: INFO: model_training: Rank 0, Epoch 4347, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,305: INFO: model_training: Rank 0, Epoch 4348, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,306: INFO: model_training: Rank 0, Epoch 4348, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,307: INFO: model_training: Rank 0, Epoch 4348, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,309: INFO: model_training: Rank 0, Epoch 4348, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,310: INFO: model_training: Rank 0, Epoch 4348, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,311: INFO: model_training: Rank 0, Epoch 4349, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,312: INFO: model_training: Rank 0, Epoch 4349, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,313: INFO: model_training: Rank 0, Epoch 4349, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,315: INFO: model_training: Rank 0, Epoch 4349, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,316: INFO: model_training: Rank 0, Epoch 4349, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,317: INFO: model_training: Rank 0, Epoch 4350, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,318: INFO: model_training: Rank 0, Epoch 4350, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,320: INFO: model_training: Rank 0, Epoch 4350, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,321: INFO: model_training: Rank 0, Epoch 4350, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,323: INFO: model_training: Rank 0, Epoch 4350, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,324: INFO: model_training: Rank 0, Epoch 4351, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,325: INFO: model_training: Rank 0, Epoch 4351, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,327: INFO: model_training: Rank 0, Epoch 4351, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,328: INFO: model_training: Rank 0, Epoch 4351, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,329: INFO: model_training: Rank 0, Epoch 4351, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,330: INFO: model_training: Rank 0, Epoch 4352, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,331: INFO: model_training: Rank 0, Epoch 4352, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,332: INFO: model_training: Rank 0, Epoch 4352, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,334: INFO: model_training: Rank 0, Epoch 4352, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,335: INFO: model_training: Rank 0, Epoch 4352, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,336: INFO: model_training: Rank 0, Epoch 4353, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,337: INFO: model_training: Rank 0, Epoch 4353, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,339: INFO: model_training: Rank 0, Epoch 4353, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,340: INFO: model_training: Rank 0, Epoch 4353, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,341: INFO: model_training: Rank 0, Epoch 4353, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,342: INFO: model_training: Rank 0, Epoch 4354, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,344: INFO: model_training: Rank 0, Epoch 4354, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,345: INFO: model_training: Rank 0, Epoch 4354, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,346: INFO: model_training: Rank 0, Epoch 4354, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,347: INFO: model_training: Rank 0, Epoch 4354, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,349: INFO: model_training: Rank 0, Epoch 4355, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,350: INFO: model_training: Rank 0, Epoch 4355, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,351: INFO: model_training: Rank 0, Epoch 4355, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,352: INFO: model_training: Rank 0, Epoch 4355, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,354: INFO: model_training: Rank 0, Epoch 4355, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,355: INFO: model_training: Rank 0, Epoch 4356, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,357: INFO: model_training: Rank 0, Epoch 4356, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,358: INFO: model_training: Rank 0, Epoch 4356, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,359: INFO: model_training: Rank 0, Epoch 4356, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,360: INFO: model_training: Rank 0, Epoch 4356, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,362: INFO: model_training: Rank 0, Epoch 4357, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,363: INFO: model_training: Rank 0, Epoch 4357, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,364: INFO: model_training: Rank 0, Epoch 4357, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,365: INFO: model_training: Rank 0, Epoch 4357, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,367: INFO: model_training: Rank 0, Epoch 4357, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,368: INFO: model_training: Rank 0, Epoch 4358, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,369: INFO: model_training: Rank 0, Epoch 4358, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,371: INFO: model_training: Rank 0, Epoch 4358, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,372: INFO: model_training: Rank 0, Epoch 4358, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,373: INFO: model_training: Rank 0, Epoch 4358, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,375: INFO: model_training: Rank 0, Epoch 4359, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,376: INFO: model_training: Rank 0, Epoch 4359, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,377: INFO: model_training: Rank 0, Epoch 4359, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,378: INFO: model_training: Rank 0, Epoch 4359, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,380: INFO: model_training: Rank 0, Epoch 4359, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,381: INFO: model_training: Rank 0, Epoch 4360, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,384: INFO: model_training: Rank 0, Epoch 4360, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,385: INFO: model_training: Rank 0, Epoch 4360, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,387: INFO: model_training: Rank 0, Epoch 4360, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,388: INFO: model_training: Rank 0, Epoch 4360, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,390: INFO: model_training: Rank 0, Epoch 4361, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,391: INFO: model_training: Rank 0, Epoch 4361, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,393: INFO: model_training: Rank 0, Epoch 4361, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,394: INFO: model_training: Rank 0, Epoch 4361, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,396: INFO: model_training: Rank 0, Epoch 4361, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,398: INFO: model_training: Rank 0, Epoch 4362, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,400: INFO: model_training: Rank 0, Epoch 4362, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,401: INFO: model_training: Rank 0, Epoch 4362, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,402: INFO: model_training: Rank 0, Epoch 4362, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,403: INFO: model_training: Rank 0, Epoch 4362, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,405: INFO: model_training: Rank 0, Epoch 4363, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,406: INFO: model_training: Rank 0, Epoch 4363, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,408: INFO: model_training: Rank 0, Epoch 4363, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,409: INFO: model_training: Rank 0, Epoch 4363, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,410: INFO: model_training: Rank 0, Epoch 4363, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,411: INFO: model_training: Rank 0, Epoch 4364, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,413: INFO: model_training: Rank 0, Epoch 4364, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,414: INFO: model_training: Rank 0, Epoch 4364, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,416: INFO: model_training: Rank 0, Epoch 4364, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,417: INFO: model_training: Rank 0, Epoch 4364, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,419: INFO: model_training: Rank 0, Epoch 4365, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,420: INFO: model_training: Rank 0, Epoch 4365, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,421: INFO: model_training: Rank 0, Epoch 4365, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,422: INFO: model_training: Rank 0, Epoch 4365, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,425: INFO: model_training: Rank 0, Epoch 4365, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,427: INFO: model_training: Rank 0, Epoch 4366, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,429: INFO: model_training: Rank 0, Epoch 4366, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,432: INFO: model_training: Rank 0, Epoch 4366, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,434: INFO: model_training: Rank 0, Epoch 4366, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,437: INFO: model_training: Rank 0, Epoch 4366, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,439: INFO: model_training: Rank 0, Epoch 4367, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,442: INFO: model_training: Rank 0, Epoch 4367, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,444: INFO: model_training: Rank 0, Epoch 4367, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,447: INFO: model_training: Rank 0, Epoch 4367, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,449: INFO: model_training: Rank 0, Epoch 4367, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,452: INFO: model_training: Rank 0, Epoch 4368, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,454: INFO: model_training: Rank 0, Epoch 4368, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,457: INFO: model_training: Rank 0, Epoch 4368, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,459: INFO: model_training: Rank 0, Epoch 4368, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,461: INFO: model_training: Rank 0, Epoch 4368, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,464: INFO: model_training: Rank 0, Epoch 4369, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,466: INFO: model_training: Rank 0, Epoch 4369, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,467: INFO: model_training: Rank 0, Epoch 4369, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,469: INFO: model_training: Rank 0, Epoch 4369, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,471: INFO: model_training: Rank 0, Epoch 4369, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,472: INFO: model_training: Rank 0, Epoch 4370, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,473: INFO: model_training: Rank 0, Epoch 4370, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,475: INFO: model_training: Rank 0, Epoch 4370, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,477: INFO: model_training: Rank 0, Epoch 4370, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,478: INFO: model_training: Rank 0, Epoch 4370, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,480: INFO: model_training: Rank 0, Epoch 4371, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,481: INFO: model_training: Rank 0, Epoch 4371, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,483: INFO: model_training: Rank 0, Epoch 4371, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,484: INFO: model_training: Rank 0, Epoch 4371, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,486: INFO: model_training: Rank 0, Epoch 4371, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,487: INFO: model_training: Rank 0, Epoch 4372, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,489: INFO: model_training: Rank 0, Epoch 4372, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,491: INFO: model_training: Rank 0, Epoch 4372, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,492: INFO: model_training: Rank 0, Epoch 4372, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,494: INFO: model_training: Rank 0, Epoch 4372, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,496: INFO: model_training: Rank 0, Epoch 4373, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,498: INFO: model_training: Rank 0, Epoch 4373, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,500: INFO: model_training: Rank 0, Epoch 4373, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,501: INFO: model_training: Rank 0, Epoch 4373, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,502: INFO: model_training: Rank 0, Epoch 4373, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,504: INFO: model_training: Rank 0, Epoch 4374, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,505: INFO: model_training: Rank 0, Epoch 4374, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,506: INFO: model_training: Rank 0, Epoch 4374, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,507: INFO: model_training: Rank 0, Epoch 4374, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,509: INFO: model_training: Rank 0, Epoch 4374, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,510: INFO: model_training: Rank 0, Epoch 4375, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,511: INFO: model_training: Rank 0, Epoch 4375, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,512: INFO: model_training: Rank 0, Epoch 4375, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,513: INFO: model_training: Rank 0, Epoch 4375, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,515: INFO: model_training: Rank 0, Epoch 4375, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,516: INFO: model_training: Rank 0, Epoch 4376, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,518: INFO: model_training: Rank 0, Epoch 4376, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,519: INFO: model_training: Rank 0, Epoch 4376, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,520: INFO: model_training: Rank 0, Epoch 4376, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,521: INFO: model_training: Rank 0, Epoch 4376, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,522: INFO: model_training: Rank 0, Epoch 4377, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,524: INFO: model_training: Rank 0, Epoch 4377, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,525: INFO: model_training: Rank 0, Epoch 4377, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,527: INFO: model_training: Rank 0, Epoch 4377, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,529: INFO: model_training: Rank 0, Epoch 4377, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,530: INFO: model_training: Rank 0, Epoch 4378, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,531: INFO: model_training: Rank 0, Epoch 4378, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,533: INFO: model_training: Rank 0, Epoch 4378, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,534: INFO: model_training: Rank 0, Epoch 4378, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,536: INFO: model_training: Rank 0, Epoch 4378, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,537: INFO: model_training: Rank 0, Epoch 4379, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,538: INFO: model_training: Rank 0, Epoch 4379, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,539: INFO: model_training: Rank 0, Epoch 4379, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,540: INFO: model_training: Rank 0, Epoch 4379, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,542: INFO: model_training: Rank 0, Epoch 4379, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,543: INFO: model_training: Rank 0, Epoch 4380, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,544: INFO: model_training: Rank 0, Epoch 4380, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,545: INFO: model_training: Rank 0, Epoch 4380, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,546: INFO: model_training: Rank 0, Epoch 4380, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,548: INFO: model_training: Rank 0, Epoch 4380, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,549: INFO: model_training: Rank 0, Epoch 4381, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,550: INFO: model_training: Rank 0, Epoch 4381, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,551: INFO: model_training: Rank 0, Epoch 4381, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,552: INFO: model_training: Rank 0, Epoch 4381, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,553: INFO: model_training: Rank 0, Epoch 4381, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,555: INFO: model_training: Rank 0, Epoch 4382, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,556: INFO: model_training: Rank 0, Epoch 4382, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,557: INFO: model_training: Rank 0, Epoch 4382, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,559: INFO: model_training: Rank 0, Epoch 4382, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,560: INFO: model_training: Rank 0, Epoch 4382, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,561: INFO: model_training: Rank 0, Epoch 4383, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,562: INFO: model_training: Rank 0, Epoch 4383, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,564: INFO: model_training: Rank 0, Epoch 4383, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,565: INFO: model_training: Rank 0, Epoch 4383, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,566: INFO: model_training: Rank 0, Epoch 4383, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,567: INFO: model_training: Rank 0, Epoch 4384, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,568: INFO: model_training: Rank 0, Epoch 4384, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,569: INFO: model_training: Rank 0, Epoch 4384, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,570: INFO: model_training: Rank 0, Epoch 4384, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,571: INFO: model_training: Rank 0, Epoch 4384, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,572: INFO: model_training: Rank 0, Epoch 4385, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,573: INFO: model_training: Rank 0, Epoch 4385, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,574: INFO: model_training: Rank 0, Epoch 4385, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,576: INFO: model_training: Rank 0, Epoch 4385, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,577: INFO: model_training: Rank 0, Epoch 4385, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,578: INFO: model_training: Rank 0, Epoch 4386, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,579: INFO: model_training: Rank 0, Epoch 4386, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,580: INFO: model_training: Rank 0, Epoch 4386, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,582: INFO: model_training: Rank 0, Epoch 4386, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,583: INFO: model_training: Rank 0, Epoch 4386, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,584: INFO: model_training: Rank 0, Epoch 4387, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,585: INFO: model_training: Rank 0, Epoch 4387, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,586: INFO: model_training: Rank 0, Epoch 4387, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,587: INFO: model_training: Rank 0, Epoch 4387, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,589: INFO: model_training: Rank 0, Epoch 4387, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,590: INFO: model_training: Rank 0, Epoch 4388, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,591: INFO: model_training: Rank 0, Epoch 4388, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,592: INFO: model_training: Rank 0, Epoch 4388, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,593: INFO: model_training: Rank 0, Epoch 4388, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,594: INFO: model_training: Rank 0, Epoch 4388, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,596: INFO: model_training: Rank 0, Epoch 4389, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,597: INFO: model_training: Rank 0, Epoch 4389, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,599: INFO: model_training: Rank 0, Epoch 4389, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,600: INFO: model_training: Rank 0, Epoch 4389, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,601: INFO: model_training: Rank 0, Epoch 4389, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,602: INFO: model_training: Rank 0, Epoch 4390, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,603: INFO: model_training: Rank 0, Epoch 4390, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,605: INFO: model_training: Rank 0, Epoch 4390, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,606: INFO: model_training: Rank 0, Epoch 4390, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,607: INFO: model_training: Rank 0, Epoch 4390, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,608: INFO: model_training: Rank 0, Epoch 4391, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,610: INFO: model_training: Rank 0, Epoch 4391, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,611: INFO: model_training: Rank 0, Epoch 4391, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,612: INFO: model_training: Rank 0, Epoch 4391, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,614: INFO: model_training: Rank 0, Epoch 4391, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,615: INFO: model_training: Rank 0, Epoch 4392, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,616: INFO: model_training: Rank 0, Epoch 4392, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,617: INFO: model_training: Rank 0, Epoch 4392, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,619: INFO: model_training: Rank 0, Epoch 4392, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,620: INFO: model_training: Rank 0, Epoch 4392, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,621: INFO: model_training: Rank 0, Epoch 4393, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,622: INFO: model_training: Rank 0, Epoch 4393, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,623: INFO: model_training: Rank 0, Epoch 4393, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,624: INFO: model_training: Rank 0, Epoch 4393, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,625: INFO: model_training: Rank 0, Epoch 4393, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,626: INFO: model_training: Rank 0, Epoch 4394, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,628: INFO: model_training: Rank 0, Epoch 4394, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,629: INFO: model_training: Rank 0, Epoch 4394, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,630: INFO: model_training: Rank 0, Epoch 4394, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,631: INFO: model_training: Rank 0, Epoch 4394, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,632: INFO: model_training: Rank 0, Epoch 4395, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,633: INFO: model_training: Rank 0, Epoch 4395, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,634: INFO: model_training: Rank 0, Epoch 4395, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,636: INFO: model_training: Rank 0, Epoch 4395, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,637: INFO: model_training: Rank 0, Epoch 4395, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,638: INFO: model_training: Rank 0, Epoch 4396, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,640: INFO: model_training: Rank 0, Epoch 4396, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,641: INFO: model_training: Rank 0, Epoch 4396, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,642: INFO: model_training: Rank 0, Epoch 4396, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,643: INFO: model_training: Rank 0, Epoch 4396, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,644: INFO: model_training: Rank 0, Epoch 4397, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,645: INFO: model_training: Rank 0, Epoch 4397, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,646: INFO: model_training: Rank 0, Epoch 4397, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,648: INFO: model_training: Rank 0, Epoch 4397, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,649: INFO: model_training: Rank 0, Epoch 4397, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,650: INFO: model_training: Rank 0, Epoch 4398, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,651: INFO: model_training: Rank 0, Epoch 4398, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,652: INFO: model_training: Rank 0, Epoch 4398, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,653: INFO: model_training: Rank 0, Epoch 4398, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,655: INFO: model_training: Rank 0, Epoch 4398, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,656: INFO: model_training: Rank 0, Epoch 4399, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,657: INFO: model_training: Rank 0, Epoch 4399, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,658: INFO: model_training: Rank 0, Epoch 4399, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,659: INFO: model_training: Rank 0, Epoch 4399, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,660: INFO: model_training: Rank 0, Epoch 4399, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,661: INFO: model_training: Rank 0, Epoch 4400, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,662: INFO: model_training: Rank 0, Epoch 4400, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,664: INFO: model_training: Rank 0, Epoch 4400, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,666: INFO: model_training: Rank 0, Epoch 4400, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,667: INFO: model_training: Rank 0, Epoch 4400, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,668: INFO: model_training: Rank 0, Epoch 4401, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,669: INFO: model_training: Rank 0, Epoch 4401, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,670: INFO: model_training: Rank 0, Epoch 4401, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,671: INFO: model_training: Rank 0, Epoch 4401, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,672: INFO: model_training: Rank 0, Epoch 4401, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,674: INFO: model_training: Rank 0, Epoch 4402, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,675: INFO: model_training: Rank 0, Epoch 4402, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,676: INFO: model_training: Rank 0, Epoch 4402, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,677: INFO: model_training: Rank 0, Epoch 4402, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,678: INFO: model_training: Rank 0, Epoch 4402, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,679: INFO: model_training: Rank 0, Epoch 4403, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,680: INFO: model_training: Rank 0, Epoch 4403, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,681: INFO: model_training: Rank 0, Epoch 4403, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,683: INFO: model_training: Rank 0, Epoch 4403, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,684: INFO: model_training: Rank 0, Epoch 4403, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,685: INFO: model_training: Rank 0, Epoch 4404, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,686: INFO: model_training: Rank 0, Epoch 4404, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,687: INFO: model_training: Rank 0, Epoch 4404, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,688: INFO: model_training: Rank 0, Epoch 4404, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,690: INFO: model_training: Rank 0, Epoch 4404, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,691: INFO: model_training: Rank 0, Epoch 4405, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,693: INFO: model_training: Rank 0, Epoch 4405, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,694: INFO: model_training: Rank 0, Epoch 4405, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,696: INFO: model_training: Rank 0, Epoch 4405, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,697: INFO: model_training: Rank 0, Epoch 4405, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,699: INFO: model_training: Rank 0, Epoch 4406, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,700: INFO: model_training: Rank 0, Epoch 4406, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,702: INFO: model_training: Rank 0, Epoch 4406, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,704: INFO: model_training: Rank 0, Epoch 4406, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,706: INFO: model_training: Rank 0, Epoch 4406, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,707: INFO: model_training: Rank 0, Epoch 4407, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,708: INFO: model_training: Rank 0, Epoch 4407, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,709: INFO: model_training: Rank 0, Epoch 4407, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,710: INFO: model_training: Rank 0, Epoch 4407, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,712: INFO: model_training: Rank 0, Epoch 4407, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,713: INFO: model_training: Rank 0, Epoch 4408, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,714: INFO: model_training: Rank 0, Epoch 4408, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,716: INFO: model_training: Rank 0, Epoch 4408, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,717: INFO: model_training: Rank 0, Epoch 4408, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,718: INFO: model_training: Rank 0, Epoch 4408, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,719: INFO: model_training: Rank 0, Epoch 4409, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,720: INFO: model_training: Rank 0, Epoch 4409, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,721: INFO: model_training: Rank 0, Epoch 4409, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,722: INFO: model_training: Rank 0, Epoch 4409, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,724: INFO: model_training: Rank 0, Epoch 4409, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,725: INFO: model_training: Rank 0, Epoch 4410, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,727: INFO: model_training: Rank 0, Epoch 4410, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,728: INFO: model_training: Rank 0, Epoch 4410, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,729: INFO: model_training: Rank 0, Epoch 4410, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,730: INFO: model_training: Rank 0, Epoch 4410, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,731: INFO: model_training: Rank 0, Epoch 4411, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,733: INFO: model_training: Rank 0, Epoch 4411, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,734: INFO: model_training: Rank 0, Epoch 4411, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,735: INFO: model_training: Rank 0, Epoch 4411, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,736: INFO: model_training: Rank 0, Epoch 4411, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,737: INFO: model_training: Rank 0, Epoch 4412, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,738: INFO: model_training: Rank 0, Epoch 4412, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,739: INFO: model_training: Rank 0, Epoch 4412, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,741: INFO: model_training: Rank 0, Epoch 4412, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,741: INFO: model_training: Rank 0, Epoch 4412, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,743: INFO: model_training: Rank 0, Epoch 4413, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,744: INFO: model_training: Rank 0, Epoch 4413, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,745: INFO: model_training: Rank 0, Epoch 4413, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,746: INFO: model_training: Rank 0, Epoch 4413, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,748: INFO: model_training: Rank 0, Epoch 4413, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,749: INFO: model_training: Rank 0, Epoch 4414, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,750: INFO: model_training: Rank 0, Epoch 4414, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,751: INFO: model_training: Rank 0, Epoch 4414, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,752: INFO: model_training: Rank 0, Epoch 4414, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,753: INFO: model_training: Rank 0, Epoch 4414, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,754: INFO: model_training: Rank 0, Epoch 4415, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,756: INFO: model_training: Rank 0, Epoch 4415, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,757: INFO: model_training: Rank 0, Epoch 4415, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,758: INFO: model_training: Rank 0, Epoch 4415, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,759: INFO: model_training: Rank 0, Epoch 4415, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,760: INFO: model_training: Rank 0, Epoch 4416, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,761: INFO: model_training: Rank 0, Epoch 4416, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,762: INFO: model_training: Rank 0, Epoch 4416, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,763: INFO: model_training: Rank 0, Epoch 4416, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,764: INFO: model_training: Rank 0, Epoch 4416, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,765: INFO: model_training: Rank 0, Epoch 4417, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,766: INFO: model_training: Rank 0, Epoch 4417, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,768: INFO: model_training: Rank 0, Epoch 4417, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,769: INFO: model_training: Rank 0, Epoch 4417, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,770: INFO: model_training: Rank 0, Epoch 4417, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,771: INFO: model_training: Rank 0, Epoch 4418, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,772: INFO: model_training: Rank 0, Epoch 4418, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,773: INFO: model_training: Rank 0, Epoch 4418, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,774: INFO: model_training: Rank 0, Epoch 4418, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,775: INFO: model_training: Rank 0, Epoch 4418, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,777: INFO: model_training: Rank 0, Epoch 4419, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,778: INFO: model_training: Rank 0, Epoch 4419, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,779: INFO: model_training: Rank 0, Epoch 4419, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,780: INFO: model_training: Rank 0, Epoch 4419, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,781: INFO: model_training: Rank 0, Epoch 4419, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,782: INFO: model_training: Rank 0, Epoch 4420, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,783: INFO: model_training: Rank 0, Epoch 4420, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,785: INFO: model_training: Rank 0, Epoch 4420, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,786: INFO: model_training: Rank 0, Epoch 4420, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,787: INFO: model_training: Rank 0, Epoch 4420, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,788: INFO: model_training: Rank 0, Epoch 4421, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,790: INFO: model_training: Rank 0, Epoch 4421, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,791: INFO: model_training: Rank 0, Epoch 4421, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,792: INFO: model_training: Rank 0, Epoch 4421, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,793: INFO: model_training: Rank 0, Epoch 4421, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,794: INFO: model_training: Rank 0, Epoch 4422, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,795: INFO: model_training: Rank 0, Epoch 4422, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,796: INFO: model_training: Rank 0, Epoch 4422, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,798: INFO: model_training: Rank 0, Epoch 4422, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,799: INFO: model_training: Rank 0, Epoch 4422, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,800: INFO: model_training: Rank 0, Epoch 4423, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,801: INFO: model_training: Rank 0, Epoch 4423, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,802: INFO: model_training: Rank 0, Epoch 4423, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,803: INFO: model_training: Rank 0, Epoch 4423, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,804: INFO: model_training: Rank 0, Epoch 4423, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,806: INFO: model_training: Rank 0, Epoch 4424, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,807: INFO: model_training: Rank 0, Epoch 4424, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,808: INFO: model_training: Rank 0, Epoch 4424, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,809: INFO: model_training: Rank 0, Epoch 4424, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,810: INFO: model_training: Rank 0, Epoch 4424, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,812: INFO: model_training: Rank 0, Epoch 4425, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,813: INFO: model_training: Rank 0, Epoch 4425, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,814: INFO: model_training: Rank 0, Epoch 4425, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,815: INFO: model_training: Rank 0, Epoch 4425, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,816: INFO: model_training: Rank 0, Epoch 4425, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,817: INFO: model_training: Rank 0, Epoch 4426, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,818: INFO: model_training: Rank 0, Epoch 4426, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,819: INFO: model_training: Rank 0, Epoch 4426, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,820: INFO: model_training: Rank 0, Epoch 4426, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,821: INFO: model_training: Rank 0, Epoch 4426, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,822: INFO: model_training: Rank 0, Epoch 4427, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,823: INFO: model_training: Rank 0, Epoch 4427, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,824: INFO: model_training: Rank 0, Epoch 4427, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,826: INFO: model_training: Rank 0, Epoch 4427, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,827: INFO: model_training: Rank 0, Epoch 4427, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,828: INFO: model_training: Rank 0, Epoch 4428, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,829: INFO: model_training: Rank 0, Epoch 4428, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,830: INFO: model_training: Rank 0, Epoch 4428, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,831: INFO: model_training: Rank 0, Epoch 4428, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,833: INFO: model_training: Rank 0, Epoch 4428, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,835: INFO: model_training: Rank 0, Epoch 4429, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,836: INFO: model_training: Rank 0, Epoch 4429, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,837: INFO: model_training: Rank 0, Epoch 4429, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,838: INFO: model_training: Rank 0, Epoch 4429, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,840: INFO: model_training: Rank 0, Epoch 4429, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,841: INFO: model_training: Rank 0, Epoch 4430, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,842: INFO: model_training: Rank 0, Epoch 4430, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,843: INFO: model_training: Rank 0, Epoch 4430, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,845: INFO: model_training: Rank 0, Epoch 4430, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,846: INFO: model_training: Rank 0, Epoch 4430, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,848: INFO: model_training: Rank 0, Epoch 4431, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,849: INFO: model_training: Rank 0, Epoch 4431, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,850: INFO: model_training: Rank 0, Epoch 4431, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,851: INFO: model_training: Rank 0, Epoch 4431, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,852: INFO: model_training: Rank 0, Epoch 4431, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,854: INFO: model_training: Rank 0, Epoch 4432, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,855: INFO: model_training: Rank 0, Epoch 4432, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,856: INFO: model_training: Rank 0, Epoch 4432, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,858: INFO: model_training: Rank 0, Epoch 4432, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,859: INFO: model_training: Rank 0, Epoch 4432, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,860: INFO: model_training: Rank 0, Epoch 4433, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,861: INFO: model_training: Rank 0, Epoch 4433, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,862: INFO: model_training: Rank 0, Epoch 4433, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,863: INFO: model_training: Rank 0, Epoch 4433, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,865: INFO: model_training: Rank 0, Epoch 4433, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,866: INFO: model_training: Rank 0, Epoch 4434, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,867: INFO: model_training: Rank 0, Epoch 4434, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,868: INFO: model_training: Rank 0, Epoch 4434, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,869: INFO: model_training: Rank 0, Epoch 4434, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,870: INFO: model_training: Rank 0, Epoch 4434, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,871: INFO: model_training: Rank 0, Epoch 4435, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,873: INFO: model_training: Rank 0, Epoch 4435, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,874: INFO: model_training: Rank 0, Epoch 4435, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,876: INFO: model_training: Rank 0, Epoch 4435, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,877: INFO: model_training: Rank 0, Epoch 4435, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,878: INFO: model_training: Rank 0, Epoch 4436, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,879: INFO: model_training: Rank 0, Epoch 4436, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,881: INFO: model_training: Rank 0, Epoch 4436, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,882: INFO: model_training: Rank 0, Epoch 4436, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,883: INFO: model_training: Rank 0, Epoch 4436, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,885: INFO: model_training: Rank 0, Epoch 4437, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,886: INFO: model_training: Rank 0, Epoch 4437, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,887: INFO: model_training: Rank 0, Epoch 4437, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,888: INFO: model_training: Rank 0, Epoch 4437, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,889: INFO: model_training: Rank 0, Epoch 4437, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,890: INFO: model_training: Rank 0, Epoch 4438, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,891: INFO: model_training: Rank 0, Epoch 4438, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,892: INFO: model_training: Rank 0, Epoch 4438, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,893: INFO: model_training: Rank 0, Epoch 4438, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,894: INFO: model_training: Rank 0, Epoch 4438, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,895: INFO: model_training: Rank 0, Epoch 4439, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,896: INFO: model_training: Rank 0, Epoch 4439, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,897: INFO: model_training: Rank 0, Epoch 4439, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,898: INFO: model_training: Rank 0, Epoch 4439, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,900: INFO: model_training: Rank 0, Epoch 4439, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,901: INFO: model_training: Rank 0, Epoch 4440, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,902: INFO: model_training: Rank 0, Epoch 4440, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,903: INFO: model_training: Rank 0, Epoch 4440, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,904: INFO: model_training: Rank 0, Epoch 4440, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,906: INFO: model_training: Rank 0, Epoch 4440, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,907: INFO: model_training: Rank 0, Epoch 4441, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,908: INFO: model_training: Rank 0, Epoch 4441, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,909: INFO: model_training: Rank 0, Epoch 4441, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,910: INFO: model_training: Rank 0, Epoch 4441, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,911: INFO: model_training: Rank 0, Epoch 4441, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,913: INFO: model_training: Rank 0, Epoch 4442, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,914: INFO: model_training: Rank 0, Epoch 4442, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,915: INFO: model_training: Rank 0, Epoch 4442, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,916: INFO: model_training: Rank 0, Epoch 4442, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,917: INFO: model_training: Rank 0, Epoch 4442, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,918: INFO: model_training: Rank 0, Epoch 4443, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,919: INFO: model_training: Rank 0, Epoch 4443, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,920: INFO: model_training: Rank 0, Epoch 4443, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,922: INFO: model_training: Rank 0, Epoch 4443, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,923: INFO: model_training: Rank 0, Epoch 4443, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,924: INFO: model_training: Rank 0, Epoch 4444, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,926: INFO: model_training: Rank 0, Epoch 4444, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,927: INFO: model_training: Rank 0, Epoch 4444, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,928: INFO: model_training: Rank 0, Epoch 4444, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,929: INFO: model_training: Rank 0, Epoch 4444, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,930: INFO: model_training: Rank 0, Epoch 4445, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,931: INFO: model_training: Rank 0, Epoch 4445, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,933: INFO: model_training: Rank 0, Epoch 4445, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,934: INFO: model_training: Rank 0, Epoch 4445, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,935: INFO: model_training: Rank 0, Epoch 4445, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,936: INFO: model_training: Rank 0, Epoch 4446, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,937: INFO: model_training: Rank 0, Epoch 4446, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,938: INFO: model_training: Rank 0, Epoch 4446, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,939: INFO: model_training: Rank 0, Epoch 4446, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,941: INFO: model_training: Rank 0, Epoch 4446, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,942: INFO: model_training: Rank 0, Epoch 4447, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,943: INFO: model_training: Rank 0, Epoch 4447, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,944: INFO: model_training: Rank 0, Epoch 4447, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,945: INFO: model_training: Rank 0, Epoch 4447, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,947: INFO: model_training: Rank 0, Epoch 4447, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,948: INFO: model_training: Rank 0, Epoch 4448, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,949: INFO: model_training: Rank 0, Epoch 4448, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,950: INFO: model_training: Rank 0, Epoch 4448, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,951: INFO: model_training: Rank 0, Epoch 4448, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,952: INFO: model_training: Rank 0, Epoch 4448, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,953: INFO: model_training: Rank 0, Epoch 4449, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,954: INFO: model_training: Rank 0, Epoch 4449, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,955: INFO: model_training: Rank 0, Epoch 4449, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,956: INFO: model_training: Rank 0, Epoch 4449, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,957: INFO: model_training: Rank 0, Epoch 4449, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,958: INFO: model_training: Rank 0, Epoch 4450, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,959: INFO: model_training: Rank 0, Epoch 4450, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,960: INFO: model_training: Rank 0, Epoch 4450, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,961: INFO: model_training: Rank 0, Epoch 4450, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,963: INFO: model_training: Rank 0, Epoch 4450, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,964: INFO: model_training: Rank 0, Epoch 4451, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,965: INFO: model_training: Rank 0, Epoch 4451, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,966: INFO: model_training: Rank 0, Epoch 4451, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,967: INFO: model_training: Rank 0, Epoch 4451, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,969: INFO: model_training: Rank 0, Epoch 4451, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,970: INFO: model_training: Rank 0, Epoch 4452, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,971: INFO: model_training: Rank 0, Epoch 4452, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,972: INFO: model_training: Rank 0, Epoch 4452, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,974: INFO: model_training: Rank 0, Epoch 4452, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,975: INFO: model_training: Rank 0, Epoch 4452, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,976: INFO: model_training: Rank 0, Epoch 4453, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,977: INFO: model_training: Rank 0, Epoch 4453, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,978: INFO: model_training: Rank 0, Epoch 4453, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,979: INFO: model_training: Rank 0, Epoch 4453, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,980: INFO: model_training: Rank 0, Epoch 4453, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,981: INFO: model_training: Rank 0, Epoch 4454, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,982: INFO: model_training: Rank 0, Epoch 4454, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,984: INFO: model_training: Rank 0, Epoch 4454, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,985: INFO: model_training: Rank 0, Epoch 4454, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,986: INFO: model_training: Rank 0, Epoch 4454, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,988: INFO: model_training: Rank 0, Epoch 4455, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,989: INFO: model_training: Rank 0, Epoch 4455, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,990: INFO: model_training: Rank 0, Epoch 4455, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,991: INFO: model_training: Rank 0, Epoch 4455, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,992: INFO: model_training: Rank 0, Epoch 4455, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,993: INFO: model_training: Rank 0, Epoch 4456, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:54,994: INFO: model_training: Rank 0, Epoch 4456, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:54,995: INFO: model_training: Rank 0, Epoch 4456, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:54,996: INFO: model_training: Rank 0, Epoch 4456, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:54,997: INFO: model_training: Rank 0, Epoch 4456, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:54,999: INFO: model_training: Rank 0, Epoch 4457, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,000: INFO: model_training: Rank 0, Epoch 4457, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,001: INFO: model_training: Rank 0, Epoch 4457, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,002: INFO: model_training: Rank 0, Epoch 4457, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,004: INFO: model_training: Rank 0, Epoch 4457, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,005: INFO: model_training: Rank 0, Epoch 4458, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,006: INFO: model_training: Rank 0, Epoch 4458, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,007: INFO: model_training: Rank 0, Epoch 4458, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,008: INFO: model_training: Rank 0, Epoch 4458, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,009: INFO: model_training: Rank 0, Epoch 4458, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,010: INFO: model_training: Rank 0, Epoch 4459, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,011: INFO: model_training: Rank 0, Epoch 4459, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,012: INFO: model_training: Rank 0, Epoch 4459, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,013: INFO: model_training: Rank 0, Epoch 4459, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,015: INFO: model_training: Rank 0, Epoch 4459, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,016: INFO: model_training: Rank 0, Epoch 4460, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,017: INFO: model_training: Rank 0, Epoch 4460, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,018: INFO: model_training: Rank 0, Epoch 4460, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,019: INFO: model_training: Rank 0, Epoch 4460, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,020: INFO: model_training: Rank 0, Epoch 4460, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,021: INFO: model_training: Rank 0, Epoch 4461, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,022: INFO: model_training: Rank 0, Epoch 4461, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,024: INFO: model_training: Rank 0, Epoch 4461, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,025: INFO: model_training: Rank 0, Epoch 4461, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,026: INFO: model_training: Rank 0, Epoch 4461, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,027: INFO: model_training: Rank 0, Epoch 4462, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,029: INFO: model_training: Rank 0, Epoch 4462, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,030: INFO: model_training: Rank 0, Epoch 4462, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,031: INFO: model_training: Rank 0, Epoch 4462, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,032: INFO: model_training: Rank 0, Epoch 4462, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,033: INFO: model_training: Rank 0, Epoch 4463, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,034: INFO: model_training: Rank 0, Epoch 4463, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,035: INFO: model_training: Rank 0, Epoch 4463, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,036: INFO: model_training: Rank 0, Epoch 4463, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,037: INFO: model_training: Rank 0, Epoch 4463, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,038: INFO: model_training: Rank 0, Epoch 4464, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,040: INFO: model_training: Rank 0, Epoch 4464, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,041: INFO: model_training: Rank 0, Epoch 4464, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,043: INFO: model_training: Rank 0, Epoch 4464, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,044: INFO: model_training: Rank 0, Epoch 4464, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,045: INFO: model_training: Rank 0, Epoch 4465, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,046: INFO: model_training: Rank 0, Epoch 4465, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,047: INFO: model_training: Rank 0, Epoch 4465, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,048: INFO: model_training: Rank 0, Epoch 4465, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,049: INFO: model_training: Rank 0, Epoch 4465, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,050: INFO: model_training: Rank 0, Epoch 4466, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,051: INFO: model_training: Rank 0, Epoch 4466, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,053: INFO: model_training: Rank 0, Epoch 4466, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,054: INFO: model_training: Rank 0, Epoch 4466, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,055: INFO: model_training: Rank 0, Epoch 4466, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,056: INFO: model_training: Rank 0, Epoch 4467, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,057: INFO: model_training: Rank 0, Epoch 4467, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,059: INFO: model_training: Rank 0, Epoch 4467, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,059: INFO: model_training: Rank 0, Epoch 4467, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,060: INFO: model_training: Rank 0, Epoch 4467, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,061: INFO: model_training: Rank 0, Epoch 4468, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,063: INFO: model_training: Rank 0, Epoch 4468, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,064: INFO: model_training: Rank 0, Epoch 4468, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,065: INFO: model_training: Rank 0, Epoch 4468, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,067: INFO: model_training: Rank 0, Epoch 4468, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,068: INFO: model_training: Rank 0, Epoch 4469, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,069: INFO: model_training: Rank 0, Epoch 4469, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,070: INFO: model_training: Rank 0, Epoch 4469, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,071: INFO: model_training: Rank 0, Epoch 4469, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,072: INFO: model_training: Rank 0, Epoch 4469, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,073: INFO: model_training: Rank 0, Epoch 4470, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,074: INFO: model_training: Rank 0, Epoch 4470, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,075: INFO: model_training: Rank 0, Epoch 4470, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,076: INFO: model_training: Rank 0, Epoch 4470, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,078: INFO: model_training: Rank 0, Epoch 4470, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,079: INFO: model_training: Rank 0, Epoch 4471, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,080: INFO: model_training: Rank 0, Epoch 4471, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,081: INFO: model_training: Rank 0, Epoch 4471, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,082: INFO: model_training: Rank 0, Epoch 4471, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,083: INFO: model_training: Rank 0, Epoch 4471, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,085: INFO: model_training: Rank 0, Epoch 4472, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,085: INFO: model_training: Rank 0, Epoch 4472, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,086: INFO: model_training: Rank 0, Epoch 4472, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,088: INFO: model_training: Rank 0, Epoch 4472, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,089: INFO: model_training: Rank 0, Epoch 4472, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,091: INFO: model_training: Rank 0, Epoch 4473, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,091: INFO: model_training: Rank 0, Epoch 4473, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,093: INFO: model_training: Rank 0, Epoch 4473, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,094: INFO: model_training: Rank 0, Epoch 4473, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,095: INFO: model_training: Rank 0, Epoch 4473, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,096: INFO: model_training: Rank 0, Epoch 4474, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,097: INFO: model_training: Rank 0, Epoch 4474, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,098: INFO: model_training: Rank 0, Epoch 4474, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,099: INFO: model_training: Rank 0, Epoch 4474, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,100: INFO: model_training: Rank 0, Epoch 4474, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,101: INFO: model_training: Rank 0, Epoch 4475, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,102: INFO: model_training: Rank 0, Epoch 4475, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,104: INFO: model_training: Rank 0, Epoch 4475, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,105: INFO: model_training: Rank 0, Epoch 4475, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,106: INFO: model_training: Rank 0, Epoch 4475, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,107: INFO: model_training: Rank 0, Epoch 4476, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,108: INFO: model_training: Rank 0, Epoch 4476, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,110: INFO: model_training: Rank 0, Epoch 4476, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,111: INFO: model_training: Rank 0, Epoch 4476, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,112: INFO: model_training: Rank 0, Epoch 4476, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,113: INFO: model_training: Rank 0, Epoch 4477, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,114: INFO: model_training: Rank 0, Epoch 4477, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,115: INFO: model_training: Rank 0, Epoch 4477, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,116: INFO: model_training: Rank 0, Epoch 4477, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,117: INFO: model_training: Rank 0, Epoch 4477, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,118: INFO: model_training: Rank 0, Epoch 4478, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,119: INFO: model_training: Rank 0, Epoch 4478, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,120: INFO: model_training: Rank 0, Epoch 4478, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,122: INFO: model_training: Rank 0, Epoch 4478, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,122: INFO: model_training: Rank 0, Epoch 4478, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,123: INFO: model_training: Rank 0, Epoch 4479, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,125: INFO: model_training: Rank 0, Epoch 4479, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,126: INFO: model_training: Rank 0, Epoch 4479, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,127: INFO: model_training: Rank 0, Epoch 4479, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,128: INFO: model_training: Rank 0, Epoch 4479, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,129: INFO: model_training: Rank 0, Epoch 4480, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,130: INFO: model_training: Rank 0, Epoch 4480, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,132: INFO: model_training: Rank 0, Epoch 4480, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,133: INFO: model_training: Rank 0, Epoch 4480, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,134: INFO: model_training: Rank 0, Epoch 4480, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,135: INFO: model_training: Rank 0, Epoch 4481, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,136: INFO: model_training: Rank 0, Epoch 4481, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,137: INFO: model_training: Rank 0, Epoch 4481, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,138: INFO: model_training: Rank 0, Epoch 4481, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,139: INFO: model_training: Rank 0, Epoch 4481, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,140: INFO: model_training: Rank 0, Epoch 4482, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,141: INFO: model_training: Rank 0, Epoch 4482, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,142: INFO: model_training: Rank 0, Epoch 4482, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,143: INFO: model_training: Rank 0, Epoch 4482, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,144: INFO: model_training: Rank 0, Epoch 4482, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,145: INFO: model_training: Rank 0, Epoch 4483, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,147: INFO: model_training: Rank 0, Epoch 4483, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,147: INFO: model_training: Rank 0, Epoch 4483, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,148: INFO: model_training: Rank 0, Epoch 4483, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,150: INFO: model_training: Rank 0, Epoch 4483, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,151: INFO: model_training: Rank 0, Epoch 4484, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,152: INFO: model_training: Rank 0, Epoch 4484, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,154: INFO: model_training: Rank 0, Epoch 4484, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,155: INFO: model_training: Rank 0, Epoch 4484, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,156: INFO: model_training: Rank 0, Epoch 4484, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,157: INFO: model_training: Rank 0, Epoch 4485, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,158: INFO: model_training: Rank 0, Epoch 4485, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,159: INFO: model_training: Rank 0, Epoch 4485, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,160: INFO: model_training: Rank 0, Epoch 4485, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,161: INFO: model_training: Rank 0, Epoch 4485, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,162: INFO: model_training: Rank 0, Epoch 4486, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,163: INFO: model_training: Rank 0, Epoch 4486, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,164: INFO: model_training: Rank 0, Epoch 4486, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,166: INFO: model_training: Rank 0, Epoch 4486, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,167: INFO: model_training: Rank 0, Epoch 4486, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,168: INFO: model_training: Rank 0, Epoch 4487, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,169: INFO: model_training: Rank 0, Epoch 4487, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,170: INFO: model_training: Rank 0, Epoch 4487, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,171: INFO: model_training: Rank 0, Epoch 4487, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,173: INFO: model_training: Rank 0, Epoch 4487, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,174: INFO: model_training: Rank 0, Epoch 4488, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,175: INFO: model_training: Rank 0, Epoch 4488, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,176: INFO: model_training: Rank 0, Epoch 4488, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,177: INFO: model_training: Rank 0, Epoch 4488, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,178: INFO: model_training: Rank 0, Epoch 4488, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,179: INFO: model_training: Rank 0, Epoch 4489, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,180: INFO: model_training: Rank 0, Epoch 4489, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,181: INFO: model_training: Rank 0, Epoch 4489, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,183: INFO: model_training: Rank 0, Epoch 4489, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,184: INFO: model_training: Rank 0, Epoch 4489, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,185: INFO: model_training: Rank 0, Epoch 4490, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,187: INFO: model_training: Rank 0, Epoch 4490, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,188: INFO: model_training: Rank 0, Epoch 4490, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,189: INFO: model_training: Rank 0, Epoch 4490, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,191: INFO: model_training: Rank 0, Epoch 4490, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,192: INFO: model_training: Rank 0, Epoch 4491, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,194: INFO: model_training: Rank 0, Epoch 4491, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,195: INFO: model_training: Rank 0, Epoch 4491, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,198: INFO: model_training: Rank 0, Epoch 4491, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,200: INFO: model_training: Rank 0, Epoch 4491, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,202: INFO: model_training: Rank 0, Epoch 4492, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,203: INFO: model_training: Rank 0, Epoch 4492, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,205: INFO: model_training: Rank 0, Epoch 4492, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,206: INFO: model_training: Rank 0, Epoch 4492, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,207: INFO: model_training: Rank 0, Epoch 4492, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,209: INFO: model_training: Rank 0, Epoch 4493, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,210: INFO: model_training: Rank 0, Epoch 4493, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,212: INFO: model_training: Rank 0, Epoch 4493, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,213: INFO: model_training: Rank 0, Epoch 4493, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,215: INFO: model_training: Rank 0, Epoch 4493, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,216: INFO: model_training: Rank 0, Epoch 4494, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,218: INFO: model_training: Rank 0, Epoch 4494, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,219: INFO: model_training: Rank 0, Epoch 4494, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,220: INFO: model_training: Rank 0, Epoch 4494, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,221: INFO: model_training: Rank 0, Epoch 4494, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,223: INFO: model_training: Rank 0, Epoch 4495, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,224: INFO: model_training: Rank 0, Epoch 4495, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,226: INFO: model_training: Rank 0, Epoch 4495, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,228: INFO: model_training: Rank 0, Epoch 4495, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,229: INFO: model_training: Rank 0, Epoch 4495, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,230: INFO: model_training: Rank 0, Epoch 4496, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,232: INFO: model_training: Rank 0, Epoch 4496, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,233: INFO: model_training: Rank 0, Epoch 4496, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,235: INFO: model_training: Rank 0, Epoch 4496, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,237: INFO: model_training: Rank 0, Epoch 4496, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,240: INFO: model_training: Rank 0, Epoch 4497, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,242: INFO: model_training: Rank 0, Epoch 4497, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,244: INFO: model_training: Rank 0, Epoch 4497, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,246: INFO: model_training: Rank 0, Epoch 4497, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,248: INFO: model_training: Rank 0, Epoch 4497, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,250: INFO: model_training: Rank 0, Epoch 4498, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,251: INFO: model_training: Rank 0, Epoch 4498, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,253: INFO: model_training: Rank 0, Epoch 4498, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,255: INFO: model_training: Rank 0, Epoch 4498, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,257: INFO: model_training: Rank 0, Epoch 4498, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,259: INFO: model_training: Rank 0, Epoch 4499, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,261: INFO: model_training: Rank 0, Epoch 4499, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,264: INFO: model_training: Rank 0, Epoch 4499, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,267: INFO: model_training: Rank 0, Epoch 4499, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,269: INFO: model_training: Rank 0, Epoch 4499, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,270: INFO: model_training: Rank 0, Epoch 4500, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,271: INFO: model_training: Rank 0, Epoch 4500, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,272: INFO: model_training: Rank 0, Epoch 4500, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,274: INFO: model_training: Rank 0, Epoch 4500, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,276: INFO: model_training: Rank 0, Epoch 4500, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,277: INFO: model_training: Rank 0, Epoch 4501, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,279: INFO: model_training: Rank 0, Epoch 4501, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,281: INFO: model_training: Rank 0, Epoch 4501, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,282: INFO: model_training: Rank 0, Epoch 4501, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,284: INFO: model_training: Rank 0, Epoch 4501, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,285: INFO: model_training: Rank 0, Epoch 4502, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,286: INFO: model_training: Rank 0, Epoch 4502, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,287: INFO: model_training: Rank 0, Epoch 4502, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,288: INFO: model_training: Rank 0, Epoch 4502, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,290: INFO: model_training: Rank 0, Epoch 4502, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,292: INFO: model_training: Rank 0, Epoch 4503, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,293: INFO: model_training: Rank 0, Epoch 4503, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,295: INFO: model_training: Rank 0, Epoch 4503, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,296: INFO: model_training: Rank 0, Epoch 4503, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,298: INFO: model_training: Rank 0, Epoch 4503, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,299: INFO: model_training: Rank 0, Epoch 4504, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,300: INFO: model_training: Rank 0, Epoch 4504, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,302: INFO: model_training: Rank 0, Epoch 4504, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,303: INFO: model_training: Rank 0, Epoch 4504, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,304: INFO: model_training: Rank 0, Epoch 4504, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,305: INFO: model_training: Rank 0, Epoch 4505, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,306: INFO: model_training: Rank 0, Epoch 4505, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,308: INFO: model_training: Rank 0, Epoch 4505, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,310: INFO: model_training: Rank 0, Epoch 4505, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,311: INFO: model_training: Rank 0, Epoch 4505, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,313: INFO: model_training: Rank 0, Epoch 4506, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,314: INFO: model_training: Rank 0, Epoch 4506, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,316: INFO: model_training: Rank 0, Epoch 4506, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,317: INFO: model_training: Rank 0, Epoch 4506, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,318: INFO: model_training: Rank 0, Epoch 4506, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,319: INFO: model_training: Rank 0, Epoch 4507, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,320: INFO: model_training: Rank 0, Epoch 4507, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,322: INFO: model_training: Rank 0, Epoch 4507, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,323: INFO: model_training: Rank 0, Epoch 4507, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,325: INFO: model_training: Rank 0, Epoch 4507, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,327: INFO: model_training: Rank 0, Epoch 4508, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,328: INFO: model_training: Rank 0, Epoch 4508, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,330: INFO: model_training: Rank 0, Epoch 4508, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,331: INFO: model_training: Rank 0, Epoch 4508, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,332: INFO: model_training: Rank 0, Epoch 4508, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,334: INFO: model_training: Rank 0, Epoch 4509, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,335: INFO: model_training: Rank 0, Epoch 4509, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,336: INFO: model_training: Rank 0, Epoch 4509, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,337: INFO: model_training: Rank 0, Epoch 4509, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,338: INFO: model_training: Rank 0, Epoch 4509, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,339: INFO: model_training: Rank 0, Epoch 4510, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,341: INFO: model_training: Rank 0, Epoch 4510, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,343: INFO: model_training: Rank 0, Epoch 4510, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,345: INFO: model_training: Rank 0, Epoch 4510, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,346: INFO: model_training: Rank 0, Epoch 4510, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,348: INFO: model_training: Rank 0, Epoch 4511, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,349: INFO: model_training: Rank 0, Epoch 4511, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,350: INFO: model_training: Rank 0, Epoch 4511, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,351: INFO: model_training: Rank 0, Epoch 4511, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,352: INFO: model_training: Rank 0, Epoch 4511, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,354: INFO: model_training: Rank 0, Epoch 4512, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,355: INFO: model_training: Rank 0, Epoch 4512, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,356: INFO: model_training: Rank 0, Epoch 4512, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,358: INFO: model_training: Rank 0, Epoch 4512, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,359: INFO: model_training: Rank 0, Epoch 4512, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,361: INFO: model_training: Rank 0, Epoch 4513, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,362: INFO: model_training: Rank 0, Epoch 4513, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,364: INFO: model_training: Rank 0, Epoch 4513, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,365: INFO: model_training: Rank 0, Epoch 4513, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,366: INFO: model_training: Rank 0, Epoch 4513, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,367: INFO: model_training: Rank 0, Epoch 4514, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,369: INFO: model_training: Rank 0, Epoch 4514, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,370: INFO: model_training: Rank 0, Epoch 4514, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,372: INFO: model_training: Rank 0, Epoch 4514, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,373: INFO: model_training: Rank 0, Epoch 4514, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,374: INFO: model_training: Rank 0, Epoch 4515, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,376: INFO: model_training: Rank 0, Epoch 4515, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,378: INFO: model_training: Rank 0, Epoch 4515, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,380: INFO: model_training: Rank 0, Epoch 4515, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,381: INFO: model_training: Rank 0, Epoch 4515, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,382: INFO: model_training: Rank 0, Epoch 4516, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,384: INFO: model_training: Rank 0, Epoch 4516, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,385: INFO: model_training: Rank 0, Epoch 4516, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,386: INFO: model_training: Rank 0, Epoch 4516, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,387: INFO: model_training: Rank 0, Epoch 4516, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,388: INFO: model_training: Rank 0, Epoch 4517, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,390: INFO: model_training: Rank 0, Epoch 4517, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,391: INFO: model_training: Rank 0, Epoch 4517, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,393: INFO: model_training: Rank 0, Epoch 4517, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,394: INFO: model_training: Rank 0, Epoch 4517, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,397: INFO: model_training: Rank 0, Epoch 4518, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,398: INFO: model_training: Rank 0, Epoch 4518, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,399: INFO: model_training: Rank 0, Epoch 4518, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,400: INFO: model_training: Rank 0, Epoch 4518, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,401: INFO: model_training: Rank 0, Epoch 4518, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,402: INFO: model_training: Rank 0, Epoch 4519, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,403: INFO: model_training: Rank 0, Epoch 4519, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,405: INFO: model_training: Rank 0, Epoch 4519, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,407: INFO: model_training: Rank 0, Epoch 4519, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,408: INFO: model_training: Rank 0, Epoch 4519, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,410: INFO: model_training: Rank 0, Epoch 4520, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,411: INFO: model_training: Rank 0, Epoch 4520, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,413: INFO: model_training: Rank 0, Epoch 4520, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,414: INFO: model_training: Rank 0, Epoch 4520, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,415: INFO: model_training: Rank 0, Epoch 4520, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,416: INFO: model_training: Rank 0, Epoch 4521, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,418: INFO: model_training: Rank 0, Epoch 4521, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,419: INFO: model_training: Rank 0, Epoch 4521, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,420: INFO: model_training: Rank 0, Epoch 4521, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,421: INFO: model_training: Rank 0, Epoch 4521, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,423: INFO: model_training: Rank 0, Epoch 4522, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,425: INFO: model_training: Rank 0, Epoch 4522, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,426: INFO: model_training: Rank 0, Epoch 4522, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,428: INFO: model_training: Rank 0, Epoch 4522, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,429: INFO: model_training: Rank 0, Epoch 4522, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,430: INFO: model_training: Rank 0, Epoch 4523, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,431: INFO: model_training: Rank 0, Epoch 4523, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,432: INFO: model_training: Rank 0, Epoch 4523, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,433: INFO: model_training: Rank 0, Epoch 4523, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,435: INFO: model_training: Rank 0, Epoch 4523, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,436: INFO: model_training: Rank 0, Epoch 4524, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,437: INFO: model_training: Rank 0, Epoch 4524, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,438: INFO: model_training: Rank 0, Epoch 4524, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,439: INFO: model_training: Rank 0, Epoch 4524, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,441: INFO: model_training: Rank 0, Epoch 4524, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,442: INFO: model_training: Rank 0, Epoch 4525, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,443: INFO: model_training: Rank 0, Epoch 4525, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,444: INFO: model_training: Rank 0, Epoch 4525, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,445: INFO: model_training: Rank 0, Epoch 4525, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,446: INFO: model_training: Rank 0, Epoch 4525, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,447: INFO: model_training: Rank 0, Epoch 4526, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,448: INFO: model_training: Rank 0, Epoch 4526, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,449: INFO: model_training: Rank 0, Epoch 4526, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,450: INFO: model_training: Rank 0, Epoch 4526, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,451: INFO: model_training: Rank 0, Epoch 4526, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,452: INFO: model_training: Rank 0, Epoch 4527, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,453: INFO: model_training: Rank 0, Epoch 4527, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,455: INFO: model_training: Rank 0, Epoch 4527, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,456: INFO: model_training: Rank 0, Epoch 4527, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,457: INFO: model_training: Rank 0, Epoch 4527, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,458: INFO: model_training: Rank 0, Epoch 4528, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,459: INFO: model_training: Rank 0, Epoch 4528, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,461: INFO: model_training: Rank 0, Epoch 4528, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,462: INFO: model_training: Rank 0, Epoch 4528, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,463: INFO: model_training: Rank 0, Epoch 4528, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,464: INFO: model_training: Rank 0, Epoch 4529, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,465: INFO: model_training: Rank 0, Epoch 4529, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,466: INFO: model_training: Rank 0, Epoch 4529, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,467: INFO: model_training: Rank 0, Epoch 4529, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,469: INFO: model_training: Rank 0, Epoch 4529, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,470: INFO: model_training: Rank 0, Epoch 4530, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,471: INFO: model_training: Rank 0, Epoch 4530, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,472: INFO: model_training: Rank 0, Epoch 4530, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,473: INFO: model_training: Rank 0, Epoch 4530, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,474: INFO: model_training: Rank 0, Epoch 4530, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,475: INFO: model_training: Rank 0, Epoch 4531, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,476: INFO: model_training: Rank 0, Epoch 4531, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,477: INFO: model_training: Rank 0, Epoch 4531, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,478: INFO: model_training: Rank 0, Epoch 4531, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,480: INFO: model_training: Rank 0, Epoch 4531, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,481: INFO: model_training: Rank 0, Epoch 4532, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,482: INFO: model_training: Rank 0, Epoch 4532, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,483: INFO: model_training: Rank 0, Epoch 4532, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,484: INFO: model_training: Rank 0, Epoch 4532, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,485: INFO: model_training: Rank 0, Epoch 4532, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,486: INFO: model_training: Rank 0, Epoch 4533, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,487: INFO: model_training: Rank 0, Epoch 4533, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,489: INFO: model_training: Rank 0, Epoch 4533, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,490: INFO: model_training: Rank 0, Epoch 4533, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,491: INFO: model_training: Rank 0, Epoch 4533, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,492: INFO: model_training: Rank 0, Epoch 4534, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,493: INFO: model_training: Rank 0, Epoch 4534, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,494: INFO: model_training: Rank 0, Epoch 4534, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,496: INFO: model_training: Rank 0, Epoch 4534, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,497: INFO: model_training: Rank 0, Epoch 4534, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,498: INFO: model_training: Rank 0, Epoch 4535, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,501: INFO: model_training: Rank 0, Epoch 4535, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,502: INFO: model_training: Rank 0, Epoch 4535, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,503: INFO: model_training: Rank 0, Epoch 4535, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,504: INFO: model_training: Rank 0, Epoch 4535, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,505: INFO: model_training: Rank 0, Epoch 4536, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,506: INFO: model_training: Rank 0, Epoch 4536, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,507: INFO: model_training: Rank 0, Epoch 4536, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,509: INFO: model_training: Rank 0, Epoch 4536, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,510: INFO: model_training: Rank 0, Epoch 4536, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,511: INFO: model_training: Rank 0, Epoch 4537, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,512: INFO: model_training: Rank 0, Epoch 4537, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,513: INFO: model_training: Rank 0, Epoch 4537, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,514: INFO: model_training: Rank 0, Epoch 4537, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,516: INFO: model_training: Rank 0, Epoch 4537, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,517: INFO: model_training: Rank 0, Epoch 4538, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,518: INFO: model_training: Rank 0, Epoch 4538, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,519: INFO: model_training: Rank 0, Epoch 4538, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,520: INFO: model_training: Rank 0, Epoch 4538, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,521: INFO: model_training: Rank 0, Epoch 4538, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,522: INFO: model_training: Rank 0, Epoch 4539, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,523: INFO: model_training: Rank 0, Epoch 4539, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,525: INFO: model_training: Rank 0, Epoch 4539, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,526: INFO: model_training: Rank 0, Epoch 4539, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,527: INFO: model_training: Rank 0, Epoch 4539, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,528: INFO: model_training: Rank 0, Epoch 4540, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,529: INFO: model_training: Rank 0, Epoch 4540, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,530: INFO: model_training: Rank 0, Epoch 4540, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,531: INFO: model_training: Rank 0, Epoch 4540, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,533: INFO: model_training: Rank 0, Epoch 4540, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,534: INFO: model_training: Rank 0, Epoch 4541, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,535: INFO: model_training: Rank 0, Epoch 4541, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,537: INFO: model_training: Rank 0, Epoch 4541, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,538: INFO: model_training: Rank 0, Epoch 4541, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,539: INFO: model_training: Rank 0, Epoch 4541, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,540: INFO: model_training: Rank 0, Epoch 4542, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,541: INFO: model_training: Rank 0, Epoch 4542, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,542: INFO: model_training: Rank 0, Epoch 4542, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,543: INFO: model_training: Rank 0, Epoch 4542, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,544: INFO: model_training: Rank 0, Epoch 4542, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,546: INFO: model_training: Rank 0, Epoch 4543, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,547: INFO: model_training: Rank 0, Epoch 4543, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,548: INFO: model_training: Rank 0, Epoch 4543, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,550: INFO: model_training: Rank 0, Epoch 4543, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,551: INFO: model_training: Rank 0, Epoch 4543, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,552: INFO: model_training: Rank 0, Epoch 4544, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,553: INFO: model_training: Rank 0, Epoch 4544, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,554: INFO: model_training: Rank 0, Epoch 4544, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,556: INFO: model_training: Rank 0, Epoch 4544, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,557: INFO: model_training: Rank 0, Epoch 4544, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,559: INFO: model_training: Rank 0, Epoch 4545, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,560: INFO: model_training: Rank 0, Epoch 4545, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,561: INFO: model_training: Rank 0, Epoch 4545, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,562: INFO: model_training: Rank 0, Epoch 4545, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,563: INFO: model_training: Rank 0, Epoch 4545, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,565: INFO: model_training: Rank 0, Epoch 4546, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,566: INFO: model_training: Rank 0, Epoch 4546, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,567: INFO: model_training: Rank 0, Epoch 4546, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,568: INFO: model_training: Rank 0, Epoch 4546, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,569: INFO: model_training: Rank 0, Epoch 4546, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,570: INFO: model_training: Rank 0, Epoch 4547, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,571: INFO: model_training: Rank 0, Epoch 4547, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,572: INFO: model_training: Rank 0, Epoch 4547, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,573: INFO: model_training: Rank 0, Epoch 4547, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,575: INFO: model_training: Rank 0, Epoch 4547, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,576: INFO: model_training: Rank 0, Epoch 4548, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,577: INFO: model_training: Rank 0, Epoch 4548, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,579: INFO: model_training: Rank 0, Epoch 4548, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,580: INFO: model_training: Rank 0, Epoch 4548, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,581: INFO: model_training: Rank 0, Epoch 4548, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,583: INFO: model_training: Rank 0, Epoch 4549, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,584: INFO: model_training: Rank 0, Epoch 4549, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,586: INFO: model_training: Rank 0, Epoch 4549, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,587: INFO: model_training: Rank 0, Epoch 4549, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,588: INFO: model_training: Rank 0, Epoch 4549, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,589: INFO: model_training: Rank 0, Epoch 4550, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,591: INFO: model_training: Rank 0, Epoch 4550, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,592: INFO: model_training: Rank 0, Epoch 4550, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,593: INFO: model_training: Rank 0, Epoch 4550, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,594: INFO: model_training: Rank 0, Epoch 4550, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,596: INFO: model_training: Rank 0, Epoch 4551, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,597: INFO: model_training: Rank 0, Epoch 4551, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,598: INFO: model_training: Rank 0, Epoch 4551, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,599: INFO: model_training: Rank 0, Epoch 4551, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,600: INFO: model_training: Rank 0, Epoch 4551, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,601: INFO: model_training: Rank 0, Epoch 4552, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,603: INFO: model_training: Rank 0, Epoch 4552, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,604: INFO: model_training: Rank 0, Epoch 4552, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,605: INFO: model_training: Rank 0, Epoch 4552, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,606: INFO: model_training: Rank 0, Epoch 4552, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,608: INFO: model_training: Rank 0, Epoch 4553, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,609: INFO: model_training: Rank 0, Epoch 4553, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,610: INFO: model_training: Rank 0, Epoch 4553, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,611: INFO: model_training: Rank 0, Epoch 4553, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,612: INFO: model_training: Rank 0, Epoch 4553, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,613: INFO: model_training: Rank 0, Epoch 4554, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,614: INFO: model_training: Rank 0, Epoch 4554, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,615: INFO: model_training: Rank 0, Epoch 4554, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,616: INFO: model_training: Rank 0, Epoch 4554, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,617: INFO: model_training: Rank 0, Epoch 4554, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,619: INFO: model_training: Rank 0, Epoch 4555, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,620: INFO: model_training: Rank 0, Epoch 4555, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,621: INFO: model_training: Rank 0, Epoch 4555, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,622: INFO: model_training: Rank 0, Epoch 4555, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,623: INFO: model_training: Rank 0, Epoch 4555, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,625: INFO: model_training: Rank 0, Epoch 4556, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,626: INFO: model_training: Rank 0, Epoch 4556, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,628: INFO: model_training: Rank 0, Epoch 4556, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,629: INFO: model_training: Rank 0, Epoch 4556, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,630: INFO: model_training: Rank 0, Epoch 4556, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,632: INFO: model_training: Rank 0, Epoch 4557, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,633: INFO: model_training: Rank 0, Epoch 4557, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,634: INFO: model_training: Rank 0, Epoch 4557, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,635: INFO: model_training: Rank 0, Epoch 4557, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,636: INFO: model_training: Rank 0, Epoch 4557, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,637: INFO: model_training: Rank 0, Epoch 4558, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,639: INFO: model_training: Rank 0, Epoch 4558, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,640: INFO: model_training: Rank 0, Epoch 4558, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,641: INFO: model_training: Rank 0, Epoch 4558, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,642: INFO: model_training: Rank 0, Epoch 4558, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,643: INFO: model_training: Rank 0, Epoch 4559, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,644: INFO: model_training: Rank 0, Epoch 4559, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,646: INFO: model_training: Rank 0, Epoch 4559, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,647: INFO: model_training: Rank 0, Epoch 4559, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,648: INFO: model_training: Rank 0, Epoch 4559, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,649: INFO: model_training: Rank 0, Epoch 4560, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,651: INFO: model_training: Rank 0, Epoch 4560, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,652: INFO: model_training: Rank 0, Epoch 4560, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,653: INFO: model_training: Rank 0, Epoch 4560, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,655: INFO: model_training: Rank 0, Epoch 4560, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,656: INFO: model_training: Rank 0, Epoch 4561, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,657: INFO: model_training: Rank 0, Epoch 4561, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,658: INFO: model_training: Rank 0, Epoch 4561, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,660: INFO: model_training: Rank 0, Epoch 4561, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,661: INFO: model_training: Rank 0, Epoch 4561, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,662: INFO: model_training: Rank 0, Epoch 4562, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,664: INFO: model_training: Rank 0, Epoch 4562, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,665: INFO: model_training: Rank 0, Epoch 4562, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,666: INFO: model_training: Rank 0, Epoch 4562, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,667: INFO: model_training: Rank 0, Epoch 4562, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,668: INFO: model_training: Rank 0, Epoch 4563, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,669: INFO: model_training: Rank 0, Epoch 4563, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,670: INFO: model_training: Rank 0, Epoch 4563, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,672: INFO: model_training: Rank 0, Epoch 4563, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,673: INFO: model_training: Rank 0, Epoch 4563, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,675: INFO: model_training: Rank 0, Epoch 4564, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,676: INFO: model_training: Rank 0, Epoch 4564, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,677: INFO: model_training: Rank 0, Epoch 4564, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,678: INFO: model_training: Rank 0, Epoch 4564, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,679: INFO: model_training: Rank 0, Epoch 4564, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,680: INFO: model_training: Rank 0, Epoch 4565, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,681: INFO: model_training: Rank 0, Epoch 4565, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,682: INFO: model_training: Rank 0, Epoch 4565, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,683: INFO: model_training: Rank 0, Epoch 4565, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,684: INFO: model_training: Rank 0, Epoch 4565, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,686: INFO: model_training: Rank 0, Epoch 4566, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,687: INFO: model_training: Rank 0, Epoch 4566, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,688: INFO: model_training: Rank 0, Epoch 4566, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,689: INFO: model_training: Rank 0, Epoch 4566, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,690: INFO: model_training: Rank 0, Epoch 4566, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,691: INFO: model_training: Rank 0, Epoch 4567, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,692: INFO: model_training: Rank 0, Epoch 4567, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,693: INFO: model_training: Rank 0, Epoch 4567, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,694: INFO: model_training: Rank 0, Epoch 4567, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,696: INFO: model_training: Rank 0, Epoch 4567, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,697: INFO: model_training: Rank 0, Epoch 4568, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,698: INFO: model_training: Rank 0, Epoch 4568, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,699: INFO: model_training: Rank 0, Epoch 4568, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,700: INFO: model_training: Rank 0, Epoch 4568, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,701: INFO: model_training: Rank 0, Epoch 4568, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,702: INFO: model_training: Rank 0, Epoch 4569, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,703: INFO: model_training: Rank 0, Epoch 4569, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,704: INFO: model_training: Rank 0, Epoch 4569, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,705: INFO: model_training: Rank 0, Epoch 4569, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,706: INFO: model_training: Rank 0, Epoch 4569, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,707: INFO: model_training: Rank 0, Epoch 4570, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,708: INFO: model_training: Rank 0, Epoch 4570, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,709: INFO: model_training: Rank 0, Epoch 4570, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,710: INFO: model_training: Rank 0, Epoch 4570, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,712: INFO: model_training: Rank 0, Epoch 4570, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,712: INFO: model_training: Rank 0, Epoch 4571, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,714: INFO: model_training: Rank 0, Epoch 4571, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,715: INFO: model_training: Rank 0, Epoch 4571, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,716: INFO: model_training: Rank 0, Epoch 4571, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,717: INFO: model_training: Rank 0, Epoch 4571, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,719: INFO: model_training: Rank 0, Epoch 4572, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,720: INFO: model_training: Rank 0, Epoch 4572, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,721: INFO: model_training: Rank 0, Epoch 4572, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,722: INFO: model_training: Rank 0, Epoch 4572, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,723: INFO: model_training: Rank 0, Epoch 4572, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,724: INFO: model_training: Rank 0, Epoch 4573, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,725: INFO: model_training: Rank 0, Epoch 4573, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,727: INFO: model_training: Rank 0, Epoch 4573, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,728: INFO: model_training: Rank 0, Epoch 4573, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,729: INFO: model_training: Rank 0, Epoch 4573, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,730: INFO: model_training: Rank 0, Epoch 4574, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,731: INFO: model_training: Rank 0, Epoch 4574, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,732: INFO: model_training: Rank 0, Epoch 4574, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,733: INFO: model_training: Rank 0, Epoch 4574, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,734: INFO: model_training: Rank 0, Epoch 4574, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,735: INFO: model_training: Rank 0, Epoch 4575, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,736: INFO: model_training: Rank 0, Epoch 4575, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,737: INFO: model_training: Rank 0, Epoch 4575, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,738: INFO: model_training: Rank 0, Epoch 4575, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,739: INFO: model_training: Rank 0, Epoch 4575, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,741: INFO: model_training: Rank 0, Epoch 4576, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,742: INFO: model_training: Rank 0, Epoch 4576, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,743: INFO: model_training: Rank 0, Epoch 4576, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,744: INFO: model_training: Rank 0, Epoch 4576, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,746: INFO: model_training: Rank 0, Epoch 4576, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,747: INFO: model_training: Rank 0, Epoch 4577, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,748: INFO: model_training: Rank 0, Epoch 4577, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,749: INFO: model_training: Rank 0, Epoch 4577, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,751: INFO: model_training: Rank 0, Epoch 4577, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,752: INFO: model_training: Rank 0, Epoch 4577, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,753: INFO: model_training: Rank 0, Epoch 4578, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,754: INFO: model_training: Rank 0, Epoch 4578, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,755: INFO: model_training: Rank 0, Epoch 4578, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,756: INFO: model_training: Rank 0, Epoch 4578, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,757: INFO: model_training: Rank 0, Epoch 4578, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,759: INFO: model_training: Rank 0, Epoch 4579, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,760: INFO: model_training: Rank 0, Epoch 4579, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,761: INFO: model_training: Rank 0, Epoch 4579, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,762: INFO: model_training: Rank 0, Epoch 4579, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,763: INFO: model_training: Rank 0, Epoch 4579, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,765: INFO: model_training: Rank 0, Epoch 4580, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,766: INFO: model_training: Rank 0, Epoch 4580, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,767: INFO: model_training: Rank 0, Epoch 4580, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,768: INFO: model_training: Rank 0, Epoch 4580, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,769: INFO: model_training: Rank 0, Epoch 4580, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,770: INFO: model_training: Rank 0, Epoch 4581, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,772: INFO: model_training: Rank 0, Epoch 4581, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,773: INFO: model_training: Rank 0, Epoch 4581, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,774: INFO: model_training: Rank 0, Epoch 4581, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,775: INFO: model_training: Rank 0, Epoch 4581, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,776: INFO: model_training: Rank 0, Epoch 4582, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,777: INFO: model_training: Rank 0, Epoch 4582, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,778: INFO: model_training: Rank 0, Epoch 4582, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,779: INFO: model_training: Rank 0, Epoch 4582, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,781: INFO: model_training: Rank 0, Epoch 4582, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,782: INFO: model_training: Rank 0, Epoch 4583, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,783: INFO: model_training: Rank 0, Epoch 4583, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,784: INFO: model_training: Rank 0, Epoch 4583, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,785: INFO: model_training: Rank 0, Epoch 4583, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,786: INFO: model_training: Rank 0, Epoch 4583, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,788: INFO: model_training: Rank 0, Epoch 4584, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,789: INFO: model_training: Rank 0, Epoch 4584, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,790: INFO: model_training: Rank 0, Epoch 4584, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,792: INFO: model_training: Rank 0, Epoch 4584, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,793: INFO: model_training: Rank 0, Epoch 4584, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,794: INFO: model_training: Rank 0, Epoch 4585, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,796: INFO: model_training: Rank 0, Epoch 4585, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,797: INFO: model_training: Rank 0, Epoch 4585, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,798: INFO: model_training: Rank 0, Epoch 4585, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,799: INFO: model_training: Rank 0, Epoch 4585, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,800: INFO: model_training: Rank 0, Epoch 4586, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,801: INFO: model_training: Rank 0, Epoch 4586, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,802: INFO: model_training: Rank 0, Epoch 4586, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,803: INFO: model_training: Rank 0, Epoch 4586, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,804: INFO: model_training: Rank 0, Epoch 4586, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,805: INFO: model_training: Rank 0, Epoch 4587, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,806: INFO: model_training: Rank 0, Epoch 4587, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,808: INFO: model_training: Rank 0, Epoch 4587, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,809: INFO: model_training: Rank 0, Epoch 4587, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,810: INFO: model_training: Rank 0, Epoch 4587, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,811: INFO: model_training: Rank 0, Epoch 4588, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,812: INFO: model_training: Rank 0, Epoch 4588, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,814: INFO: model_training: Rank 0, Epoch 4588, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,815: INFO: model_training: Rank 0, Epoch 4588, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,816: INFO: model_training: Rank 0, Epoch 4588, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,817: INFO: model_training: Rank 0, Epoch 4589, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,818: INFO: model_training: Rank 0, Epoch 4589, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,819: INFO: model_training: Rank 0, Epoch 4589, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,820: INFO: model_training: Rank 0, Epoch 4589, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,821: INFO: model_training: Rank 0, Epoch 4589, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,823: INFO: model_training: Rank 0, Epoch 4590, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,824: INFO: model_training: Rank 0, Epoch 4590, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,825: INFO: model_training: Rank 0, Epoch 4590, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,826: INFO: model_training: Rank 0, Epoch 4590, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,828: INFO: model_training: Rank 0, Epoch 4590, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,829: INFO: model_training: Rank 0, Epoch 4591, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,830: INFO: model_training: Rank 0, Epoch 4591, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,831: INFO: model_training: Rank 0, Epoch 4591, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,832: INFO: model_training: Rank 0, Epoch 4591, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,833: INFO: model_training: Rank 0, Epoch 4591, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,835: INFO: model_training: Rank 0, Epoch 4592, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,836: INFO: model_training: Rank 0, Epoch 4592, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,837: INFO: model_training: Rank 0, Epoch 4592, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,838: INFO: model_training: Rank 0, Epoch 4592, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,839: INFO: model_training: Rank 0, Epoch 4592, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,840: INFO: model_training: Rank 0, Epoch 4593, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,841: INFO: model_training: Rank 0, Epoch 4593, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,842: INFO: model_training: Rank 0, Epoch 4593, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,843: INFO: model_training: Rank 0, Epoch 4593, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,845: INFO: model_training: Rank 0, Epoch 4593, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,846: INFO: model_training: Rank 0, Epoch 4594, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,847: INFO: model_training: Rank 0, Epoch 4594, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,848: INFO: model_training: Rank 0, Epoch 4594, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,849: INFO: model_training: Rank 0, Epoch 4594, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,851: INFO: model_training: Rank 0, Epoch 4594, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,852: INFO: model_training: Rank 0, Epoch 4595, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,853: INFO: model_training: Rank 0, Epoch 4595, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,854: INFO: model_training: Rank 0, Epoch 4595, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,855: INFO: model_training: Rank 0, Epoch 4595, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,856: INFO: model_training: Rank 0, Epoch 4595, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,857: INFO: model_training: Rank 0, Epoch 4596, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,858: INFO: model_training: Rank 0, Epoch 4596, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,860: INFO: model_training: Rank 0, Epoch 4596, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,861: INFO: model_training: Rank 0, Epoch 4596, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,862: INFO: model_training: Rank 0, Epoch 4596, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,863: INFO: model_training: Rank 0, Epoch 4597, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,864: INFO: model_training: Rank 0, Epoch 4597, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,865: INFO: model_training: Rank 0, Epoch 4597, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,867: INFO: model_training: Rank 0, Epoch 4597, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,869: INFO: model_training: Rank 0, Epoch 4597, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,870: INFO: model_training: Rank 0, Epoch 4598, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,872: INFO: model_training: Rank 0, Epoch 4598, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,873: INFO: model_training: Rank 0, Epoch 4598, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,875: INFO: model_training: Rank 0, Epoch 4598, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,876: INFO: model_training: Rank 0, Epoch 4598, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,877: INFO: model_training: Rank 0, Epoch 4599, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,878: INFO: model_training: Rank 0, Epoch 4599, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,879: INFO: model_training: Rank 0, Epoch 4599, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,880: INFO: model_training: Rank 0, Epoch 4599, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,881: INFO: model_training: Rank 0, Epoch 4599, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,882: INFO: model_training: Rank 0, Epoch 4600, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,883: INFO: model_training: Rank 0, Epoch 4600, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,884: INFO: model_training: Rank 0, Epoch 4600, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,886: INFO: model_training: Rank 0, Epoch 4600, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,887: INFO: model_training: Rank 0, Epoch 4600, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,889: INFO: model_training: Rank 0, Epoch 4601, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,890: INFO: model_training: Rank 0, Epoch 4601, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,891: INFO: model_training: Rank 0, Epoch 4601, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,893: INFO: model_training: Rank 0, Epoch 4601, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,895: INFO: model_training: Rank 0, Epoch 4601, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,897: INFO: model_training: Rank 0, Epoch 4602, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,899: INFO: model_training: Rank 0, Epoch 4602, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,900: INFO: model_training: Rank 0, Epoch 4602, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,901: INFO: model_training: Rank 0, Epoch 4602, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,903: INFO: model_training: Rank 0, Epoch 4602, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,904: INFO: model_training: Rank 0, Epoch 4603, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,905: INFO: model_training: Rank 0, Epoch 4603, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,907: INFO: model_training: Rank 0, Epoch 4603, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,908: INFO: model_training: Rank 0, Epoch 4603, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,909: INFO: model_training: Rank 0, Epoch 4603, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,910: INFO: model_training: Rank 0, Epoch 4604, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,912: INFO: model_training: Rank 0, Epoch 4604, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,913: INFO: model_training: Rank 0, Epoch 4604, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,915: INFO: model_training: Rank 0, Epoch 4604, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,916: INFO: model_training: Rank 0, Epoch 4604, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,918: INFO: model_training: Rank 0, Epoch 4605, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,920: INFO: model_training: Rank 0, Epoch 4605, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,922: INFO: model_training: Rank 0, Epoch 4605, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,924: INFO: model_training: Rank 0, Epoch 4605, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,926: INFO: model_training: Rank 0, Epoch 4605, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,928: INFO: model_training: Rank 0, Epoch 4606, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,930: INFO: model_training: Rank 0, Epoch 4606, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,933: INFO: model_training: Rank 0, Epoch 4606, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,935: INFO: model_training: Rank 0, Epoch 4606, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,937: INFO: model_training: Rank 0, Epoch 4606, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,940: INFO: model_training: Rank 0, Epoch 4607, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,942: INFO: model_training: Rank 0, Epoch 4607, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,945: INFO: model_training: Rank 0, Epoch 4607, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,947: INFO: model_training: Rank 0, Epoch 4607, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,948: INFO: model_training: Rank 0, Epoch 4607, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,950: INFO: model_training: Rank 0, Epoch 4608, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,951: INFO: model_training: Rank 0, Epoch 4608, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,953: INFO: model_training: Rank 0, Epoch 4608, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,954: INFO: model_training: Rank 0, Epoch 4608, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,956: INFO: model_training: Rank 0, Epoch 4608, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,957: INFO: model_training: Rank 0, Epoch 4609, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,958: INFO: model_training: Rank 0, Epoch 4609, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,960: INFO: model_training: Rank 0, Epoch 4609, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,961: INFO: model_training: Rank 0, Epoch 4609, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,963: INFO: model_training: Rank 0, Epoch 4609, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,964: INFO: model_training: Rank 0, Epoch 4610, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,966: INFO: model_training: Rank 0, Epoch 4610, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,967: INFO: model_training: Rank 0, Epoch 4610, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,969: INFO: model_training: Rank 0, Epoch 4610, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,971: INFO: model_training: Rank 0, Epoch 4610, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,973: INFO: model_training: Rank 0, Epoch 4611, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,974: INFO: model_training: Rank 0, Epoch 4611, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,975: INFO: model_training: Rank 0, Epoch 4611, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,977: INFO: model_training: Rank 0, Epoch 4611, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,978: INFO: model_training: Rank 0, Epoch 4611, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,980: INFO: model_training: Rank 0, Epoch 4612, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,981: INFO: model_training: Rank 0, Epoch 4612, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,983: INFO: model_training: Rank 0, Epoch 4612, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,984: INFO: model_training: Rank 0, Epoch 4612, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,986: INFO: model_training: Rank 0, Epoch 4612, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,987: INFO: model_training: Rank 0, Epoch 4613, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,988: INFO: model_training: Rank 0, Epoch 4613, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,989: INFO: model_training: Rank 0, Epoch 4613, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,990: INFO: model_training: Rank 0, Epoch 4613, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:55,992: INFO: model_training: Rank 0, Epoch 4613, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:55,994: INFO: model_training: Rank 0, Epoch 4614, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:55,996: INFO: model_training: Rank 0, Epoch 4614, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:55,997: INFO: model_training: Rank 0, Epoch 4614, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:55,998: INFO: model_training: Rank 0, Epoch 4614, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,000: INFO: model_training: Rank 0, Epoch 4614, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,001: INFO: model_training: Rank 0, Epoch 4615, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,003: INFO: model_training: Rank 0, Epoch 4615, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,004: INFO: model_training: Rank 0, Epoch 4615, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,005: INFO: model_training: Rank 0, Epoch 4615, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,006: INFO: model_training: Rank 0, Epoch 4615, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,008: INFO: model_training: Rank 0, Epoch 4616, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,009: INFO: model_training: Rank 0, Epoch 4616, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,011: INFO: model_training: Rank 0, Epoch 4616, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,012: INFO: model_training: Rank 0, Epoch 4616, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,013: INFO: model_training: Rank 0, Epoch 4616, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,014: INFO: model_training: Rank 0, Epoch 4617, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,016: INFO: model_training: Rank 0, Epoch 4617, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,017: INFO: model_training: Rank 0, Epoch 4617, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,018: INFO: model_training: Rank 0, Epoch 4617, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,019: INFO: model_training: Rank 0, Epoch 4617, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,021: INFO: model_training: Rank 0, Epoch 4618, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,022: INFO: model_training: Rank 0, Epoch 4618, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,024: INFO: model_training: Rank 0, Epoch 4618, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,025: INFO: model_training: Rank 0, Epoch 4618, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,026: INFO: model_training: Rank 0, Epoch 4618, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,028: INFO: model_training: Rank 0, Epoch 4619, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,029: INFO: model_training: Rank 0, Epoch 4619, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,030: INFO: model_training: Rank 0, Epoch 4619, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,032: INFO: model_training: Rank 0, Epoch 4619, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,033: INFO: model_training: Rank 0, Epoch 4619, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,035: INFO: model_training: Rank 0, Epoch 4620, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,036: INFO: model_training: Rank 0, Epoch 4620, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,037: INFO: model_training: Rank 0, Epoch 4620, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,038: INFO: model_training: Rank 0, Epoch 4620, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,039: INFO: model_training: Rank 0, Epoch 4620, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,041: INFO: model_training: Rank 0, Epoch 4621, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,042: INFO: model_training: Rank 0, Epoch 4621, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,044: INFO: model_training: Rank 0, Epoch 4621, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,046: INFO: model_training: Rank 0, Epoch 4621, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,047: INFO: model_training: Rank 0, Epoch 4621, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,048: INFO: model_training: Rank 0, Epoch 4622, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,050: INFO: model_training: Rank 0, Epoch 4622, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,051: INFO: model_training: Rank 0, Epoch 4622, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,053: INFO: model_training: Rank 0, Epoch 4622, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,054: INFO: model_training: Rank 0, Epoch 4622, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,055: INFO: model_training: Rank 0, Epoch 4623, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,057: INFO: model_training: Rank 0, Epoch 4623, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,058: INFO: model_training: Rank 0, Epoch 4623, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,059: INFO: model_training: Rank 0, Epoch 4623, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,061: INFO: model_training: Rank 0, Epoch 4623, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,062: INFO: model_training: Rank 0, Epoch 4624, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,063: INFO: model_training: Rank 0, Epoch 4624, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,064: INFO: model_training: Rank 0, Epoch 4624, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,066: INFO: model_training: Rank 0, Epoch 4624, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,067: INFO: model_training: Rank 0, Epoch 4624, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,069: INFO: model_training: Rank 0, Epoch 4625, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,071: INFO: model_training: Rank 0, Epoch 4625, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,072: INFO: model_training: Rank 0, Epoch 4625, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,073: INFO: model_training: Rank 0, Epoch 4625, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,075: INFO: model_training: Rank 0, Epoch 4625, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,077: INFO: model_training: Rank 0, Epoch 4626, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,078: INFO: model_training: Rank 0, Epoch 4626, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,079: INFO: model_training: Rank 0, Epoch 4626, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,080: INFO: model_training: Rank 0, Epoch 4626, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,081: INFO: model_training: Rank 0, Epoch 4626, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,083: INFO: model_training: Rank 0, Epoch 4627, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,085: INFO: model_training: Rank 0, Epoch 4627, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,086: INFO: model_training: Rank 0, Epoch 4627, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,087: INFO: model_training: Rank 0, Epoch 4627, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,089: INFO: model_training: Rank 0, Epoch 4627, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,090: INFO: model_training: Rank 0, Epoch 4628, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,092: INFO: model_training: Rank 0, Epoch 4628, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,094: INFO: model_training: Rank 0, Epoch 4628, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,095: INFO: model_training: Rank 0, Epoch 4628, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,096: INFO: model_training: Rank 0, Epoch 4628, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,097: INFO: model_training: Rank 0, Epoch 4629, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,099: INFO: model_training: Rank 0, Epoch 4629, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,100: INFO: model_training: Rank 0, Epoch 4629, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,102: INFO: model_training: Rank 0, Epoch 4629, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,103: INFO: model_training: Rank 0, Epoch 4629, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,104: INFO: model_training: Rank 0, Epoch 4630, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,106: INFO: model_training: Rank 0, Epoch 4630, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,107: INFO: model_training: Rank 0, Epoch 4630, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,108: INFO: model_training: Rank 0, Epoch 4630, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,110: INFO: model_training: Rank 0, Epoch 4630, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,112: INFO: model_training: Rank 0, Epoch 4631, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,113: INFO: model_training: Rank 0, Epoch 4631, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,114: INFO: model_training: Rank 0, Epoch 4631, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,116: INFO: model_training: Rank 0, Epoch 4631, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,117: INFO: model_training: Rank 0, Epoch 4631, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,119: INFO: model_training: Rank 0, Epoch 4632, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,120: INFO: model_training: Rank 0, Epoch 4632, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,121: INFO: model_training: Rank 0, Epoch 4632, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,123: INFO: model_training: Rank 0, Epoch 4632, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,124: INFO: model_training: Rank 0, Epoch 4632, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,126: INFO: model_training: Rank 0, Epoch 4633, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,127: INFO: model_training: Rank 0, Epoch 4633, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,129: INFO: model_training: Rank 0, Epoch 4633, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,130: INFO: model_training: Rank 0, Epoch 4633, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,131: INFO: model_training: Rank 0, Epoch 4633, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,133: INFO: model_training: Rank 0, Epoch 4634, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,134: INFO: model_training: Rank 0, Epoch 4634, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,136: INFO: model_training: Rank 0, Epoch 4634, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,137: INFO: model_training: Rank 0, Epoch 4634, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,139: INFO: model_training: Rank 0, Epoch 4634, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,140: INFO: model_training: Rank 0, Epoch 4635, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,141: INFO: model_training: Rank 0, Epoch 4635, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,143: INFO: model_training: Rank 0, Epoch 4635, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,144: INFO: model_training: Rank 0, Epoch 4635, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,146: INFO: model_training: Rank 0, Epoch 4635, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,147: INFO: model_training: Rank 0, Epoch 4636, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,148: INFO: model_training: Rank 0, Epoch 4636, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,149: INFO: model_training: Rank 0, Epoch 4636, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,150: INFO: model_training: Rank 0, Epoch 4636, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,151: INFO: model_training: Rank 0, Epoch 4636, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,153: INFO: model_training: Rank 0, Epoch 4637, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,154: INFO: model_training: Rank 0, Epoch 4637, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,156: INFO: model_training: Rank 0, Epoch 4637, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,157: INFO: model_training: Rank 0, Epoch 4637, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,158: INFO: model_training: Rank 0, Epoch 4637, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,160: INFO: model_training: Rank 0, Epoch 4638, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,161: INFO: model_training: Rank 0, Epoch 4638, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,162: INFO: model_training: Rank 0, Epoch 4638, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,163: INFO: model_training: Rank 0, Epoch 4638, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,165: INFO: model_training: Rank 0, Epoch 4638, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,167: INFO: model_training: Rank 0, Epoch 4639, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,168: INFO: model_training: Rank 0, Epoch 4639, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,169: INFO: model_training: Rank 0, Epoch 4639, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,170: INFO: model_training: Rank 0, Epoch 4639, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,172: INFO: model_training: Rank 0, Epoch 4639, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,173: INFO: model_training: Rank 0, Epoch 4640, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,174: INFO: model_training: Rank 0, Epoch 4640, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,175: INFO: model_training: Rank 0, Epoch 4640, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,177: INFO: model_training: Rank 0, Epoch 4640, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,179: INFO: model_training: Rank 0, Epoch 4640, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,180: INFO: model_training: Rank 0, Epoch 4641, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,181: INFO: model_training: Rank 0, Epoch 4641, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,182: INFO: model_training: Rank 0, Epoch 4641, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,183: INFO: model_training: Rank 0, Epoch 4641, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,185: INFO: model_training: Rank 0, Epoch 4641, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,187: INFO: model_training: Rank 0, Epoch 4642, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,188: INFO: model_training: Rank 0, Epoch 4642, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,189: INFO: model_training: Rank 0, Epoch 4642, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,191: INFO: model_training: Rank 0, Epoch 4642, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,192: INFO: model_training: Rank 0, Epoch 4642, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,193: INFO: model_training: Rank 0, Epoch 4643, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,195: INFO: model_training: Rank 0, Epoch 4643, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,197: INFO: model_training: Rank 0, Epoch 4643, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,198: INFO: model_training: Rank 0, Epoch 4643, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,200: INFO: model_training: Rank 0, Epoch 4643, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,201: INFO: model_training: Rank 0, Epoch 4644, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,202: INFO: model_training: Rank 0, Epoch 4644, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,204: INFO: model_training: Rank 0, Epoch 4644, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,205: INFO: model_training: Rank 0, Epoch 4644, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,206: INFO: model_training: Rank 0, Epoch 4644, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,207: INFO: model_training: Rank 0, Epoch 4645, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,208: INFO: model_training: Rank 0, Epoch 4645, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,210: INFO: model_training: Rank 0, Epoch 4645, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,211: INFO: model_training: Rank 0, Epoch 4645, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,212: INFO: model_training: Rank 0, Epoch 4645, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,213: INFO: model_training: Rank 0, Epoch 4646, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,215: INFO: model_training: Rank 0, Epoch 4646, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,217: INFO: model_training: Rank 0, Epoch 4646, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,218: INFO: model_training: Rank 0, Epoch 4646, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,219: INFO: model_training: Rank 0, Epoch 4646, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,220: INFO: model_training: Rank 0, Epoch 4647, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,222: INFO: model_training: Rank 0, Epoch 4647, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,223: INFO: model_training: Rank 0, Epoch 4647, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,225: INFO: model_training: Rank 0, Epoch 4647, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,226: INFO: model_training: Rank 0, Epoch 4647, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,227: INFO: model_training: Rank 0, Epoch 4648, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,228: INFO: model_training: Rank 0, Epoch 4648, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,230: INFO: model_training: Rank 0, Epoch 4648, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,231: INFO: model_training: Rank 0, Epoch 4648, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,232: INFO: model_training: Rank 0, Epoch 4648, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,233: INFO: model_training: Rank 0, Epoch 4649, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,235: INFO: model_training: Rank 0, Epoch 4649, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,237: INFO: model_training: Rank 0, Epoch 4649, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,238: INFO: model_training: Rank 0, Epoch 4649, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,239: INFO: model_training: Rank 0, Epoch 4649, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,241: INFO: model_training: Rank 0, Epoch 4650, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,242: INFO: model_training: Rank 0, Epoch 4650, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,244: INFO: model_training: Rank 0, Epoch 4650, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,245: INFO: model_training: Rank 0, Epoch 4650, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,246: INFO: model_training: Rank 0, Epoch 4650, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,247: INFO: model_training: Rank 0, Epoch 4651, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,249: INFO: model_training: Rank 0, Epoch 4651, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,250: INFO: model_training: Rank 0, Epoch 4651, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,252: INFO: model_training: Rank 0, Epoch 4651, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,253: INFO: model_training: Rank 0, Epoch 4651, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,254: INFO: model_training: Rank 0, Epoch 4652, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,255: INFO: model_training: Rank 0, Epoch 4652, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,257: INFO: model_training: Rank 0, Epoch 4652, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,258: INFO: model_training: Rank 0, Epoch 4652, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,260: INFO: model_training: Rank 0, Epoch 4652, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,262: INFO: model_training: Rank 0, Epoch 4653, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,263: INFO: model_training: Rank 0, Epoch 4653, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,265: INFO: model_training: Rank 0, Epoch 4653, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,267: INFO: model_training: Rank 0, Epoch 4653, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,268: INFO: model_training: Rank 0, Epoch 4653, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,270: INFO: model_training: Rank 0, Epoch 4654, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,271: INFO: model_training: Rank 0, Epoch 4654, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,272: INFO: model_training: Rank 0, Epoch 4654, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,273: INFO: model_training: Rank 0, Epoch 4654, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,275: INFO: model_training: Rank 0, Epoch 4654, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,276: INFO: model_training: Rank 0, Epoch 4655, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,278: INFO: model_training: Rank 0, Epoch 4655, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,279: INFO: model_training: Rank 0, Epoch 4655, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,280: INFO: model_training: Rank 0, Epoch 4655, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,282: INFO: model_training: Rank 0, Epoch 4655, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,284: INFO: model_training: Rank 0, Epoch 4656, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,286: INFO: model_training: Rank 0, Epoch 4656, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,287: INFO: model_training: Rank 0, Epoch 4656, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,289: INFO: model_training: Rank 0, Epoch 4656, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,290: INFO: model_training: Rank 0, Epoch 4656, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,292: INFO: model_training: Rank 0, Epoch 4657, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,294: INFO: model_training: Rank 0, Epoch 4657, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,295: INFO: model_training: Rank 0, Epoch 4657, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,297: INFO: model_training: Rank 0, Epoch 4657, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,298: INFO: model_training: Rank 0, Epoch 4657, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,300: INFO: model_training: Rank 0, Epoch 4658, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,301: INFO: model_training: Rank 0, Epoch 4658, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,303: INFO: model_training: Rank 0, Epoch 4658, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,305: INFO: model_training: Rank 0, Epoch 4658, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,306: INFO: model_training: Rank 0, Epoch 4658, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,308: INFO: model_training: Rank 0, Epoch 4659, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,310: INFO: model_training: Rank 0, Epoch 4659, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,312: INFO: model_training: Rank 0, Epoch 4659, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,313: INFO: model_training: Rank 0, Epoch 4659, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,314: INFO: model_training: Rank 0, Epoch 4659, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,316: INFO: model_training: Rank 0, Epoch 4660, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,317: INFO: model_training: Rank 0, Epoch 4660, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,319: INFO: model_training: Rank 0, Epoch 4660, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,320: INFO: model_training: Rank 0, Epoch 4660, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,322: INFO: model_training: Rank 0, Epoch 4660, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,324: INFO: model_training: Rank 0, Epoch 4661, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,326: INFO: model_training: Rank 0, Epoch 4661, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,328: INFO: model_training: Rank 0, Epoch 4661, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,329: INFO: model_training: Rank 0, Epoch 4661, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,330: INFO: model_training: Rank 0, Epoch 4661, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,331: INFO: model_training: Rank 0, Epoch 4662, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,333: INFO: model_training: Rank 0, Epoch 4662, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,334: INFO: model_training: Rank 0, Epoch 4662, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,336: INFO: model_training: Rank 0, Epoch 4662, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,337: INFO: model_training: Rank 0, Epoch 4662, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,339: INFO: model_training: Rank 0, Epoch 4663, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,340: INFO: model_training: Rank 0, Epoch 4663, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,341: INFO: model_training: Rank 0, Epoch 4663, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,343: INFO: model_training: Rank 0, Epoch 4663, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,345: INFO: model_training: Rank 0, Epoch 4663, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,346: INFO: model_training: Rank 0, Epoch 4664, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,347: INFO: model_training: Rank 0, Epoch 4664, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,349: INFO: model_training: Rank 0, Epoch 4664, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,350: INFO: model_training: Rank 0, Epoch 4664, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,352: INFO: model_training: Rank 0, Epoch 4664, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,354: INFO: model_training: Rank 0, Epoch 4665, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,355: INFO: model_training: Rank 0, Epoch 4665, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,357: INFO: model_training: Rank 0, Epoch 4665, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,358: INFO: model_training: Rank 0, Epoch 4665, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,359: INFO: model_training: Rank 0, Epoch 4665, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,361: INFO: model_training: Rank 0, Epoch 4666, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,362: INFO: model_training: Rank 0, Epoch 4666, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,363: INFO: model_training: Rank 0, Epoch 4666, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,364: INFO: model_training: Rank 0, Epoch 4666, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,366: INFO: model_training: Rank 0, Epoch 4666, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,367: INFO: model_training: Rank 0, Epoch 4667, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,369: INFO: model_training: Rank 0, Epoch 4667, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,370: INFO: model_training: Rank 0, Epoch 4667, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,371: INFO: model_training: Rank 0, Epoch 4667, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,372: INFO: model_training: Rank 0, Epoch 4667, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,374: INFO: model_training: Rank 0, Epoch 4668, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,375: INFO: model_training: Rank 0, Epoch 4668, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,376: INFO: model_training: Rank 0, Epoch 4668, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,379: INFO: model_training: Rank 0, Epoch 4668, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,380: INFO: model_training: Rank 0, Epoch 4668, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,382: INFO: model_training: Rank 0, Epoch 4669, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,383: INFO: model_training: Rank 0, Epoch 4669, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,385: INFO: model_training: Rank 0, Epoch 4669, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,387: INFO: model_training: Rank 0, Epoch 4669, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,388: INFO: model_training: Rank 0, Epoch 4669, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,389: INFO: model_training: Rank 0, Epoch 4670, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,390: INFO: model_training: Rank 0, Epoch 4670, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,392: INFO: model_training: Rank 0, Epoch 4670, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,393: INFO: model_training: Rank 0, Epoch 4670, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,395: INFO: model_training: Rank 0, Epoch 4670, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,396: INFO: model_training: Rank 0, Epoch 4671, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,397: INFO: model_training: Rank 0, Epoch 4671, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,398: INFO: model_training: Rank 0, Epoch 4671, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,400: INFO: model_training: Rank 0, Epoch 4671, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,401: INFO: model_training: Rank 0, Epoch 4671, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,403: INFO: model_training: Rank 0, Epoch 4672, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,405: INFO: model_training: Rank 0, Epoch 4672, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,406: INFO: model_training: Rank 0, Epoch 4672, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,408: INFO: model_training: Rank 0, Epoch 4672, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,409: INFO: model_training: Rank 0, Epoch 4672, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,410: INFO: model_training: Rank 0, Epoch 4673, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,412: INFO: model_training: Rank 0, Epoch 4673, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,413: INFO: model_training: Rank 0, Epoch 4673, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,415: INFO: model_training: Rank 0, Epoch 4673, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,416: INFO: model_training: Rank 0, Epoch 4673, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,418: INFO: model_training: Rank 0, Epoch 4674, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,419: INFO: model_training: Rank 0, Epoch 4674, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,421: INFO: model_training: Rank 0, Epoch 4674, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,422: INFO: model_training: Rank 0, Epoch 4674, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,423: INFO: model_training: Rank 0, Epoch 4674, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,425: INFO: model_training: Rank 0, Epoch 4675, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,426: INFO: model_training: Rank 0, Epoch 4675, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,428: INFO: model_training: Rank 0, Epoch 4675, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,429: INFO: model_training: Rank 0, Epoch 4675, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,430: INFO: model_training: Rank 0, Epoch 4675, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,432: INFO: model_training: Rank 0, Epoch 4676, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,433: INFO: model_training: Rank 0, Epoch 4676, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,435: INFO: model_training: Rank 0, Epoch 4676, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,437: INFO: model_training: Rank 0, Epoch 4676, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,438: INFO: model_training: Rank 0, Epoch 4676, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,439: INFO: model_training: Rank 0, Epoch 4677, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,440: INFO: model_training: Rank 0, Epoch 4677, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,442: INFO: model_training: Rank 0, Epoch 4677, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,444: INFO: model_training: Rank 0, Epoch 4677, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,445: INFO: model_training: Rank 0, Epoch 4677, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,446: INFO: model_training: Rank 0, Epoch 4678, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,447: INFO: model_training: Rank 0, Epoch 4678, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,448: INFO: model_training: Rank 0, Epoch 4678, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,450: INFO: model_training: Rank 0, Epoch 4678, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,451: INFO: model_training: Rank 0, Epoch 4678, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,452: INFO: model_training: Rank 0, Epoch 4679, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,453: INFO: model_training: Rank 0, Epoch 4679, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,454: INFO: model_training: Rank 0, Epoch 4679, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,457: INFO: model_training: Rank 0, Epoch 4679, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,458: INFO: model_training: Rank 0, Epoch 4679, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,459: INFO: model_training: Rank 0, Epoch 4680, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,461: INFO: model_training: Rank 0, Epoch 4680, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,462: INFO: model_training: Rank 0, Epoch 4680, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,463: INFO: model_training: Rank 0, Epoch 4680, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,464: INFO: model_training: Rank 0, Epoch 4680, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,466: INFO: model_training: Rank 0, Epoch 4681, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,467: INFO: model_training: Rank 0, Epoch 4681, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,468: INFO: model_training: Rank 0, Epoch 4681, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,469: INFO: model_training: Rank 0, Epoch 4681, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,471: INFO: model_training: Rank 0, Epoch 4681, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,472: INFO: model_training: Rank 0, Epoch 4682, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,473: INFO: model_training: Rank 0, Epoch 4682, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,474: INFO: model_training: Rank 0, Epoch 4682, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,475: INFO: model_training: Rank 0, Epoch 4682, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,477: INFO: model_training: Rank 0, Epoch 4682, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,478: INFO: model_training: Rank 0, Epoch 4683, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,479: INFO: model_training: Rank 0, Epoch 4683, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,481: INFO: model_training: Rank 0, Epoch 4683, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,482: INFO: model_training: Rank 0, Epoch 4683, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,484: INFO: model_training: Rank 0, Epoch 4683, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,485: INFO: model_training: Rank 0, Epoch 4684, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,487: INFO: model_training: Rank 0, Epoch 4684, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,488: INFO: model_training: Rank 0, Epoch 4684, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,489: INFO: model_training: Rank 0, Epoch 4684, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,490: INFO: model_training: Rank 0, Epoch 4684, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,492: INFO: model_training: Rank 0, Epoch 4685, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,493: INFO: model_training: Rank 0, Epoch 4685, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,495: INFO: model_training: Rank 0, Epoch 4685, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,496: INFO: model_training: Rank 0, Epoch 4685, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,497: INFO: model_training: Rank 0, Epoch 4685, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,499: INFO: model_training: Rank 0, Epoch 4686, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,500: INFO: model_training: Rank 0, Epoch 4686, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,502: INFO: model_training: Rank 0, Epoch 4686, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,503: INFO: model_training: Rank 0, Epoch 4686, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,504: INFO: model_training: Rank 0, Epoch 4686, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,506: INFO: model_training: Rank 0, Epoch 4687, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,508: INFO: model_training: Rank 0, Epoch 4687, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,509: INFO: model_training: Rank 0, Epoch 4687, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,510: INFO: model_training: Rank 0, Epoch 4687, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,511: INFO: model_training: Rank 0, Epoch 4687, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,513: INFO: model_training: Rank 0, Epoch 4688, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,514: INFO: model_training: Rank 0, Epoch 4688, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,515: INFO: model_training: Rank 0, Epoch 4688, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,516: INFO: model_training: Rank 0, Epoch 4688, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,517: INFO: model_training: Rank 0, Epoch 4688, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,519: INFO: model_training: Rank 0, Epoch 4689, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,521: INFO: model_training: Rank 0, Epoch 4689, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,522: INFO: model_training: Rank 0, Epoch 4689, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,523: INFO: model_training: Rank 0, Epoch 4689, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,525: INFO: model_training: Rank 0, Epoch 4689, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,527: INFO: model_training: Rank 0, Epoch 4690, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,528: INFO: model_training: Rank 0, Epoch 4690, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,529: INFO: model_training: Rank 0, Epoch 4690, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,530: INFO: model_training: Rank 0, Epoch 4690, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,531: INFO: model_training: Rank 0, Epoch 4690, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,533: INFO: model_training: Rank 0, Epoch 4691, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,534: INFO: model_training: Rank 0, Epoch 4691, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,536: INFO: model_training: Rank 0, Epoch 4691, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,537: INFO: model_training: Rank 0, Epoch 4691, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,538: INFO: model_training: Rank 0, Epoch 4691, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,539: INFO: model_training: Rank 0, Epoch 4692, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,541: INFO: model_training: Rank 0, Epoch 4692, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,542: INFO: model_training: Rank 0, Epoch 4692, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,544: INFO: model_training: Rank 0, Epoch 4692, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,545: INFO: model_training: Rank 0, Epoch 4692, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,546: INFO: model_training: Rank 0, Epoch 4693, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,547: INFO: model_training: Rank 0, Epoch 4693, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,549: INFO: model_training: Rank 0, Epoch 4693, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,550: INFO: model_training: Rank 0, Epoch 4693, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,551: INFO: model_training: Rank 0, Epoch 4693, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,553: INFO: model_training: Rank 0, Epoch 4694, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,554: INFO: model_training: Rank 0, Epoch 4694, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,555: INFO: model_training: Rank 0, Epoch 4694, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,557: INFO: model_training: Rank 0, Epoch 4694, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,558: INFO: model_training: Rank 0, Epoch 4694, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,562: INFO: model_training: Rank 0, Epoch 4695, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,573: INFO: model_training: Rank 0, Epoch 4695, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,581: INFO: model_training: Rank 0, Epoch 4695, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,587: INFO: model_training: Rank 0, Epoch 4695, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,588: INFO: model_training: Rank 0, Epoch 4695, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,590: INFO: model_training: Rank 0, Epoch 4696, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,595: INFO: model_training: Rank 0, Epoch 4696, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,597: INFO: model_training: Rank 0, Epoch 4696, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,599: INFO: model_training: Rank 0, Epoch 4696, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,601: INFO: model_training: Rank 0, Epoch 4696, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,602: INFO: model_training: Rank 0, Epoch 4697, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,603: INFO: model_training: Rank 0, Epoch 4697, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,605: INFO: model_training: Rank 0, Epoch 4697, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,606: INFO: model_training: Rank 0, Epoch 4697, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,607: INFO: model_training: Rank 0, Epoch 4697, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,609: INFO: model_training: Rank 0, Epoch 4698, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,611: INFO: model_training: Rank 0, Epoch 4698, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,612: INFO: model_training: Rank 0, Epoch 4698, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,613: INFO: model_training: Rank 0, Epoch 4698, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,614: INFO: model_training: Rank 0, Epoch 4698, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,615: INFO: model_training: Rank 0, Epoch 4699, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,617: INFO: model_training: Rank 0, Epoch 4699, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,618: INFO: model_training: Rank 0, Epoch 4699, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,620: INFO: model_training: Rank 0, Epoch 4699, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,621: INFO: model_training: Rank 0, Epoch 4699, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,622: INFO: model_training: Rank 0, Epoch 4700, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,623: INFO: model_training: Rank 0, Epoch 4700, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,625: INFO: model_training: Rank 0, Epoch 4700, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,626: INFO: model_training: Rank 0, Epoch 4700, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,628: INFO: model_training: Rank 0, Epoch 4700, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,629: INFO: model_training: Rank 0, Epoch 4701, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,630: INFO: model_training: Rank 0, Epoch 4701, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,631: INFO: model_training: Rank 0, Epoch 4701, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,633: INFO: model_training: Rank 0, Epoch 4701, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,635: INFO: model_training: Rank 0, Epoch 4701, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,636: INFO: model_training: Rank 0, Epoch 4702, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,638: INFO: model_training: Rank 0, Epoch 4702, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,639: INFO: model_training: Rank 0, Epoch 4702, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,640: INFO: model_training: Rank 0, Epoch 4702, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,641: INFO: model_training: Rank 0, Epoch 4702, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,642: INFO: model_training: Rank 0, Epoch 4703, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,645: INFO: model_training: Rank 0, Epoch 4703, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,646: INFO: model_training: Rank 0, Epoch 4703, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,647: INFO: model_training: Rank 0, Epoch 4703, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,648: INFO: model_training: Rank 0, Epoch 4703, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,650: INFO: model_training: Rank 0, Epoch 4704, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,651: INFO: model_training: Rank 0, Epoch 4704, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,653: INFO: model_training: Rank 0, Epoch 4704, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,654: INFO: model_training: Rank 0, Epoch 4704, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,655: INFO: model_training: Rank 0, Epoch 4704, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,657: INFO: model_training: Rank 0, Epoch 4705, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,658: INFO: model_training: Rank 0, Epoch 4705, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,660: INFO: model_training: Rank 0, Epoch 4705, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,662: INFO: model_training: Rank 0, Epoch 4705, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,663: INFO: model_training: Rank 0, Epoch 4705, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,665: INFO: model_training: Rank 0, Epoch 4706, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,666: INFO: model_training: Rank 0, Epoch 4706, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,667: INFO: model_training: Rank 0, Epoch 4706, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,669: INFO: model_training: Rank 0, Epoch 4706, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,670: INFO: model_training: Rank 0, Epoch 4706, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,672: INFO: model_training: Rank 0, Epoch 4707, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,674: INFO: model_training: Rank 0, Epoch 4707, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,675: INFO: model_training: Rank 0, Epoch 4707, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,677: INFO: model_training: Rank 0, Epoch 4707, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,679: INFO: model_training: Rank 0, Epoch 4707, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,680: INFO: model_training: Rank 0, Epoch 4708, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,682: INFO: model_training: Rank 0, Epoch 4708, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,683: INFO: model_training: Rank 0, Epoch 4708, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,685: INFO: model_training: Rank 0, Epoch 4708, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,687: INFO: model_training: Rank 0, Epoch 4708, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,688: INFO: model_training: Rank 0, Epoch 4709, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,690: INFO: model_training: Rank 0, Epoch 4709, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,691: INFO: model_training: Rank 0, Epoch 4709, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,693: INFO: model_training: Rank 0, Epoch 4709, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,695: INFO: model_training: Rank 0, Epoch 4709, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,696: INFO: model_training: Rank 0, Epoch 4710, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,697: INFO: model_training: Rank 0, Epoch 4710, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,699: INFO: model_training: Rank 0, Epoch 4710, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,700: INFO: model_training: Rank 0, Epoch 4710, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,702: INFO: model_training: Rank 0, Epoch 4710, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,703: INFO: model_training: Rank 0, Epoch 4711, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,704: INFO: model_training: Rank 0, Epoch 4711, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,705: INFO: model_training: Rank 0, Epoch 4711, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,706: INFO: model_training: Rank 0, Epoch 4711, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,708: INFO: model_training: Rank 0, Epoch 4711, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,709: INFO: model_training: Rank 0, Epoch 4712, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,710: INFO: model_training: Rank 0, Epoch 4712, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,712: INFO: model_training: Rank 0, Epoch 4712, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,713: INFO: model_training: Rank 0, Epoch 4712, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,714: INFO: model_training: Rank 0, Epoch 4712, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,716: INFO: model_training: Rank 0, Epoch 4713, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,717: INFO: model_training: Rank 0, Epoch 4713, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,719: INFO: model_training: Rank 0, Epoch 4713, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,720: INFO: model_training: Rank 0, Epoch 4713, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,721: INFO: model_training: Rank 0, Epoch 4713, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,722: INFO: model_training: Rank 0, Epoch 4714, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,723: INFO: model_training: Rank 0, Epoch 4714, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,725: INFO: model_training: Rank 0, Epoch 4714, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,726: INFO: model_training: Rank 0, Epoch 4714, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,727: INFO: model_training: Rank 0, Epoch 4714, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,729: INFO: model_training: Rank 0, Epoch 4715, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,730: INFO: model_training: Rank 0, Epoch 4715, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,731: INFO: model_training: Rank 0, Epoch 4715, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,732: INFO: model_training: Rank 0, Epoch 4715, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,733: INFO: model_training: Rank 0, Epoch 4715, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,735: INFO: model_training: Rank 0, Epoch 4716, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,736: INFO: model_training: Rank 0, Epoch 4716, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,737: INFO: model_training: Rank 0, Epoch 4716, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,739: INFO: model_training: Rank 0, Epoch 4716, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,740: INFO: model_training: Rank 0, Epoch 4716, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,741: INFO: model_training: Rank 0, Epoch 4717, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,742: INFO: model_training: Rank 0, Epoch 4717, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,744: INFO: model_training: Rank 0, Epoch 4717, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,745: INFO: model_training: Rank 0, Epoch 4717, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,746: INFO: model_training: Rank 0, Epoch 4717, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,748: INFO: model_training: Rank 0, Epoch 4718, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,749: INFO: model_training: Rank 0, Epoch 4718, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,750: INFO: model_training: Rank 0, Epoch 4718, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,752: INFO: model_training: Rank 0, Epoch 4718, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,753: INFO: model_training: Rank 0, Epoch 4718, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,754: INFO: model_training: Rank 0, Epoch 4719, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,755: INFO: model_training: Rank 0, Epoch 4719, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,757: INFO: model_training: Rank 0, Epoch 4719, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,758: INFO: model_training: Rank 0, Epoch 4719, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,759: INFO: model_training: Rank 0, Epoch 4719, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,760: INFO: model_training: Rank 0, Epoch 4720, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,762: INFO: model_training: Rank 0, Epoch 4720, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,763: INFO: model_training: Rank 0, Epoch 4720, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,764: INFO: model_training: Rank 0, Epoch 4720, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,766: INFO: model_training: Rank 0, Epoch 4720, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,767: INFO: model_training: Rank 0, Epoch 4721, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,769: INFO: model_training: Rank 0, Epoch 4721, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,770: INFO: model_training: Rank 0, Epoch 4721, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,771: INFO: model_training: Rank 0, Epoch 4721, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,773: INFO: model_training: Rank 0, Epoch 4721, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,774: INFO: model_training: Rank 0, Epoch 4722, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,775: INFO: model_training: Rank 0, Epoch 4722, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,777: INFO: model_training: Rank 0, Epoch 4722, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,778: INFO: model_training: Rank 0, Epoch 4722, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,779: INFO: model_training: Rank 0, Epoch 4722, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,780: INFO: model_training: Rank 0, Epoch 4723, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,782: INFO: model_training: Rank 0, Epoch 4723, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,784: INFO: model_training: Rank 0, Epoch 4723, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,785: INFO: model_training: Rank 0, Epoch 4723, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,787: INFO: model_training: Rank 0, Epoch 4723, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,788: INFO: model_training: Rank 0, Epoch 4724, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,789: INFO: model_training: Rank 0, Epoch 4724, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,791: INFO: model_training: Rank 0, Epoch 4724, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,792: INFO: model_training: Rank 0, Epoch 4724, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,794: INFO: model_training: Rank 0, Epoch 4724, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,795: INFO: model_training: Rank 0, Epoch 4725, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,796: INFO: model_training: Rank 0, Epoch 4725, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,798: INFO: model_training: Rank 0, Epoch 4725, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,799: INFO: model_training: Rank 0, Epoch 4725, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,800: INFO: model_training: Rank 0, Epoch 4725, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,802: INFO: model_training: Rank 0, Epoch 4726, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,804: INFO: model_training: Rank 0, Epoch 4726, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,805: INFO: model_training: Rank 0, Epoch 4726, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,806: INFO: model_training: Rank 0, Epoch 4726, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,808: INFO: model_training: Rank 0, Epoch 4726, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,809: INFO: model_training: Rank 0, Epoch 4727, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,811: INFO: model_training: Rank 0, Epoch 4727, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,812: INFO: model_training: Rank 0, Epoch 4727, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,813: INFO: model_training: Rank 0, Epoch 4727, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,814: INFO: model_training: Rank 0, Epoch 4727, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,816: INFO: model_training: Rank 0, Epoch 4728, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,817: INFO: model_training: Rank 0, Epoch 4728, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,819: INFO: model_training: Rank 0, Epoch 4728, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,820: INFO: model_training: Rank 0, Epoch 4728, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,821: INFO: model_training: Rank 0, Epoch 4728, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,823: INFO: model_training: Rank 0, Epoch 4729, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,824: INFO: model_training: Rank 0, Epoch 4729, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,826: INFO: model_training: Rank 0, Epoch 4729, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,828: INFO: model_training: Rank 0, Epoch 4729, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,829: INFO: model_training: Rank 0, Epoch 4729, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,830: INFO: model_training: Rank 0, Epoch 4730, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,832: INFO: model_training: Rank 0, Epoch 4730, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,833: INFO: model_training: Rank 0, Epoch 4730, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,835: INFO: model_training: Rank 0, Epoch 4730, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,836: INFO: model_training: Rank 0, Epoch 4730, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,837: INFO: model_training: Rank 0, Epoch 4731, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,838: INFO: model_training: Rank 0, Epoch 4731, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,840: INFO: model_training: Rank 0, Epoch 4731, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,841: INFO: model_training: Rank 0, Epoch 4731, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,842: INFO: model_training: Rank 0, Epoch 4731, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,845: INFO: model_training: Rank 0, Epoch 4732, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,846: INFO: model_training: Rank 0, Epoch 4732, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,847: INFO: model_training: Rank 0, Epoch 4732, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,849: INFO: model_training: Rank 0, Epoch 4732, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,850: INFO: model_training: Rank 0, Epoch 4732, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,852: INFO: model_training: Rank 0, Epoch 4733, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,853: INFO: model_training: Rank 0, Epoch 4733, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,854: INFO: model_training: Rank 0, Epoch 4733, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,855: INFO: model_training: Rank 0, Epoch 4733, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,857: INFO: model_training: Rank 0, Epoch 4733, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,858: INFO: model_training: Rank 0, Epoch 4734, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,860: INFO: model_training: Rank 0, Epoch 4734, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,861: INFO: model_training: Rank 0, Epoch 4734, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,863: INFO: model_training: Rank 0, Epoch 4734, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,864: INFO: model_training: Rank 0, Epoch 4734, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,865: INFO: model_training: Rank 0, Epoch 4735, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,867: INFO: model_training: Rank 0, Epoch 4735, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,868: INFO: model_training: Rank 0, Epoch 4735, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,870: INFO: model_training: Rank 0, Epoch 4735, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,871: INFO: model_training: Rank 0, Epoch 4735, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,872: INFO: model_training: Rank 0, Epoch 4736, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,874: INFO: model_training: Rank 0, Epoch 4736, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,875: INFO: model_training: Rank 0, Epoch 4736, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,877: INFO: model_training: Rank 0, Epoch 4736, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,878: INFO: model_training: Rank 0, Epoch 4736, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,879: INFO: model_training: Rank 0, Epoch 4737, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,880: INFO: model_training: Rank 0, Epoch 4737, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,881: INFO: model_training: Rank 0, Epoch 4737, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,883: INFO: model_training: Rank 0, Epoch 4737, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,885: INFO: model_training: Rank 0, Epoch 4737, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,887: INFO: model_training: Rank 0, Epoch 4738, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,888: INFO: model_training: Rank 0, Epoch 4738, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,889: INFO: model_training: Rank 0, Epoch 4738, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,890: INFO: model_training: Rank 0, Epoch 4738, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,892: INFO: model_training: Rank 0, Epoch 4738, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,893: INFO: model_training: Rank 0, Epoch 4739, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,895: INFO: model_training: Rank 0, Epoch 4739, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,896: INFO: model_training: Rank 0, Epoch 4739, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,897: INFO: model_training: Rank 0, Epoch 4739, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,898: INFO: model_training: Rank 0, Epoch 4739, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,900: INFO: model_training: Rank 0, Epoch 4740, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,902: INFO: model_training: Rank 0, Epoch 4740, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,903: INFO: model_training: Rank 0, Epoch 4740, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,904: INFO: model_training: Rank 0, Epoch 4740, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,905: INFO: model_training: Rank 0, Epoch 4740, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,907: INFO: model_training: Rank 0, Epoch 4741, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,908: INFO: model_training: Rank 0, Epoch 4741, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,909: INFO: model_training: Rank 0, Epoch 4741, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,911: INFO: model_training: Rank 0, Epoch 4741, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,912: INFO: model_training: Rank 0, Epoch 4741, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,913: INFO: model_training: Rank 0, Epoch 4742, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,914: INFO: model_training: Rank 0, Epoch 4742, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,915: INFO: model_training: Rank 0, Epoch 4742, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,917: INFO: model_training: Rank 0, Epoch 4742, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,919: INFO: model_training: Rank 0, Epoch 4742, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,920: INFO: model_training: Rank 0, Epoch 4743, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,921: INFO: model_training: Rank 0, Epoch 4743, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,923: INFO: model_training: Rank 0, Epoch 4743, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,924: INFO: model_training: Rank 0, Epoch 4743, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,925: INFO: model_training: Rank 0, Epoch 4743, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,927: INFO: model_training: Rank 0, Epoch 4744, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,928: INFO: model_training: Rank 0, Epoch 4744, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,929: INFO: model_training: Rank 0, Epoch 4744, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,930: INFO: model_training: Rank 0, Epoch 4744, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,934: INFO: model_training: Rank 0, Epoch 4744, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,936: INFO: model_training: Rank 0, Epoch 4745, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,938: INFO: model_training: Rank 0, Epoch 4745, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,943: INFO: model_training: Rank 0, Epoch 4745, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,946: INFO: model_training: Rank 0, Epoch 4745, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,949: INFO: model_training: Rank 0, Epoch 4745, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,952: INFO: model_training: Rank 0, Epoch 4746, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,954: INFO: model_training: Rank 0, Epoch 4746, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,957: INFO: model_training: Rank 0, Epoch 4746, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,960: INFO: model_training: Rank 0, Epoch 4746, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,964: INFO: model_training: Rank 0, Epoch 4746, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,967: INFO: model_training: Rank 0, Epoch 4747, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,969: INFO: model_training: Rank 0, Epoch 4747, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,971: INFO: model_training: Rank 0, Epoch 4747, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,974: INFO: model_training: Rank 0, Epoch 4747, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,976: INFO: model_training: Rank 0, Epoch 4747, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,979: INFO: model_training: Rank 0, Epoch 4748, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,980: INFO: model_training: Rank 0, Epoch 4748, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,982: INFO: model_training: Rank 0, Epoch 4748, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,983: INFO: model_training: Rank 0, Epoch 4748, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,984: INFO: model_training: Rank 0, Epoch 4748, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,987: INFO: model_training: Rank 0, Epoch 4749, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,988: INFO: model_training: Rank 0, Epoch 4749, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,990: INFO: model_training: Rank 0, Epoch 4749, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,991: INFO: model_training: Rank 0, Epoch 4749, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,992: INFO: model_training: Rank 0, Epoch 4749, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:56,993: INFO: model_training: Rank 0, Epoch 4750, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:56,996: INFO: model_training: Rank 0, Epoch 4750, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:56,997: INFO: model_training: Rank 0, Epoch 4750, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:56,998: INFO: model_training: Rank 0, Epoch 4750, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:56,999: INFO: model_training: Rank 0, Epoch 4750, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,001: INFO: model_training: Rank 0, Epoch 4751, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,002: INFO: model_training: Rank 0, Epoch 4751, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,004: INFO: model_training: Rank 0, Epoch 4751, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,006: INFO: model_training: Rank 0, Epoch 4751, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,007: INFO: model_training: Rank 0, Epoch 4751, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,009: INFO: model_training: Rank 0, Epoch 4752, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,010: INFO: model_training: Rank 0, Epoch 4752, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,012: INFO: model_training: Rank 0, Epoch 4752, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,013: INFO: model_training: Rank 0, Epoch 4752, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,015: INFO: model_training: Rank 0, Epoch 4752, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,017: INFO: model_training: Rank 0, Epoch 4753, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,018: INFO: model_training: Rank 0, Epoch 4753, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,020: INFO: model_training: Rank 0, Epoch 4753, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,021: INFO: model_training: Rank 0, Epoch 4753, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,023: INFO: model_training: Rank 0, Epoch 4753, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,024: INFO: model_training: Rank 0, Epoch 4754, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,025: INFO: model_training: Rank 0, Epoch 4754, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,027: INFO: model_training: Rank 0, Epoch 4754, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,029: INFO: model_training: Rank 0, Epoch 4754, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,030: INFO: model_training: Rank 0, Epoch 4754, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,031: INFO: model_training: Rank 0, Epoch 4755, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,032: INFO: model_training: Rank 0, Epoch 4755, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,034: INFO: model_training: Rank 0, Epoch 4755, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,036: INFO: model_training: Rank 0, Epoch 4755, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,038: INFO: model_training: Rank 0, Epoch 4755, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,039: INFO: model_training: Rank 0, Epoch 4756, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,040: INFO: model_training: Rank 0, Epoch 4756, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,042: INFO: model_training: Rank 0, Epoch 4756, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,043: INFO: model_training: Rank 0, Epoch 4756, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,045: INFO: model_training: Rank 0, Epoch 4756, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,046: INFO: model_training: Rank 0, Epoch 4757, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,047: INFO: model_training: Rank 0, Epoch 4757, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,048: INFO: model_training: Rank 0, Epoch 4757, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,049: INFO: model_training: Rank 0, Epoch 4757, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,051: INFO: model_training: Rank 0, Epoch 4757, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,052: INFO: model_training: Rank 0, Epoch 4758, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,054: INFO: model_training: Rank 0, Epoch 4758, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,056: INFO: model_training: Rank 0, Epoch 4758, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,057: INFO: model_training: Rank 0, Epoch 4758, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,058: INFO: model_training: Rank 0, Epoch 4758, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,060: INFO: model_training: Rank 0, Epoch 4759, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,061: INFO: model_training: Rank 0, Epoch 4759, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,063: INFO: model_training: Rank 0, Epoch 4759, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,064: INFO: model_training: Rank 0, Epoch 4759, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,066: INFO: model_training: Rank 0, Epoch 4759, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,067: INFO: model_training: Rank 0, Epoch 4760, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,069: INFO: model_training: Rank 0, Epoch 4760, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,071: INFO: model_training: Rank 0, Epoch 4760, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,072: INFO: model_training: Rank 0, Epoch 4760, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,073: INFO: model_training: Rank 0, Epoch 4760, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,075: INFO: model_training: Rank 0, Epoch 4761, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,077: INFO: model_training: Rank 0, Epoch 4761, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,079: INFO: model_training: Rank 0, Epoch 4761, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,080: INFO: model_training: Rank 0, Epoch 4761, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,081: INFO: model_training: Rank 0, Epoch 4761, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,082: INFO: model_training: Rank 0, Epoch 4762, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,083: INFO: model_training: Rank 0, Epoch 4762, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,085: INFO: model_training: Rank 0, Epoch 4762, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,087: INFO: model_training: Rank 0, Epoch 4762, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,088: INFO: model_training: Rank 0, Epoch 4762, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,089: INFO: model_training: Rank 0, Epoch 4763, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,090: INFO: model_training: Rank 0, Epoch 4763, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,092: INFO: model_training: Rank 0, Epoch 4763, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,094: INFO: model_training: Rank 0, Epoch 4763, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,096: INFO: model_training: Rank 0, Epoch 4763, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,097: INFO: model_training: Rank 0, Epoch 4764, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,098: INFO: model_training: Rank 0, Epoch 4764, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,099: INFO: model_training: Rank 0, Epoch 4764, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,100: INFO: model_training: Rank 0, Epoch 4764, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,103: INFO: model_training: Rank 0, Epoch 4764, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,104: INFO: model_training: Rank 0, Epoch 4765, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,105: INFO: model_training: Rank 0, Epoch 4765, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,106: INFO: model_training: Rank 0, Epoch 4765, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,107: INFO: model_training: Rank 0, Epoch 4765, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,108: INFO: model_training: Rank 0, Epoch 4765, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,110: INFO: model_training: Rank 0, Epoch 4766, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,112: INFO: model_training: Rank 0, Epoch 4766, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,113: INFO: model_training: Rank 0, Epoch 4766, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,114: INFO: model_training: Rank 0, Epoch 4766, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,115: INFO: model_training: Rank 0, Epoch 4766, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,117: INFO: model_training: Rank 0, Epoch 4767, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,118: INFO: model_training: Rank 0, Epoch 4767, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,120: INFO: model_training: Rank 0, Epoch 4767, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,121: INFO: model_training: Rank 0, Epoch 4767, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,122: INFO: model_training: Rank 0, Epoch 4767, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,124: INFO: model_training: Rank 0, Epoch 4768, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,125: INFO: model_training: Rank 0, Epoch 4768, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,127: INFO: model_training: Rank 0, Epoch 4768, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,128: INFO: model_training: Rank 0, Epoch 4768, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,130: INFO: model_training: Rank 0, Epoch 4768, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,131: INFO: model_training: Rank 0, Epoch 4769, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,132: INFO: model_training: Rank 0, Epoch 4769, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,134: INFO: model_training: Rank 0, Epoch 4769, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,136: INFO: model_training: Rank 0, Epoch 4769, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,137: INFO: model_training: Rank 0, Epoch 4769, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,138: INFO: model_training: Rank 0, Epoch 4770, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,139: INFO: model_training: Rank 0, Epoch 4770, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,140: INFO: model_training: Rank 0, Epoch 4770, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,142: INFO: model_training: Rank 0, Epoch 4770, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,144: INFO: model_training: Rank 0, Epoch 4770, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,145: INFO: model_training: Rank 0, Epoch 4771, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,146: INFO: model_training: Rank 0, Epoch 4771, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,148: INFO: model_training: Rank 0, Epoch 4771, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,149: INFO: model_training: Rank 0, Epoch 4771, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,151: INFO: model_training: Rank 0, Epoch 4771, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,153: INFO: model_training: Rank 0, Epoch 4772, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,154: INFO: model_training: Rank 0, Epoch 4772, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,156: INFO: model_training: Rank 0, Epoch 4772, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,157: INFO: model_training: Rank 0, Epoch 4772, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,158: INFO: model_training: Rank 0, Epoch 4772, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,159: INFO: model_training: Rank 0, Epoch 4773, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,161: INFO: model_training: Rank 0, Epoch 4773, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,162: INFO: model_training: Rank 0, Epoch 4773, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,164: INFO: model_training: Rank 0, Epoch 4773, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,165: INFO: model_training: Rank 0, Epoch 4773, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,167: INFO: model_training: Rank 0, Epoch 4774, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,169: INFO: model_training: Rank 0, Epoch 4774, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,170: INFO: model_training: Rank 0, Epoch 4774, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,171: INFO: model_training: Rank 0, Epoch 4774, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,172: INFO: model_training: Rank 0, Epoch 4774, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,173: INFO: model_training: Rank 0, Epoch 4775, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,175: INFO: model_training: Rank 0, Epoch 4775, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,177: INFO: model_training: Rank 0, Epoch 4775, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,178: INFO: model_training: Rank 0, Epoch 4775, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,179: INFO: model_training: Rank 0, Epoch 4775, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,181: INFO: model_training: Rank 0, Epoch 4776, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,182: INFO: model_training: Rank 0, Epoch 4776, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,183: INFO: model_training: Rank 0, Epoch 4776, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,185: INFO: model_training: Rank 0, Epoch 4776, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,186: INFO: model_training: Rank 0, Epoch 4776, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,188: INFO: model_training: Rank 0, Epoch 4777, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,189: INFO: model_training: Rank 0, Epoch 4777, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,190: INFO: model_training: Rank 0, Epoch 4777, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,192: INFO: model_training: Rank 0, Epoch 4777, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,194: INFO: model_training: Rank 0, Epoch 4777, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,195: INFO: model_training: Rank 0, Epoch 4778, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,196: INFO: model_training: Rank 0, Epoch 4778, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,197: INFO: model_training: Rank 0, Epoch 4778, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,199: INFO: model_training: Rank 0, Epoch 4778, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,200: INFO: model_training: Rank 0, Epoch 4778, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,201: INFO: model_training: Rank 0, Epoch 4779, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,203: INFO: model_training: Rank 0, Epoch 4779, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,204: INFO: model_training: Rank 0, Epoch 4779, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,205: INFO: model_training: Rank 0, Epoch 4779, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,207: INFO: model_training: Rank 0, Epoch 4779, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,209: INFO: model_training: Rank 0, Epoch 4780, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,210: INFO: model_training: Rank 0, Epoch 4780, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,211: INFO: model_training: Rank 0, Epoch 4780, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,213: INFO: model_training: Rank 0, Epoch 4780, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,214: INFO: model_training: Rank 0, Epoch 4780, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,215: INFO: model_training: Rank 0, Epoch 4781, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,216: INFO: model_training: Rank 0, Epoch 4781, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,218: INFO: model_training: Rank 0, Epoch 4781, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,220: INFO: model_training: Rank 0, Epoch 4781, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,220: INFO: model_training: Rank 0, Epoch 4781, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,222: INFO: model_training: Rank 0, Epoch 4782, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,223: INFO: model_training: Rank 0, Epoch 4782, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,224: INFO: model_training: Rank 0, Epoch 4782, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,227: INFO: model_training: Rank 0, Epoch 4782, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,228: INFO: model_training: Rank 0, Epoch 4782, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,229: INFO: model_training: Rank 0, Epoch 4783, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,230: INFO: model_training: Rank 0, Epoch 4783, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,231: INFO: model_training: Rank 0, Epoch 4783, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,232: INFO: model_training: Rank 0, Epoch 4783, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,233: INFO: model_training: Rank 0, Epoch 4783, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,235: INFO: model_training: Rank 0, Epoch 4784, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,237: INFO: model_training: Rank 0, Epoch 4784, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,238: INFO: model_training: Rank 0, Epoch 4784, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,239: INFO: model_training: Rank 0, Epoch 4784, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,240: INFO: model_training: Rank 0, Epoch 4784, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,242: INFO: model_training: Rank 0, Epoch 4785, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,243: INFO: model_training: Rank 0, Epoch 4785, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,245: INFO: model_training: Rank 0, Epoch 4785, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,247: INFO: model_training: Rank 0, Epoch 4785, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,248: INFO: model_training: Rank 0, Epoch 4785, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,249: INFO: model_training: Rank 0, Epoch 4786, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,251: INFO: model_training: Rank 0, Epoch 4786, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,252: INFO: model_training: Rank 0, Epoch 4786, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,254: INFO: model_training: Rank 0, Epoch 4786, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,255: INFO: model_training: Rank 0, Epoch 4786, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,256: INFO: model_training: Rank 0, Epoch 4787, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,257: INFO: model_training: Rank 0, Epoch 4787, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,258: INFO: model_training: Rank 0, Epoch 4787, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,260: INFO: model_training: Rank 0, Epoch 4787, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,262: INFO: model_training: Rank 0, Epoch 4787, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,263: INFO: model_training: Rank 0, Epoch 4788, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,265: INFO: model_training: Rank 0, Epoch 4788, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,266: INFO: model_training: Rank 0, Epoch 4788, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,267: INFO: model_training: Rank 0, Epoch 4788, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,269: INFO: model_training: Rank 0, Epoch 4788, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,270: INFO: model_training: Rank 0, Epoch 4789, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,272: INFO: model_training: Rank 0, Epoch 4789, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,273: INFO: model_training: Rank 0, Epoch 4789, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,274: INFO: model_training: Rank 0, Epoch 4789, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,276: INFO: model_training: Rank 0, Epoch 4789, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,278: INFO: model_training: Rank 0, Epoch 4790, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,279: INFO: model_training: Rank 0, Epoch 4790, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,280: INFO: model_training: Rank 0, Epoch 4790, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,282: INFO: model_training: Rank 0, Epoch 4790, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,283: INFO: model_training: Rank 0, Epoch 4790, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,285: INFO: model_training: Rank 0, Epoch 4791, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,286: INFO: model_training: Rank 0, Epoch 4791, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,288: INFO: model_training: Rank 0, Epoch 4791, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,289: INFO: model_training: Rank 0, Epoch 4791, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,290: INFO: model_training: Rank 0, Epoch 4791, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,292: INFO: model_training: Rank 0, Epoch 4792, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,293: INFO: model_training: Rank 0, Epoch 4792, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,295: INFO: model_training: Rank 0, Epoch 4792, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,297: INFO: model_training: Rank 0, Epoch 4792, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,298: INFO: model_training: Rank 0, Epoch 4792, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,300: INFO: model_training: Rank 0, Epoch 4793, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,301: INFO: model_training: Rank 0, Epoch 4793, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,302: INFO: model_training: Rank 0, Epoch 4793, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,304: INFO: model_training: Rank 0, Epoch 4793, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,305: INFO: model_training: Rank 0, Epoch 4793, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,307: INFO: model_training: Rank 0, Epoch 4794, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,308: INFO: model_training: Rank 0, Epoch 4794, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,310: INFO: model_training: Rank 0, Epoch 4794, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,311: INFO: model_training: Rank 0, Epoch 4794, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,312: INFO: model_training: Rank 0, Epoch 4794, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,314: INFO: model_training: Rank 0, Epoch 4795, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,315: INFO: model_training: Rank 0, Epoch 4795, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,317: INFO: model_training: Rank 0, Epoch 4795, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,319: INFO: model_training: Rank 0, Epoch 4795, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,320: INFO: model_training: Rank 0, Epoch 4795, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,321: INFO: model_training: Rank 0, Epoch 4796, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,322: INFO: model_training: Rank 0, Epoch 4796, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,323: INFO: model_training: Rank 0, Epoch 4796, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,325: INFO: model_training: Rank 0, Epoch 4796, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,328: INFO: model_training: Rank 0, Epoch 4796, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,329: INFO: model_training: Rank 0, Epoch 4797, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,330: INFO: model_training: Rank 0, Epoch 4797, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,331: INFO: model_training: Rank 0, Epoch 4797, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,332: INFO: model_training: Rank 0, Epoch 4797, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,334: INFO: model_training: Rank 0, Epoch 4797, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,335: INFO: model_training: Rank 0, Epoch 4798, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,336: INFO: model_training: Rank 0, Epoch 4798, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,337: INFO: model_training: Rank 0, Epoch 4798, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,339: INFO: model_training: Rank 0, Epoch 4798, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,340: INFO: model_training: Rank 0, Epoch 4798, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,341: INFO: model_training: Rank 0, Epoch 4799, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,342: INFO: model_training: Rank 0, Epoch 4799, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,344: INFO: model_training: Rank 0, Epoch 4799, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,345: INFO: model_training: Rank 0, Epoch 4799, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,347: INFO: model_training: Rank 0, Epoch 4799, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,348: INFO: model_training: Rank 0, Epoch 4800, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,349: INFO: model_training: Rank 0, Epoch 4800, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,351: INFO: model_training: Rank 0, Epoch 4800, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,352: INFO: model_training: Rank 0, Epoch 4800, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,353: INFO: model_training: Rank 0, Epoch 4800, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,354: INFO: model_training: Rank 0, Epoch 4801, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,355: INFO: model_training: Rank 0, Epoch 4801, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,357: INFO: model_training: Rank 0, Epoch 4801, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,358: INFO: model_training: Rank 0, Epoch 4801, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,359: INFO: model_training: Rank 0, Epoch 4801, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,361: INFO: model_training: Rank 0, Epoch 4802, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,363: INFO: model_training: Rank 0, Epoch 4802, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,365: INFO: model_training: Rank 0, Epoch 4802, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,366: INFO: model_training: Rank 0, Epoch 4802, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,367: INFO: model_training: Rank 0, Epoch 4802, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,369: INFO: model_training: Rank 0, Epoch 4803, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,370: INFO: model_training: Rank 0, Epoch 4803, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,371: INFO: model_training: Rank 0, Epoch 4803, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,372: INFO: model_training: Rank 0, Epoch 4803, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,374: INFO: model_training: Rank 0, Epoch 4803, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,375: INFO: model_training: Rank 0, Epoch 4804, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,376: INFO: model_training: Rank 0, Epoch 4804, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,378: INFO: model_training: Rank 0, Epoch 4804, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,379: INFO: model_training: Rank 0, Epoch 4804, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,380: INFO: model_training: Rank 0, Epoch 4804, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,381: INFO: model_training: Rank 0, Epoch 4805, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,383: INFO: model_training: Rank 0, Epoch 4805, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,386: INFO: model_training: Rank 0, Epoch 4805, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,387: INFO: model_training: Rank 0, Epoch 4805, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,388: INFO: model_training: Rank 0, Epoch 4805, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,389: INFO: model_training: Rank 0, Epoch 4806, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,391: INFO: model_training: Rank 0, Epoch 4806, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,392: INFO: model_training: Rank 0, Epoch 4806, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,393: INFO: model_training: Rank 0, Epoch 4806, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,395: INFO: model_training: Rank 0, Epoch 4806, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,396: INFO: model_training: Rank 0, Epoch 4807, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,397: INFO: model_training: Rank 0, Epoch 4807, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,399: INFO: model_training: Rank 0, Epoch 4807, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,400: INFO: model_training: Rank 0, Epoch 4807, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,402: INFO: model_training: Rank 0, Epoch 4807, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,403: INFO: model_training: Rank 0, Epoch 4808, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,405: INFO: model_training: Rank 0, Epoch 4808, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,406: INFO: model_training: Rank 0, Epoch 4808, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,407: INFO: model_training: Rank 0, Epoch 4808, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,408: INFO: model_training: Rank 0, Epoch 4808, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,410: INFO: model_training: Rank 0, Epoch 4809, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,411: INFO: model_training: Rank 0, Epoch 4809, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,412: INFO: model_training: Rank 0, Epoch 4809, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,414: INFO: model_training: Rank 0, Epoch 4809, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,415: INFO: model_training: Rank 0, Epoch 4809, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,417: INFO: model_training: Rank 0, Epoch 4810, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,418: INFO: model_training: Rank 0, Epoch 4810, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,420: INFO: model_training: Rank 0, Epoch 4810, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,421: INFO: model_training: Rank 0, Epoch 4810, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,422: INFO: model_training: Rank 0, Epoch 4810, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,423: INFO: model_training: Rank 0, Epoch 4811, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,425: INFO: model_training: Rank 0, Epoch 4811, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,426: INFO: model_training: Rank 0, Epoch 4811, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,428: INFO: model_training: Rank 0, Epoch 4811, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,429: INFO: model_training: Rank 0, Epoch 4811, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,430: INFO: model_training: Rank 0, Epoch 4812, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,431: INFO: model_training: Rank 0, Epoch 4812, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,433: INFO: model_training: Rank 0, Epoch 4812, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,435: INFO: model_training: Rank 0, Epoch 4812, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,436: INFO: model_training: Rank 0, Epoch 4812, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,437: INFO: model_training: Rank 0, Epoch 4813, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,439: INFO: model_training: Rank 0, Epoch 4813, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,441: INFO: model_training: Rank 0, Epoch 4813, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,442: INFO: model_training: Rank 0, Epoch 4813, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,443: INFO: model_training: Rank 0, Epoch 4813, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,445: INFO: model_training: Rank 0, Epoch 4814, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,446: INFO: model_training: Rank 0, Epoch 4814, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,447: INFO: model_training: Rank 0, Epoch 4814, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,448: INFO: model_training: Rank 0, Epoch 4814, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,450: INFO: model_training: Rank 0, Epoch 4814, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,451: INFO: model_training: Rank 0, Epoch 4815, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,453: INFO: model_training: Rank 0, Epoch 4815, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,455: INFO: model_training: Rank 0, Epoch 4815, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,456: INFO: model_training: Rank 0, Epoch 4815, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,457: INFO: model_training: Rank 0, Epoch 4815, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,460: INFO: model_training: Rank 0, Epoch 4816, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,461: INFO: model_training: Rank 0, Epoch 4816, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,462: INFO: model_training: Rank 0, Epoch 4816, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,463: INFO: model_training: Rank 0, Epoch 4816, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,465: INFO: model_training: Rank 0, Epoch 4816, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,466: INFO: model_training: Rank 0, Epoch 4817, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,467: INFO: model_training: Rank 0, Epoch 4817, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,469: INFO: model_training: Rank 0, Epoch 4817, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,471: INFO: model_training: Rank 0, Epoch 4817, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,473: INFO: model_training: Rank 0, Epoch 4817, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,475: INFO: model_training: Rank 0, Epoch 4818, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,476: INFO: model_training: Rank 0, Epoch 4818, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,480: INFO: model_training: Rank 0, Epoch 4818, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,484: INFO: model_training: Rank 0, Epoch 4818, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,488: INFO: model_training: Rank 0, Epoch 4818, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,493: INFO: model_training: Rank 0, Epoch 4819, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,502: INFO: model_training: Rank 0, Epoch 4819, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,506: INFO: model_training: Rank 0, Epoch 4819, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,508: INFO: model_training: Rank 0, Epoch 4819, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,509: INFO: model_training: Rank 0, Epoch 4819, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,513: INFO: model_training: Rank 0, Epoch 4820, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,516: INFO: model_training: Rank 0, Epoch 4820, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,518: INFO: model_training: Rank 0, Epoch 4820, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,519: INFO: model_training: Rank 0, Epoch 4820, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,521: INFO: model_training: Rank 0, Epoch 4820, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,522: INFO: model_training: Rank 0, Epoch 4821, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,524: INFO: model_training: Rank 0, Epoch 4821, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,525: INFO: model_training: Rank 0, Epoch 4821, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,527: INFO: model_training: Rank 0, Epoch 4821, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,528: INFO: model_training: Rank 0, Epoch 4821, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,529: INFO: model_training: Rank 0, Epoch 4822, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,530: INFO: model_training: Rank 0, Epoch 4822, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,532: INFO: model_training: Rank 0, Epoch 4822, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,533: INFO: model_training: Rank 0, Epoch 4822, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,535: INFO: model_training: Rank 0, Epoch 4822, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,536: INFO: model_training: Rank 0, Epoch 4823, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,537: INFO: model_training: Rank 0, Epoch 4823, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,538: INFO: model_training: Rank 0, Epoch 4823, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,540: INFO: model_training: Rank 0, Epoch 4823, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,541: INFO: model_training: Rank 0, Epoch 4823, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,542: INFO: model_training: Rank 0, Epoch 4824, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,544: INFO: model_training: Rank 0, Epoch 4824, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,546: INFO: model_training: Rank 0, Epoch 4824, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,547: INFO: model_training: Rank 0, Epoch 4824, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,548: INFO: model_training: Rank 0, Epoch 4824, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,549: INFO: model_training: Rank 0, Epoch 4825, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,550: INFO: model_training: Rank 0, Epoch 4825, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,552: INFO: model_training: Rank 0, Epoch 4825, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,553: INFO: model_training: Rank 0, Epoch 4825, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,555: INFO: model_training: Rank 0, Epoch 4825, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,556: INFO: model_training: Rank 0, Epoch 4826, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,557: INFO: model_training: Rank 0, Epoch 4826, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,558: INFO: model_training: Rank 0, Epoch 4826, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,561: INFO: model_training: Rank 0, Epoch 4826, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,562: INFO: model_training: Rank 0, Epoch 4826, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,563: INFO: model_training: Rank 0, Epoch 4827, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,565: INFO: model_training: Rank 0, Epoch 4827, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,566: INFO: model_training: Rank 0, Epoch 4827, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,567: INFO: model_training: Rank 0, Epoch 4827, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,569: INFO: model_training: Rank 0, Epoch 4827, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,570: INFO: model_training: Rank 0, Epoch 4828, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,571: INFO: model_training: Rank 0, Epoch 4828, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,573: INFO: model_training: Rank 0, Epoch 4828, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,574: INFO: model_training: Rank 0, Epoch 4828, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,575: INFO: model_training: Rank 0, Epoch 4828, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,577: INFO: model_training: Rank 0, Epoch 4829, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,578: INFO: model_training: Rank 0, Epoch 4829, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,579: INFO: model_training: Rank 0, Epoch 4829, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,581: INFO: model_training: Rank 0, Epoch 4829, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,582: INFO: model_training: Rank 0, Epoch 4829, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,583: INFO: model_training: Rank 0, Epoch 4830, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,585: INFO: model_training: Rank 0, Epoch 4830, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,586: INFO: model_training: Rank 0, Epoch 4830, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,587: INFO: model_training: Rank 0, Epoch 4830, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,589: INFO: model_training: Rank 0, Epoch 4830, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,590: INFO: model_training: Rank 0, Epoch 4831, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,591: INFO: model_training: Rank 0, Epoch 4831, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,592: INFO: model_training: Rank 0, Epoch 4831, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,593: INFO: model_training: Rank 0, Epoch 4831, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,595: INFO: model_training: Rank 0, Epoch 4831, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,597: INFO: model_training: Rank 0, Epoch 4832, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,599: INFO: model_training: Rank 0, Epoch 4832, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,600: INFO: model_training: Rank 0, Epoch 4832, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,603: INFO: model_training: Rank 0, Epoch 4832, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,604: INFO: model_training: Rank 0, Epoch 4832, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,605: INFO: model_training: Rank 0, Epoch 4833, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,607: INFO: model_training: Rank 0, Epoch 4833, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,608: INFO: model_training: Rank 0, Epoch 4833, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,609: INFO: model_training: Rank 0, Epoch 4833, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,611: INFO: model_training: Rank 0, Epoch 4833, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,612: INFO: model_training: Rank 0, Epoch 4834, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,613: INFO: model_training: Rank 0, Epoch 4834, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,614: INFO: model_training: Rank 0, Epoch 4834, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,615: INFO: model_training: Rank 0, Epoch 4834, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,617: INFO: model_training: Rank 0, Epoch 4834, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,619: INFO: model_training: Rank 0, Epoch 4835, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,620: INFO: model_training: Rank 0, Epoch 4835, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,621: INFO: model_training: Rank 0, Epoch 4835, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,622: INFO: model_training: Rank 0, Epoch 4835, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,624: INFO: model_training: Rank 0, Epoch 4835, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,625: INFO: model_training: Rank 0, Epoch 4836, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,626: INFO: model_training: Rank 0, Epoch 4836, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,628: INFO: model_training: Rank 0, Epoch 4836, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,629: INFO: model_training: Rank 0, Epoch 4836, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,630: INFO: model_training: Rank 0, Epoch 4836, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,632: INFO: model_training: Rank 0, Epoch 4837, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,633: INFO: model_training: Rank 0, Epoch 4837, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,634: INFO: model_training: Rank 0, Epoch 4837, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,636: INFO: model_training: Rank 0, Epoch 4837, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,638: INFO: model_training: Rank 0, Epoch 4837, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,639: INFO: model_training: Rank 0, Epoch 4838, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,640: INFO: model_training: Rank 0, Epoch 4838, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,641: INFO: model_training: Rank 0, Epoch 4838, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,643: INFO: model_training: Rank 0, Epoch 4838, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,645: INFO: model_training: Rank 0, Epoch 4838, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,646: INFO: model_training: Rank 0, Epoch 4839, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,647: INFO: model_training: Rank 0, Epoch 4839, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,648: INFO: model_training: Rank 0, Epoch 4839, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,650: INFO: model_training: Rank 0, Epoch 4839, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,651: INFO: model_training: Rank 0, Epoch 4839, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,653: INFO: model_training: Rank 0, Epoch 4840, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,654: INFO: model_training: Rank 0, Epoch 4840, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,655: INFO: model_training: Rank 0, Epoch 4840, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,657: INFO: model_training: Rank 0, Epoch 4840, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,658: INFO: model_training: Rank 0, Epoch 4840, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,659: INFO: model_training: Rank 0, Epoch 4841, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,661: INFO: model_training: Rank 0, Epoch 4841, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,662: INFO: model_training: Rank 0, Epoch 4841, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,663: INFO: model_training: Rank 0, Epoch 4841, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,664: INFO: model_training: Rank 0, Epoch 4841, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,665: INFO: model_training: Rank 0, Epoch 4842, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,667: INFO: model_training: Rank 0, Epoch 4842, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,668: INFO: model_training: Rank 0, Epoch 4842, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,670: INFO: model_training: Rank 0, Epoch 4842, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,671: INFO: model_training: Rank 0, Epoch 4842, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,673: INFO: model_training: Rank 0, Epoch 4843, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,675: INFO: model_training: Rank 0, Epoch 4843, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,676: INFO: model_training: Rank 0, Epoch 4843, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,677: INFO: model_training: Rank 0, Epoch 4843, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,678: INFO: model_training: Rank 0, Epoch 4843, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,679: INFO: model_training: Rank 0, Epoch 4844, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,681: INFO: model_training: Rank 0, Epoch 4844, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,682: INFO: model_training: Rank 0, Epoch 4844, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,683: INFO: model_training: Rank 0, Epoch 4844, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,684: INFO: model_training: Rank 0, Epoch 4844, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,686: INFO: model_training: Rank 0, Epoch 4845, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,687: INFO: model_training: Rank 0, Epoch 4845, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,688: INFO: model_training: Rank 0, Epoch 4845, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,689: INFO: model_training: Rank 0, Epoch 4845, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,690: INFO: model_training: Rank 0, Epoch 4845, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,692: INFO: model_training: Rank 0, Epoch 4846, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,694: INFO: model_training: Rank 0, Epoch 4846, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,695: INFO: model_training: Rank 0, Epoch 4846, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,697: INFO: model_training: Rank 0, Epoch 4846, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,698: INFO: model_training: Rank 0, Epoch 4846, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,699: INFO: model_training: Rank 0, Epoch 4847, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,699: INFO: model_training: Rank 0, Epoch 4847, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,701: INFO: model_training: Rank 0, Epoch 4847, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,702: INFO: model_training: Rank 0, Epoch 4847, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,703: INFO: model_training: Rank 0, Epoch 4847, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,704: INFO: model_training: Rank 0, Epoch 4848, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,705: INFO: model_training: Rank 0, Epoch 4848, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,707: INFO: model_training: Rank 0, Epoch 4848, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,708: INFO: model_training: Rank 0, Epoch 4848, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,710: INFO: model_training: Rank 0, Epoch 4848, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,711: INFO: model_training: Rank 0, Epoch 4849, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,712: INFO: model_training: Rank 0, Epoch 4849, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,713: INFO: model_training: Rank 0, Epoch 4849, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,715: INFO: model_training: Rank 0, Epoch 4849, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,716: INFO: model_training: Rank 0, Epoch 4849, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,717: INFO: model_training: Rank 0, Epoch 4850, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,719: INFO: model_training: Rank 0, Epoch 4850, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,720: INFO: model_training: Rank 0, Epoch 4850, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,721: INFO: model_training: Rank 0, Epoch 4850, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,722: INFO: model_training: Rank 0, Epoch 4850, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,723: INFO: model_training: Rank 0, Epoch 4851, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,725: INFO: model_training: Rank 0, Epoch 4851, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,726: INFO: model_training: Rank 0, Epoch 4851, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,728: INFO: model_training: Rank 0, Epoch 4851, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,730: INFO: model_training: Rank 0, Epoch 4851, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,731: INFO: model_training: Rank 0, Epoch 4852, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,732: INFO: model_training: Rank 0, Epoch 4852, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,733: INFO: model_training: Rank 0, Epoch 4852, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,734: INFO: model_training: Rank 0, Epoch 4852, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,736: INFO: model_training: Rank 0, Epoch 4852, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,738: INFO: model_training: Rank 0, Epoch 4853, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,739: INFO: model_training: Rank 0, Epoch 4853, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,740: INFO: model_training: Rank 0, Epoch 4853, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,741: INFO: model_training: Rank 0, Epoch 4853, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,742: INFO: model_training: Rank 0, Epoch 4853, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,745: INFO: model_training: Rank 0, Epoch 4854, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,746: INFO: model_training: Rank 0, Epoch 4854, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,748: INFO: model_training: Rank 0, Epoch 4854, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,749: INFO: model_training: Rank 0, Epoch 4854, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,750: INFO: model_training: Rank 0, Epoch 4854, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,752: INFO: model_training: Rank 0, Epoch 4855, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,753: INFO: model_training: Rank 0, Epoch 4855, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,754: INFO: model_training: Rank 0, Epoch 4855, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,756: INFO: model_training: Rank 0, Epoch 4855, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,757: INFO: model_training: Rank 0, Epoch 4855, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,758: INFO: model_training: Rank 0, Epoch 4856, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,760: INFO: model_training: Rank 0, Epoch 4856, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,761: INFO: model_training: Rank 0, Epoch 4856, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,762: INFO: model_training: Rank 0, Epoch 4856, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,763: INFO: model_training: Rank 0, Epoch 4856, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,765: INFO: model_training: Rank 0, Epoch 4857, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,766: INFO: model_training: Rank 0, Epoch 4857, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,767: INFO: model_training: Rank 0, Epoch 4857, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,770: INFO: model_training: Rank 0, Epoch 4857, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,771: INFO: model_training: Rank 0, Epoch 4857, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,772: INFO: model_training: Rank 0, Epoch 4858, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,773: INFO: model_training: Rank 0, Epoch 4858, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,775: INFO: model_training: Rank 0, Epoch 4858, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,776: INFO: model_training: Rank 0, Epoch 4858, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,777: INFO: model_training: Rank 0, Epoch 4858, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,779: INFO: model_training: Rank 0, Epoch 4859, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,780: INFO: model_training: Rank 0, Epoch 4859, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,781: INFO: model_training: Rank 0, Epoch 4859, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,782: INFO: model_training: Rank 0, Epoch 4859, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,784: INFO: model_training: Rank 0, Epoch 4859, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,786: INFO: model_training: Rank 0, Epoch 4860, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,787: INFO: model_training: Rank 0, Epoch 4860, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,789: INFO: model_training: Rank 0, Epoch 4860, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,790: INFO: model_training: Rank 0, Epoch 4860, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,791: INFO: model_training: Rank 0, Epoch 4860, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,793: INFO: model_training: Rank 0, Epoch 4861, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,795: INFO: model_training: Rank 0, Epoch 4861, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,796: INFO: model_training: Rank 0, Epoch 4861, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,797: INFO: model_training: Rank 0, Epoch 4861, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,798: INFO: model_training: Rank 0, Epoch 4861, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,799: INFO: model_training: Rank 0, Epoch 4862, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,800: INFO: model_training: Rank 0, Epoch 4862, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,801: INFO: model_training: Rank 0, Epoch 4862, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,803: INFO: model_training: Rank 0, Epoch 4862, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,804: INFO: model_training: Rank 0, Epoch 4862, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,806: INFO: model_training: Rank 0, Epoch 4863, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,807: INFO: model_training: Rank 0, Epoch 4863, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,808: INFO: model_training: Rank 0, Epoch 4863, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,810: INFO: model_training: Rank 0, Epoch 4863, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,811: INFO: model_training: Rank 0, Epoch 4863, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,813: INFO: model_training: Rank 0, Epoch 4864, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,814: INFO: model_training: Rank 0, Epoch 4864, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,815: INFO: model_training: Rank 0, Epoch 4864, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,816: INFO: model_training: Rank 0, Epoch 4864, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,817: INFO: model_training: Rank 0, Epoch 4864, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,819: INFO: model_training: Rank 0, Epoch 4865, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,820: INFO: model_training: Rank 0, Epoch 4865, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,821: INFO: model_training: Rank 0, Epoch 4865, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,822: INFO: model_training: Rank 0, Epoch 4865, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,824: INFO: model_training: Rank 0, Epoch 4865, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,825: INFO: model_training: Rank 0, Epoch 4866, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,827: INFO: model_training: Rank 0, Epoch 4866, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,829: INFO: model_training: Rank 0, Epoch 4866, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,830: INFO: model_training: Rank 0, Epoch 4866, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,831: INFO: model_training: Rank 0, Epoch 4866, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,832: INFO: model_training: Rank 0, Epoch 4867, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,833: INFO: model_training: Rank 0, Epoch 4867, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,835: INFO: model_training: Rank 0, Epoch 4867, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,837: INFO: model_training: Rank 0, Epoch 4867, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,838: INFO: model_training: Rank 0, Epoch 4867, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,839: INFO: model_training: Rank 0, Epoch 4868, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,840: INFO: model_training: Rank 0, Epoch 4868, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,841: INFO: model_training: Rank 0, Epoch 4868, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,842: INFO: model_training: Rank 0, Epoch 4868, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,844: INFO: model_training: Rank 0, Epoch 4868, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,846: INFO: model_training: Rank 0, Epoch 4869, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,847: INFO: model_training: Rank 0, Epoch 4869, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,848: INFO: model_training: Rank 0, Epoch 4869, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,849: INFO: model_training: Rank 0, Epoch 4869, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,851: INFO: model_training: Rank 0, Epoch 4869, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,853: INFO: model_training: Rank 0, Epoch 4870, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,854: INFO: model_training: Rank 0, Epoch 4870, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,855: INFO: model_training: Rank 0, Epoch 4870, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,857: INFO: model_training: Rank 0, Epoch 4870, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,858: INFO: model_training: Rank 0, Epoch 4870, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,859: INFO: model_training: Rank 0, Epoch 4871, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,860: INFO: model_training: Rank 0, Epoch 4871, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,861: INFO: model_training: Rank 0, Epoch 4871, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,863: INFO: model_training: Rank 0, Epoch 4871, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,864: INFO: model_training: Rank 0, Epoch 4871, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,866: INFO: model_training: Rank 0, Epoch 4872, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,867: INFO: model_training: Rank 0, Epoch 4872, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,869: INFO: model_training: Rank 0, Epoch 4872, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,870: INFO: model_training: Rank 0, Epoch 4872, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,871: INFO: model_training: Rank 0, Epoch 4872, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,873: INFO: model_training: Rank 0, Epoch 4873, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,874: INFO: model_training: Rank 0, Epoch 4873, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,875: INFO: model_training: Rank 0, Epoch 4873, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,877: INFO: model_training: Rank 0, Epoch 4873, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,879: INFO: model_training: Rank 0, Epoch 4873, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,880: INFO: model_training: Rank 0, Epoch 4874, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,881: INFO: model_training: Rank 0, Epoch 4874, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,882: INFO: model_training: Rank 0, Epoch 4874, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,883: INFO: model_training: Rank 0, Epoch 4874, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,885: INFO: model_training: Rank 0, Epoch 4874, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,887: INFO: model_training: Rank 0, Epoch 4875, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,888: INFO: model_training: Rank 0, Epoch 4875, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,890: INFO: model_training: Rank 0, Epoch 4875, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,891: INFO: model_training: Rank 0, Epoch 4875, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,892: INFO: model_training: Rank 0, Epoch 4875, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,893: INFO: model_training: Rank 0, Epoch 4876, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,895: INFO: model_training: Rank 0, Epoch 4876, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,896: INFO: model_training: Rank 0, Epoch 4876, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,897: INFO: model_training: Rank 0, Epoch 4876, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,898: INFO: model_training: Rank 0, Epoch 4876, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,899: INFO: model_training: Rank 0, Epoch 4877, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,901: INFO: model_training: Rank 0, Epoch 4877, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,903: INFO: model_training: Rank 0, Epoch 4877, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,904: INFO: model_training: Rank 0, Epoch 4877, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,906: INFO: model_training: Rank 0, Epoch 4877, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,907: INFO: model_training: Rank 0, Epoch 4878, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,909: INFO: model_training: Rank 0, Epoch 4878, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,910: INFO: model_training: Rank 0, Epoch 4878, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,911: INFO: model_training: Rank 0, Epoch 4878, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,913: INFO: model_training: Rank 0, Epoch 4878, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,914: INFO: model_training: Rank 0, Epoch 4879, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,915: INFO: model_training: Rank 0, Epoch 4879, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,916: INFO: model_training: Rank 0, Epoch 4879, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,917: INFO: model_training: Rank 0, Epoch 4879, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,918: INFO: model_training: Rank 0, Epoch 4879, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,920: INFO: model_training: Rank 0, Epoch 4880, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,921: INFO: model_training: Rank 0, Epoch 4880, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,922: INFO: model_training: Rank 0, Epoch 4880, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,923: INFO: model_training: Rank 0, Epoch 4880, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,925: INFO: model_training: Rank 0, Epoch 4880, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,927: INFO: model_training: Rank 0, Epoch 4881, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,928: INFO: model_training: Rank 0, Epoch 4881, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,929: INFO: model_training: Rank 0, Epoch 4881, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,931: INFO: model_training: Rank 0, Epoch 4881, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,932: INFO: model_training: Rank 0, Epoch 4881, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,933: INFO: model_training: Rank 0, Epoch 4882, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,935: INFO: model_training: Rank 0, Epoch 4882, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,936: INFO: model_training: Rank 0, Epoch 4882, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,937: INFO: model_training: Rank 0, Epoch 4882, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,938: INFO: model_training: Rank 0, Epoch 4882, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,939: INFO: model_training: Rank 0, Epoch 4883, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,940: INFO: model_training: Rank 0, Epoch 4883, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,942: INFO: model_training: Rank 0, Epoch 4883, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,944: INFO: model_training: Rank 0, Epoch 4883, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,946: INFO: model_training: Rank 0, Epoch 4883, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,947: INFO: model_training: Rank 0, Epoch 4884, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,948: INFO: model_training: Rank 0, Epoch 4884, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,949: INFO: model_training: Rank 0, Epoch 4884, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,951: INFO: model_training: Rank 0, Epoch 4884, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,952: INFO: model_training: Rank 0, Epoch 4884, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,954: INFO: model_training: Rank 0, Epoch 4885, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,955: INFO: model_training: Rank 0, Epoch 4885, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,956: INFO: model_training: Rank 0, Epoch 4885, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,957: INFO: model_training: Rank 0, Epoch 4885, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,959: INFO: model_training: Rank 0, Epoch 4885, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,961: INFO: model_training: Rank 0, Epoch 4886, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,962: INFO: model_training: Rank 0, Epoch 4886, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,963: INFO: model_training: Rank 0, Epoch 4886, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,965: INFO: model_training: Rank 0, Epoch 4886, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,966: INFO: model_training: Rank 0, Epoch 4886, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,968: INFO: model_training: Rank 0, Epoch 4887, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,970: INFO: model_training: Rank 0, Epoch 4887, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,971: INFO: model_training: Rank 0, Epoch 4887, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,972: INFO: model_training: Rank 0, Epoch 4887, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,973: INFO: model_training: Rank 0, Epoch 4887, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,974: INFO: model_training: Rank 0, Epoch 4888, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,976: INFO: model_training: Rank 0, Epoch 4888, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,977: INFO: model_training: Rank 0, Epoch 4888, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,978: INFO: model_training: Rank 0, Epoch 4888, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,979: INFO: model_training: Rank 0, Epoch 4888, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,981: INFO: model_training: Rank 0, Epoch 4889, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,982: INFO: model_training: Rank 0, Epoch 4889, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,984: INFO: model_training: Rank 0, Epoch 4889, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,985: INFO: model_training: Rank 0, Epoch 4889, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,987: INFO: model_training: Rank 0, Epoch 4889, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,988: INFO: model_training: Rank 0, Epoch 4890, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,989: INFO: model_training: Rank 0, Epoch 4890, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,991: INFO: model_training: Rank 0, Epoch 4890, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,992: INFO: model_training: Rank 0, Epoch 4890, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:57,994: INFO: model_training: Rank 0, Epoch 4890, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:57,995: INFO: model_training: Rank 0, Epoch 4891, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:57,996: INFO: model_training: Rank 0, Epoch 4891, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:57,997: INFO: model_training: Rank 0, Epoch 4891, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:57,998: INFO: model_training: Rank 0, Epoch 4891, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,000: INFO: model_training: Rank 0, Epoch 4891, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,002: INFO: model_training: Rank 0, Epoch 4892, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,003: INFO: model_training: Rank 0, Epoch 4892, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,005: INFO: model_training: Rank 0, Epoch 4892, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,007: INFO: model_training: Rank 0, Epoch 4892, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,008: INFO: model_training: Rank 0, Epoch 4892, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,009: INFO: model_training: Rank 0, Epoch 4893, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,011: INFO: model_training: Rank 0, Epoch 4893, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,012: INFO: model_training: Rank 0, Epoch 4893, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,014: INFO: model_training: Rank 0, Epoch 4893, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,015: INFO: model_training: Rank 0, Epoch 4893, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,017: INFO: model_training: Rank 0, Epoch 4894, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,018: INFO: model_training: Rank 0, Epoch 4894, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,020: INFO: model_training: Rank 0, Epoch 4894, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,021: INFO: model_training: Rank 0, Epoch 4894, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,022: INFO: model_training: Rank 0, Epoch 4894, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,023: INFO: model_training: Rank 0, Epoch 4895, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,025: INFO: model_training: Rank 0, Epoch 4895, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,027: INFO: model_training: Rank 0, Epoch 4895, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,029: INFO: model_training: Rank 0, Epoch 4895, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,030: INFO: model_training: Rank 0, Epoch 4895, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,031: INFO: model_training: Rank 0, Epoch 4896, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,032: INFO: model_training: Rank 0, Epoch 4896, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,034: INFO: model_training: Rank 0, Epoch 4896, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,035: INFO: model_training: Rank 0, Epoch 4896, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,037: INFO: model_training: Rank 0, Epoch 4896, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,038: INFO: model_training: Rank 0, Epoch 4897, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,039: INFO: model_training: Rank 0, Epoch 4897, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,040: INFO: model_training: Rank 0, Epoch 4897, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,041: INFO: model_training: Rank 0, Epoch 4897, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,043: INFO: model_training: Rank 0, Epoch 4897, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,045: INFO: model_training: Rank 0, Epoch 4898, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,046: INFO: model_training: Rank 0, Epoch 4898, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,047: INFO: model_training: Rank 0, Epoch 4898, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,048: INFO: model_training: Rank 0, Epoch 4898, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,050: INFO: model_training: Rank 0, Epoch 4898, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,051: INFO: model_training: Rank 0, Epoch 4899, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,053: INFO: model_training: Rank 0, Epoch 4899, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,054: INFO: model_training: Rank 0, Epoch 4899, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,056: INFO: model_training: Rank 0, Epoch 4899, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,057: INFO: model_training: Rank 0, Epoch 4899, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,058: INFO: model_training: Rank 0, Epoch 4900, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,060: INFO: model_training: Rank 0, Epoch 4900, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,062: INFO: model_training: Rank 0, Epoch 4900, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,063: INFO: model_training: Rank 0, Epoch 4900, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,064: INFO: model_training: Rank 0, Epoch 4900, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,066: INFO: model_training: Rank 0, Epoch 4901, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,067: INFO: model_training: Rank 0, Epoch 4901, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,069: INFO: model_training: Rank 0, Epoch 4901, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,070: INFO: model_training: Rank 0, Epoch 4901, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,071: INFO: model_training: Rank 0, Epoch 4901, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,072: INFO: model_training: Rank 0, Epoch 4902, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,074: INFO: model_training: Rank 0, Epoch 4902, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,075: INFO: model_training: Rank 0, Epoch 4902, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,077: INFO: model_training: Rank 0, Epoch 4902, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,078: INFO: model_training: Rank 0, Epoch 4902, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,079: INFO: model_training: Rank 0, Epoch 4903, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,081: INFO: model_training: Rank 0, Epoch 4903, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,082: INFO: model_training: Rank 0, Epoch 4903, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,084: INFO: model_training: Rank 0, Epoch 4903, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,086: INFO: model_training: Rank 0, Epoch 4903, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,087: INFO: model_training: Rank 0, Epoch 4904, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,088: INFO: model_training: Rank 0, Epoch 4904, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,089: INFO: model_training: Rank 0, Epoch 4904, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,090: INFO: model_training: Rank 0, Epoch 4904, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,092: INFO: model_training: Rank 0, Epoch 4904, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,093: INFO: model_training: Rank 0, Epoch 4905, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,095: INFO: model_training: Rank 0, Epoch 4905, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,096: INFO: model_training: Rank 0, Epoch 4905, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,097: INFO: model_training: Rank 0, Epoch 4905, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,098: INFO: model_training: Rank 0, Epoch 4905, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,100: INFO: model_training: Rank 0, Epoch 4906, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,103: INFO: model_training: Rank 0, Epoch 4906, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,104: INFO: model_training: Rank 0, Epoch 4906, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,105: INFO: model_training: Rank 0, Epoch 4906, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,107: INFO: model_training: Rank 0, Epoch 4906, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,108: INFO: model_training: Rank 0, Epoch 4907, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,109: INFO: model_training: Rank 0, Epoch 4907, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,111: INFO: model_training: Rank 0, Epoch 4907, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,112: INFO: model_training: Rank 0, Epoch 4907, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,113: INFO: model_training: Rank 0, Epoch 4907, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,115: INFO: model_training: Rank 0, Epoch 4908, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,116: INFO: model_training: Rank 0, Epoch 4908, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,117: INFO: model_training: Rank 0, Epoch 4908, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,118: INFO: model_training: Rank 0, Epoch 4908, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,120: INFO: model_training: Rank 0, Epoch 4908, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,122: INFO: model_training: Rank 0, Epoch 4909, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,123: INFO: model_training: Rank 0, Epoch 4909, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,124: INFO: model_training: Rank 0, Epoch 4909, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,126: INFO: model_training: Rank 0, Epoch 4909, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,128: INFO: model_training: Rank 0, Epoch 4909, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,129: INFO: model_training: Rank 0, Epoch 4910, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,130: INFO: model_training: Rank 0, Epoch 4910, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,132: INFO: model_training: Rank 0, Epoch 4910, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,133: INFO: model_training: Rank 0, Epoch 4910, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,134: INFO: model_training: Rank 0, Epoch 4910, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,136: INFO: model_training: Rank 0, Epoch 4911, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,138: INFO: model_training: Rank 0, Epoch 4911, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,139: INFO: model_training: Rank 0, Epoch 4911, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,141: INFO: model_training: Rank 0, Epoch 4911, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,142: INFO: model_training: Rank 0, Epoch 4911, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,143: INFO: model_training: Rank 0, Epoch 4912, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,144: INFO: model_training: Rank 0, Epoch 4912, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,146: INFO: model_training: Rank 0, Epoch 4912, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,147: INFO: model_training: Rank 0, Epoch 4912, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,148: INFO: model_training: Rank 0, Epoch 4912, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,150: INFO: model_training: Rank 0, Epoch 4913, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,151: INFO: model_training: Rank 0, Epoch 4913, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,153: INFO: model_training: Rank 0, Epoch 4913, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,154: INFO: model_training: Rank 0, Epoch 4913, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,156: INFO: model_training: Rank 0, Epoch 4913, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,157: INFO: model_training: Rank 0, Epoch 4914, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,158: INFO: model_training: Rank 0, Epoch 4914, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,160: INFO: model_training: Rank 0, Epoch 4914, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,162: INFO: model_training: Rank 0, Epoch 4914, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,163: INFO: model_training: Rank 0, Epoch 4914, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,164: INFO: model_training: Rank 0, Epoch 4915, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,165: INFO: model_training: Rank 0, Epoch 4915, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,167: INFO: model_training: Rank 0, Epoch 4915, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,168: INFO: model_training: Rank 0, Epoch 4915, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,170: INFO: model_training: Rank 0, Epoch 4915, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,171: INFO: model_training: Rank 0, Epoch 4916, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,172: INFO: model_training: Rank 0, Epoch 4916, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,173: INFO: model_training: Rank 0, Epoch 4916, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,175: INFO: model_training: Rank 0, Epoch 4916, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,176: INFO: model_training: Rank 0, Epoch 4916, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,177: INFO: model_training: Rank 0, Epoch 4917, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,179: INFO: model_training: Rank 0, Epoch 4917, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,180: INFO: model_training: Rank 0, Epoch 4917, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,182: INFO: model_training: Rank 0, Epoch 4917, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,183: INFO: model_training: Rank 0, Epoch 4917, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,184: INFO: model_training: Rank 0, Epoch 4918, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,186: INFO: model_training: Rank 0, Epoch 4918, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,188: INFO: model_training: Rank 0, Epoch 4918, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,189: INFO: model_training: Rank 0, Epoch 4918, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,190: INFO: model_training: Rank 0, Epoch 4918, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,192: INFO: model_training: Rank 0, Epoch 4919, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,193: INFO: model_training: Rank 0, Epoch 4919, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,195: INFO: model_training: Rank 0, Epoch 4919, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,196: INFO: model_training: Rank 0, Epoch 4919, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,197: INFO: model_training: Rank 0, Epoch 4919, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,199: INFO: model_training: Rank 0, Epoch 4920, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,200: INFO: model_training: Rank 0, Epoch 4920, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,202: INFO: model_training: Rank 0, Epoch 4920, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,203: INFO: model_training: Rank 0, Epoch 4920, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,204: INFO: model_training: Rank 0, Epoch 4920, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,206: INFO: model_training: Rank 0, Epoch 4921, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,207: INFO: model_training: Rank 0, Epoch 4921, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,208: INFO: model_training: Rank 0, Epoch 4921, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,210: INFO: model_training: Rank 0, Epoch 4921, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,212: INFO: model_training: Rank 0, Epoch 4921, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,213: INFO: model_training: Rank 0, Epoch 4922, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,214: INFO: model_training: Rank 0, Epoch 4922, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,215: INFO: model_training: Rank 0, Epoch 4922, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,217: INFO: model_training: Rank 0, Epoch 4922, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,219: INFO: model_training: Rank 0, Epoch 4922, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,221: INFO: model_training: Rank 0, Epoch 4923, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,222: INFO: model_training: Rank 0, Epoch 4923, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,223: INFO: model_training: Rank 0, Epoch 4923, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,225: INFO: model_training: Rank 0, Epoch 4923, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,226: INFO: model_training: Rank 0, Epoch 4923, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,228: INFO: model_training: Rank 0, Epoch 4924, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,229: INFO: model_training: Rank 0, Epoch 4924, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,230: INFO: model_training: Rank 0, Epoch 4924, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,232: INFO: model_training: Rank 0, Epoch 4924, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,233: INFO: model_training: Rank 0, Epoch 4924, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,234: INFO: model_training: Rank 0, Epoch 4925, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,236: INFO: model_training: Rank 0, Epoch 4925, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,238: INFO: model_training: Rank 0, Epoch 4925, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,239: INFO: model_training: Rank 0, Epoch 4925, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,241: INFO: model_training: Rank 0, Epoch 4925, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,242: INFO: model_training: Rank 0, Epoch 4926, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,243: INFO: model_training: Rank 0, Epoch 4926, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,245: INFO: model_training: Rank 0, Epoch 4926, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,246: INFO: model_training: Rank 0, Epoch 4926, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,247: INFO: model_training: Rank 0, Epoch 4926, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,249: INFO: model_training: Rank 0, Epoch 4927, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,250: INFO: model_training: Rank 0, Epoch 4927, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,252: INFO: model_training: Rank 0, Epoch 4927, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,253: INFO: model_training: Rank 0, Epoch 4927, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,254: INFO: model_training: Rank 0, Epoch 4927, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,256: INFO: model_training: Rank 0, Epoch 4928, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,257: INFO: model_training: Rank 0, Epoch 4928, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,258: INFO: model_training: Rank 0, Epoch 4928, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,260: INFO: model_training: Rank 0, Epoch 4928, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,261: INFO: model_training: Rank 0, Epoch 4928, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,262: INFO: model_training: Rank 0, Epoch 4929, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,263: INFO: model_training: Rank 0, Epoch 4929, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,264: INFO: model_training: Rank 0, Epoch 4929, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,266: INFO: model_training: Rank 0, Epoch 4929, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,267: INFO: model_training: Rank 0, Epoch 4929, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,269: INFO: model_training: Rank 0, Epoch 4930, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,274: INFO: model_training: Rank 0, Epoch 4930, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,280: INFO: model_training: Rank 0, Epoch 4930, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,285: INFO: model_training: Rank 0, Epoch 4930, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,296: INFO: model_training: Rank 0, Epoch 4930, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,299: INFO: model_training: Rank 0, Epoch 4931, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,301: INFO: model_training: Rank 0, Epoch 4931, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,304: INFO: model_training: Rank 0, Epoch 4931, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,308: INFO: model_training: Rank 0, Epoch 4931, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,311: INFO: model_training: Rank 0, Epoch 4931, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,313: INFO: model_training: Rank 0, Epoch 4932, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,314: INFO: model_training: Rank 0, Epoch 4932, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,316: INFO: model_training: Rank 0, Epoch 4932, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,317: INFO: model_training: Rank 0, Epoch 4932, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,319: INFO: model_training: Rank 0, Epoch 4932, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,321: INFO: model_training: Rank 0, Epoch 4933, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,322: INFO: model_training: Rank 0, Epoch 4933, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,324: INFO: model_training: Rank 0, Epoch 4933, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,325: INFO: model_training: Rank 0, Epoch 4933, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,327: INFO: model_training: Rank 0, Epoch 4933, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,329: INFO: model_training: Rank 0, Epoch 4934, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,331: INFO: model_training: Rank 0, Epoch 4934, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,332: INFO: model_training: Rank 0, Epoch 4934, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,334: INFO: model_training: Rank 0, Epoch 4934, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,335: INFO: model_training: Rank 0, Epoch 4934, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,337: INFO: model_training: Rank 0, Epoch 4935, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,338: INFO: model_training: Rank 0, Epoch 4935, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,340: INFO: model_training: Rank 0, Epoch 4935, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,341: INFO: model_training: Rank 0, Epoch 4935, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,342: INFO: model_training: Rank 0, Epoch 4935, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,344: INFO: model_training: Rank 0, Epoch 4936, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,345: INFO: model_training: Rank 0, Epoch 4936, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,346: INFO: model_training: Rank 0, Epoch 4936, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,347: INFO: model_training: Rank 0, Epoch 4936, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,349: INFO: model_training: Rank 0, Epoch 4936, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,350: INFO: model_training: Rank 0, Epoch 4937, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,353: INFO: model_training: Rank 0, Epoch 4937, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,354: INFO: model_training: Rank 0, Epoch 4937, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,356: INFO: model_training: Rank 0, Epoch 4937, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,357: INFO: model_training: Rank 0, Epoch 4937, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,359: INFO: model_training: Rank 0, Epoch 4938, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,360: INFO: model_training: Rank 0, Epoch 4938, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,362: INFO: model_training: Rank 0, Epoch 4938, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,363: INFO: model_training: Rank 0, Epoch 4938, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,365: INFO: model_training: Rank 0, Epoch 4938, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,366: INFO: model_training: Rank 0, Epoch 4939, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,367: INFO: model_training: Rank 0, Epoch 4939, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,369: INFO: model_training: Rank 0, Epoch 4939, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,370: INFO: model_training: Rank 0, Epoch 4939, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,371: INFO: model_training: Rank 0, Epoch 4939, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,373: INFO: model_training: Rank 0, Epoch 4940, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,375: INFO: model_training: Rank 0, Epoch 4940, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,376: INFO: model_training: Rank 0, Epoch 4940, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,378: INFO: model_training: Rank 0, Epoch 4940, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,379: INFO: model_training: Rank 0, Epoch 4940, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,381: INFO: model_training: Rank 0, Epoch 4941, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,382: INFO: model_training: Rank 0, Epoch 4941, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,383: INFO: model_training: Rank 0, Epoch 4941, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,385: INFO: model_training: Rank 0, Epoch 4941, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,387: INFO: model_training: Rank 0, Epoch 4941, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,388: INFO: model_training: Rank 0, Epoch 4942, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,389: INFO: model_training: Rank 0, Epoch 4942, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,391: INFO: model_training: Rank 0, Epoch 4942, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,392: INFO: model_training: Rank 0, Epoch 4942, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,394: INFO: model_training: Rank 0, Epoch 4942, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,396: INFO: model_training: Rank 0, Epoch 4943, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,397: INFO: model_training: Rank 0, Epoch 4943, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,398: INFO: model_training: Rank 0, Epoch 4943, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,399: INFO: model_training: Rank 0, Epoch 4943, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,401: INFO: model_training: Rank 0, Epoch 4943, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,402: INFO: model_training: Rank 0, Epoch 4944, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,404: INFO: model_training: Rank 0, Epoch 4944, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,405: INFO: model_training: Rank 0, Epoch 4944, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,406: INFO: model_training: Rank 0, Epoch 4944, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,407: INFO: model_training: Rank 0, Epoch 4944, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,409: INFO: model_training: Rank 0, Epoch 4945, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,410: INFO: model_training: Rank 0, Epoch 4945, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,412: INFO: model_training: Rank 0, Epoch 4945, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,414: INFO: model_training: Rank 0, Epoch 4945, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,415: INFO: model_training: Rank 0, Epoch 4945, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,416: INFO: model_training: Rank 0, Epoch 4946, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,418: INFO: model_training: Rank 0, Epoch 4946, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,420: INFO: model_training: Rank 0, Epoch 4946, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,422: INFO: model_training: Rank 0, Epoch 4946, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,423: INFO: model_training: Rank 0, Epoch 4946, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,425: INFO: model_training: Rank 0, Epoch 4947, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,427: INFO: model_training: Rank 0, Epoch 4947, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,429: INFO: model_training: Rank 0, Epoch 4947, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,431: INFO: model_training: Rank 0, Epoch 4947, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,432: INFO: model_training: Rank 0, Epoch 4947, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,434: INFO: model_training: Rank 0, Epoch 4948, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,438: INFO: model_training: Rank 0, Epoch 4948, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,439: INFO: model_training: Rank 0, Epoch 4948, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,441: INFO: model_training: Rank 0, Epoch 4948, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,442: INFO: model_training: Rank 0, Epoch 4948, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,445: INFO: model_training: Rank 0, Epoch 4949, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,446: INFO: model_training: Rank 0, Epoch 4949, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,447: INFO: model_training: Rank 0, Epoch 4949, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,449: INFO: model_training: Rank 0, Epoch 4949, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,450: INFO: model_training: Rank 0, Epoch 4949, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,451: INFO: model_training: Rank 0, Epoch 4950, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,453: INFO: model_training: Rank 0, Epoch 4950, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,454: INFO: model_training: Rank 0, Epoch 4950, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,456: INFO: model_training: Rank 0, Epoch 4950, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,457: INFO: model_training: Rank 0, Epoch 4950, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,459: INFO: model_training: Rank 0, Epoch 4951, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,461: INFO: model_training: Rank 0, Epoch 4951, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,463: INFO: model_training: Rank 0, Epoch 4951, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,464: INFO: model_training: Rank 0, Epoch 4951, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,465: INFO: model_training: Rank 0, Epoch 4951, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,467: INFO: model_training: Rank 0, Epoch 4952, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,468: INFO: model_training: Rank 0, Epoch 4952, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,470: INFO: model_training: Rank 0, Epoch 4952, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,471: INFO: model_training: Rank 0, Epoch 4952, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,472: INFO: model_training: Rank 0, Epoch 4952, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,473: INFO: model_training: Rank 0, Epoch 4953, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,475: INFO: model_training: Rank 0, Epoch 4953, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,477: INFO: model_training: Rank 0, Epoch 4953, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,478: INFO: model_training: Rank 0, Epoch 4953, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,480: INFO: model_training: Rank 0, Epoch 4953, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,481: INFO: model_training: Rank 0, Epoch 4954, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,483: INFO: model_training: Rank 0, Epoch 4954, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,484: INFO: model_training: Rank 0, Epoch 4954, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,486: INFO: model_training: Rank 0, Epoch 4954, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,487: INFO: model_training: Rank 0, Epoch 4954, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,488: INFO: model_training: Rank 0, Epoch 4955, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,489: INFO: model_training: Rank 0, Epoch 4955, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,491: INFO: model_training: Rank 0, Epoch 4955, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,492: INFO: model_training: Rank 0, Epoch 4955, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,494: INFO: model_training: Rank 0, Epoch 4955, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,495: INFO: model_training: Rank 0, Epoch 4956, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,497: INFO: model_training: Rank 0, Epoch 4956, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,498: INFO: model_training: Rank 0, Epoch 4956, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,500: INFO: model_training: Rank 0, Epoch 4956, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,502: INFO: model_training: Rank 0, Epoch 4956, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,503: INFO: model_training: Rank 0, Epoch 4957, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,505: INFO: model_training: Rank 0, Epoch 4957, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,505: INFO: model_training: Rank 0, Epoch 4957, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,507: INFO: model_training: Rank 0, Epoch 4957, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,508: INFO: model_training: Rank 0, Epoch 4957, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,510: INFO: model_training: Rank 0, Epoch 4958, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,512: INFO: model_training: Rank 0, Epoch 4958, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,513: INFO: model_training: Rank 0, Epoch 4958, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,514: INFO: model_training: Rank 0, Epoch 4958, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,516: INFO: model_training: Rank 0, Epoch 4958, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,517: INFO: model_training: Rank 0, Epoch 4959, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,518: INFO: model_training: Rank 0, Epoch 4959, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,521: INFO: model_training: Rank 0, Epoch 4959, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,522: INFO: model_training: Rank 0, Epoch 4959, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,523: INFO: model_training: Rank 0, Epoch 4959, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,525: INFO: model_training: Rank 0, Epoch 4960, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,526: INFO: model_training: Rank 0, Epoch 4960, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,528: INFO: model_training: Rank 0, Epoch 4960, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,529: INFO: model_training: Rank 0, Epoch 4960, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,530: INFO: model_training: Rank 0, Epoch 4960, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,531: INFO: model_training: Rank 0, Epoch 4961, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,532: INFO: model_training: Rank 0, Epoch 4961, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,533: INFO: model_training: Rank 0, Epoch 4961, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,535: INFO: model_training: Rank 0, Epoch 4961, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,537: INFO: model_training: Rank 0, Epoch 4961, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,538: INFO: model_training: Rank 0, Epoch 4962, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,539: INFO: model_training: Rank 0, Epoch 4962, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,541: INFO: model_training: Rank 0, Epoch 4962, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,542: INFO: model_training: Rank 0, Epoch 4962, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,544: INFO: model_training: Rank 0, Epoch 4962, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,545: INFO: model_training: Rank 0, Epoch 4963, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,546: INFO: model_training: Rank 0, Epoch 4963, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,548: INFO: model_training: Rank 0, Epoch 4963, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,549: INFO: model_training: Rank 0, Epoch 4963, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,550: INFO: model_training: Rank 0, Epoch 4963, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,552: INFO: model_training: Rank 0, Epoch 4964, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,553: INFO: model_training: Rank 0, Epoch 4964, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,554: INFO: model_training: Rank 0, Epoch 4964, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,556: INFO: model_training: Rank 0, Epoch 4964, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,557: INFO: model_training: Rank 0, Epoch 4964, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,559: INFO: model_training: Rank 0, Epoch 4965, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,561: INFO: model_training: Rank 0, Epoch 4965, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,562: INFO: model_training: Rank 0, Epoch 4965, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,563: INFO: model_training: Rank 0, Epoch 4965, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,565: INFO: model_training: Rank 0, Epoch 4965, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,566: INFO: model_training: Rank 0, Epoch 4966, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,567: INFO: model_training: Rank 0, Epoch 4966, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,569: INFO: model_training: Rank 0, Epoch 4966, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,571: INFO: model_training: Rank 0, Epoch 4966, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,572: INFO: model_training: Rank 0, Epoch 4966, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,573: INFO: model_training: Rank 0, Epoch 4967, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,575: INFO: model_training: Rank 0, Epoch 4967, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,576: INFO: model_training: Rank 0, Epoch 4967, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,577: INFO: model_training: Rank 0, Epoch 4967, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,579: INFO: model_training: Rank 0, Epoch 4967, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,580: INFO: model_training: Rank 0, Epoch 4968, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,582: INFO: model_training: Rank 0, Epoch 4968, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,583: INFO: model_training: Rank 0, Epoch 4968, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,584: INFO: model_training: Rank 0, Epoch 4968, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,586: INFO: model_training: Rank 0, Epoch 4968, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,587: INFO: model_training: Rank 0, Epoch 4969, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,588: INFO: model_training: Rank 0, Epoch 4969, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,589: INFO: model_training: Rank 0, Epoch 4969, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,591: INFO: model_training: Rank 0, Epoch 4969, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,592: INFO: model_training: Rank 0, Epoch 4969, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,594: INFO: model_training: Rank 0, Epoch 4970, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,596: INFO: model_training: Rank 0, Epoch 4970, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,597: INFO: model_training: Rank 0, Epoch 4970, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,598: INFO: model_training: Rank 0, Epoch 4970, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,600: INFO: model_training: Rank 0, Epoch 4970, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,601: INFO: model_training: Rank 0, Epoch 4971, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,603: INFO: model_training: Rank 0, Epoch 4971, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,604: INFO: model_training: Rank 0, Epoch 4971, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,605: INFO: model_training: Rank 0, Epoch 4971, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,607: INFO: model_training: Rank 0, Epoch 4971, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,608: INFO: model_training: Rank 0, Epoch 4972, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,609: INFO: model_training: Rank 0, Epoch 4972, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,611: INFO: model_training: Rank 0, Epoch 4972, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,612: INFO: model_training: Rank 0, Epoch 4972, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,613: INFO: model_training: Rank 0, Epoch 4972, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,615: INFO: model_training: Rank 0, Epoch 4973, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,616: INFO: model_training: Rank 0, Epoch 4973, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,617: INFO: model_training: Rank 0, Epoch 4973, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,620: INFO: model_training: Rank 0, Epoch 4973, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,621: INFO: model_training: Rank 0, Epoch 4973, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,622: INFO: model_training: Rank 0, Epoch 4974, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,623: INFO: model_training: Rank 0, Epoch 4974, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,625: INFO: model_training: Rank 0, Epoch 4974, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,626: INFO: model_training: Rank 0, Epoch 4974, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,628: INFO: model_training: Rank 0, Epoch 4974, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,629: INFO: model_training: Rank 0, Epoch 4975, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,630: INFO: model_training: Rank 0, Epoch 4975, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,632: INFO: model_training: Rank 0, Epoch 4975, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,633: INFO: model_training: Rank 0, Epoch 4975, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,635: INFO: model_training: Rank 0, Epoch 4975, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,637: INFO: model_training: Rank 0, Epoch 4976, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,638: INFO: model_training: Rank 0, Epoch 4976, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,639: INFO: model_training: Rank 0, Epoch 4976, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,641: INFO: model_training: Rank 0, Epoch 4976, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,642: INFO: model_training: Rank 0, Epoch 4976, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,644: INFO: model_training: Rank 0, Epoch 4977, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,645: INFO: model_training: Rank 0, Epoch 4977, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,647: INFO: model_training: Rank 0, Epoch 4977, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,648: INFO: model_training: Rank 0, Epoch 4977, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,649: INFO: model_training: Rank 0, Epoch 4977, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,650: INFO: model_training: Rank 0, Epoch 4978, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,652: INFO: model_training: Rank 0, Epoch 4978, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,654: INFO: model_training: Rank 0, Epoch 4978, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,655: INFO: model_training: Rank 0, Epoch 4978, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,656: INFO: model_training: Rank 0, Epoch 4978, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,658: INFO: model_training: Rank 0, Epoch 4979, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,659: INFO: model_training: Rank 0, Epoch 4979, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,661: INFO: model_training: Rank 0, Epoch 4979, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,663: INFO: model_training: Rank 0, Epoch 4979, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,664: INFO: model_training: Rank 0, Epoch 4979, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,665: INFO: model_training: Rank 0, Epoch 4980, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,666: INFO: model_training: Rank 0, Epoch 4980, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,667: INFO: model_training: Rank 0, Epoch 4980, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,669: INFO: model_training: Rank 0, Epoch 4980, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,670: INFO: model_training: Rank 0, Epoch 4980, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,672: INFO: model_training: Rank 0, Epoch 4981, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,673: INFO: model_training: Rank 0, Epoch 4981, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,674: INFO: model_training: Rank 0, Epoch 4981, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,675: INFO: model_training: Rank 0, Epoch 4981, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,677: INFO: model_training: Rank 0, Epoch 4981, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,679: INFO: model_training: Rank 0, Epoch 4982, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,680: INFO: model_training: Rank 0, Epoch 4982, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,681: INFO: model_training: Rank 0, Epoch 4982, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,682: INFO: model_training: Rank 0, Epoch 4982, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,684: INFO: model_training: Rank 0, Epoch 4982, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,686: INFO: model_training: Rank 0, Epoch 4983, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,687: INFO: model_training: Rank 0, Epoch 4983, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,688: INFO: model_training: Rank 0, Epoch 4983, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,689: INFO: model_training: Rank 0, Epoch 4983, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,691: INFO: model_training: Rank 0, Epoch 4983, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,692: INFO: model_training: Rank 0, Epoch 4984, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,694: INFO: model_training: Rank 0, Epoch 4984, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,695: INFO: model_training: Rank 0, Epoch 4984, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,697: INFO: model_training: Rank 0, Epoch 4984, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,699: INFO: model_training: Rank 0, Epoch 4984, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,700: INFO: model_training: Rank 0, Epoch 4985, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,701: INFO: model_training: Rank 0, Epoch 4985, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,703: INFO: model_training: Rank 0, Epoch 4985, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,704: INFO: model_training: Rank 0, Epoch 4985, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,705: INFO: model_training: Rank 0, Epoch 4985, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,706: INFO: model_training: Rank 0, Epoch 4986, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,708: INFO: model_training: Rank 0, Epoch 4986, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,709: INFO: model_training: Rank 0, Epoch 4986, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,711: INFO: model_training: Rank 0, Epoch 4986, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,712: INFO: model_training: Rank 0, Epoch 4986, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,714: INFO: model_training: Rank 0, Epoch 4987, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,715: INFO: model_training: Rank 0, Epoch 4987, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,717: INFO: model_training: Rank 0, Epoch 4987, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,718: INFO: model_training: Rank 0, Epoch 4987, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,720: INFO: model_training: Rank 0, Epoch 4987, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,721: INFO: model_training: Rank 0, Epoch 4988, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,723: INFO: model_training: Rank 0, Epoch 4988, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,724: INFO: model_training: Rank 0, Epoch 4988, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,725: INFO: model_training: Rank 0, Epoch 4988, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,727: INFO: model_training: Rank 0, Epoch 4988, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,729: INFO: model_training: Rank 0, Epoch 4989, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,730: INFO: model_training: Rank 0, Epoch 4989, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,731: INFO: model_training: Rank 0, Epoch 4989, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,732: INFO: model_training: Rank 0, Epoch 4989, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,734: INFO: model_training: Rank 0, Epoch 4989, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,736: INFO: model_training: Rank 0, Epoch 4990, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,738: INFO: model_training: Rank 0, Epoch 4990, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,740: INFO: model_training: Rank 0, Epoch 4990, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,741: INFO: model_training: Rank 0, Epoch 4990, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,742: INFO: model_training: Rank 0, Epoch 4990, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,744: INFO: model_training: Rank 0, Epoch 4991, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,745: INFO: model_training: Rank 0, Epoch 4991, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,747: INFO: model_training: Rank 0, Epoch 4991, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,748: INFO: model_training: Rank 0, Epoch 4991, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,749: INFO: model_training: Rank 0, Epoch 4991, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,750: INFO: model_training: Rank 0, Epoch 4992, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,752: INFO: model_training: Rank 0, Epoch 4992, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,754: INFO: model_training: Rank 0, Epoch 4992, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,755: INFO: model_training: Rank 0, Epoch 4992, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,756: INFO: model_training: Rank 0, Epoch 4992, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,757: INFO: model_training: Rank 0, Epoch 4993, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,759: INFO: model_training: Rank 0, Epoch 4993, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,760: INFO: model_training: Rank 0, Epoch 4993, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,762: INFO: model_training: Rank 0, Epoch 4993, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,764: INFO: model_training: Rank 0, Epoch 4993, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,765: INFO: model_training: Rank 0, Epoch 4994, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,766: INFO: model_training: Rank 0, Epoch 4994, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,767: INFO: model_training: Rank 0, Epoch 4994, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,769: INFO: model_training: Rank 0, Epoch 4994, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,771: INFO: model_training: Rank 0, Epoch 4994, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,772: INFO: model_training: Rank 0, Epoch 4995, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,774: INFO: model_training: Rank 0, Epoch 4995, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,775: INFO: model_training: Rank 0, Epoch 4995, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,776: INFO: model_training: Rank 0, Epoch 4995, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,778: INFO: model_training: Rank 0, Epoch 4995, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,779: INFO: model_training: Rank 0, Epoch 4996, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,780: INFO: model_training: Rank 0, Epoch 4996, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,782: INFO: model_training: Rank 0, Epoch 4996, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,784: INFO: model_training: Rank 0, Epoch 4996, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,785: INFO: model_training: Rank 0, Epoch 4996, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,787: INFO: model_training: Rank 0, Epoch 4997, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,788: INFO: model_training: Rank 0, Epoch 4997, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,790: INFO: model_training: Rank 0, Epoch 4997, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,791: INFO: model_training: Rank 0, Epoch 4997, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,793: INFO: model_training: Rank 0, Epoch 4997, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,795: INFO: model_training: Rank 0, Epoch 4998, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,796: INFO: model_training: Rank 0, Epoch 4998, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,797: INFO: model_training: Rank 0, Epoch 4998, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,799: INFO: model_training: Rank 0, Epoch 4998, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,800: INFO: model_training: Rank 0, Epoch 4998, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,802: INFO: model_training: Rank 0, Epoch 4999, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,804: INFO: model_training: Rank 0, Epoch 4999, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,805: INFO: model_training: Rank 0, Epoch 4999, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,807: INFO: model_training: Rank 0, Epoch 4999, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,808: INFO: model_training: Rank 0, Epoch 4999, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,809: INFO: model_training: Rank 0, Epoch 5000, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,811: INFO: model_training: Rank 0, Epoch 5000, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,812: INFO: model_training: Rank 0, Epoch 5000, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,814: INFO: model_training: Rank 0, Epoch 5000, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,815: INFO: model_training: Rank 0, Epoch 5000, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,816: INFO: model_training: Rank 0, Epoch 5001, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,818: INFO: model_training: Rank 0, Epoch 5001, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,819: INFO: model_training: Rank 0, Epoch 5001, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,821: INFO: model_training: Rank 0, Epoch 5001, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,822: INFO: model_training: Rank 0, Epoch 5001, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,824: INFO: model_training: Rank 0, Epoch 5002, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,825: INFO: model_training: Rank 0, Epoch 5002, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,827: INFO: model_training: Rank 0, Epoch 5002, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,829: INFO: model_training: Rank 0, Epoch 5002, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,830: INFO: model_training: Rank 0, Epoch 5002, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,831: INFO: model_training: Rank 0, Epoch 5003, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,832: INFO: model_training: Rank 0, Epoch 5003, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,833: INFO: model_training: Rank 0, Epoch 5003, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,834: INFO: model_training: Rank 0, Epoch 5003, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,836: INFO: model_training: Rank 0, Epoch 5003, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,838: INFO: model_training: Rank 0, Epoch 5004, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,839: INFO: model_training: Rank 0, Epoch 5004, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,840: INFO: model_training: Rank 0, Epoch 5004, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,841: INFO: model_training: Rank 0, Epoch 5004, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,843: INFO: model_training: Rank 0, Epoch 5004, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,845: INFO: model_training: Rank 0, Epoch 5005, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,847: INFO: model_training: Rank 0, Epoch 5005, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,848: INFO: model_training: Rank 0, Epoch 5005, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,849: INFO: model_training: Rank 0, Epoch 5005, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,850: INFO: model_training: Rank 0, Epoch 5005, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,852: INFO: model_training: Rank 0, Epoch 5006, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,854: INFO: model_training: Rank 0, Epoch 5006, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,855: INFO: model_training: Rank 0, Epoch 5006, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,856: INFO: model_training: Rank 0, Epoch 5006, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,857: INFO: model_training: Rank 0, Epoch 5006, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,859: INFO: model_training: Rank 0, Epoch 5007, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,860: INFO: model_training: Rank 0, Epoch 5007, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,861: INFO: model_training: Rank 0, Epoch 5007, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,863: INFO: model_training: Rank 0, Epoch 5007, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,865: INFO: model_training: Rank 0, Epoch 5007, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,867: INFO: model_training: Rank 0, Epoch 5008, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,868: INFO: model_training: Rank 0, Epoch 5008, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,870: INFO: model_training: Rank 0, Epoch 5008, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,871: INFO: model_training: Rank 0, Epoch 5008, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,872: INFO: model_training: Rank 0, Epoch 5008, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,874: INFO: model_training: Rank 0, Epoch 5009, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,875: INFO: model_training: Rank 0, Epoch 5009, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,876: INFO: model_training: Rank 0, Epoch 5009, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,878: INFO: model_training: Rank 0, Epoch 5009, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,880: INFO: model_training: Rank 0, Epoch 5009, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,881: INFO: model_training: Rank 0, Epoch 5010, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,882: INFO: model_training: Rank 0, Epoch 5010, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,883: INFO: model_training: Rank 0, Epoch 5010, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,886: INFO: model_training: Rank 0, Epoch 5010, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,887: INFO: model_training: Rank 0, Epoch 5010, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,889: INFO: model_training: Rank 0, Epoch 5011, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,890: INFO: model_training: Rank 0, Epoch 5011, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,891: INFO: model_training: Rank 0, Epoch 5011, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,893: INFO: model_training: Rank 0, Epoch 5011, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,895: INFO: model_training: Rank 0, Epoch 5011, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,896: INFO: model_training: Rank 0, Epoch 5012, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,897: INFO: model_training: Rank 0, Epoch 5012, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,899: INFO: model_training: Rank 0, Epoch 5012, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,900: INFO: model_training: Rank 0, Epoch 5012, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,902: INFO: model_training: Rank 0, Epoch 5012, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,904: INFO: model_training: Rank 0, Epoch 5013, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,906: INFO: model_training: Rank 0, Epoch 5013, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,908: INFO: model_training: Rank 0, Epoch 5013, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,909: INFO: model_training: Rank 0, Epoch 5013, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,910: INFO: model_training: Rank 0, Epoch 5013, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,911: INFO: model_training: Rank 0, Epoch 5014, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,913: INFO: model_training: Rank 0, Epoch 5014, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,915: INFO: model_training: Rank 0, Epoch 5014, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,916: INFO: model_training: Rank 0, Epoch 5014, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,917: INFO: model_training: Rank 0, Epoch 5014, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,918: INFO: model_training: Rank 0, Epoch 5015, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,920: INFO: model_training: Rank 0, Epoch 5015, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,921: INFO: model_training: Rank 0, Epoch 5015, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,923: INFO: model_training: Rank 0, Epoch 5015, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,924: INFO: model_training: Rank 0, Epoch 5015, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,925: INFO: model_training: Rank 0, Epoch 5016, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,927: INFO: model_training: Rank 0, Epoch 5016, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,929: INFO: model_training: Rank 0, Epoch 5016, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,930: INFO: model_training: Rank 0, Epoch 5016, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,931: INFO: model_training: Rank 0, Epoch 5016, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,932: INFO: model_training: Rank 0, Epoch 5017, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,934: INFO: model_training: Rank 0, Epoch 5017, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,935: INFO: model_training: Rank 0, Epoch 5017, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,936: INFO: model_training: Rank 0, Epoch 5017, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,938: INFO: model_training: Rank 0, Epoch 5017, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,939: INFO: model_training: Rank 0, Epoch 5018, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,941: INFO: model_training: Rank 0, Epoch 5018, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,942: INFO: model_training: Rank 0, Epoch 5018, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,943: INFO: model_training: Rank 0, Epoch 5018, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,945: INFO: model_training: Rank 0, Epoch 5018, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,947: INFO: model_training: Rank 0, Epoch 5019, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,948: INFO: model_training: Rank 0, Epoch 5019, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,950: INFO: model_training: Rank 0, Epoch 5019, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,951: INFO: model_training: Rank 0, Epoch 5019, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,952: INFO: model_training: Rank 0, Epoch 5019, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,954: INFO: model_training: Rank 0, Epoch 5020, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,955: INFO: model_training: Rank 0, Epoch 5020, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,956: INFO: model_training: Rank 0, Epoch 5020, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,958: INFO: model_training: Rank 0, Epoch 5020, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,959: INFO: model_training: Rank 0, Epoch 5020, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,961: INFO: model_training: Rank 0, Epoch 5021, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,962: INFO: model_training: Rank 0, Epoch 5021, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,964: INFO: model_training: Rank 0, Epoch 5021, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,965: INFO: model_training: Rank 0, Epoch 5021, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,967: INFO: model_training: Rank 0, Epoch 5021, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,968: INFO: model_training: Rank 0, Epoch 5022, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,970: INFO: model_training: Rank 0, Epoch 5022, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,971: INFO: model_training: Rank 0, Epoch 5022, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,972: INFO: model_training: Rank 0, Epoch 5022, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,974: INFO: model_training: Rank 0, Epoch 5022, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,975: INFO: model_training: Rank 0, Epoch 5023, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,976: INFO: model_training: Rank 0, Epoch 5023, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,978: INFO: model_training: Rank 0, Epoch 5023, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,979: INFO: model_training: Rank 0, Epoch 5023, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,980: INFO: model_training: Rank 0, Epoch 5023, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,982: INFO: model_training: Rank 0, Epoch 5024, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,983: INFO: model_training: Rank 0, Epoch 5024, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,984: INFO: model_training: Rank 0, Epoch 5024, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,986: INFO: model_training: Rank 0, Epoch 5024, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,988: INFO: model_training: Rank 0, Epoch 5024, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,989: INFO: model_training: Rank 0, Epoch 5025, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,991: INFO: model_training: Rank 0, Epoch 5025, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,992: INFO: model_training: Rank 0, Epoch 5025, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:58,993: INFO: model_training: Rank 0, Epoch 5025, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:58,995: INFO: model_training: Rank 0, Epoch 5025, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:58,996: INFO: model_training: Rank 0, Epoch 5026, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:58,998: INFO: model_training: Rank 0, Epoch 5026, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:58,999: INFO: model_training: Rank 0, Epoch 5026, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,000: INFO: model_training: Rank 0, Epoch 5026, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,003: INFO: model_training: Rank 0, Epoch 5026, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,004: INFO: model_training: Rank 0, Epoch 5027, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,005: INFO: model_training: Rank 0, Epoch 5027, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,007: INFO: model_training: Rank 0, Epoch 5027, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,009: INFO: model_training: Rank 0, Epoch 5027, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,011: INFO: model_training: Rank 0, Epoch 5027, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,012: INFO: model_training: Rank 0, Epoch 5028, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,013: INFO: model_training: Rank 0, Epoch 5028, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,015: INFO: model_training: Rank 0, Epoch 5028, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,016: INFO: model_training: Rank 0, Epoch 5028, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,017: INFO: model_training: Rank 0, Epoch 5028, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,019: INFO: model_training: Rank 0, Epoch 5029, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,020: INFO: model_training: Rank 0, Epoch 5029, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,022: INFO: model_training: Rank 0, Epoch 5029, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,024: INFO: model_training: Rank 0, Epoch 5029, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,025: INFO: model_training: Rank 0, Epoch 5029, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,026: INFO: model_training: Rank 0, Epoch 5030, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,028: INFO: model_training: Rank 0, Epoch 5030, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,029: INFO: model_training: Rank 0, Epoch 5030, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,031: INFO: model_training: Rank 0, Epoch 5030, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,032: INFO: model_training: Rank 0, Epoch 5030, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,033: INFO: model_training: Rank 0, Epoch 5031, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,035: INFO: model_training: Rank 0, Epoch 5031, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,037: INFO: model_training: Rank 0, Epoch 5031, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,038: INFO: model_training: Rank 0, Epoch 5031, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,040: INFO: model_training: Rank 0, Epoch 5031, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,041: INFO: model_training: Rank 0, Epoch 5032, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,042: INFO: model_training: Rank 0, Epoch 5032, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,044: INFO: model_training: Rank 0, Epoch 5032, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,045: INFO: model_training: Rank 0, Epoch 5032, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,047: INFO: model_training: Rank 0, Epoch 5032, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,048: INFO: model_training: Rank 0, Epoch 5033, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,049: INFO: model_training: Rank 0, Epoch 5033, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,051: INFO: model_training: Rank 0, Epoch 5033, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,053: INFO: model_training: Rank 0, Epoch 5033, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,055: INFO: model_training: Rank 0, Epoch 5033, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,057: INFO: model_training: Rank 0, Epoch 5034, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,058: INFO: model_training: Rank 0, Epoch 5034, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,059: INFO: model_training: Rank 0, Epoch 5034, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,061: INFO: model_training: Rank 0, Epoch 5034, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,063: INFO: model_training: Rank 0, Epoch 5034, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,064: INFO: model_training: Rank 0, Epoch 5035, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,066: INFO: model_training: Rank 0, Epoch 5035, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,067: INFO: model_training: Rank 0, Epoch 5035, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,068: INFO: model_training: Rank 0, Epoch 5035, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,070: INFO: model_training: Rank 0, Epoch 5035, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,072: INFO: model_training: Rank 0, Epoch 5036, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,073: INFO: model_training: Rank 0, Epoch 5036, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,075: INFO: model_training: Rank 0, Epoch 5036, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,076: INFO: model_training: Rank 0, Epoch 5036, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,078: INFO: model_training: Rank 0, Epoch 5036, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,079: INFO: model_training: Rank 0, Epoch 5037, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,080: INFO: model_training: Rank 0, Epoch 5037, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,081: INFO: model_training: Rank 0, Epoch 5037, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,083: INFO: model_training: Rank 0, Epoch 5037, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,084: INFO: model_training: Rank 0, Epoch 5037, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,086: INFO: model_training: Rank 0, Epoch 5038, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,087: INFO: model_training: Rank 0, Epoch 5038, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,089: INFO: model_training: Rank 0, Epoch 5038, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,090: INFO: model_training: Rank 0, Epoch 5038, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,091: INFO: model_training: Rank 0, Epoch 5038, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,093: INFO: model_training: Rank 0, Epoch 5039, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,095: INFO: model_training: Rank 0, Epoch 5039, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,096: INFO: model_training: Rank 0, Epoch 5039, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,097: INFO: model_training: Rank 0, Epoch 5039, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,099: INFO: model_training: Rank 0, Epoch 5039, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,100: INFO: model_training: Rank 0, Epoch 5040, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,102: INFO: model_training: Rank 0, Epoch 5040, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,104: INFO: model_training: Rank 0, Epoch 5040, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,105: INFO: model_training: Rank 0, Epoch 5040, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,107: INFO: model_training: Rank 0, Epoch 5040, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,108: INFO: model_training: Rank 0, Epoch 5041, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,110: INFO: model_training: Rank 0, Epoch 5041, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,112: INFO: model_training: Rank 0, Epoch 5041, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,113: INFO: model_training: Rank 0, Epoch 5041, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,114: INFO: model_training: Rank 0, Epoch 5041, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,116: INFO: model_training: Rank 0, Epoch 5042, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,117: INFO: model_training: Rank 0, Epoch 5042, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,119: INFO: model_training: Rank 0, Epoch 5042, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,121: INFO: model_training: Rank 0, Epoch 5042, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,122: INFO: model_training: Rank 0, Epoch 5042, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,124: INFO: model_training: Rank 0, Epoch 5043, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,125: INFO: model_training: Rank 0, Epoch 5043, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,127: INFO: model_training: Rank 0, Epoch 5043, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,129: INFO: model_training: Rank 0, Epoch 5043, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,130: INFO: model_training: Rank 0, Epoch 5043, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,131: INFO: model_training: Rank 0, Epoch 5044, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,133: INFO: model_training: Rank 0, Epoch 5044, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,134: INFO: model_training: Rank 0, Epoch 5044, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,136: INFO: model_training: Rank 0, Epoch 5044, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,137: INFO: model_training: Rank 0, Epoch 5044, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,139: INFO: model_training: Rank 0, Epoch 5045, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,140: INFO: model_training: Rank 0, Epoch 5045, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,141: INFO: model_training: Rank 0, Epoch 5045, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,142: INFO: model_training: Rank 0, Epoch 5045, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,144: INFO: model_training: Rank 0, Epoch 5045, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,146: INFO: model_training: Rank 0, Epoch 5046, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,147: INFO: model_training: Rank 0, Epoch 5046, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,148: INFO: model_training: Rank 0, Epoch 5046, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,149: INFO: model_training: Rank 0, Epoch 5046, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,152: INFO: model_training: Rank 0, Epoch 5046, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,153: INFO: model_training: Rank 0, Epoch 5047, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,155: INFO: model_training: Rank 0, Epoch 5047, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,156: INFO: model_training: Rank 0, Epoch 5047, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,157: INFO: model_training: Rank 0, Epoch 5047, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,159: INFO: model_training: Rank 0, Epoch 5047, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,160: INFO: model_training: Rank 0, Epoch 5048, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,162: INFO: model_training: Rank 0, Epoch 5048, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,163: INFO: model_training: Rank 0, Epoch 5048, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,164: INFO: model_training: Rank 0, Epoch 5048, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,166: INFO: model_training: Rank 0, Epoch 5048, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,167: INFO: model_training: Rank 0, Epoch 5049, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,169: INFO: model_training: Rank 0, Epoch 5049, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,170: INFO: model_training: Rank 0, Epoch 5049, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,172: INFO: model_training: Rank 0, Epoch 5049, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,173: INFO: model_training: Rank 0, Epoch 5049, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,175: INFO: model_training: Rank 0, Epoch 5050, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,176: INFO: model_training: Rank 0, Epoch 5050, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,178: INFO: model_training: Rank 0, Epoch 5050, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,179: INFO: model_training: Rank 0, Epoch 5050, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,180: INFO: model_training: Rank 0, Epoch 5050, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,182: INFO: model_training: Rank 0, Epoch 5051, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,183: INFO: model_training: Rank 0, Epoch 5051, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,185: INFO: model_training: Rank 0, Epoch 5051, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,186: INFO: model_training: Rank 0, Epoch 5051, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,187: INFO: model_training: Rank 0, Epoch 5051, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,189: INFO: model_training: Rank 0, Epoch 5052, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,190: INFO: model_training: Rank 0, Epoch 5052, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,191: INFO: model_training: Rank 0, Epoch 5052, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,192: INFO: model_training: Rank 0, Epoch 5052, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,194: INFO: model_training: Rank 0, Epoch 5052, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,195: INFO: model_training: Rank 0, Epoch 5053, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,197: INFO: model_training: Rank 0, Epoch 5053, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,199: INFO: model_training: Rank 0, Epoch 5053, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,200: INFO: model_training: Rank 0, Epoch 5053, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,202: INFO: model_training: Rank 0, Epoch 5053, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,203: INFO: model_training: Rank 0, Epoch 5054, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,205: INFO: model_training: Rank 0, Epoch 5054, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,206: INFO: model_training: Rank 0, Epoch 5054, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,207: INFO: model_training: Rank 0, Epoch 5054, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,208: INFO: model_training: Rank 0, Epoch 5054, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,210: INFO: model_training: Rank 0, Epoch 5055, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,212: INFO: model_training: Rank 0, Epoch 5055, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,215: INFO: model_training: Rank 0, Epoch 5055, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,217: INFO: model_training: Rank 0, Epoch 5055, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,218: INFO: model_training: Rank 0, Epoch 5055, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,220: INFO: model_training: Rank 0, Epoch 5056, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,221: INFO: model_training: Rank 0, Epoch 5056, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,222: INFO: model_training: Rank 0, Epoch 5056, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,223: INFO: model_training: Rank 0, Epoch 5056, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,225: INFO: model_training: Rank 0, Epoch 5056, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,226: INFO: model_training: Rank 0, Epoch 5057, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,228: INFO: model_training: Rank 0, Epoch 5057, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,229: INFO: model_training: Rank 0, Epoch 5057, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,230: INFO: model_training: Rank 0, Epoch 5057, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,232: INFO: model_training: Rank 0, Epoch 5057, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,233: INFO: model_training: Rank 0, Epoch 5058, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,235: INFO: model_training: Rank 0, Epoch 5058, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,237: INFO: model_training: Rank 0, Epoch 5058, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,238: INFO: model_training: Rank 0, Epoch 5058, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,239: INFO: model_training: Rank 0, Epoch 5058, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,240: INFO: model_training: Rank 0, Epoch 5059, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,241: INFO: model_training: Rank 0, Epoch 5059, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,242: INFO: model_training: Rank 0, Epoch 5059, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,244: INFO: model_training: Rank 0, Epoch 5059, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,246: INFO: model_training: Rank 0, Epoch 5059, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,247: INFO: model_training: Rank 0, Epoch 5060, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,249: INFO: model_training: Rank 0, Epoch 5060, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,250: INFO: model_training: Rank 0, Epoch 5060, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,252: INFO: model_training: Rank 0, Epoch 5060, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,253: INFO: model_training: Rank 0, Epoch 5060, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,255: INFO: model_training: Rank 0, Epoch 5061, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,256: INFO: model_training: Rank 0, Epoch 5061, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,257: INFO: model_training: Rank 0, Epoch 5061, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,258: INFO: model_training: Rank 0, Epoch 5061, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,259: INFO: model_training: Rank 0, Epoch 5061, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,261: INFO: model_training: Rank 0, Epoch 5062, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,263: INFO: model_training: Rank 0, Epoch 5062, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,265: INFO: model_training: Rank 0, Epoch 5062, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,266: INFO: model_training: Rank 0, Epoch 5062, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,267: INFO: model_training: Rank 0, Epoch 5062, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,269: INFO: model_training: Rank 0, Epoch 5063, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,270: INFO: model_training: Rank 0, Epoch 5063, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,271: INFO: model_training: Rank 0, Epoch 5063, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,272: INFO: model_training: Rank 0, Epoch 5063, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,274: INFO: model_training: Rank 0, Epoch 5063, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,275: INFO: model_training: Rank 0, Epoch 5064, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,277: INFO: model_training: Rank 0, Epoch 5064, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,279: INFO: model_training: Rank 0, Epoch 5064, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,280: INFO: model_training: Rank 0, Epoch 5064, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,282: INFO: model_training: Rank 0, Epoch 5064, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,283: INFO: model_training: Rank 0, Epoch 5065, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,284: INFO: model_training: Rank 0, Epoch 5065, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,286: INFO: model_training: Rank 0, Epoch 5065, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,287: INFO: model_training: Rank 0, Epoch 5065, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,288: INFO: model_training: Rank 0, Epoch 5065, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,289: INFO: model_training: Rank 0, Epoch 5066, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,291: INFO: model_training: Rank 0, Epoch 5066, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,292: INFO: model_training: Rank 0, Epoch 5066, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,294: INFO: model_training: Rank 0, Epoch 5066, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,296: INFO: model_training: Rank 0, Epoch 5066, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,297: INFO: model_training: Rank 0, Epoch 5067, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,299: INFO: model_training: Rank 0, Epoch 5067, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,300: INFO: model_training: Rank 0, Epoch 5067, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,301: INFO: model_training: Rank 0, Epoch 5067, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,303: INFO: model_training: Rank 0, Epoch 5067, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,304: INFO: model_training: Rank 0, Epoch 5068, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,305: INFO: model_training: Rank 0, Epoch 5068, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,307: INFO: model_training: Rank 0, Epoch 5068, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,308: INFO: model_training: Rank 0, Epoch 5068, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,309: INFO: model_training: Rank 0, Epoch 5068, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,311: INFO: model_training: Rank 0, Epoch 5069, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,312: INFO: model_training: Rank 0, Epoch 5069, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,313: INFO: model_training: Rank 0, Epoch 5069, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,315: INFO: model_training: Rank 0, Epoch 5069, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,316: INFO: model_training: Rank 0, Epoch 5069, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,318: INFO: model_training: Rank 0, Epoch 5070, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,319: INFO: model_training: Rank 0, Epoch 5070, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,321: INFO: model_training: Rank 0, Epoch 5070, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,322: INFO: model_training: Rank 0, Epoch 5070, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,324: INFO: model_training: Rank 0, Epoch 5070, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,325: INFO: model_training: Rank 0, Epoch 5071, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,327: INFO: model_training: Rank 0, Epoch 5071, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,328: INFO: model_training: Rank 0, Epoch 5071, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,330: INFO: model_training: Rank 0, Epoch 5071, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,331: INFO: model_training: Rank 0, Epoch 5071, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,332: INFO: model_training: Rank 0, Epoch 5072, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,333: INFO: model_training: Rank 0, Epoch 5072, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,335: INFO: model_training: Rank 0, Epoch 5072, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,337: INFO: model_training: Rank 0, Epoch 5072, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,338: INFO: model_training: Rank 0, Epoch 5072, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,339: INFO: model_training: Rank 0, Epoch 5073, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,341: INFO: model_training: Rank 0, Epoch 5073, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,342: INFO: model_training: Rank 0, Epoch 5073, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,345: INFO: model_training: Rank 0, Epoch 5073, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,346: INFO: model_training: Rank 0, Epoch 5073, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,347: INFO: model_training: Rank 0, Epoch 5074, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,348: INFO: model_training: Rank 0, Epoch 5074, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,349: INFO: model_training: Rank 0, Epoch 5074, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,351: INFO: model_training: Rank 0, Epoch 5074, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,352: INFO: model_training: Rank 0, Epoch 5074, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,354: INFO: model_training: Rank 0, Epoch 5075, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,355: INFO: model_training: Rank 0, Epoch 5075, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,356: INFO: model_training: Rank 0, Epoch 5075, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,358: INFO: model_training: Rank 0, Epoch 5075, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,360: INFO: model_training: Rank 0, Epoch 5075, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,362: INFO: model_training: Rank 0, Epoch 5076, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,363: INFO: model_training: Rank 0, Epoch 5076, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,364: INFO: model_training: Rank 0, Epoch 5076, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,365: INFO: model_training: Rank 0, Epoch 5076, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,367: INFO: model_training: Rank 0, Epoch 5076, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,368: INFO: model_training: Rank 0, Epoch 5077, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,370: INFO: model_training: Rank 0, Epoch 5077, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,371: INFO: model_training: Rank 0, Epoch 5077, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,372: INFO: model_training: Rank 0, Epoch 5077, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,374: INFO: model_training: Rank 0, Epoch 5077, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,376: INFO: model_training: Rank 0, Epoch 5078, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,378: INFO: model_training: Rank 0, Epoch 5078, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,379: INFO: model_training: Rank 0, Epoch 5078, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,380: INFO: model_training: Rank 0, Epoch 5078, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,382: INFO: model_training: Rank 0, Epoch 5078, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,383: INFO: model_training: Rank 0, Epoch 5079, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,385: INFO: model_training: Rank 0, Epoch 5079, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,386: INFO: model_training: Rank 0, Epoch 5079, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,388: INFO: model_training: Rank 0, Epoch 5079, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,389: INFO: model_training: Rank 0, Epoch 5079, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,390: INFO: model_training: Rank 0, Epoch 5080, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,392: INFO: model_training: Rank 0, Epoch 5080, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,394: INFO: model_training: Rank 0, Epoch 5080, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,395: INFO: model_training: Rank 0, Epoch 5080, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,397: INFO: model_training: Rank 0, Epoch 5080, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,398: INFO: model_training: Rank 0, Epoch 5081, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,399: INFO: model_training: Rank 0, Epoch 5081, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,400: INFO: model_training: Rank 0, Epoch 5081, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,402: INFO: model_training: Rank 0, Epoch 5081, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,403: INFO: model_training: Rank 0, Epoch 5081, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,405: INFO: model_training: Rank 0, Epoch 5082, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,406: INFO: model_training: Rank 0, Epoch 5082, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,408: INFO: model_training: Rank 0, Epoch 5082, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,409: INFO: model_training: Rank 0, Epoch 5082, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,411: INFO: model_training: Rank 0, Epoch 5082, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,413: INFO: model_training: Rank 0, Epoch 5083, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,414: INFO: model_training: Rank 0, Epoch 5083, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,416: INFO: model_training: Rank 0, Epoch 5083, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,417: INFO: model_training: Rank 0, Epoch 5083, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,418: INFO: model_training: Rank 0, Epoch 5083, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,420: INFO: model_training: Rank 0, Epoch 5084, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,421: INFO: model_training: Rank 0, Epoch 5084, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,423: INFO: model_training: Rank 0, Epoch 5084, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,425: INFO: model_training: Rank 0, Epoch 5084, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,426: INFO: model_training: Rank 0, Epoch 5084, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,427: INFO: model_training: Rank 0, Epoch 5085, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,429: INFO: model_training: Rank 0, Epoch 5085, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,430: INFO: model_training: Rank 0, Epoch 5085, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,432: INFO: model_training: Rank 0, Epoch 5085, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,433: INFO: model_training: Rank 0, Epoch 5085, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,435: INFO: model_training: Rank 0, Epoch 5086, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,436: INFO: model_training: Rank 0, Epoch 5086, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,437: INFO: model_training: Rank 0, Epoch 5086, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,438: INFO: model_training: Rank 0, Epoch 5086, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,440: INFO: model_training: Rank 0, Epoch 5086, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,441: INFO: model_training: Rank 0, Epoch 5087, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,442: INFO: model_training: Rank 0, Epoch 5087, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,444: INFO: model_training: Rank 0, Epoch 5087, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,446: INFO: model_training: Rank 0, Epoch 5087, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,447: INFO: model_training: Rank 0, Epoch 5087, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,448: INFO: model_training: Rank 0, Epoch 5088, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,450: INFO: model_training: Rank 0, Epoch 5088, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,451: INFO: model_training: Rank 0, Epoch 5088, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,453: INFO: model_training: Rank 0, Epoch 5088, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,454: INFO: model_training: Rank 0, Epoch 5088, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,456: INFO: model_training: Rank 0, Epoch 5089, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,458: INFO: model_training: Rank 0, Epoch 5089, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,459: INFO: model_training: Rank 0, Epoch 5089, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,461: INFO: model_training: Rank 0, Epoch 5089, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,462: INFO: model_training: Rank 0, Epoch 5089, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,464: INFO: model_training: Rank 0, Epoch 5090, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,465: INFO: model_training: Rank 0, Epoch 5090, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,466: INFO: model_training: Rank 0, Epoch 5090, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,467: INFO: model_training: Rank 0, Epoch 5090, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,469: INFO: model_training: Rank 0, Epoch 5090, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,470: INFO: model_training: Rank 0, Epoch 5091, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,472: INFO: model_training: Rank 0, Epoch 5091, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,473: INFO: model_training: Rank 0, Epoch 5091, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,475: INFO: model_training: Rank 0, Epoch 5091, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,476: INFO: model_training: Rank 0, Epoch 5091, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,478: INFO: model_training: Rank 0, Epoch 5092, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,479: INFO: model_training: Rank 0, Epoch 5092, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,481: INFO: model_training: Rank 0, Epoch 5092, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,482: INFO: model_training: Rank 0, Epoch 5092, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,483: INFO: model_training: Rank 0, Epoch 5092, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,485: INFO: model_training: Rank 0, Epoch 5093, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,486: INFO: model_training: Rank 0, Epoch 5093, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,488: INFO: model_training: Rank 0, Epoch 5093, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,489: INFO: model_training: Rank 0, Epoch 5093, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,490: INFO: model_training: Rank 0, Epoch 5093, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,492: INFO: model_training: Rank 0, Epoch 5094, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,494: INFO: model_training: Rank 0, Epoch 5094, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,496: INFO: model_training: Rank 0, Epoch 5094, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,497: INFO: model_training: Rank 0, Epoch 5094, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,498: INFO: model_training: Rank 0, Epoch 5094, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,500: INFO: model_training: Rank 0, Epoch 5095, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,501: INFO: model_training: Rank 0, Epoch 5095, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,503: INFO: model_training: Rank 0, Epoch 5095, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,505: INFO: model_training: Rank 0, Epoch 5095, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,506: INFO: model_training: Rank 0, Epoch 5095, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,507: INFO: model_training: Rank 0, Epoch 5096, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,509: INFO: model_training: Rank 0, Epoch 5096, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,511: INFO: model_training: Rank 0, Epoch 5096, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,512: INFO: model_training: Rank 0, Epoch 5096, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,514: INFO: model_training: Rank 0, Epoch 5096, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,515: INFO: model_training: Rank 0, Epoch 5097, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,516: INFO: model_training: Rank 0, Epoch 5097, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,517: INFO: model_training: Rank 0, Epoch 5097, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,519: INFO: model_training: Rank 0, Epoch 5097, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,520: INFO: model_training: Rank 0, Epoch 5097, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,522: INFO: model_training: Rank 0, Epoch 5098, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,523: INFO: model_training: Rank 0, Epoch 5098, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,525: INFO: model_training: Rank 0, Epoch 5098, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,526: INFO: model_training: Rank 0, Epoch 5098, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,528: INFO: model_training: Rank 0, Epoch 5098, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,529: INFO: model_training: Rank 0, Epoch 5099, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,531: INFO: model_training: Rank 0, Epoch 5099, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,532: INFO: model_training: Rank 0, Epoch 5099, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,533: INFO: model_training: Rank 0, Epoch 5099, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,535: INFO: model_training: Rank 0, Epoch 5099, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,536: INFO: model_training: Rank 0, Epoch 5100, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,537: INFO: model_training: Rank 0, Epoch 5100, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,538: INFO: model_training: Rank 0, Epoch 5100, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,540: INFO: model_training: Rank 0, Epoch 5100, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,542: INFO: model_training: Rank 0, Epoch 5100, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,543: INFO: model_training: Rank 0, Epoch 5101, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,545: INFO: model_training: Rank 0, Epoch 5101, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,546: INFO: model_training: Rank 0, Epoch 5101, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,548: INFO: model_training: Rank 0, Epoch 5101, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,549: INFO: model_training: Rank 0, Epoch 5101, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,550: INFO: model_training: Rank 0, Epoch 5102, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,551: INFO: model_training: Rank 0, Epoch 5102, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,553: INFO: model_training: Rank 0, Epoch 5102, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,554: INFO: model_training: Rank 0, Epoch 5102, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,556: INFO: model_training: Rank 0, Epoch 5102, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,557: INFO: model_training: Rank 0, Epoch 5103, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,558: INFO: model_training: Rank 0, Epoch 5103, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,560: INFO: model_training: Rank 0, Epoch 5103, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,561: INFO: model_training: Rank 0, Epoch 5103, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,562: INFO: model_training: Rank 0, Epoch 5103, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,564: INFO: model_training: Rank 0, Epoch 5104, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,565: INFO: model_training: Rank 0, Epoch 5104, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,566: INFO: model_training: Rank 0, Epoch 5104, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,568: INFO: model_training: Rank 0, Epoch 5104, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,569: INFO: model_training: Rank 0, Epoch 5104, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,571: INFO: model_training: Rank 0, Epoch 5105, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,572: INFO: model_training: Rank 0, Epoch 5105, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,574: INFO: model_training: Rank 0, Epoch 5105, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,575: INFO: model_training: Rank 0, Epoch 5105, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,577: INFO: model_training: Rank 0, Epoch 5105, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,578: INFO: model_training: Rank 0, Epoch 5106, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,579: INFO: model_training: Rank 0, Epoch 5106, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,580: INFO: model_training: Rank 0, Epoch 5106, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,582: INFO: model_training: Rank 0, Epoch 5106, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,583: INFO: model_training: Rank 0, Epoch 5106, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,584: INFO: model_training: Rank 0, Epoch 5107, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,587: INFO: model_training: Rank 0, Epoch 5107, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,588: INFO: model_training: Rank 0, Epoch 5107, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,589: INFO: model_training: Rank 0, Epoch 5107, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,591: INFO: model_training: Rank 0, Epoch 5107, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,592: INFO: model_training: Rank 0, Epoch 5108, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,594: INFO: model_training: Rank 0, Epoch 5108, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,596: INFO: model_training: Rank 0, Epoch 5108, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,597: INFO: model_training: Rank 0, Epoch 5108, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,598: INFO: model_training: Rank 0, Epoch 5108, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,599: INFO: model_training: Rank 0, Epoch 5109, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,601: INFO: model_training: Rank 0, Epoch 5109, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,603: INFO: model_training: Rank 0, Epoch 5109, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,604: INFO: model_training: Rank 0, Epoch 5109, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,605: INFO: model_training: Rank 0, Epoch 5109, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,606: INFO: model_training: Rank 0, Epoch 5110, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,608: INFO: model_training: Rank 0, Epoch 5110, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,609: INFO: model_training: Rank 0, Epoch 5110, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,611: INFO: model_training: Rank 0, Epoch 5110, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,612: INFO: model_training: Rank 0, Epoch 5110, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,614: INFO: model_training: Rank 0, Epoch 5111, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,616: INFO: model_training: Rank 0, Epoch 5111, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,618: INFO: model_training: Rank 0, Epoch 5111, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,620: INFO: model_training: Rank 0, Epoch 5111, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,621: INFO: model_training: Rank 0, Epoch 5111, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,622: INFO: model_training: Rank 0, Epoch 5112, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,623: INFO: model_training: Rank 0, Epoch 5112, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,625: INFO: model_training: Rank 0, Epoch 5112, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,627: INFO: model_training: Rank 0, Epoch 5112, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,628: INFO: model_training: Rank 0, Epoch 5112, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,630: INFO: model_training: Rank 0, Epoch 5113, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,631: INFO: model_training: Rank 0, Epoch 5113, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,632: INFO: model_training: Rank 0, Epoch 5113, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,634: INFO: model_training: Rank 0, Epoch 5113, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,635: INFO: model_training: Rank 0, Epoch 5113, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,637: INFO: model_training: Rank 0, Epoch 5114, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,638: INFO: model_training: Rank 0, Epoch 5114, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,639: INFO: model_training: Rank 0, Epoch 5114, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,640: INFO: model_training: Rank 0, Epoch 5114, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,641: INFO: model_training: Rank 0, Epoch 5114, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,643: INFO: model_training: Rank 0, Epoch 5115, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,645: INFO: model_training: Rank 0, Epoch 5115, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,646: INFO: model_training: Rank 0, Epoch 5115, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,647: INFO: model_training: Rank 0, Epoch 5115, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,648: INFO: model_training: Rank 0, Epoch 5115, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,650: INFO: model_training: Rank 0, Epoch 5116, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,651: INFO: model_training: Rank 0, Epoch 5116, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,653: INFO: model_training: Rank 0, Epoch 5116, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,654: INFO: model_training: Rank 0, Epoch 5116, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,655: INFO: model_training: Rank 0, Epoch 5116, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,656: INFO: model_training: Rank 0, Epoch 5117, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,658: INFO: model_training: Rank 0, Epoch 5117, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,660: INFO: model_training: Rank 0, Epoch 5117, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,662: INFO: model_training: Rank 0, Epoch 5117, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,663: INFO: model_training: Rank 0, Epoch 5117, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,664: INFO: model_training: Rank 0, Epoch 5118, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,665: INFO: model_training: Rank 0, Epoch 5118, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,666: INFO: model_training: Rank 0, Epoch 5118, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,667: INFO: model_training: Rank 0, Epoch 5118, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,670: INFO: model_training: Rank 0, Epoch 5118, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,671: INFO: model_training: Rank 0, Epoch 5119, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,673: INFO: model_training: Rank 0, Epoch 5119, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,674: INFO: model_training: Rank 0, Epoch 5119, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,675: INFO: model_training: Rank 0, Epoch 5119, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,677: INFO: model_training: Rank 0, Epoch 5119, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,679: INFO: model_training: Rank 0, Epoch 5120, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,680: INFO: model_training: Rank 0, Epoch 5120, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,681: INFO: model_training: Rank 0, Epoch 5120, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,682: INFO: model_training: Rank 0, Epoch 5120, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,683: INFO: model_training: Rank 0, Epoch 5120, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,685: INFO: model_training: Rank 0, Epoch 5121, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,687: INFO: model_training: Rank 0, Epoch 5121, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,688: INFO: model_training: Rank 0, Epoch 5121, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,689: INFO: model_training: Rank 0, Epoch 5121, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,690: INFO: model_training: Rank 0, Epoch 5121, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,692: INFO: model_training: Rank 0, Epoch 5122, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,694: INFO: model_training: Rank 0, Epoch 5122, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,695: INFO: model_training: Rank 0, Epoch 5122, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,697: INFO: model_training: Rank 0, Epoch 5122, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,698: INFO: model_training: Rank 0, Epoch 5122, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,699: INFO: model_training: Rank 0, Epoch 5123, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,700: INFO: model_training: Rank 0, Epoch 5123, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,701: INFO: model_training: Rank 0, Epoch 5123, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,703: INFO: model_training: Rank 0, Epoch 5123, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,704: INFO: model_training: Rank 0, Epoch 5123, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,706: INFO: model_training: Rank 0, Epoch 5124, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,707: INFO: model_training: Rank 0, Epoch 5124, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,708: INFO: model_training: Rank 0, Epoch 5124, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,710: INFO: model_training: Rank 0, Epoch 5124, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,712: INFO: model_training: Rank 0, Epoch 5124, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,713: INFO: model_training: Rank 0, Epoch 5125, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,714: INFO: model_training: Rank 0, Epoch 5125, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,715: INFO: model_training: Rank 0, Epoch 5125, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,717: INFO: model_training: Rank 0, Epoch 5125, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,718: INFO: model_training: Rank 0, Epoch 5125, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,720: INFO: model_training: Rank 0, Epoch 5126, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,721: INFO: model_training: Rank 0, Epoch 5126, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,722: INFO: model_training: Rank 0, Epoch 5126, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,724: INFO: model_training: Rank 0, Epoch 5126, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,726: INFO: model_training: Rank 0, Epoch 5126, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,727: INFO: model_training: Rank 0, Epoch 5127, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,729: INFO: model_training: Rank 0, Epoch 5127, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,730: INFO: model_training: Rank 0, Epoch 5127, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,731: INFO: model_training: Rank 0, Epoch 5127, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,733: INFO: model_training: Rank 0, Epoch 5127, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,734: INFO: model_training: Rank 0, Epoch 5128, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,736: INFO: model_training: Rank 0, Epoch 5128, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,738: INFO: model_training: Rank 0, Epoch 5128, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,739: INFO: model_training: Rank 0, Epoch 5128, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,740: INFO: model_training: Rank 0, Epoch 5128, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,741: INFO: model_training: Rank 0, Epoch 5129, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,743: INFO: model_training: Rank 0, Epoch 5129, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,745: INFO: model_training: Rank 0, Epoch 5129, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,746: INFO: model_training: Rank 0, Epoch 5129, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,747: INFO: model_training: Rank 0, Epoch 5129, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,748: INFO: model_training: Rank 0, Epoch 5130, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,750: INFO: model_training: Rank 0, Epoch 5130, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,751: INFO: model_training: Rank 0, Epoch 5130, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,752: INFO: model_training: Rank 0, Epoch 5130, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,754: INFO: model_training: Rank 0, Epoch 5130, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,755: INFO: model_training: Rank 0, Epoch 5131, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,756: INFO: model_training: Rank 0, Epoch 5131, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,758: INFO: model_training: Rank 0, Epoch 5131, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,759: INFO: model_training: Rank 0, Epoch 5131, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,760: INFO: model_training: Rank 0, Epoch 5131, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,762: INFO: model_training: Rank 0, Epoch 5132, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,763: INFO: model_training: Rank 0, Epoch 5132, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,764: INFO: model_training: Rank 0, Epoch 5132, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,765: INFO: model_training: Rank 0, Epoch 5132, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,767: INFO: model_training: Rank 0, Epoch 5132, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,768: INFO: model_training: Rank 0, Epoch 5133, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,770: INFO: model_training: Rank 0, Epoch 5133, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,771: INFO: model_training: Rank 0, Epoch 5133, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,773: INFO: model_training: Rank 0, Epoch 5133, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,774: INFO: model_training: Rank 0, Epoch 5133, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,775: INFO: model_training: Rank 0, Epoch 5134, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,777: INFO: model_training: Rank 0, Epoch 5134, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,779: INFO: model_training: Rank 0, Epoch 5134, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,780: INFO: model_training: Rank 0, Epoch 5134, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,781: INFO: model_training: Rank 0, Epoch 5134, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,783: INFO: model_training: Rank 0, Epoch 5135, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,784: INFO: model_training: Rank 0, Epoch 5135, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,786: INFO: model_training: Rank 0, Epoch 5135, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,788: INFO: model_training: Rank 0, Epoch 5135, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,789: INFO: model_training: Rank 0, Epoch 5135, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,790: INFO: model_training: Rank 0, Epoch 5136, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,791: INFO: model_training: Rank 0, Epoch 5136, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,793: INFO: model_training: Rank 0, Epoch 5136, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,801: INFO: model_training: Rank 0, Epoch 5136, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,810: INFO: model_training: Rank 0, Epoch 5136, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,820: INFO: model_training: Rank 0, Epoch 5137, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,822: INFO: model_training: Rank 0, Epoch 5137, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,823: INFO: model_training: Rank 0, Epoch 5137, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,827: INFO: model_training: Rank 0, Epoch 5137, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,831: INFO: model_training: Rank 0, Epoch 5137, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,833: INFO: model_training: Rank 0, Epoch 5138, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,835: INFO: model_training: Rank 0, Epoch 5138, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,836: INFO: model_training: Rank 0, Epoch 5138, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,838: INFO: model_training: Rank 0, Epoch 5138, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,839: INFO: model_training: Rank 0, Epoch 5138, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,841: INFO: model_training: Rank 0, Epoch 5139, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,842: INFO: model_training: Rank 0, Epoch 5139, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,843: INFO: model_training: Rank 0, Epoch 5139, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,845: INFO: model_training: Rank 0, Epoch 5139, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,846: INFO: model_training: Rank 0, Epoch 5139, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,848: INFO: model_training: Rank 0, Epoch 5140, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,849: INFO: model_training: Rank 0, Epoch 5140, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,850: INFO: model_training: Rank 0, Epoch 5140, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,852: INFO: model_training: Rank 0, Epoch 5140, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,853: INFO: model_training: Rank 0, Epoch 5140, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,855: INFO: model_training: Rank 0, Epoch 5141, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,856: INFO: model_training: Rank 0, Epoch 5141, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,857: INFO: model_training: Rank 0, Epoch 5141, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,859: INFO: model_training: Rank 0, Epoch 5141, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,860: INFO: model_training: Rank 0, Epoch 5141, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,862: INFO: model_training: Rank 0, Epoch 5142, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,863: INFO: model_training: Rank 0, Epoch 5142, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,865: INFO: model_training: Rank 0, Epoch 5142, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,866: INFO: model_training: Rank 0, Epoch 5142, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,867: INFO: model_training: Rank 0, Epoch 5142, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,868: INFO: model_training: Rank 0, Epoch 5143, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,870: INFO: model_training: Rank 0, Epoch 5143, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,871: INFO: model_training: Rank 0, Epoch 5143, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,872: INFO: model_training: Rank 0, Epoch 5143, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,874: INFO: model_training: Rank 0, Epoch 5143, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,875: INFO: model_training: Rank 0, Epoch 5144, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,877: INFO: model_training: Rank 0, Epoch 5144, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,879: INFO: model_training: Rank 0, Epoch 5144, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,880: INFO: model_training: Rank 0, Epoch 5144, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,881: INFO: model_training: Rank 0, Epoch 5144, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,882: INFO: model_training: Rank 0, Epoch 5145, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,883: INFO: model_training: Rank 0, Epoch 5145, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,885: INFO: model_training: Rank 0, Epoch 5145, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,886: INFO: model_training: Rank 0, Epoch 5145, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,888: INFO: model_training: Rank 0, Epoch 5145, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,889: INFO: model_training: Rank 0, Epoch 5146, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,890: INFO: model_training: Rank 0, Epoch 5146, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,892: INFO: model_training: Rank 0, Epoch 5146, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,893: INFO: model_training: Rank 0, Epoch 5146, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,895: INFO: model_training: Rank 0, Epoch 5146, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,896: INFO: model_training: Rank 0, Epoch 5147, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,897: INFO: model_training: Rank 0, Epoch 5147, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,898: INFO: model_training: Rank 0, Epoch 5147, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,899: INFO: model_training: Rank 0, Epoch 5147, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,900: INFO: model_training: Rank 0, Epoch 5147, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,902: INFO: model_training: Rank 0, Epoch 5148, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,903: INFO: model_training: Rank 0, Epoch 5148, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,904: INFO: model_training: Rank 0, Epoch 5148, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,906: INFO: model_training: Rank 0, Epoch 5148, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,907: INFO: model_training: Rank 0, Epoch 5148, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,909: INFO: model_training: Rank 0, Epoch 5149, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,911: INFO: model_training: Rank 0, Epoch 5149, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,912: INFO: model_training: Rank 0, Epoch 5149, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,913: INFO: model_training: Rank 0, Epoch 5149, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,914: INFO: model_training: Rank 0, Epoch 5149, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,916: INFO: model_training: Rank 0, Epoch 5150, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,917: INFO: model_training: Rank 0, Epoch 5150, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,918: INFO: model_training: Rank 0, Epoch 5150, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,920: INFO: model_training: Rank 0, Epoch 5150, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,922: INFO: model_training: Rank 0, Epoch 5150, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,923: INFO: model_training: Rank 0, Epoch 5151, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,924: INFO: model_training: Rank 0, Epoch 5151, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,926: INFO: model_training: Rank 0, Epoch 5151, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,927: INFO: model_training: Rank 0, Epoch 5151, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,929: INFO: model_training: Rank 0, Epoch 5151, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,931: INFO: model_training: Rank 0, Epoch 5152, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,932: INFO: model_training: Rank 0, Epoch 5152, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,933: INFO: model_training: Rank 0, Epoch 5152, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,934: INFO: model_training: Rank 0, Epoch 5152, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,936: INFO: model_training: Rank 0, Epoch 5152, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,938: INFO: model_training: Rank 0, Epoch 5153, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,939: INFO: model_training: Rank 0, Epoch 5153, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,941: INFO: model_training: Rank 0, Epoch 5153, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,942: INFO: model_training: Rank 0, Epoch 5153, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,943: INFO: model_training: Rank 0, Epoch 5153, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,945: INFO: model_training: Rank 0, Epoch 5154, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,946: INFO: model_training: Rank 0, Epoch 5154, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,948: INFO: model_training: Rank 0, Epoch 5154, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,949: INFO: model_training: Rank 0, Epoch 5154, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,950: INFO: model_training: Rank 0, Epoch 5154, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,951: INFO: model_training: Rank 0, Epoch 5155, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,953: INFO: model_training: Rank 0, Epoch 5155, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,954: INFO: model_training: Rank 0, Epoch 5155, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,956: INFO: model_training: Rank 0, Epoch 5155, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,957: INFO: model_training: Rank 0, Epoch 5155, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,958: INFO: model_training: Rank 0, Epoch 5156, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,959: INFO: model_training: Rank 0, Epoch 5156, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,961: INFO: model_training: Rank 0, Epoch 5156, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,962: INFO: model_training: Rank 0, Epoch 5156, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,964: INFO: model_training: Rank 0, Epoch 5156, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,965: INFO: model_training: Rank 0, Epoch 5157, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,966: INFO: model_training: Rank 0, Epoch 5157, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,967: INFO: model_training: Rank 0, Epoch 5157, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,969: INFO: model_training: Rank 0, Epoch 5157, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,971: INFO: model_training: Rank 0, Epoch 5157, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,972: INFO: model_training: Rank 0, Epoch 5158, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,973: INFO: model_training: Rank 0, Epoch 5158, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,974: INFO: model_training: Rank 0, Epoch 5158, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,976: INFO: model_training: Rank 0, Epoch 5158, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,977: INFO: model_training: Rank 0, Epoch 5158, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,978: INFO: model_training: Rank 0, Epoch 5159, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,980: INFO: model_training: Rank 0, Epoch 5159, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,981: INFO: model_training: Rank 0, Epoch 5159, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,982: INFO: model_training: Rank 0, Epoch 5159, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,983: INFO: model_training: Rank 0, Epoch 5159, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,986: INFO: model_training: Rank 0, Epoch 5160, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,987: INFO: model_training: Rank 0, Epoch 5160, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,988: INFO: model_training: Rank 0, Epoch 5160, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,990: INFO: model_training: Rank 0, Epoch 5160, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,991: INFO: model_training: Rank 0, Epoch 5160, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,992: INFO: model_training: Rank 0, Epoch 5161, Batch 0, Loss: 0.0500]
[2025-05-20 22:30:59,994: INFO: model_training: Rank 0, Epoch 5161, Batch 1, Loss: 0.0500]
[2025-05-20 22:30:59,995: INFO: model_training: Rank 0, Epoch 5161, Batch 2, Loss: 0.0500]
[2025-05-20 22:30:59,997: INFO: model_training: Rank 0, Epoch 5161, Batch 3, Loss: 0.0500]
[2025-05-20 22:30:59,998: INFO: model_training: Rank 0, Epoch 5161, Batch 4, Loss: 0.0500]
[2025-05-20 22:30:59,999: INFO: model_training: Rank 0, Epoch 5162, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,001: INFO: model_training: Rank 0, Epoch 5162, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,003: INFO: model_training: Rank 0, Epoch 5162, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,004: INFO: model_training: Rank 0, Epoch 5162, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,005: INFO: model_training: Rank 0, Epoch 5162, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,007: INFO: model_training: Rank 0, Epoch 5163, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,008: INFO: model_training: Rank 0, Epoch 5163, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,009: INFO: model_training: Rank 0, Epoch 5163, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,011: INFO: model_training: Rank 0, Epoch 5163, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,012: INFO: model_training: Rank 0, Epoch 5163, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,013: INFO: model_training: Rank 0, Epoch 5164, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,015: INFO: model_training: Rank 0, Epoch 5164, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,016: INFO: model_training: Rank 0, Epoch 5164, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,017: INFO: model_training: Rank 0, Epoch 5164, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,019: INFO: model_training: Rank 0, Epoch 5164, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,020: INFO: model_training: Rank 0, Epoch 5165, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,022: INFO: model_training: Rank 0, Epoch 5165, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,023: INFO: model_training: Rank 0, Epoch 5165, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,024: INFO: model_training: Rank 0, Epoch 5165, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,026: INFO: model_training: Rank 0, Epoch 5165, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,027: INFO: model_training: Rank 0, Epoch 5166, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,028: INFO: model_training: Rank 0, Epoch 5166, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,029: INFO: model_training: Rank 0, Epoch 5166, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,031: INFO: model_training: Rank 0, Epoch 5166, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,032: INFO: model_training: Rank 0, Epoch 5166, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,034: INFO: model_training: Rank 0, Epoch 5167, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,035: INFO: model_training: Rank 0, Epoch 5167, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,036: INFO: model_training: Rank 0, Epoch 5167, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,038: INFO: model_training: Rank 0, Epoch 5167, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,039: INFO: model_training: Rank 0, Epoch 5167, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,040: INFO: model_training: Rank 0, Epoch 5168, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,042: INFO: model_training: Rank 0, Epoch 5168, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,043: INFO: model_training: Rank 0, Epoch 5168, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,045: INFO: model_training: Rank 0, Epoch 5168, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,046: INFO: model_training: Rank 0, Epoch 5168, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,047: INFO: model_training: Rank 0, Epoch 5169, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,048: INFO: model_training: Rank 0, Epoch 5169, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,050: INFO: model_training: Rank 0, Epoch 5169, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,051: INFO: model_training: Rank 0, Epoch 5169, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,053: INFO: model_training: Rank 0, Epoch 5169, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,055: INFO: model_training: Rank 0, Epoch 5170, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,056: INFO: model_training: Rank 0, Epoch 5170, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,057: INFO: model_training: Rank 0, Epoch 5170, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,059: INFO: model_training: Rank 0, Epoch 5170, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,060: INFO: model_training: Rank 0, Epoch 5170, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,062: INFO: model_training: Rank 0, Epoch 5171, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,063: INFO: model_training: Rank 0, Epoch 5171, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,065: INFO: model_training: Rank 0, Epoch 5171, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,066: INFO: model_training: Rank 0, Epoch 5171, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,068: INFO: model_training: Rank 0, Epoch 5171, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,069: INFO: model_training: Rank 0, Epoch 5172, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,071: INFO: model_training: Rank 0, Epoch 5172, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,072: INFO: model_training: Rank 0, Epoch 5172, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,073: INFO: model_training: Rank 0, Epoch 5172, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,074: INFO: model_training: Rank 0, Epoch 5172, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,075: INFO: model_training: Rank 0, Epoch 5173, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,077: INFO: model_training: Rank 0, Epoch 5173, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,078: INFO: model_training: Rank 0, Epoch 5173, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,080: INFO: model_training: Rank 0, Epoch 5173, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,081: INFO: model_training: Rank 0, Epoch 5173, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,083: INFO: model_training: Rank 0, Epoch 5174, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,084: INFO: model_training: Rank 0, Epoch 5174, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,086: INFO: model_training: Rank 0, Epoch 5174, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,087: INFO: model_training: Rank 0, Epoch 5174, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,088: INFO: model_training: Rank 0, Epoch 5174, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,090: INFO: model_training: Rank 0, Epoch 5175, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,091: INFO: model_training: Rank 0, Epoch 5175, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,092: INFO: model_training: Rank 0, Epoch 5175, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,094: INFO: model_training: Rank 0, Epoch 5175, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,095: INFO: model_training: Rank 0, Epoch 5175, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,097: INFO: model_training: Rank 0, Epoch 5176, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,099: INFO: model_training: Rank 0, Epoch 5176, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,100: INFO: model_training: Rank 0, Epoch 5176, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,101: INFO: model_training: Rank 0, Epoch 5176, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,102: INFO: model_training: Rank 0, Epoch 5176, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,104: INFO: model_training: Rank 0, Epoch 5177, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,105: INFO: model_training: Rank 0, Epoch 5177, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,106: INFO: model_training: Rank 0, Epoch 5177, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,107: INFO: model_training: Rank 0, Epoch 5177, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,108: INFO: model_training: Rank 0, Epoch 5177, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,110: INFO: model_training: Rank 0, Epoch 5178, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,112: INFO: model_training: Rank 0, Epoch 5178, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,113: INFO: model_training: Rank 0, Epoch 5178, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,115: INFO: model_training: Rank 0, Epoch 5178, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,116: INFO: model_training: Rank 0, Epoch 5178, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,118: INFO: model_training: Rank 0, Epoch 5179, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,119: INFO: model_training: Rank 0, Epoch 5179, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,120: INFO: model_training: Rank 0, Epoch 5179, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,121: INFO: model_training: Rank 0, Epoch 5179, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,123: INFO: model_training: Rank 0, Epoch 5179, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,124: INFO: model_training: Rank 0, Epoch 5180, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,125: INFO: model_training: Rank 0, Epoch 5180, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,127: INFO: model_training: Rank 0, Epoch 5180, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,128: INFO: model_training: Rank 0, Epoch 5180, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,130: INFO: model_training: Rank 0, Epoch 5180, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,131: INFO: model_training: Rank 0, Epoch 5181, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,132: INFO: model_training: Rank 0, Epoch 5181, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,134: INFO: model_training: Rank 0, Epoch 5181, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,136: INFO: model_training: Rank 0, Epoch 5181, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,137: INFO: model_training: Rank 0, Epoch 5181, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,138: INFO: model_training: Rank 0, Epoch 5182, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,139: INFO: model_training: Rank 0, Epoch 5182, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,140: INFO: model_training: Rank 0, Epoch 5182, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,142: INFO: model_training: Rank 0, Epoch 5182, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,143: INFO: model_training: Rank 0, Epoch 5182, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,145: INFO: model_training: Rank 0, Epoch 5183, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,147: INFO: model_training: Rank 0, Epoch 5183, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,148: INFO: model_training: Rank 0, Epoch 5183, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,149: INFO: model_training: Rank 0, Epoch 5183, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,150: INFO: model_training: Rank 0, Epoch 5183, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,152: INFO: model_training: Rank 0, Epoch 5184, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,153: INFO: model_training: Rank 0, Epoch 5184, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,155: INFO: model_training: Rank 0, Epoch 5184, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,156: INFO: model_training: Rank 0, Epoch 5184, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,157: INFO: model_training: Rank 0, Epoch 5184, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,158: INFO: model_training: Rank 0, Epoch 5185, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,159: INFO: model_training: Rank 0, Epoch 5185, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,161: INFO: model_training: Rank 0, Epoch 5185, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,163: INFO: model_training: Rank 0, Epoch 5185, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,165: INFO: model_training: Rank 0, Epoch 5185, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,166: INFO: model_training: Rank 0, Epoch 5186, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,167: INFO: model_training: Rank 0, Epoch 5186, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,169: INFO: model_training: Rank 0, Epoch 5186, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,170: INFO: model_training: Rank 0, Epoch 5186, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,171: INFO: model_training: Rank 0, Epoch 5186, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,173: INFO: model_training: Rank 0, Epoch 5187, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,174: INFO: model_training: Rank 0, Epoch 5187, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,175: INFO: model_training: Rank 0, Epoch 5187, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,177: INFO: model_training: Rank 0, Epoch 5187, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,179: INFO: model_training: Rank 0, Epoch 5187, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,180: INFO: model_training: Rank 0, Epoch 5188, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,182: INFO: model_training: Rank 0, Epoch 5188, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,183: INFO: model_training: Rank 0, Epoch 5188, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,184: INFO: model_training: Rank 0, Epoch 5188, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,186: INFO: model_training: Rank 0, Epoch 5188, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,187: INFO: model_training: Rank 0, Epoch 5189, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,189: INFO: model_training: Rank 0, Epoch 5189, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,190: INFO: model_training: Rank 0, Epoch 5189, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,192: INFO: model_training: Rank 0, Epoch 5189, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,193: INFO: model_training: Rank 0, Epoch 5189, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,195: INFO: model_training: Rank 0, Epoch 5190, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,196: INFO: model_training: Rank 0, Epoch 5190, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,197: INFO: model_training: Rank 0, Epoch 5190, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,199: INFO: model_training: Rank 0, Epoch 5190, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,200: INFO: model_training: Rank 0, Epoch 5190, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,201: INFO: model_training: Rank 0, Epoch 5191, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,203: INFO: model_training: Rank 0, Epoch 5191, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,205: INFO: model_training: Rank 0, Epoch 5191, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,206: INFO: model_training: Rank 0, Epoch 5191, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,207: INFO: model_training: Rank 0, Epoch 5191, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,209: INFO: model_training: Rank 0, Epoch 5192, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,210: INFO: model_training: Rank 0, Epoch 5192, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,212: INFO: model_training: Rank 0, Epoch 5192, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,214: INFO: model_training: Rank 0, Epoch 5192, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,215: INFO: model_training: Rank 0, Epoch 5192, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,216: INFO: model_training: Rank 0, Epoch 5193, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,218: INFO: model_training: Rank 0, Epoch 5193, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,219: INFO: model_training: Rank 0, Epoch 5193, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,221: INFO: model_training: Rank 0, Epoch 5193, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,222: INFO: model_training: Rank 0, Epoch 5193, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,223: INFO: model_training: Rank 0, Epoch 5194, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,224: INFO: model_training: Rank 0, Epoch 5194, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,225: INFO: model_training: Rank 0, Epoch 5194, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,228: INFO: model_training: Rank 0, Epoch 5194, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,229: INFO: model_training: Rank 0, Epoch 5194, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,230: INFO: model_training: Rank 0, Epoch 5195, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,232: INFO: model_training: Rank 0, Epoch 5195, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,233: INFO: model_training: Rank 0, Epoch 5195, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,234: INFO: model_training: Rank 0, Epoch 5195, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,236: INFO: model_training: Rank 0, Epoch 5195, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,238: INFO: model_training: Rank 0, Epoch 5196, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,239: INFO: model_training: Rank 0, Epoch 5196, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,240: INFO: model_training: Rank 0, Epoch 5196, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,241: INFO: model_training: Rank 0, Epoch 5196, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,242: INFO: model_training: Rank 0, Epoch 5196, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,244: INFO: model_training: Rank 0, Epoch 5197, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,246: INFO: model_training: Rank 0, Epoch 5197, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,248: INFO: model_training: Rank 0, Epoch 5197, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,249: INFO: model_training: Rank 0, Epoch 5197, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,250: INFO: model_training: Rank 0, Epoch 5197, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,251: INFO: model_training: Rank 0, Epoch 5198, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,253: INFO: model_training: Rank 0, Epoch 5198, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,255: INFO: model_training: Rank 0, Epoch 5198, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,256: INFO: model_training: Rank 0, Epoch 5198, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,257: INFO: model_training: Rank 0, Epoch 5198, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,259: INFO: model_training: Rank 0, Epoch 5199, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,260: INFO: model_training: Rank 0, Epoch 5199, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,262: INFO: model_training: Rank 0, Epoch 5199, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,263: INFO: model_training: Rank 0, Epoch 5199, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,265: INFO: model_training: Rank 0, Epoch 5199, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,266: INFO: model_training: Rank 0, Epoch 5200, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,267: INFO: model_training: Rank 0, Epoch 5200, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,269: INFO: model_training: Rank 0, Epoch 5200, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,270: INFO: model_training: Rank 0, Epoch 5200, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,272: INFO: model_training: Rank 0, Epoch 5200, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,273: INFO: model_training: Rank 0, Epoch 5201, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,274: INFO: model_training: Rank 0, Epoch 5201, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,276: INFO: model_training: Rank 0, Epoch 5201, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,277: INFO: model_training: Rank 0, Epoch 5201, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,279: INFO: model_training: Rank 0, Epoch 5201, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,281: INFO: model_training: Rank 0, Epoch 5202, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,282: INFO: model_training: Rank 0, Epoch 5202, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,284: INFO: model_training: Rank 0, Epoch 5202, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,285: INFO: model_training: Rank 0, Epoch 5202, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,287: INFO: model_training: Rank 0, Epoch 5202, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,288: INFO: model_training: Rank 0, Epoch 5203, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,290: INFO: model_training: Rank 0, Epoch 5203, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,291: INFO: model_training: Rank 0, Epoch 5203, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,292: INFO: model_training: Rank 0, Epoch 5203, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,293: INFO: model_training: Rank 0, Epoch 5203, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,296: INFO: model_training: Rank 0, Epoch 5204, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,297: INFO: model_training: Rank 0, Epoch 5204, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,298: INFO: model_training: Rank 0, Epoch 5204, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,299: INFO: model_training: Rank 0, Epoch 5204, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,301: INFO: model_training: Rank 0, Epoch 5204, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,302: INFO: model_training: Rank 0, Epoch 5205, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,304: INFO: model_training: Rank 0, Epoch 5205, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,305: INFO: model_training: Rank 0, Epoch 5205, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,307: INFO: model_training: Rank 0, Epoch 5205, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,308: INFO: model_training: Rank 0, Epoch 5205, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,309: INFO: model_training: Rank 0, Epoch 5206, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,311: INFO: model_training: Rank 0, Epoch 5206, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,313: INFO: model_training: Rank 0, Epoch 5206, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,314: INFO: model_training: Rank 0, Epoch 5206, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,316: INFO: model_training: Rank 0, Epoch 5206, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,317: INFO: model_training: Rank 0, Epoch 5207, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,319: INFO: model_training: Rank 0, Epoch 5207, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,320: INFO: model_training: Rank 0, Epoch 5207, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,322: INFO: model_training: Rank 0, Epoch 5207, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,323: INFO: model_training: Rank 0, Epoch 5207, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,324: INFO: model_training: Rank 0, Epoch 5208, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,326: INFO: model_training: Rank 0, Epoch 5208, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,327: INFO: model_training: Rank 0, Epoch 5208, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,329: INFO: model_training: Rank 0, Epoch 5208, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,331: INFO: model_training: Rank 0, Epoch 5208, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,332: INFO: model_training: Rank 0, Epoch 5209, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,333: INFO: model_training: Rank 0, Epoch 5209, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,334: INFO: model_training: Rank 0, Epoch 5209, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,336: INFO: model_training: Rank 0, Epoch 5209, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,337: INFO: model_training: Rank 0, Epoch 5209, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,339: INFO: model_training: Rank 0, Epoch 5210, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,340: INFO: model_training: Rank 0, Epoch 5210, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,341: INFO: model_training: Rank 0, Epoch 5210, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,342: INFO: model_training: Rank 0, Epoch 5210, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,344: INFO: model_training: Rank 0, Epoch 5210, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,346: INFO: model_training: Rank 0, Epoch 5211, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,348: INFO: model_training: Rank 0, Epoch 5211, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,349: INFO: model_training: Rank 0, Epoch 5211, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,350: INFO: model_training: Rank 0, Epoch 5211, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,351: INFO: model_training: Rank 0, Epoch 5211, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,353: INFO: model_training: Rank 0, Epoch 5212, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,355: INFO: model_training: Rank 0, Epoch 5212, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,357: INFO: model_training: Rank 0, Epoch 5212, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,358: INFO: model_training: Rank 0, Epoch 5212, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,359: INFO: model_training: Rank 0, Epoch 5212, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,360: INFO: model_training: Rank 0, Epoch 5213, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,363: INFO: model_training: Rank 0, Epoch 5213, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,364: INFO: model_training: Rank 0, Epoch 5213, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,365: INFO: model_training: Rank 0, Epoch 5213, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,367: INFO: model_training: Rank 0, Epoch 5213, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,368: INFO: model_training: Rank 0, Epoch 5214, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,369: INFO: model_training: Rank 0, Epoch 5214, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,371: INFO: model_training: Rank 0, Epoch 5214, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,372: INFO: model_training: Rank 0, Epoch 5214, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,373: INFO: model_training: Rank 0, Epoch 5214, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,375: INFO: model_training: Rank 0, Epoch 5215, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,376: INFO: model_training: Rank 0, Epoch 5215, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,378: INFO: model_training: Rank 0, Epoch 5215, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,380: INFO: model_training: Rank 0, Epoch 5215, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,381: INFO: model_training: Rank 0, Epoch 5215, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,383: INFO: model_training: Rank 0, Epoch 5216, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,384: INFO: model_training: Rank 0, Epoch 5216, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,386: INFO: model_training: Rank 0, Epoch 5216, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,388: INFO: model_training: Rank 0, Epoch 5216, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,390: INFO: model_training: Rank 0, Epoch 5216, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,391: INFO: model_training: Rank 0, Epoch 5217, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,392: INFO: model_training: Rank 0, Epoch 5217, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,394: INFO: model_training: Rank 0, Epoch 5217, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,396: INFO: model_training: Rank 0, Epoch 5217, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,397: INFO: model_training: Rank 0, Epoch 5217, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,399: INFO: model_training: Rank 0, Epoch 5218, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,400: INFO: model_training: Rank 0, Epoch 5218, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,401: INFO: model_training: Rank 0, Epoch 5218, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,403: INFO: model_training: Rank 0, Epoch 5218, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,404: INFO: model_training: Rank 0, Epoch 5218, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,406: INFO: model_training: Rank 0, Epoch 5219, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,407: INFO: model_training: Rank 0, Epoch 5219, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,408: INFO: model_training: Rank 0, Epoch 5219, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,410: INFO: model_training: Rank 0, Epoch 5219, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,411: INFO: model_training: Rank 0, Epoch 5219, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,413: INFO: model_training: Rank 0, Epoch 5220, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,415: INFO: model_training: Rank 0, Epoch 5220, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,416: INFO: model_training: Rank 0, Epoch 5220, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,417: INFO: model_training: Rank 0, Epoch 5220, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,419: INFO: model_training: Rank 0, Epoch 5220, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,420: INFO: model_training: Rank 0, Epoch 5221, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,422: INFO: model_training: Rank 0, Epoch 5221, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,423: INFO: model_training: Rank 0, Epoch 5221, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,424: INFO: model_training: Rank 0, Epoch 5221, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,426: INFO: model_training: Rank 0, Epoch 5221, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,427: INFO: model_training: Rank 0, Epoch 5222, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,429: INFO: model_training: Rank 0, Epoch 5222, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,430: INFO: model_training: Rank 0, Epoch 5222, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,432: INFO: model_training: Rank 0, Epoch 5222, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,433: INFO: model_training: Rank 0, Epoch 5222, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,435: INFO: model_training: Rank 0, Epoch 5223, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,436: INFO: model_training: Rank 0, Epoch 5223, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,438: INFO: model_training: Rank 0, Epoch 5223, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,439: INFO: model_training: Rank 0, Epoch 5223, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,440: INFO: model_training: Rank 0, Epoch 5223, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,441: INFO: model_training: Rank 0, Epoch 5224, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,443: INFO: model_training: Rank 0, Epoch 5224, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,444: INFO: model_training: Rank 0, Epoch 5224, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,446: INFO: model_training: Rank 0, Epoch 5224, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,448: INFO: model_training: Rank 0, Epoch 5224, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,449: INFO: model_training: Rank 0, Epoch 5225, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,450: INFO: model_training: Rank 0, Epoch 5225, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,452: INFO: model_training: Rank 0, Epoch 5225, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,454: INFO: model_training: Rank 0, Epoch 5225, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,455: INFO: model_training: Rank 0, Epoch 5225, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,456: INFO: model_training: Rank 0, Epoch 5226, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,457: INFO: model_training: Rank 0, Epoch 5226, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,458: INFO: model_training: Rank 0, Epoch 5226, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,460: INFO: model_training: Rank 0, Epoch 5226, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,461: INFO: model_training: Rank 0, Epoch 5226, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,463: INFO: model_training: Rank 0, Epoch 5227, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,464: INFO: model_training: Rank 0, Epoch 5227, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,466: INFO: model_training: Rank 0, Epoch 5227, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,467: INFO: model_training: Rank 0, Epoch 5227, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,468: INFO: model_training: Rank 0, Epoch 5227, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,470: INFO: model_training: Rank 0, Epoch 5228, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,471: INFO: model_training: Rank 0, Epoch 5228, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,472: INFO: model_training: Rank 0, Epoch 5228, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,473: INFO: model_training: Rank 0, Epoch 5228, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,474: INFO: model_training: Rank 0, Epoch 5228, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,476: INFO: model_training: Rank 0, Epoch 5229, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,477: INFO: model_training: Rank 0, Epoch 5229, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,479: INFO: model_training: Rank 0, Epoch 5229, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,481: INFO: model_training: Rank 0, Epoch 5229, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,482: INFO: model_training: Rank 0, Epoch 5229, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,483: INFO: model_training: Rank 0, Epoch 5230, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,486: INFO: model_training: Rank 0, Epoch 5230, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,487: INFO: model_training: Rank 0, Epoch 5230, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,488: INFO: model_training: Rank 0, Epoch 5230, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,490: INFO: model_training: Rank 0, Epoch 5230, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,491: INFO: model_training: Rank 0, Epoch 5231, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,492: INFO: model_training: Rank 0, Epoch 5231, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,494: INFO: model_training: Rank 0, Epoch 5231, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,496: INFO: model_training: Rank 0, Epoch 5231, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,497: INFO: model_training: Rank 0, Epoch 5231, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,499: INFO: model_training: Rank 0, Epoch 5232, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,500: INFO: model_training: Rank 0, Epoch 5232, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,501: INFO: model_training: Rank 0, Epoch 5232, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,502: INFO: model_training: Rank 0, Epoch 5232, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,504: INFO: model_training: Rank 0, Epoch 5232, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,505: INFO: model_training: Rank 0, Epoch 5233, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,506: INFO: model_training: Rank 0, Epoch 5233, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,508: INFO: model_training: Rank 0, Epoch 5233, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,509: INFO: model_training: Rank 0, Epoch 5233, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,511: INFO: model_training: Rank 0, Epoch 5233, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,512: INFO: model_training: Rank 0, Epoch 5234, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,514: INFO: model_training: Rank 0, Epoch 5234, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,515: INFO: model_training: Rank 0, Epoch 5234, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,516: INFO: model_training: Rank 0, Epoch 5234, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,518: INFO: model_training: Rank 0, Epoch 5234, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,520: INFO: model_training: Rank 0, Epoch 5235, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,521: INFO: model_training: Rank 0, Epoch 5235, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,522: INFO: model_training: Rank 0, Epoch 5235, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,524: INFO: model_training: Rank 0, Epoch 5235, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,525: INFO: model_training: Rank 0, Epoch 5235, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,527: INFO: model_training: Rank 0, Epoch 5236, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,528: INFO: model_training: Rank 0, Epoch 5236, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,530: INFO: model_training: Rank 0, Epoch 5236, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,531: INFO: model_training: Rank 0, Epoch 5236, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,532: INFO: model_training: Rank 0, Epoch 5236, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,533: INFO: model_training: Rank 0, Epoch 5237, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,535: INFO: model_training: Rank 0, Epoch 5237, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,537: INFO: model_training: Rank 0, Epoch 5237, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,539: INFO: model_training: Rank 0, Epoch 5237, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,540: INFO: model_training: Rank 0, Epoch 5237, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,541: INFO: model_training: Rank 0, Epoch 5238, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,543: INFO: model_training: Rank 0, Epoch 5238, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,545: INFO: model_training: Rank 0, Epoch 5238, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,546: INFO: model_training: Rank 0, Epoch 5238, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,547: INFO: model_training: Rank 0, Epoch 5238, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,549: INFO: model_training: Rank 0, Epoch 5239, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,550: INFO: model_training: Rank 0, Epoch 5239, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,552: INFO: model_training: Rank 0, Epoch 5239, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,553: INFO: model_training: Rank 0, Epoch 5239, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,555: INFO: model_training: Rank 0, Epoch 5239, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,556: INFO: model_training: Rank 0, Epoch 5240, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,557: INFO: model_training: Rank 0, Epoch 5240, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,558: INFO: model_training: Rank 0, Epoch 5240, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,560: INFO: model_training: Rank 0, Epoch 5240, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,562: INFO: model_training: Rank 0, Epoch 5240, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,563: INFO: model_training: Rank 0, Epoch 5241, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,565: INFO: model_training: Rank 0, Epoch 5241, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,566: INFO: model_training: Rank 0, Epoch 5241, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,567: INFO: model_training: Rank 0, Epoch 5241, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,568: INFO: model_training: Rank 0, Epoch 5241, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,570: INFO: model_training: Rank 0, Epoch 5242, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,572: INFO: model_training: Rank 0, Epoch 5242, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,573: INFO: model_training: Rank 0, Epoch 5242, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,574: INFO: model_training: Rank 0, Epoch 5242, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,576: INFO: model_training: Rank 0, Epoch 5242, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,577: INFO: model_training: Rank 0, Epoch 5243, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,579: INFO: model_training: Rank 0, Epoch 5243, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,580: INFO: model_training: Rank 0, Epoch 5243, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,581: INFO: model_training: Rank 0, Epoch 5243, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,582: INFO: model_training: Rank 0, Epoch 5243, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,583: INFO: model_training: Rank 0, Epoch 5244, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,585: INFO: model_training: Rank 0, Epoch 5244, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,587: INFO: model_training: Rank 0, Epoch 5244, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,588: INFO: model_training: Rank 0, Epoch 5244, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,589: INFO: model_training: Rank 0, Epoch 5244, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,590: INFO: model_training: Rank 0, Epoch 5245, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,592: INFO: model_training: Rank 0, Epoch 5245, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,593: INFO: model_training: Rank 0, Epoch 5245, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,594: INFO: model_training: Rank 0, Epoch 5245, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,595: INFO: model_training: Rank 0, Epoch 5245, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,597: INFO: model_training: Rank 0, Epoch 5246, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,598: INFO: model_training: Rank 0, Epoch 5246, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,599: INFO: model_training: Rank 0, Epoch 5246, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,600: INFO: model_training: Rank 0, Epoch 5246, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,603: INFO: model_training: Rank 0, Epoch 5246, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,604: INFO: model_training: Rank 0, Epoch 5247, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,605: INFO: model_training: Rank 0, Epoch 5247, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,607: INFO: model_training: Rank 0, Epoch 5247, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,608: INFO: model_training: Rank 0, Epoch 5247, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,609: INFO: model_training: Rank 0, Epoch 5247, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,611: INFO: model_training: Rank 0, Epoch 5248, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,612: INFO: model_training: Rank 0, Epoch 5248, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,614: INFO: model_training: Rank 0, Epoch 5248, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,615: INFO: model_training: Rank 0, Epoch 5248, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,616: INFO: model_training: Rank 0, Epoch 5248, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,617: INFO: model_training: Rank 0, Epoch 5249, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,620: INFO: model_training: Rank 0, Epoch 5249, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,621: INFO: model_training: Rank 0, Epoch 5249, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,622: INFO: model_training: Rank 0, Epoch 5249, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,623: INFO: model_training: Rank 0, Epoch 5249, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,625: INFO: model_training: Rank 0, Epoch 5250, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,626: INFO: model_training: Rank 0, Epoch 5250, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,628: INFO: model_training: Rank 0, Epoch 5250, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,630: INFO: model_training: Rank 0, Epoch 5250, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,631: INFO: model_training: Rank 0, Epoch 5250, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,632: INFO: model_training: Rank 0, Epoch 5251, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,633: INFO: model_training: Rank 0, Epoch 5251, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,635: INFO: model_training: Rank 0, Epoch 5251, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,637: INFO: model_training: Rank 0, Epoch 5251, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,639: INFO: model_training: Rank 0, Epoch 5251, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,640: INFO: model_training: Rank 0, Epoch 5252, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,641: INFO: model_training: Rank 0, Epoch 5252, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,642: INFO: model_training: Rank 0, Epoch 5252, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,644: INFO: model_training: Rank 0, Epoch 5252, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,645: INFO: model_training: Rank 0, Epoch 5252, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,646: INFO: model_training: Rank 0, Epoch 5253, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,648: INFO: model_training: Rank 0, Epoch 5253, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,649: INFO: model_training: Rank 0, Epoch 5253, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,650: INFO: model_training: Rank 0, Epoch 5253, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,652: INFO: model_training: Rank 0, Epoch 5253, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,653: INFO: model_training: Rank 0, Epoch 5254, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,655: INFO: model_training: Rank 0, Epoch 5254, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,657: INFO: model_training: Rank 0, Epoch 5254, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,658: INFO: model_training: Rank 0, Epoch 5254, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,659: INFO: model_training: Rank 0, Epoch 5254, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,661: INFO: model_training: Rank 0, Epoch 5255, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,662: INFO: model_training: Rank 0, Epoch 5255, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,663: INFO: model_training: Rank 0, Epoch 5255, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,665: INFO: model_training: Rank 0, Epoch 5255, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,666: INFO: model_training: Rank 0, Epoch 5255, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,667: INFO: model_training: Rank 0, Epoch 5256, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,669: INFO: model_training: Rank 0, Epoch 5256, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,670: INFO: model_training: Rank 0, Epoch 5256, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,672: INFO: model_training: Rank 0, Epoch 5256, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,674: INFO: model_training: Rank 0, Epoch 5256, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,675: INFO: model_training: Rank 0, Epoch 5257, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,676: INFO: model_training: Rank 0, Epoch 5257, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,678: INFO: model_training: Rank 0, Epoch 5257, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,679: INFO: model_training: Rank 0, Epoch 5257, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,681: INFO: model_training: Rank 0, Epoch 5257, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,682: INFO: model_training: Rank 0, Epoch 5258, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,683: INFO: model_training: Rank 0, Epoch 5258, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,684: INFO: model_training: Rank 0, Epoch 5258, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,686: INFO: model_training: Rank 0, Epoch 5258, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,687: INFO: model_training: Rank 0, Epoch 5258, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,689: INFO: model_training: Rank 0, Epoch 5259, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,691: INFO: model_training: Rank 0, Epoch 5259, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,692: INFO: model_training: Rank 0, Epoch 5259, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,693: INFO: model_training: Rank 0, Epoch 5259, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,695: INFO: model_training: Rank 0, Epoch 5259, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,697: INFO: model_training: Rank 0, Epoch 5260, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,698: INFO: model_training: Rank 0, Epoch 5260, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,699: INFO: model_training: Rank 0, Epoch 5260, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,700: INFO: model_training: Rank 0, Epoch 5260, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,702: INFO: model_training: Rank 0, Epoch 5260, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,703: INFO: model_training: Rank 0, Epoch 5261, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,705: INFO: model_training: Rank 0, Epoch 5261, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,707: INFO: model_training: Rank 0, Epoch 5261, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,708: INFO: model_training: Rank 0, Epoch 5261, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,709: INFO: model_training: Rank 0, Epoch 5261, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,710: INFO: model_training: Rank 0, Epoch 5262, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,711: INFO: model_training: Rank 0, Epoch 5262, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,713: INFO: model_training: Rank 0, Epoch 5262, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,714: INFO: model_training: Rank 0, Epoch 5262, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,715: INFO: model_training: Rank 0, Epoch 5262, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,716: INFO: model_training: Rank 0, Epoch 5263, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,718: INFO: model_training: Rank 0, Epoch 5263, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,719: INFO: model_training: Rank 0, Epoch 5263, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,720: INFO: model_training: Rank 0, Epoch 5263, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,722: INFO: model_training: Rank 0, Epoch 5263, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,724: INFO: model_training: Rank 0, Epoch 5264, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,725: INFO: model_training: Rank 0, Epoch 5264, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,727: INFO: model_training: Rank 0, Epoch 5264, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,728: INFO: model_training: Rank 0, Epoch 5264, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,729: INFO: model_training: Rank 0, Epoch 5264, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,731: INFO: model_training: Rank 0, Epoch 5265, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,732: INFO: model_training: Rank 0, Epoch 5265, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,733: INFO: model_training: Rank 0, Epoch 5265, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,734: INFO: model_training: Rank 0, Epoch 5265, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,736: INFO: model_training: Rank 0, Epoch 5265, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,737: INFO: model_training: Rank 0, Epoch 5266, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,739: INFO: model_training: Rank 0, Epoch 5266, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,740: INFO: model_training: Rank 0, Epoch 5266, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,742: INFO: model_training: Rank 0, Epoch 5266, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,743: INFO: model_training: Rank 0, Epoch 5266, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,745: INFO: model_training: Rank 0, Epoch 5267, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,746: INFO: model_training: Rank 0, Epoch 5267, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,748: INFO: model_training: Rank 0, Epoch 5267, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,749: INFO: model_training: Rank 0, Epoch 5267, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,750: INFO: model_training: Rank 0, Epoch 5267, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,751: INFO: model_training: Rank 0, Epoch 5268, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,753: INFO: model_training: Rank 0, Epoch 5268, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,754: INFO: model_training: Rank 0, Epoch 5268, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,756: INFO: model_training: Rank 0, Epoch 5268, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,757: INFO: model_training: Rank 0, Epoch 5268, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,758: INFO: model_training: Rank 0, Epoch 5269, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,760: INFO: model_training: Rank 0, Epoch 5269, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,761: INFO: model_training: Rank 0, Epoch 5269, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,763: INFO: model_training: Rank 0, Epoch 5269, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,764: INFO: model_training: Rank 0, Epoch 5269, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,765: INFO: model_training: Rank 0, Epoch 5270, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,766: INFO: model_training: Rank 0, Epoch 5270, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,768: INFO: model_training: Rank 0, Epoch 5270, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,769: INFO: model_training: Rank 0, Epoch 5270, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,770: INFO: model_training: Rank 0, Epoch 5270, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,772: INFO: model_training: Rank 0, Epoch 5271, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,774: INFO: model_training: Rank 0, Epoch 5271, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,775: INFO: model_training: Rank 0, Epoch 5271, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,776: INFO: model_training: Rank 0, Epoch 5271, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,778: INFO: model_training: Rank 0, Epoch 5271, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,779: INFO: model_training: Rank 0, Epoch 5272, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,781: INFO: model_training: Rank 0, Epoch 5272, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,782: INFO: model_training: Rank 0, Epoch 5272, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,783: INFO: model_training: Rank 0, Epoch 5272, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,785: INFO: model_training: Rank 0, Epoch 5272, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,787: INFO: model_training: Rank 0, Epoch 5273, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,788: INFO: model_training: Rank 0, Epoch 5273, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,789: INFO: model_training: Rank 0, Epoch 5273, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,791: INFO: model_training: Rank 0, Epoch 5273, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,792: INFO: model_training: Rank 0, Epoch 5273, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,794: INFO: model_training: Rank 0, Epoch 5274, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,795: INFO: model_training: Rank 0, Epoch 5274, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,796: INFO: model_training: Rank 0, Epoch 5274, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,798: INFO: model_training: Rank 0, Epoch 5274, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,799: INFO: model_training: Rank 0, Epoch 5274, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,800: INFO: model_training: Rank 0, Epoch 5275, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,801: INFO: model_training: Rank 0, Epoch 5275, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,803: INFO: model_training: Rank 0, Epoch 5275, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,805: INFO: model_training: Rank 0, Epoch 5275, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,806: INFO: model_training: Rank 0, Epoch 5275, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,808: INFO: model_training: Rank 0, Epoch 5276, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,810: INFO: model_training: Rank 0, Epoch 5276, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,811: INFO: model_training: Rank 0, Epoch 5276, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,813: INFO: model_training: Rank 0, Epoch 5276, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,814: INFO: model_training: Rank 0, Epoch 5276, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,816: INFO: model_training: Rank 0, Epoch 5277, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,817: INFO: model_training: Rank 0, Epoch 5277, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,819: INFO: model_training: Rank 0, Epoch 5277, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,820: INFO: model_training: Rank 0, Epoch 5277, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,821: INFO: model_training: Rank 0, Epoch 5277, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,823: INFO: model_training: Rank 0, Epoch 5278, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,824: INFO: model_training: Rank 0, Epoch 5278, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,826: INFO: model_training: Rank 0, Epoch 5278, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,828: INFO: model_training: Rank 0, Epoch 5278, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,829: INFO: model_training: Rank 0, Epoch 5278, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,831: INFO: model_training: Rank 0, Epoch 5279, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,832: INFO: model_training: Rank 0, Epoch 5279, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,833: INFO: model_training: Rank 0, Epoch 5279, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,835: INFO: model_training: Rank 0, Epoch 5279, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,837: INFO: model_training: Rank 0, Epoch 5279, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,838: INFO: model_training: Rank 0, Epoch 5280, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,840: INFO: model_training: Rank 0, Epoch 5280, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,841: INFO: model_training: Rank 0, Epoch 5280, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,842: INFO: model_training: Rank 0, Epoch 5280, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,843: INFO: model_training: Rank 0, Epoch 5280, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,846: INFO: model_training: Rank 0, Epoch 5281, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,847: INFO: model_training: Rank 0, Epoch 5281, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,848: INFO: model_training: Rank 0, Epoch 5281, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,849: INFO: model_training: Rank 0, Epoch 5281, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,850: INFO: model_training: Rank 0, Epoch 5281, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,852: INFO: model_training: Rank 0, Epoch 5282, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,853: INFO: model_training: Rank 0, Epoch 5282, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,855: INFO: model_training: Rank 0, Epoch 5282, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,856: INFO: model_training: Rank 0, Epoch 5282, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,857: INFO: model_training: Rank 0, Epoch 5282, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,859: INFO: model_training: Rank 0, Epoch 5283, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,860: INFO: model_training: Rank 0, Epoch 5283, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,862: INFO: model_training: Rank 0, Epoch 5283, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,864: INFO: model_training: Rank 0, Epoch 5283, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,865: INFO: model_training: Rank 0, Epoch 5283, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,866: INFO: model_training: Rank 0, Epoch 5284, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,868: INFO: model_training: Rank 0, Epoch 5284, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,869: INFO: model_training: Rank 0, Epoch 5284, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,871: INFO: model_training: Rank 0, Epoch 5284, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,872: INFO: model_training: Rank 0, Epoch 5284, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,873: INFO: model_training: Rank 0, Epoch 5285, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,875: INFO: model_training: Rank 0, Epoch 5285, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,876: INFO: model_training: Rank 0, Epoch 5285, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,878: INFO: model_training: Rank 0, Epoch 5285, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,880: INFO: model_training: Rank 0, Epoch 5285, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,881: INFO: model_training: Rank 0, Epoch 5286, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,882: INFO: model_training: Rank 0, Epoch 5286, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,884: INFO: model_training: Rank 0, Epoch 5286, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,886: INFO: model_training: Rank 0, Epoch 5286, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,887: INFO: model_training: Rank 0, Epoch 5286, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,889: INFO: model_training: Rank 0, Epoch 5287, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,890: INFO: model_training: Rank 0, Epoch 5287, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,891: INFO: model_training: Rank 0, Epoch 5287, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,892: INFO: model_training: Rank 0, Epoch 5287, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,894: INFO: model_training: Rank 0, Epoch 5287, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,896: INFO: model_training: Rank 0, Epoch 5288, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,897: INFO: model_training: Rank 0, Epoch 5288, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,899: INFO: model_training: Rank 0, Epoch 5288, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,900: INFO: model_training: Rank 0, Epoch 5288, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,902: INFO: model_training: Rank 0, Epoch 5288, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,903: INFO: model_training: Rank 0, Epoch 5289, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,905: INFO: model_training: Rank 0, Epoch 5289, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,906: INFO: model_training: Rank 0, Epoch 5289, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,907: INFO: model_training: Rank 0, Epoch 5289, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,909: INFO: model_training: Rank 0, Epoch 5289, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,911: INFO: model_training: Rank 0, Epoch 5290, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,912: INFO: model_training: Rank 0, Epoch 5290, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,913: INFO: model_training: Rank 0, Epoch 5290, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,915: INFO: model_training: Rank 0, Epoch 5290, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,916: INFO: model_training: Rank 0, Epoch 5290, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,918: INFO: model_training: Rank 0, Epoch 5291, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,919: INFO: model_training: Rank 0, Epoch 5291, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,921: INFO: model_training: Rank 0, Epoch 5291, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,922: INFO: model_training: Rank 0, Epoch 5291, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,923: INFO: model_training: Rank 0, Epoch 5291, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,924: INFO: model_training: Rank 0, Epoch 5292, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,926: INFO: model_training: Rank 0, Epoch 5292, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,927: INFO: model_training: Rank 0, Epoch 5292, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,929: INFO: model_training: Rank 0, Epoch 5292, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,930: INFO: model_training: Rank 0, Epoch 5292, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,932: INFO: model_training: Rank 0, Epoch 5293, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,934: INFO: model_training: Rank 0, Epoch 5293, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,935: INFO: model_training: Rank 0, Epoch 5293, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,937: INFO: model_training: Rank 0, Epoch 5293, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,938: INFO: model_training: Rank 0, Epoch 5293, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,939: INFO: model_training: Rank 0, Epoch 5294, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,940: INFO: model_training: Rank 0, Epoch 5294, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,941: INFO: model_training: Rank 0, Epoch 5294, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,943: INFO: model_training: Rank 0, Epoch 5294, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,944: INFO: model_training: Rank 0, Epoch 5294, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,946: INFO: model_training: Rank 0, Epoch 5295, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,948: INFO: model_training: Rank 0, Epoch 5295, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,949: INFO: model_training: Rank 0, Epoch 5295, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,950: INFO: model_training: Rank 0, Epoch 5295, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,952: INFO: model_training: Rank 0, Epoch 5295, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,953: INFO: model_training: Rank 0, Epoch 5296, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,955: INFO: model_training: Rank 0, Epoch 5296, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,956: INFO: model_training: Rank 0, Epoch 5296, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,957: INFO: model_training: Rank 0, Epoch 5296, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,958: INFO: model_training: Rank 0, Epoch 5296, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,960: INFO: model_training: Rank 0, Epoch 5297, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,961: INFO: model_training: Rank 0, Epoch 5297, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,963: INFO: model_training: Rank 0, Epoch 5297, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,965: INFO: model_training: Rank 0, Epoch 5297, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,966: INFO: model_training: Rank 0, Epoch 5297, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,968: INFO: model_training: Rank 0, Epoch 5298, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,969: INFO: model_training: Rank 0, Epoch 5298, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,971: INFO: model_training: Rank 0, Epoch 5298, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,972: INFO: model_training: Rank 0, Epoch 5298, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,973: INFO: model_training: Rank 0, Epoch 5298, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,975: INFO: model_training: Rank 0, Epoch 5299, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,977: INFO: model_training: Rank 0, Epoch 5299, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,978: INFO: model_training: Rank 0, Epoch 5299, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,980: INFO: model_training: Rank 0, Epoch 5299, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,981: INFO: model_training: Rank 0, Epoch 5299, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,982: INFO: model_training: Rank 0, Epoch 5300, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,983: INFO: model_training: Rank 0, Epoch 5300, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,985: INFO: model_training: Rank 0, Epoch 5300, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,986: INFO: model_training: Rank 0, Epoch 5300, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,988: INFO: model_training: Rank 0, Epoch 5300, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,989: INFO: model_training: Rank 0, Epoch 5301, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,990: INFO: model_training: Rank 0, Epoch 5301, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,991: INFO: model_training: Rank 0, Epoch 5301, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,992: INFO: model_training: Rank 0, Epoch 5301, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:00,994: INFO: model_training: Rank 0, Epoch 5301, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:00,996: INFO: model_training: Rank 0, Epoch 5302, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:00,997: INFO: model_training: Rank 0, Epoch 5302, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:00,998: INFO: model_training: Rank 0, Epoch 5302, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:00,999: INFO: model_training: Rank 0, Epoch 5302, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,001: INFO: model_training: Rank 0, Epoch 5302, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,003: INFO: model_training: Rank 0, Epoch 5303, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,004: INFO: model_training: Rank 0, Epoch 5303, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,005: INFO: model_training: Rank 0, Epoch 5303, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,007: INFO: model_training: Rank 0, Epoch 5303, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,008: INFO: model_training: Rank 0, Epoch 5303, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,009: INFO: model_training: Rank 0, Epoch 5304, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,010: INFO: model_training: Rank 0, Epoch 5304, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,012: INFO: model_training: Rank 0, Epoch 5304, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,014: INFO: model_training: Rank 0, Epoch 5304, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,015: INFO: model_training: Rank 0, Epoch 5304, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,016: INFO: model_training: Rank 0, Epoch 5305, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,018: INFO: model_training: Rank 0, Epoch 5305, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,019: INFO: model_training: Rank 0, Epoch 5305, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,021: INFO: model_training: Rank 0, Epoch 5305, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,022: INFO: model_training: Rank 0, Epoch 5305, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,023: INFO: model_training: Rank 0, Epoch 5306, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,024: INFO: model_training: Rank 0, Epoch 5306, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,026: INFO: model_training: Rank 0, Epoch 5306, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,027: INFO: model_training: Rank 0, Epoch 5306, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,028: INFO: model_training: Rank 0, Epoch 5306, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,030: INFO: model_training: Rank 0, Epoch 5307, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,031: INFO: model_training: Rank 0, Epoch 5307, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,033: INFO: model_training: Rank 0, Epoch 5307, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,034: INFO: model_training: Rank 0, Epoch 5307, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,036: INFO: model_training: Rank 0, Epoch 5307, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,037: INFO: model_training: Rank 0, Epoch 5308, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,038: INFO: model_training: Rank 0, Epoch 5308, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,039: INFO: model_training: Rank 0, Epoch 5308, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,040: INFO: model_training: Rank 0, Epoch 5308, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,042: INFO: model_training: Rank 0, Epoch 5308, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,043: INFO: model_training: Rank 0, Epoch 5309, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,045: INFO: model_training: Rank 0, Epoch 5309, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,047: INFO: model_training: Rank 0, Epoch 5309, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,048: INFO: model_training: Rank 0, Epoch 5309, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,049: INFO: model_training: Rank 0, Epoch 5309, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,050: INFO: model_training: Rank 0, Epoch 5310, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,052: INFO: model_training: Rank 0, Epoch 5310, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,054: INFO: model_training: Rank 0, Epoch 5310, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,055: INFO: model_training: Rank 0, Epoch 5310, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,056: INFO: model_training: Rank 0, Epoch 5310, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,058: INFO: model_training: Rank 0, Epoch 5311, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,059: INFO: model_training: Rank 0, Epoch 5311, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,060: INFO: model_training: Rank 0, Epoch 5311, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,062: INFO: model_training: Rank 0, Epoch 5311, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,063: INFO: model_training: Rank 0, Epoch 5311, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,065: INFO: model_training: Rank 0, Epoch 5312, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,066: INFO: model_training: Rank 0, Epoch 5312, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,067: INFO: model_training: Rank 0, Epoch 5312, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,069: INFO: model_training: Rank 0, Epoch 5312, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,070: INFO: model_training: Rank 0, Epoch 5312, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,071: INFO: model_training: Rank 0, Epoch 5313, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,072: INFO: model_training: Rank 0, Epoch 5313, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,074: INFO: model_training: Rank 0, Epoch 5313, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,075: INFO: model_training: Rank 0, Epoch 5313, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,076: INFO: model_training: Rank 0, Epoch 5313, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,078: INFO: model_training: Rank 0, Epoch 5314, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,079: INFO: model_training: Rank 0, Epoch 5314, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,081: INFO: model_training: Rank 0, Epoch 5314, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,082: INFO: model_training: Rank 0, Epoch 5314, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,084: INFO: model_training: Rank 0, Epoch 5314, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,085: INFO: model_training: Rank 0, Epoch 5315, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,087: INFO: model_training: Rank 0, Epoch 5315, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,088: INFO: model_training: Rank 0, Epoch 5315, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,089: INFO: model_training: Rank 0, Epoch 5315, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,090: INFO: model_training: Rank 0, Epoch 5315, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,092: INFO: model_training: Rank 0, Epoch 5316, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,093: INFO: model_training: Rank 0, Epoch 5316, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,095: INFO: model_training: Rank 0, Epoch 5316, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,096: INFO: model_training: Rank 0, Epoch 5316, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,098: INFO: model_training: Rank 0, Epoch 5316, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,099: INFO: model_training: Rank 0, Epoch 5317, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,100: INFO: model_training: Rank 0, Epoch 5317, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,101: INFO: model_training: Rank 0, Epoch 5317, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,103: INFO: model_training: Rank 0, Epoch 5317, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,105: INFO: model_training: Rank 0, Epoch 5317, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,106: INFO: model_training: Rank 0, Epoch 5318, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,107: INFO: model_training: Rank 0, Epoch 5318, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,108: INFO: model_training: Rank 0, Epoch 5318, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,110: INFO: model_training: Rank 0, Epoch 5318, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,111: INFO: model_training: Rank 0, Epoch 5318, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,112: INFO: model_training: Rank 0, Epoch 5319, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,113: INFO: model_training: Rank 0, Epoch 5319, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,115: INFO: model_training: Rank 0, Epoch 5319, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,116: INFO: model_training: Rank 0, Epoch 5319, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,117: INFO: model_training: Rank 0, Epoch 5319, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,119: INFO: model_training: Rank 0, Epoch 5320, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,120: INFO: model_training: Rank 0, Epoch 5320, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,122: INFO: model_training: Rank 0, Epoch 5320, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,123: INFO: model_training: Rank 0, Epoch 5320, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,124: INFO: model_training: Rank 0, Epoch 5320, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,125: INFO: model_training: Rank 0, Epoch 5321, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,127: INFO: model_training: Rank 0, Epoch 5321, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,128: INFO: model_training: Rank 0, Epoch 5321, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,130: INFO: model_training: Rank 0, Epoch 5321, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,131: INFO: model_training: Rank 0, Epoch 5321, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,133: INFO: model_training: Rank 0, Epoch 5322, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,135: INFO: model_training: Rank 0, Epoch 5322, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,136: INFO: model_training: Rank 0, Epoch 5322, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,137: INFO: model_training: Rank 0, Epoch 5322, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,139: INFO: model_training: Rank 0, Epoch 5322, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,140: INFO: model_training: Rank 0, Epoch 5323, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,141: INFO: model_training: Rank 0, Epoch 5323, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,143: INFO: model_training: Rank 0, Epoch 5323, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,144: INFO: model_training: Rank 0, Epoch 5323, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,146: INFO: model_training: Rank 0, Epoch 5323, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,147: INFO: model_training: Rank 0, Epoch 5324, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,149: INFO: model_training: Rank 0, Epoch 5324, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,151: INFO: model_training: Rank 0, Epoch 5324, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,153: INFO: model_training: Rank 0, Epoch 5324, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,154: INFO: model_training: Rank 0, Epoch 5324, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,156: INFO: model_training: Rank 0, Epoch 5325, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,157: INFO: model_training: Rank 0, Epoch 5325, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,159: INFO: model_training: Rank 0, Epoch 5325, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,161: INFO: model_training: Rank 0, Epoch 5325, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,162: INFO: model_training: Rank 0, Epoch 5325, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,164: INFO: model_training: Rank 0, Epoch 5326, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,165: INFO: model_training: Rank 0, Epoch 5326, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,167: INFO: model_training: Rank 0, Epoch 5326, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,168: INFO: model_training: Rank 0, Epoch 5326, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,170: INFO: model_training: Rank 0, Epoch 5326, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,171: INFO: model_training: Rank 0, Epoch 5327, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,173: INFO: model_training: Rank 0, Epoch 5327, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,174: INFO: model_training: Rank 0, Epoch 5327, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,175: INFO: model_training: Rank 0, Epoch 5327, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,177: INFO: model_training: Rank 0, Epoch 5327, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,178: INFO: model_training: Rank 0, Epoch 5328, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,179: INFO: model_training: Rank 0, Epoch 5328, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,181: INFO: model_training: Rank 0, Epoch 5328, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,182: INFO: model_training: Rank 0, Epoch 5328, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,184: INFO: model_training: Rank 0, Epoch 5328, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,185: INFO: model_training: Rank 0, Epoch 5329, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,187: INFO: model_training: Rank 0, Epoch 5329, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,188: INFO: model_training: Rank 0, Epoch 5329, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,189: INFO: model_training: Rank 0, Epoch 5329, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,190: INFO: model_training: Rank 0, Epoch 5329, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,191: INFO: model_training: Rank 0, Epoch 5330, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,193: INFO: model_training: Rank 0, Epoch 5330, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,194: INFO: model_training: Rank 0, Epoch 5330, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,196: INFO: model_training: Rank 0, Epoch 5330, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,197: INFO: model_training: Rank 0, Epoch 5330, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,199: INFO: model_training: Rank 0, Epoch 5331, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,200: INFO: model_training: Rank 0, Epoch 5331, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,202: INFO: model_training: Rank 0, Epoch 5331, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,203: INFO: model_training: Rank 0, Epoch 5331, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,204: INFO: model_training: Rank 0, Epoch 5331, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,205: INFO: model_training: Rank 0, Epoch 5332, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,207: INFO: model_training: Rank 0, Epoch 5332, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,208: INFO: model_training: Rank 0, Epoch 5332, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,209: INFO: model_training: Rank 0, Epoch 5332, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,211: INFO: model_training: Rank 0, Epoch 5332, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,212: INFO: model_training: Rank 0, Epoch 5333, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,214: INFO: model_training: Rank 0, Epoch 5333, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,215: INFO: model_training: Rank 0, Epoch 5333, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,217: INFO: model_training: Rank 0, Epoch 5333, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,218: INFO: model_training: Rank 0, Epoch 5333, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,220: INFO: model_training: Rank 0, Epoch 5334, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,221: INFO: model_training: Rank 0, Epoch 5334, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,223: INFO: model_training: Rank 0, Epoch 5334, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,224: INFO: model_training: Rank 0, Epoch 5334, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,225: INFO: model_training: Rank 0, Epoch 5334, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,227: INFO: model_training: Rank 0, Epoch 5335, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,228: INFO: model_training: Rank 0, Epoch 5335, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,229: INFO: model_training: Rank 0, Epoch 5335, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,230: INFO: model_training: Rank 0, Epoch 5335, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,232: INFO: model_training: Rank 0, Epoch 5335, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,233: INFO: model_training: Rank 0, Epoch 5336, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,234: INFO: model_training: Rank 0, Epoch 5336, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,236: INFO: model_training: Rank 0, Epoch 5336, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,237: INFO: model_training: Rank 0, Epoch 5336, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,239: INFO: model_training: Rank 0, Epoch 5336, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,240: INFO: model_training: Rank 0, Epoch 5337, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,241: INFO: model_training: Rank 0, Epoch 5337, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,242: INFO: model_training: Rank 0, Epoch 5337, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,244: INFO: model_training: Rank 0, Epoch 5337, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,245: INFO: model_training: Rank 0, Epoch 5337, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,246: INFO: model_training: Rank 0, Epoch 5338, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,248: INFO: model_training: Rank 0, Epoch 5338, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,250: INFO: model_training: Rank 0, Epoch 5338, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,251: INFO: model_training: Rank 0, Epoch 5338, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,252: INFO: model_training: Rank 0, Epoch 5338, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,253: INFO: model_training: Rank 0, Epoch 5339, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,255: INFO: model_training: Rank 0, Epoch 5339, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,256: INFO: model_training: Rank 0, Epoch 5339, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,258: INFO: model_training: Rank 0, Epoch 5339, Batch 3, Loss: 0.0500]
[2025-05-20 22:31:01,259: INFO: model_training: Rank 0, Epoch 5339, Batch 4, Loss: 0.0500]
[2025-05-20 22:31:01,260: INFO: model_training: Rank 0, Epoch 5340, Batch 0, Loss: 0.0500]
[2025-05-20 22:31:01,262: INFO: model_training: Rank 0, Epoch 5340, Batch 1, Loss: 0.0500]
[2025-05-20 22:31:01,263: INFO: model_training: Rank 0, Epoch 5340, Batch 2, Loss: 0.0500]
[2025-05-20 22:31:01,265: INFO: model_training: Rank 0, Epoch 5340, Batch 3, Loss: 0.0500]
[2025-05-20 22:33:40,688: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:33:40,690: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:33:40,691: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:33:40,691: INFO: common: created directory at: artifacts]
[2025-05-20 22:33:40,691: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:33:40,691: INFO: data_ingestion: File already exists]
[2025-05-20 22:33:40,691: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:33:40,691: INFO: main: *******************]
[2025-05-20 22:33:40,691: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:33:40,692: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:33:40,693: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:33:40,693: INFO: common: created directory at: artifacts]
[2025-05-20 22:33:40,693: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:33:47,411: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:33:47,413: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:33:47,724: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 1.9780]
[2025-05-20 22:33:48,453: INFO: distributed: Reducer buckets have been rebuilt in this iteration.]
[2025-05-20 22:33:48,649: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9472]
[2025-05-20 22:33:49,542: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.8730]
[2025-05-20 22:33:50,495: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.9105]
[2025-05-20 22:33:51,318: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8573]
[2025-05-20 22:33:52,087: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.8345]
[2025-05-20 22:33:52,318: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.8073]
[2025-05-20 22:33:52,525: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.7372]
[2025-05-20 22:33:52,728: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.6855]
[2025-05-20 22:33:52,935: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6611]
[2025-05-20 22:33:53,134: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6555]
[2025-05-20 22:33:53,332: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.6285]
[2025-05-20 22:33:53,538: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.5910]
[2025-05-20 22:33:53,745: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.5606]
[2025-05-20 22:33:53,946: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.4699]
[2025-05-20 22:33:54,148: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.5172]
[2025-05-20 22:33:54,353: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4500]
[2025-05-20 22:33:54,563: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.3842]
[2025-05-20 22:33:54,769: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.4078]
[2025-05-20 22:33:54,970: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.3803]
[2025-05-20 22:33:55,169: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.2967]
[2025-05-20 22:33:55,373: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.2811]
[2025-05-20 22:33:55,589: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.2400]
[2025-05-20 22:33:55,794: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2607]
[2025-05-20 22:33:56,001: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.1925]
[2025-05-20 22:33:56,205: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.2222]
[2025-05-20 22:33:56,420: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.2013]
[2025-05-20 22:33:56,625: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.1788]
[2025-05-20 22:33:56,826: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.1197]
[2025-05-20 22:33:57,026: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.1337]
[2025-05-20 22:33:57,239: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.1061]
[2025-05-20 22:33:57,467: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.0585]
[2025-05-20 22:33:57,695: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0631]
[2025-05-20 22:33:57,967: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 1.0584]
[2025-05-20 22:33:58,213: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 1.0208]
[2025-05-20 22:33:58,435: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 0.9918]
[2025-05-20 22:33:58,647: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.9700]
[2025-05-20 22:33:58,852: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9231]
[2025-05-20 22:33:59,056: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.9714]
[2025-05-20 22:33:59,262: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.9135]
[2025-05-20 22:33:59,494: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.8889]
[2025-05-20 22:33:59,714: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.9068]
[2025-05-20 22:33:59,949: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.8449]
[2025-05-20 22:34:00,174: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8767]
[2025-05-20 22:34:00,403: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8417]
[2025-05-20 22:34:00,632: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.7880]
[2025-05-20 22:34:00,864: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.8346]
[2025-05-20 22:34:01,081: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.7740]
[2025-05-20 22:34:01,308: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.7659]
[2025-05-20 22:34:01,543: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7488]
[2025-05-20 22:34:01,774: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.7317]
[2025-05-20 22:34:02,003: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.7326]
[2025-05-20 22:34:02,215: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.7182]
[2025-05-20 22:34:02,425: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.6710]
[2025-05-20 22:34:02,644: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.6884]
[2025-05-20 22:34:02,858: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6861]
[2025-05-20 22:34:03,071: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.6793]
[2025-05-20 22:34:03,292: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.6213]
[2025-05-20 22:34:03,506: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.5903]
[2025-05-20 22:34:03,721: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.5762]
[2025-05-20 22:34:03,938: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.6360]
[2025-05-20 22:34:04,187: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.5483]
[2025-05-20 22:34:04,393: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5829]
[2025-05-20 22:34:04,605: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.5796]
[2025-05-20 22:34:04,809: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5117]
[2025-05-20 22:34:05,014: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.5347]
[2025-05-20 22:34:05,218: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.5394]
[2025-05-20 22:34:05,425: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.5356]
[2025-05-20 22:34:05,631: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.4963]
[2025-05-20 22:34:05,837: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.4727]
[2025-05-20 22:34:06,039: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.5030]
[2025-05-20 22:34:06,241: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.4979]
[2025-05-20 22:34:06,445: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4517]
[2025-05-20 22:34:06,651: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4226]
[2025-05-20 22:34:06,852: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4865]
[2025-05-20 22:34:07,055: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4505]
[2025-05-20 22:34:07,259: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.4145]
[2025-05-20 22:34:07,464: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.4658]
[2025-05-20 22:34:07,666: INFO: model_training: Rank 0, Epoch 16, Batch 3, Loss: 0.3938]
[2025-05-20 22:34:07,866: INFO: model_training: Rank 0, Epoch 16, Batch 4, Loss: 0.4074]
[2025-05-20 22:34:08,069: INFO: model_training: Rank 0, Epoch 17, Batch 0, Loss: 0.4392]
[2025-05-20 22:34:08,272: INFO: model_training: Rank 0, Epoch 17, Batch 1, Loss: 0.3912]
[2025-05-20 22:34:08,475: INFO: model_training: Rank 0, Epoch 17, Batch 2, Loss: 0.3683]
[2025-05-20 22:34:08,688: INFO: model_training: Rank 0, Epoch 17, Batch 3, Loss: 0.3387]
[2025-05-20 22:34:08,891: INFO: model_training: Rank 0, Epoch 17, Batch 4, Loss: 0.3572]
[2025-05-20 22:34:09,094: INFO: model_training: Rank 0, Epoch 18, Batch 0, Loss: 0.3810]
[2025-05-20 22:34:09,295: INFO: model_training: Rank 0, Epoch 18, Batch 1, Loss: 0.3142]
[2025-05-20 22:34:09,499: INFO: model_training: Rank 0, Epoch 18, Batch 2, Loss: 0.3920]
[2025-05-20 22:34:09,735: INFO: model_training: Rank 0, Epoch 18, Batch 3, Loss: 0.3397]
[2025-05-20 22:34:09,936: INFO: model_training: Rank 0, Epoch 18, Batch 4, Loss: 0.3154]
[2025-05-20 22:34:10,133: INFO: model_training: Rank 0, Epoch 19, Batch 0, Loss: 0.3681]
[2025-05-20 22:34:10,332: INFO: model_training: Rank 0, Epoch 19, Batch 1, Loss: 0.3466]
[2025-05-20 22:34:10,532: INFO: model_training: Rank 0, Epoch 19, Batch 2, Loss: 0.2906]
[2025-05-20 22:34:10,744: INFO: model_training: Rank 0, Epoch 19, Batch 3, Loss: 0.3197]
[2025-05-20 22:34:10,950: INFO: model_training: Rank 0, Epoch 19, Batch 4, Loss: 0.2512]
[2025-05-20 22:34:11,157: INFO: model_training: Rank 0, Epoch 20, Batch 0, Loss: 0.2716]
[2025-05-20 22:34:11,399: INFO: model_training: Rank 0, Epoch 20, Batch 1, Loss: 0.2516]
[2025-05-20 22:34:11,603: INFO: model_training: Rank 0, Epoch 20, Batch 2, Loss: 0.2881]
[2025-05-20 22:34:11,857: INFO: model_training: Rank 0, Epoch 20, Batch 3, Loss: 0.3054]
[2025-05-20 22:34:12,110: INFO: model_training: Rank 0, Epoch 20, Batch 4, Loss: 0.2743]
[2025-05-20 22:34:12,349: INFO: model_training: Rank 0, Epoch 21, Batch 0, Loss: 0.3093]
[2025-05-20 22:34:12,571: INFO: model_training: Rank 0, Epoch 21, Batch 1, Loss: 0.2886]
[2025-05-20 22:34:12,782: INFO: model_training: Rank 0, Epoch 21, Batch 2, Loss: 0.2326]
[2025-05-20 22:34:12,982: INFO: model_training: Rank 0, Epoch 21, Batch 3, Loss: 0.2433]
[2025-05-20 22:34:13,183: INFO: model_training: Rank 0, Epoch 21, Batch 4, Loss: 0.2488]
[2025-05-20 22:34:13,428: INFO: model_training: Rank 0, Epoch 22, Batch 0, Loss: 0.2149]
[2025-05-20 22:34:13,665: INFO: model_training: Rank 0, Epoch 22, Batch 1, Loss: 0.2048]
[2025-05-20 22:34:13,913: INFO: model_training: Rank 0, Epoch 22, Batch 2, Loss: 0.1975]
[2025-05-20 22:34:14,145: INFO: model_training: Rank 0, Epoch 22, Batch 3, Loss: 0.1876]
[2025-05-20 22:34:14,368: INFO: model_training: Rank 0, Epoch 22, Batch 4, Loss: 0.1786]
[2025-05-20 22:34:14,602: INFO: model_training: Rank 0, Epoch 23, Batch 0, Loss: 0.1905]
[2025-05-20 22:34:14,817: INFO: model_training: Rank 0, Epoch 23, Batch 1, Loss: 0.2611]
[2025-05-20 22:34:15,022: INFO: model_training: Rank 0, Epoch 23, Batch 2, Loss: 0.2112]
[2025-05-20 22:34:15,246: INFO: model_training: Rank 0, Epoch 23, Batch 3, Loss: 0.2244]
[2025-05-20 22:34:15,469: INFO: model_training: Rank 0, Epoch 23, Batch 4, Loss: 0.2377]
[2025-05-20 22:34:15,668: INFO: model_training: Rank 0, Epoch 24, Batch 0, Loss: 0.1547]
[2025-05-20 22:34:15,872: INFO: model_training: Rank 0, Epoch 24, Batch 1, Loss: 0.2083]
[2025-05-20 22:34:16,081: INFO: model_training: Rank 0, Epoch 24, Batch 2, Loss: 0.1633]
[2025-05-20 22:34:16,284: INFO: model_training: Rank 0, Epoch 24, Batch 3, Loss: 0.1901]
[2025-05-20 22:34:16,486: INFO: model_training: Rank 0, Epoch 24, Batch 4, Loss: 0.1582]
[2025-05-20 22:34:16,709: INFO: model_training: Rank 0, Epoch 25, Batch 0, Loss: 0.2117]
[2025-05-20 22:34:16,909: INFO: model_training: Rank 0, Epoch 25, Batch 1, Loss: 0.2067]
[2025-05-20 22:34:17,130: INFO: model_training: Rank 0, Epoch 25, Batch 2, Loss: 0.1863]
[2025-05-20 22:34:17,350: INFO: model_training: Rank 0, Epoch 25, Batch 3, Loss: 0.2017]
[2025-05-20 22:34:17,570: INFO: model_training: Rank 0, Epoch 25, Batch 4, Loss: 0.1619]
[2025-05-20 22:34:17,777: INFO: model_training: Rank 0, Epoch 26, Batch 0, Loss: 0.1463]
[2025-05-20 22:34:17,998: INFO: model_training: Rank 0, Epoch 26, Batch 1, Loss: 0.1933]
[2025-05-20 22:34:18,213: INFO: model_training: Rank 0, Epoch 26, Batch 2, Loss: 0.1379]
[2025-05-20 22:34:18,424: INFO: model_training: Rank 0, Epoch 26, Batch 3, Loss: 0.1604]
[2025-05-20 22:34:18,631: INFO: model_training: Rank 0, Epoch 26, Batch 4, Loss: 0.1101]
[2025-05-20 22:34:18,845: INFO: model_training: Rank 0, Epoch 27, Batch 0, Loss: 0.1091]
[2025-05-20 22:34:19,088: INFO: model_training: Rank 0, Epoch 27, Batch 1, Loss: 0.1345]
[2025-05-20 22:34:19,294: INFO: model_training: Rank 0, Epoch 27, Batch 2, Loss: 0.1218]
[2025-05-20 22:34:19,501: INFO: model_training: Rank 0, Epoch 27, Batch 3, Loss: 0.1189]
[2025-05-20 22:34:19,718: INFO: model_training: Rank 0, Epoch 27, Batch 4, Loss: 0.1193]
[2025-05-20 22:34:19,930: INFO: model_training: Rank 0, Epoch 28, Batch 0, Loss: 0.1516]
[2025-05-20 22:34:20,160: INFO: model_training: Rank 0, Epoch 28, Batch 1, Loss: 0.1600]
[2025-05-20 22:34:20,390: INFO: model_training: Rank 0, Epoch 28, Batch 2, Loss: 0.0808]
[2025-05-20 22:34:20,601: INFO: model_training: Rank 0, Epoch 28, Batch 3, Loss: 0.0928]
[2025-05-20 22:34:20,805: INFO: model_training: Rank 0, Epoch 28, Batch 4, Loss: 0.1480]
[2025-05-20 22:34:21,021: INFO: model_training: Rank 0, Epoch 29, Batch 0, Loss: 0.1483]
[2025-05-20 22:34:21,257: INFO: model_training: Rank 0, Epoch 29, Batch 1, Loss: 0.0788]
[2025-05-20 22:34:21,473: INFO: model_training: Rank 0, Epoch 29, Batch 2, Loss: 0.1390]
[2025-05-20 22:34:21,688: INFO: model_training: Rank 0, Epoch 29, Batch 3, Loss: 0.1022]
[2025-05-20 22:34:21,902: INFO: model_training: Rank 0, Epoch 29, Batch 4, Loss: 0.1138]
[2025-05-20 22:34:22,114: INFO: model_training: Rank 0, Epoch 30, Batch 0, Loss: 0.1112]
[2025-05-20 22:34:22,329: INFO: model_training: Rank 0, Epoch 30, Batch 1, Loss: 0.1055]
[2025-05-20 22:34:22,531: INFO: model_training: Rank 0, Epoch 30, Batch 2, Loss: 0.0943]
[2025-05-20 22:34:22,727: INFO: model_training: Rank 0, Epoch 30, Batch 3, Loss: 0.0537]
[2025-05-20 22:34:22,928: INFO: model_training: Rank 0, Epoch 30, Batch 4, Loss: 0.1225]
[2025-05-20 22:34:23,138: INFO: model_training: Rank 0, Epoch 31, Batch 0, Loss: 0.0649]
[2025-05-20 22:34:23,344: INFO: model_training: Rank 0, Epoch 31, Batch 1, Loss: 0.0950]
[2025-05-20 22:34:23,574: INFO: model_training: Rank 0, Epoch 31, Batch 2, Loss: 0.1332]
[2025-05-20 22:34:23,785: INFO: model_training: Rank 0, Epoch 31, Batch 3, Loss: 0.0820]
[2025-05-20 22:34:24,003: INFO: model_training: Rank 0, Epoch 31, Batch 4, Loss: 0.1113]
[2025-05-20 22:34:24,217: INFO: model_training: Rank 0, Epoch 32, Batch 0, Loss: 0.1083]
[2025-05-20 22:34:24,419: INFO: model_training: Rank 0, Epoch 32, Batch 1, Loss: 0.0789]
[2025-05-20 22:34:24,623: INFO: model_training: Rank 0, Epoch 32, Batch 2, Loss: 0.0500]
[2025-05-20 22:34:24,820: INFO: model_training: Rank 0, Epoch 32, Batch 3, Loss: 0.0500]
[2025-05-20 22:34:25,049: INFO: model_training: Rank 0, Epoch 32, Batch 4, Loss: 0.0500]
[2025-05-20 22:34:25,271: INFO: model_training: Rank 0, Epoch 33, Batch 0, Loss: 0.1092]
[2025-05-20 22:34:25,476: INFO: model_training: Rank 0, Epoch 33, Batch 1, Loss: 0.0600]
[2025-05-20 22:34:25,694: INFO: model_training: Rank 0, Epoch 33, Batch 2, Loss: 0.0727]
[2025-05-20 22:34:25,896: INFO: model_training: Rank 0, Epoch 33, Batch 3, Loss: 0.1152]
[2025-05-20 22:34:26,099: INFO: model_training: Rank 0, Epoch 33, Batch 4, Loss: 0.0500]
[2025-05-20 22:34:26,299: INFO: model_training: Rank 0, Epoch 34, Batch 0, Loss: 0.0709]
[2025-05-20 22:34:26,500: INFO: model_training: Rank 0, Epoch 34, Batch 1, Loss: 0.0500]
[2025-05-20 22:34:26,699: INFO: model_training: Rank 0, Epoch 34, Batch 2, Loss: 0.1109]
[2025-05-20 22:34:26,906: INFO: model_training: Rank 0, Epoch 34, Batch 3, Loss: 0.0643]
[2025-05-20 22:34:27,115: INFO: model_training: Rank 0, Epoch 34, Batch 4, Loss: 0.1138]
[2025-05-20 22:34:27,314: INFO: model_training: Rank 0, Epoch 35, Batch 0, Loss: 0.0675]
[2025-05-20 22:34:27,516: INFO: model_training: Rank 0, Epoch 35, Batch 1, Loss: 0.0500]
[2025-05-20 22:34:27,722: INFO: model_training: Rank 0, Epoch 35, Batch 2, Loss: 0.1017]
[2025-05-20 22:34:27,943: INFO: model_training: Rank 0, Epoch 35, Batch 3, Loss: 0.0500]
[2025-05-20 22:34:28,148: INFO: model_training: Rank 0, Epoch 35, Batch 4, Loss: 0.0975]
[2025-05-20 22:34:28,354: INFO: model_training: Rank 0, Epoch 36, Batch 0, Loss: 0.0539]
[2025-05-20 22:34:28,563: INFO: model_training: Rank 0, Epoch 36, Batch 1, Loss: 0.0833]
[2025-05-20 22:34:28,813: INFO: model_training: Rank 0, Epoch 36, Batch 2, Loss: 0.0500]
[2025-05-20 22:34:29,021: INFO: model_training: Rank 0, Epoch 36, Batch 3, Loss: 0.0651]
[2025-05-20 22:34:29,225: INFO: model_training: Rank 0, Epoch 36, Batch 4, Loss: 0.0985]
[2025-05-20 22:34:29,429: INFO: model_training: Rank 0, Epoch 37, Batch 0, Loss: 0.0500]
[2025-05-20 22:34:29,641: INFO: model_training: Rank 0, Epoch 37, Batch 1, Loss: 0.0500]
[2025-05-20 22:49:17,195: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:49:17,197: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:49:17,198: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:49:17,198: INFO: common: created directory at: artifacts]
[2025-05-20 22:49:17,198: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:49:17,199: INFO: data_ingestion: File already exists]
[2025-05-20 22:49:17,199: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:49:17,199: INFO: main: *******************]
[2025-05-20 22:49:17,199: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:49:17,200: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:49:17,201: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:49:17,201: INFO: common: created directory at: artifacts]
[2025-05-20 22:49:17,201: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:49:23,984: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:49:23,986: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:49:24,320: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 2.0412]
[2025-05-20 22:49:25,220: INFO: distributed: Reducer buckets have been rebuilt in this iteration.]
[2025-05-20 22:49:25,463: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9694]
[2025-05-20 22:49:26,394: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.8987]
[2025-05-20 22:49:27,251: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.9189]
[2025-05-20 22:49:28,114: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8517]
[2025-05-20 22:49:28,972: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.7988]
[2025-05-20 22:49:29,186: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.7976]
[2025-05-20 22:49:29,407: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.7552]
[2025-05-20 22:49:29,622: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.6615]
[2025-05-20 22:49:29,837: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6338]
[2025-05-20 22:49:30,047: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6725]
[2025-05-20 22:49:30,248: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.6424]
[2025-05-20 22:49:30,451: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.5484]
[2025-05-20 22:49:30,654: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.5564]
[2025-05-20 22:49:30,860: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.4878]
[2025-05-20 22:49:31,065: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.4545]
[2025-05-20 22:49:31,280: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4319]
[2025-05-20 22:49:31,489: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.4219]
[2025-05-20 22:49:31,732: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.4068]
[2025-05-20 22:49:31,946: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.3130]
[2025-05-20 22:49:32,225: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.3635]
[2025-05-20 22:49:32,506: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.3049]
[2025-05-20 22:49:32,763: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.2945]
[2025-05-20 22:49:33,007: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2413]
[2025-05-20 22:49:33,250: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.1841]
[2025-05-20 22:49:33,474: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.1603]
[2025-05-20 22:49:33,707: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.1627]
[2025-05-20 22:49:33,935: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.1271]
[2025-05-20 22:49:34,177: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.1669]
[2025-05-20 22:49:34,387: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.1045]
[2025-05-20 22:49:34,610: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.1311]
[2025-05-20 22:49:34,823: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.1189]
[2025-05-20 22:49:35,034: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0737]
[2025-05-20 22:49:35,248: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 0.9806]
[2025-05-20 22:49:35,462: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 0.9737]
[2025-05-20 22:49:35,678: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 1.0100]
[2025-05-20 22:49:35,888: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.9543]
[2025-05-20 22:49:36,101: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9453]
[2025-05-20 22:49:36,311: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.9329]
[2025-05-20 22:49:36,519: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.9084]
[2025-05-20 22:49:36,729: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.8956]
[2025-05-20 22:49:36,939: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.8367]
[2025-05-20 22:49:37,154: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.9006]
[2025-05-20 22:49:37,413: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8322]
[2025-05-20 22:49:37,653: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8131]
[2025-05-20 22:49:37,931: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.7613]
[2025-05-20 22:49:38,178: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.7991]
[2025-05-20 22:49:38,401: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.7489]
[2025-05-20 22:49:38,623: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.8047]
[2025-05-20 22:49:38,830: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7674]
[2025-05-20 22:49:39,037: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.6924]
[2025-05-20 22:49:39,247: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.7562]
[2025-05-20 22:49:39,486: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.7195]
[2025-05-20 22:49:39,716: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.6662]
[2025-05-20 22:49:39,963: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.6564]
[2025-05-20 22:49:40,212: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6271]
[2025-05-20 22:49:40,450: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.6073]
[2025-05-20 22:49:40,690: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.6613]
[2025-05-20 22:49:40,909: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.6399]
[2025-05-20 22:49:41,114: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.6459]
[2025-05-20 22:49:41,334: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.6054]
[2025-05-20 22:49:41,545: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.6010]
[2025-05-20 22:49:41,754: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5356]
[2025-05-20 22:49:41,982: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.5693]
[2025-05-20 22:49:42,196: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5268]
[2025-05-20 22:49:42,402: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.5772]
[2025-05-20 22:49:42,608: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.4900]
[2025-05-20 22:49:42,813: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.5621]
[2025-05-20 22:49:43,036: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.5134]
[2025-05-20 22:49:43,252: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.4703]
[2025-05-20 22:49:43,458: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.5015]
[2025-05-20 22:49:43,666: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.4563]
[2025-05-20 22:49:43,873: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4524]
[2025-05-20 22:49:44,134: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4814]
[2025-05-20 22:49:44,342: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4729]
[2025-05-20 22:49:44,559: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4286]
[2025-05-20 22:49:44,770: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.3822]
[2025-05-20 22:49:44,981: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.3758]
[2025-05-20 22:50:17,702: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-20 22:50:17,703: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:50:17,704: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:50:17,704: INFO: common: created directory at: artifacts]
[2025-05-20 22:50:17,704: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-20 22:50:17,704: INFO: data_ingestion: File already exists]
[2025-05-20 22:50:17,705: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-20 22:50:17,705: INFO: main: *******************]
[2025-05-20 22:50:17,705: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-20 22:50:17,706: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-20 22:50:17,707: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-20 22:50:17,707: INFO: common: created directory at: artifacts]
[2025-05-20 22:50:17,707: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-20 22:50:24,503: INFO: distributed_c10d: Added key: store_based_barrier_key:1 to store for rank: 0]
[2025-05-20 22:50:24,504: INFO: distributed_c10d: Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 1 nodes.]
[2025-05-20 22:50:24,804: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 1.9723]
[2025-05-20 22:50:25,489: INFO: distributed: Reducer buckets have been rebuilt in this iteration.]
[2025-05-20 22:50:25,701: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9236]
[2025-05-20 22:50:26,449: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.9407]
[2025-05-20 22:50:27,204: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.8420]
[2025-05-20 22:50:27,930: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8301]
[2025-05-20 22:50:28,665: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.7855]
[2025-05-20 22:50:28,877: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.7712]
[2025-05-20 22:50:29,103: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.7395]
[2025-05-20 22:50:29,351: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.6556]
[2025-05-20 22:50:29,601: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6353]
[2025-05-20 22:50:29,876: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.6225]
[2025-05-20 22:50:30,143: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.5900]
[2025-05-20 22:50:30,394: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.5656]
[2025-05-20 22:50:30,603: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.5612]
[2025-05-20 22:50:30,807: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.4812]
[2025-05-20 22:50:31,011: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.4808]
[2025-05-20 22:50:31,245: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4805]
[2025-05-20 22:50:31,459: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.4576]
[2025-05-20 22:50:31,672: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.4265]
[2025-05-20 22:50:31,878: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.3831]
[2025-05-20 22:50:32,105: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.3212]
[2025-05-20 22:50:32,324: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.3526]
[2025-05-20 22:50:32,534: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.2809]
[2025-05-20 22:50:32,739: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2133]
[2025-05-20 22:50:32,944: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.2769]
[2025-05-20 22:50:33,153: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.1882]
[2025-05-20 22:50:33,381: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.1456]
[2025-05-20 22:50:33,592: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.1886]
[2025-05-20 22:50:33,802: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.1059]
[2025-05-20 22:50:34,007: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.0953]
[2025-05-20 22:50:34,208: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.1128]
[2025-05-20 22:50:34,419: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.0556]
[2025-05-20 22:50:34,620: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0693]
[2025-05-20 22:50:34,827: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 1.0717]
[2025-05-20 22:50:35,063: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 0.9804]
[2025-05-20 22:50:35,289: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 0.9518]
[2025-05-20 22:50:35,541: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 0.9570]
[2025-05-20 22:50:35,772: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9733]
[2025-05-20 22:50:36,013: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.9606]
[2025-05-20 22:50:36,259: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.8756]
[2025-05-20 22:50:36,502: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.8559]
[2025-05-20 22:50:36,766: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.9090]
[2025-05-20 22:50:36,972: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.8079]
[2025-05-20 22:50:37,176: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8596]
[2025-05-20 22:50:37,374: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8171]
[2025-05-20 22:50:37,609: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.7688]
[2025-05-20 22:50:37,871: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.7485]
[2025-05-20 22:50:38,128: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.7851]
[2025-05-20 22:50:38,365: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.7543]
[2025-05-20 22:50:38,623: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7476]
[2025-05-20 22:50:38,862: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.7332]
[2025-05-20 22:50:39,087: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.7457]
[2025-05-20 22:50:39,303: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.7303]
[2025-05-20 22:50:39,534: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.6370]
[2025-05-20 22:50:39,756: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.6611]
[2025-05-20 22:50:39,976: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6111]
[2025-05-20 22:50:40,208: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.5985]
[2025-05-20 22:50:40,438: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.6265]
[2025-05-20 22:50:40,660: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.6025]
[2025-05-20 22:50:40,886: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.5847]
[2025-05-20 22:50:41,106: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.5860]
[2025-05-20 22:50:41,340: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.5632]
[2025-05-20 22:50:41,569: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5748]
[2025-05-20 22:50:41,796: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.5699]
[2025-05-20 22:50:42,029: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5569]
[2025-05-20 22:50:42,277: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.5583]
[2025-05-20 22:50:42,504: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.5443]
[2025-05-20 22:50:42,734: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.5452]
[2025-05-20 22:50:42,962: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.5441]
[2025-05-20 22:50:43,194: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.4547]
[2025-05-20 22:50:43,417: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.5316]
[2025-05-20 22:50:43,638: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.4910]
[2025-05-20 22:50:43,860: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4869]
[2025-05-20 22:50:44,101: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4548]
[2025-05-20 22:50:44,330: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4924]
[2025-05-20 22:50:44,562: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4729]
[2025-05-20 22:50:44,804: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.4132]
[2025-05-20 22:50:45,033: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.3962]
[2025-05-20 22:50:45,274: INFO: model_training: Rank 0, Epoch 16, Batch 3, Loss: 0.3907]
[2025-05-20 22:50:45,536: INFO: model_training: Rank 0, Epoch 16, Batch 4, Loss: 0.3717]
[2025-05-20 22:50:45,763: INFO: model_training: Rank 0, Epoch 17, Batch 0, Loss: 0.4261]
[2025-05-20 22:50:45,982: INFO: model_training: Rank 0, Epoch 17, Batch 1, Loss: 0.4254]
[2025-05-20 22:50:46,222: INFO: model_training: Rank 0, Epoch 17, Batch 2, Loss: 0.3630]
[2025-05-20 22:50:46,454: INFO: model_training: Rank 0, Epoch 17, Batch 3, Loss: 0.3342]
[2025-05-20 22:50:46,682: INFO: model_training: Rank 0, Epoch 17, Batch 4, Loss: 0.4013]
[2025-05-20 22:50:46,923: INFO: model_training: Rank 0, Epoch 18, Batch 0, Loss: 0.3593]
[2025-05-20 22:50:47,162: INFO: model_training: Rank 0, Epoch 18, Batch 1, Loss: 0.3954]
[2025-05-20 22:50:47,386: INFO: model_training: Rank 0, Epoch 18, Batch 2, Loss: 0.3526]
[2025-05-20 22:50:47,612: INFO: model_training: Rank 0, Epoch 18, Batch 3, Loss: 0.3346]
[2025-05-20 22:50:47,866: INFO: model_training: Rank 0, Epoch 18, Batch 4, Loss: 0.3731]
[2025-05-20 22:50:48,134: INFO: model_training: Rank 0, Epoch 19, Batch 0, Loss: 0.3236]
[2025-05-20 22:50:48,384: INFO: model_training: Rank 0, Epoch 19, Batch 1, Loss: 0.2892]
[2025-05-20 22:50:48,620: INFO: model_training: Rank 0, Epoch 19, Batch 2, Loss: 0.2681]
[2025-05-20 22:50:48,840: INFO: model_training: Rank 0, Epoch 19, Batch 3, Loss: 0.3069]
[2025-05-20 22:50:49,061: INFO: model_training: Rank 0, Epoch 19, Batch 4, Loss: 0.3463]
[2025-05-20 22:50:49,279: INFO: model_training: Rank 0, Epoch 20, Batch 0, Loss: 0.2963]
[2025-05-20 22:50:49,654: INFO: model_training: Rank 0, Epoch 20, Batch 1, Loss: 0.2505]
[2025-05-20 22:50:49,879: INFO: model_training: Rank 0, Epoch 20, Batch 2, Loss: 0.2763]
[2025-05-20 22:50:50,097: INFO: model_training: Rank 0, Epoch 20, Batch 3, Loss: 0.2914]
[2025-05-20 22:50:50,318: INFO: model_training: Rank 0, Epoch 20, Batch 4, Loss: 0.2839]
[2025-05-20 22:50:50,530: INFO: model_training: Rank 0, Epoch 21, Batch 0, Loss: 0.2578]
[2025-05-20 22:50:50,743: INFO: model_training: Rank 0, Epoch 21, Batch 1, Loss: 0.2450]
[2025-05-20 22:50:50,980: INFO: model_training: Rank 0, Epoch 21, Batch 2, Loss: 0.2800]
[2025-05-20 22:50:51,202: INFO: model_training: Rank 0, Epoch 21, Batch 3, Loss: 0.2781]
[2025-05-20 22:50:51,415: INFO: model_training: Rank 0, Epoch 21, Batch 4, Loss: 0.2918]
[2025-05-20 22:50:51,625: INFO: model_training: Rank 0, Epoch 22, Batch 0, Loss: 0.2778]
[2025-05-20 22:50:51,856: INFO: model_training: Rank 0, Epoch 22, Batch 1, Loss: 0.2215]
[2025-05-20 22:50:52,070: INFO: model_training: Rank 0, Epoch 22, Batch 2, Loss: 0.1967]
[2025-05-20 22:50:52,283: INFO: model_training: Rank 0, Epoch 22, Batch 3, Loss: 0.2560]
[2025-05-20 22:50:52,488: INFO: model_training: Rank 0, Epoch 22, Batch 4, Loss: 0.2182]
[2025-05-20 22:50:52,710: INFO: model_training: Rank 0, Epoch 23, Batch 0, Loss: 0.2122]
[2025-05-20 22:50:52,928: INFO: model_training: Rank 0, Epoch 23, Batch 1, Loss: 0.1661]
[2025-05-20 22:50:53,138: INFO: model_training: Rank 0, Epoch 23, Batch 2, Loss: 0.2201]
[2025-05-20 22:50:53,358: INFO: model_training: Rank 0, Epoch 23, Batch 3, Loss: 0.1731]
[2025-05-20 22:50:53,605: INFO: model_training: Rank 0, Epoch 23, Batch 4, Loss: 0.1712]
[2025-05-20 22:50:53,846: INFO: model_training: Rank 0, Epoch 24, Batch 0, Loss: 0.1887]
[2025-05-20 22:50:54,096: INFO: model_training: Rank 0, Epoch 24, Batch 1, Loss: 0.2148]
[2025-05-20 22:50:54,347: INFO: model_training: Rank 0, Epoch 24, Batch 2, Loss: 0.1430]
[2025-05-20 22:50:54,593: INFO: model_training: Rank 0, Epoch 24, Batch 3, Loss: 0.2310]
[2025-05-20 22:50:54,851: INFO: model_training: Rank 0, Epoch 24, Batch 4, Loss: 0.1393]
[2025-05-20 22:50:55,058: INFO: model_training: Rank 0, Epoch 25, Batch 0, Loss: 0.1648]
[2025-05-20 22:50:55,261: INFO: model_training: Rank 0, Epoch 25, Batch 1, Loss: 0.1682]
[2025-05-20 22:50:55,462: INFO: model_training: Rank 0, Epoch 25, Batch 2, Loss: 0.1846]
[2025-05-20 22:50:55,664: INFO: model_training: Rank 0, Epoch 25, Batch 3, Loss: 0.1426]
[2025-05-20 22:50:55,899: INFO: model_training: Rank 0, Epoch 25, Batch 4, Loss: 0.1369]
[2025-05-20 22:50:56,110: INFO: model_training: Rank 0, Epoch 26, Batch 0, Loss: 0.1195]
[2025-05-20 22:50:56,325: INFO: model_training: Rank 0, Epoch 26, Batch 1, Loss: 0.1634]
[2025-05-20 22:50:56,525: INFO: model_training: Rank 0, Epoch 26, Batch 2, Loss: 0.1819]
[2025-05-20 22:50:56,723: INFO: model_training: Rank 0, Epoch 26, Batch 3, Loss: 0.1252]
[2025-05-20 22:50:56,921: INFO: model_training: Rank 0, Epoch 26, Batch 4, Loss: 0.1483]
[2025-05-20 22:50:57,122: INFO: model_training: Rank 0, Epoch 27, Batch 0, Loss: 0.1034]
[2025-05-20 22:50:57,336: INFO: model_training: Rank 0, Epoch 27, Batch 1, Loss: 0.1087]
[2025-05-20 22:50:57,541: INFO: model_training: Rank 0, Epoch 27, Batch 2, Loss: 0.1727]
[2025-05-20 22:50:57,766: INFO: model_training: Rank 0, Epoch 27, Batch 3, Loss: 0.1773]
[2025-05-20 22:50:57,962: INFO: model_training: Rank 0, Epoch 27, Batch 4, Loss: 0.0955]
[2025-05-20 22:50:58,156: INFO: model_training: Rank 0, Epoch 28, Batch 0, Loss: 0.1031]
[2025-05-20 22:50:58,364: INFO: model_training: Rank 0, Epoch 28, Batch 1, Loss: 0.1405]
[2025-05-20 22:50:58,584: INFO: model_training: Rank 0, Epoch 28, Batch 2, Loss: 0.1561]
[2025-05-20 22:50:58,804: INFO: model_training: Rank 0, Epoch 28, Batch 3, Loss: 0.1621]
[2025-05-20 22:50:59,018: INFO: model_training: Rank 0, Epoch 28, Batch 4, Loss: 0.1079]
[2025-05-20 22:50:59,234: INFO: model_training: Rank 0, Epoch 29, Batch 0, Loss: 0.1565]
[2025-05-20 22:50:59,440: INFO: model_training: Rank 0, Epoch 29, Batch 1, Loss: 0.1440]
[2025-05-20 22:50:59,637: INFO: model_training: Rank 0, Epoch 29, Batch 2, Loss: 0.0856]
[2025-05-20 22:50:59,834: INFO: model_training: Rank 0, Epoch 29, Batch 3, Loss: 0.0638]
[2025-05-20 22:51:00,031: INFO: model_training: Rank 0, Epoch 29, Batch 4, Loss: 0.1024]
[2025-05-20 22:51:00,232: INFO: model_training: Rank 0, Epoch 30, Batch 0, Loss: 0.1175]
[2025-05-20 22:51:00,440: INFO: model_training: Rank 0, Epoch 30, Batch 1, Loss: 0.0807]
[2025-05-20 22:51:00,647: INFO: model_training: Rank 0, Epoch 30, Batch 2, Loss: 0.0632]
[2025-05-20 22:51:00,856: INFO: model_training: Rank 0, Epoch 30, Batch 3, Loss: 0.1106]
[2025-05-20 22:51:01,058: INFO: model_training: Rank 0, Epoch 30, Batch 4, Loss: 0.1260]
[2025-05-20 22:51:01,257: INFO: model_training: Rank 0, Epoch 31, Batch 0, Loss: 0.0959]
[2025-05-20 22:51:01,453: INFO: model_training: Rank 0, Epoch 31, Batch 1, Loss: 0.1413]
[2025-05-20 22:51:01,652: INFO: model_training: Rank 0, Epoch 31, Batch 2, Loss: 0.1157]
[2025-05-20 22:51:01,854: INFO: model_training: Rank 0, Epoch 31, Batch 3, Loss: 0.1227]
[2025-05-20 22:51:02,052: INFO: model_training: Rank 0, Epoch 31, Batch 4, Loss: 0.1106]
[2025-05-20 22:51:02,256: INFO: model_training: Rank 0, Epoch 32, Batch 0, Loss: 0.0955]
[2025-05-20 22:51:02,457: INFO: model_training: Rank 0, Epoch 32, Batch 1, Loss: 0.1350]
[2025-05-20 22:51:02,658: INFO: model_training: Rank 0, Epoch 32, Batch 2, Loss: 0.0917]
[2025-05-20 22:51:02,856: INFO: model_training: Rank 0, Epoch 32, Batch 3, Loss: 0.1279]
[2025-05-20 22:51:03,059: INFO: model_training: Rank 0, Epoch 32, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:03,260: INFO: model_training: Rank 0, Epoch 33, Batch 0, Loss: 0.0517]
[2025-05-20 22:51:03,463: INFO: model_training: Rank 0, Epoch 33, Batch 1, Loss: 0.1153]
[2025-05-20 22:51:03,660: INFO: model_training: Rank 0, Epoch 33, Batch 2, Loss: 0.1043]
[2025-05-20 22:51:03,858: INFO: model_training: Rank 0, Epoch 33, Batch 3, Loss: 0.1048]
[2025-05-20 22:51:04,059: INFO: model_training: Rank 0, Epoch 33, Batch 4, Loss: 0.0701]
[2025-05-20 22:51:04,267: INFO: model_training: Rank 0, Epoch 34, Batch 0, Loss: 0.1207]
[2025-05-20 22:51:04,470: INFO: model_training: Rank 0, Epoch 34, Batch 1, Loss: 0.0982]
[2025-05-20 22:51:04,670: INFO: model_training: Rank 0, Epoch 34, Batch 2, Loss: 0.1044]
[2025-05-20 22:51:04,880: INFO: model_training: Rank 0, Epoch 34, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:05,088: INFO: model_training: Rank 0, Epoch 34, Batch 4, Loss: 0.0753]
[2025-05-20 22:51:05,292: INFO: model_training: Rank 0, Epoch 35, Batch 0, Loss: 0.0587]
[2025-05-20 22:51:05,498: INFO: model_training: Rank 0, Epoch 35, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:05,699: INFO: model_training: Rank 0, Epoch 35, Batch 2, Loss: 0.0547]
[2025-05-20 22:51:05,905: INFO: model_training: Rank 0, Epoch 35, Batch 3, Loss: 0.0742]
[2025-05-20 22:51:06,111: INFO: model_training: Rank 0, Epoch 35, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:06,317: INFO: model_training: Rank 0, Epoch 36, Batch 0, Loss: 0.0889]
[2025-05-20 22:51:06,531: INFO: model_training: Rank 0, Epoch 36, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:06,750: INFO: model_training: Rank 0, Epoch 36, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:06,964: INFO: model_training: Rank 0, Epoch 36, Batch 3, Loss: 0.0845]
[2025-05-20 22:51:07,174: INFO: model_training: Rank 0, Epoch 36, Batch 4, Loss: 0.0768]
[2025-05-20 22:51:07,393: INFO: model_training: Rank 0, Epoch 37, Batch 0, Loss: 0.0964]
[2025-05-20 22:51:07,618: INFO: model_training: Rank 0, Epoch 37, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:07,836: INFO: model_training: Rank 0, Epoch 37, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:08,059: INFO: model_training: Rank 0, Epoch 37, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:08,288: INFO: model_training: Rank 0, Epoch 37, Batch 4, Loss: 0.0866]
[2025-05-20 22:51:08,519: INFO: model_training: Rank 0, Epoch 38, Batch 0, Loss: 0.0723]
[2025-05-20 22:51:08,744: INFO: model_training: Rank 0, Epoch 38, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:08,972: INFO: model_training: Rank 0, Epoch 38, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:09,198: INFO: model_training: Rank 0, Epoch 38, Batch 3, Loss: 0.0691]
[2025-05-20 22:51:09,425: INFO: model_training: Rank 0, Epoch 38, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:09,658: INFO: model_training: Rank 0, Epoch 39, Batch 0, Loss: 0.0887]
[2025-05-20 22:51:09,885: INFO: model_training: Rank 0, Epoch 39, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:10,111: INFO: model_training: Rank 0, Epoch 39, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:10,335: INFO: model_training: Rank 0, Epoch 39, Batch 3, Loss: 0.0522]
[2025-05-20 22:51:10,564: INFO: model_training: Rank 0, Epoch 39, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:10,793: INFO: model_training: Rank 0, Epoch 40, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:11,021: INFO: model_training: Rank 0, Epoch 40, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:11,252: INFO: model_training: Rank 0, Epoch 40, Batch 2, Loss: 0.0612]
[2025-05-20 22:51:11,475: INFO: model_training: Rank 0, Epoch 40, Batch 3, Loss: 0.0829]
[2025-05-20 22:51:11,700: INFO: model_training: Rank 0, Epoch 40, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:11,930: INFO: model_training: Rank 0, Epoch 41, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:12,161: INFO: model_training: Rank 0, Epoch 41, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:12,398: INFO: model_training: Rank 0, Epoch 41, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:12,632: INFO: model_training: Rank 0, Epoch 41, Batch 3, Loss: 0.0640]
[2025-05-20 22:51:12,861: INFO: model_training: Rank 0, Epoch 41, Batch 4, Loss: 0.0756]
[2025-05-20 22:51:13,095: INFO: model_training: Rank 0, Epoch 42, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:13,334: INFO: model_training: Rank 0, Epoch 42, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:13,585: INFO: model_training: Rank 0, Epoch 42, Batch 2, Loss: 0.0656]
[2025-05-20 22:51:13,889: INFO: model_training: Rank 0, Epoch 42, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:14,166: INFO: model_training: Rank 0, Epoch 42, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:14,442: INFO: model_training: Rank 0, Epoch 43, Batch 0, Loss: 0.0668]
[2025-05-20 22:51:14,750: INFO: model_training: Rank 0, Epoch 43, Batch 1, Loss: 0.0513]
[2025-05-20 22:51:14,987: INFO: model_training: Rank 0, Epoch 43, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:15,194: INFO: model_training: Rank 0, Epoch 43, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:15,401: INFO: model_training: Rank 0, Epoch 43, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:15,608: INFO: model_training: Rank 0, Epoch 44, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:15,840: INFO: model_training: Rank 0, Epoch 44, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:16,074: INFO: model_training: Rank 0, Epoch 44, Batch 2, Loss: 0.0659]
[2025-05-20 22:51:16,299: INFO: model_training: Rank 0, Epoch 44, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:16,505: INFO: model_training: Rank 0, Epoch 44, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:16,705: INFO: model_training: Rank 0, Epoch 45, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:16,910: INFO: model_training: Rank 0, Epoch 45, Batch 1, Loss: 0.0557]
[2025-05-20 22:51:17,113: INFO: model_training: Rank 0, Epoch 45, Batch 2, Loss: 0.0533]
[2025-05-20 22:51:17,321: INFO: model_training: Rank 0, Epoch 45, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:17,523: INFO: model_training: Rank 0, Epoch 45, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:17,720: INFO: model_training: Rank 0, Epoch 46, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:17,923: INFO: model_training: Rank 0, Epoch 46, Batch 1, Loss: 0.0664]
[2025-05-20 22:51:18,144: INFO: model_training: Rank 0, Epoch 46, Batch 2, Loss: 0.0696]
[2025-05-20 22:51:18,346: INFO: model_training: Rank 0, Epoch 46, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:18,547: INFO: model_training: Rank 0, Epoch 46, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:18,754: INFO: model_training: Rank 0, Epoch 47, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:18,998: INFO: model_training: Rank 0, Epoch 47, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:19,221: INFO: model_training: Rank 0, Epoch 47, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:19,429: INFO: model_training: Rank 0, Epoch 47, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:19,633: INFO: model_training: Rank 0, Epoch 47, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:19,838: INFO: model_training: Rank 0, Epoch 48, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:20,079: INFO: model_training: Rank 0, Epoch 48, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:20,320: INFO: model_training: Rank 0, Epoch 48, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:20,544: INFO: model_training: Rank 0, Epoch 48, Batch 3, Loss: 0.0629]
[2025-05-20 22:51:20,776: INFO: model_training: Rank 0, Epoch 48, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:21,006: INFO: model_training: Rank 0, Epoch 49, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:21,214: INFO: model_training: Rank 0, Epoch 49, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:21,420: INFO: model_training: Rank 0, Epoch 49, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:21,629: INFO: model_training: Rank 0, Epoch 49, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:21,829: INFO: model_training: Rank 0, Epoch 49, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:22,073: INFO: model_training: Rank 0, Epoch 50, Batch 0, Loss: 0.0512]
[2025-05-20 22:51:22,293: INFO: model_training: Rank 0, Epoch 50, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:22,523: INFO: model_training: Rank 0, Epoch 50, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:22,746: INFO: model_training: Rank 0, Epoch 50, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:22,953: INFO: model_training: Rank 0, Epoch 50, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:23,161: INFO: model_training: Rank 0, Epoch 51, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:23,375: INFO: model_training: Rank 0, Epoch 51, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:23,605: INFO: model_training: Rank 0, Epoch 51, Batch 2, Loss: 0.0522]
[2025-05-20 22:51:23,820: INFO: model_training: Rank 0, Epoch 51, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:24,024: INFO: model_training: Rank 0, Epoch 51, Batch 4, Loss: 0.0514]
[2025-05-20 22:51:24,226: INFO: model_training: Rank 0, Epoch 52, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:24,471: INFO: model_training: Rank 0, Epoch 52, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:24,690: INFO: model_training: Rank 0, Epoch 52, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:24,917: INFO: model_training: Rank 0, Epoch 52, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:25,128: INFO: model_training: Rank 0, Epoch 52, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:25,342: INFO: model_training: Rank 0, Epoch 53, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:25,542: INFO: model_training: Rank 0, Epoch 53, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:25,748: INFO: model_training: Rank 0, Epoch 53, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:25,948: INFO: model_training: Rank 0, Epoch 53, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:26,181: INFO: model_training: Rank 0, Epoch 53, Batch 4, Loss: 0.0540]
[2025-05-20 22:51:26,398: INFO: model_training: Rank 0, Epoch 54, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:26,605: INFO: model_training: Rank 0, Epoch 54, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:26,820: INFO: model_training: Rank 0, Epoch 54, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:27,034: INFO: model_training: Rank 0, Epoch 54, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:27,243: INFO: model_training: Rank 0, Epoch 54, Batch 4, Loss: 0.0564]
[2025-05-20 22:51:27,454: INFO: model_training: Rank 0, Epoch 55, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:27,670: INFO: model_training: Rank 0, Epoch 55, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:27,873: INFO: model_training: Rank 0, Epoch 55, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:28,076: INFO: model_training: Rank 0, Epoch 55, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:28,281: INFO: model_training: Rank 0, Epoch 55, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:28,503: INFO: model_training: Rank 0, Epoch 56, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:28,717: INFO: model_training: Rank 0, Epoch 56, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:28,934: INFO: model_training: Rank 0, Epoch 56, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:29,145: INFO: model_training: Rank 0, Epoch 56, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:29,369: INFO: model_training: Rank 0, Epoch 56, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:29,577: INFO: model_training: Rank 0, Epoch 57, Batch 0, Loss: 0.0546]
[2025-05-20 22:51:29,781: INFO: model_training: Rank 0, Epoch 57, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:29,987: INFO: model_training: Rank 0, Epoch 57, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:30,195: INFO: model_training: Rank 0, Epoch 57, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:30,405: INFO: model_training: Rank 0, Epoch 57, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:30,614: INFO: model_training: Rank 0, Epoch 58, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:30,823: INFO: model_training: Rank 0, Epoch 58, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:31,038: INFO: model_training: Rank 0, Epoch 58, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:31,251: INFO: model_training: Rank 0, Epoch 58, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:31,460: INFO: model_training: Rank 0, Epoch 58, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:31,663: INFO: model_training: Rank 0, Epoch 59, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:31,870: INFO: model_training: Rank 0, Epoch 59, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:32,075: INFO: model_training: Rank 0, Epoch 59, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:32,276: INFO: model_training: Rank 0, Epoch 59, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:32,511: INFO: model_training: Rank 0, Epoch 59, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:32,723: INFO: model_training: Rank 0, Epoch 60, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:32,939: INFO: model_training: Rank 0, Epoch 60, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:33,180: INFO: model_training: Rank 0, Epoch 60, Batch 2, Loss: 0.0506]
[2025-05-20 22:51:33,435: INFO: model_training: Rank 0, Epoch 60, Batch 3, Loss: 0.0545]
[2025-05-20 22:51:33,687: INFO: model_training: Rank 0, Epoch 60, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:33,924: INFO: model_training: Rank 0, Epoch 61, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:34,164: INFO: model_training: Rank 0, Epoch 61, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:34,427: INFO: model_training: Rank 0, Epoch 61, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:34,665: INFO: model_training: Rank 0, Epoch 61, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:34,871: INFO: model_training: Rank 0, Epoch 61, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:35,073: INFO: model_training: Rank 0, Epoch 62, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:35,275: INFO: model_training: Rank 0, Epoch 62, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:35,485: INFO: model_training: Rank 0, Epoch 62, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:35,716: INFO: model_training: Rank 0, Epoch 62, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:35,959: INFO: model_training: Rank 0, Epoch 62, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:36,198: INFO: model_training: Rank 0, Epoch 63, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:36,424: INFO: model_training: Rank 0, Epoch 63, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:36,648: INFO: model_training: Rank 0, Epoch 63, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:36,858: INFO: model_training: Rank 0, Epoch 63, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:37,069: INFO: model_training: Rank 0, Epoch 63, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:37,273: INFO: model_training: Rank 0, Epoch 64, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:37,500: INFO: model_training: Rank 0, Epoch 64, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:37,731: INFO: model_training: Rank 0, Epoch 64, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:37,944: INFO: model_training: Rank 0, Epoch 64, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:38,199: INFO: model_training: Rank 0, Epoch 64, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:38,430: INFO: model_training: Rank 0, Epoch 65, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:38,642: INFO: model_training: Rank 0, Epoch 65, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:38,868: INFO: model_training: Rank 0, Epoch 65, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:39,133: INFO: model_training: Rank 0, Epoch 65, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:39,361: INFO: model_training: Rank 0, Epoch 65, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:39,567: INFO: model_training: Rank 0, Epoch 66, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:39,772: INFO: model_training: Rank 0, Epoch 66, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:39,987: INFO: model_training: Rank 0, Epoch 66, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:40,187: INFO: model_training: Rank 0, Epoch 66, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:40,397: INFO: model_training: Rank 0, Epoch 66, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:40,615: INFO: model_training: Rank 0, Epoch 67, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:40,834: INFO: model_training: Rank 0, Epoch 67, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:41,046: INFO: model_training: Rank 0, Epoch 67, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:41,251: INFO: model_training: Rank 0, Epoch 67, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:41,451: INFO: model_training: Rank 0, Epoch 67, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:41,658: INFO: model_training: Rank 0, Epoch 68, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:41,864: INFO: model_training: Rank 0, Epoch 68, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:42,070: INFO: model_training: Rank 0, Epoch 68, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:42,283: INFO: model_training: Rank 0, Epoch 68, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:42,516: INFO: model_training: Rank 0, Epoch 68, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:42,721: INFO: model_training: Rank 0, Epoch 69, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:42,933: INFO: model_training: Rank 0, Epoch 69, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:43,146: INFO: model_training: Rank 0, Epoch 69, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:43,357: INFO: model_training: Rank 0, Epoch 69, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:43,563: INFO: model_training: Rank 0, Epoch 69, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:43,771: INFO: model_training: Rank 0, Epoch 70, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:43,996: INFO: model_training: Rank 0, Epoch 70, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:44,203: INFO: model_training: Rank 0, Epoch 70, Batch 2, Loss: 0.0500]
[2025-05-20 22:51:44,406: INFO: model_training: Rank 0, Epoch 70, Batch 3, Loss: 0.0500]
[2025-05-20 22:51:44,610: INFO: model_training: Rank 0, Epoch 70, Batch 4, Loss: 0.0500]
[2025-05-20 22:51:44,816: INFO: model_training: Rank 0, Epoch 71, Batch 0, Loss: 0.0500]
[2025-05-20 22:51:45,024: INFO: model_training: Rank 0, Epoch 71, Batch 1, Loss: 0.0500]
[2025-05-20 22:51:45,230: INFO: model_training: Rank 0, Epoch 71, Batch 2, Loss: 0.0500]
[2025-05-23 12:16:01,999: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-23 12:16:02,001: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-23 12:16:02,002: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-23 12:16:02,002: INFO: common: created directory at: artifacts]
[2025-05-23 12:16:02,002: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-23 12:16:02,002: INFO: data_ingestion: File already exists]
[2025-05-23 12:16:02,002: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-23 12:16:02,002: INFO: main: *******************]
[2025-05-23 12:16:02,002: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-23 12:16:02,003: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-23 12:16:02,004: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-23 12:16:02,004: INFO: common: created directory at: artifacts]
[2025-05-23 12:16:02,004: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-23 12:16:02,004: ERROR: main: module 'wandb' has no attribute 'init']
Traceback (most recent call last):
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/main.py", line 38, in <module>
    model_trainer.main()
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/pipeline/stage_04_model_trainer.py", line 13, in main
    model_trainer_config = ModelTrainer(config=model_trainer_config)
  File "/Users/nagarajpoojari/Desktop/agentic/Distributed-MoE/distributed/Distributed-ML/src/DistributedML/components/model_training.py", line 51, in __init__
    wandb.init(project="moe-kubectl", mode="online")
AttributeError: module 'wandb' has no attribute 'init'
[2025-05-23 12:16:23,718: INFO: main: >>>>>> stage Data Ingestion stage started <<<<<<]
[2025-05-23 12:16:23,720: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-23 12:16:23,720: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-23 12:16:23,721: INFO: common: created directory at: artifacts]
[2025-05-23 12:16:23,721: INFO: common: created directory at: artifacts/data_ingestion]
[2025-05-23 12:16:23,721: INFO: data_ingestion: File already exists]
[2025-05-23 12:16:23,721: INFO: main: >>>>>> stage Data Ingestion stage completed <<<<<<]
[2025-05-23 12:16:23,721: INFO: main: *******************]
[2025-05-23 12:16:23,721: INFO: main: >>>>>> stage Training started <<<<<<]
[2025-05-23 12:16:23,722: INFO: common: yaml file: config/config.yaml loaded successfully]
[2025-05-23 12:16:23,723: INFO: common: yaml file: params.yaml loaded successfully]
[2025-05-23 12:16:23,723: INFO: common: created directory at: artifacts]
[2025-05-23 12:16:23,723: INFO: common: created directory at: artifacts/model_trainer]
[2025-05-23 12:16:28,614: INFO: model_training: Rank 0, Epoch 1, Batch 0, Loss: 2.0097]
[2025-05-23 12:16:29,302: INFO: model_training: Rank 0, Epoch 1, Batch 1, Loss: 1.9313]
[2025-05-23 12:16:29,958: INFO: model_training: Rank 0, Epoch 1, Batch 2, Loss: 1.9118]
[2025-05-23 12:16:30,627: INFO: model_training: Rank 0, Epoch 1, Batch 3, Loss: 1.8967]
[2025-05-23 12:16:31,301: INFO: model_training: Rank 0, Epoch 1, Batch 4, Loss: 1.8210]
[2025-05-23 12:16:31,994: INFO: model_training: Rank 0, Epoch 2, Batch 0, Loss: 1.8132]
[2025-05-23 12:16:32,163: INFO: model_training: Rank 0, Epoch 2, Batch 1, Loss: 1.7719]
[2025-05-23 12:16:32,330: INFO: model_training: Rank 0, Epoch 2, Batch 2, Loss: 1.6959]
[2025-05-23 12:16:32,493: INFO: model_training: Rank 0, Epoch 2, Batch 3, Loss: 1.6873]
[2025-05-23 12:16:32,741: INFO: model_training: Rank 0, Epoch 2, Batch 4, Loss: 1.6623]
[2025-05-23 12:16:32,929: INFO: model_training: Rank 0, Epoch 3, Batch 0, Loss: 1.5895]
[2025-05-23 12:16:33,128: INFO: model_training: Rank 0, Epoch 3, Batch 1, Loss: 1.6054]
[2025-05-23 12:16:33,297: INFO: model_training: Rank 0, Epoch 3, Batch 2, Loss: 1.5857]
[2025-05-23 12:16:33,468: INFO: model_training: Rank 0, Epoch 3, Batch 3, Loss: 1.5253]
[2025-05-23 12:16:33,637: INFO: model_training: Rank 0, Epoch 3, Batch 4, Loss: 1.4882]
[2025-05-23 12:16:33,809: INFO: model_training: Rank 0, Epoch 4, Batch 0, Loss: 1.4880]
[2025-05-23 12:16:33,991: INFO: model_training: Rank 0, Epoch 4, Batch 1, Loss: 1.4127]
[2025-05-23 12:16:34,162: INFO: model_training: Rank 0, Epoch 4, Batch 2, Loss: 1.4029]
[2025-05-23 12:16:34,336: INFO: model_training: Rank 0, Epoch 4, Batch 3, Loss: 1.3980]
[2025-05-23 12:16:34,501: INFO: model_training: Rank 0, Epoch 4, Batch 4, Loss: 1.4025]
[2025-05-23 12:16:34,666: INFO: model_training: Rank 0, Epoch 5, Batch 0, Loss: 1.3021]
[2025-05-23 12:16:34,838: INFO: model_training: Rank 0, Epoch 5, Batch 1, Loss: 1.2956]
[2025-05-23 12:16:35,001: INFO: model_training: Rank 0, Epoch 5, Batch 2, Loss: 1.2563]
[2025-05-23 12:16:35,166: INFO: model_training: Rank 0, Epoch 5, Batch 3, Loss: 1.2379]
[2025-05-23 12:16:35,335: INFO: model_training: Rank 0, Epoch 5, Batch 4, Loss: 1.2192]
[2025-05-23 12:16:35,504: INFO: model_training: Rank 0, Epoch 6, Batch 0, Loss: 1.2511]
[2025-05-23 12:16:35,669: INFO: model_training: Rank 0, Epoch 6, Batch 1, Loss: 1.1961]
[2025-05-23 12:16:35,841: INFO: model_training: Rank 0, Epoch 6, Batch 2, Loss: 1.1192]
[2025-05-23 12:16:36,014: INFO: model_training: Rank 0, Epoch 6, Batch 3, Loss: 1.1616]
[2025-05-23 12:16:36,183: INFO: model_training: Rank 0, Epoch 6, Batch 4, Loss: 1.1582]
[2025-05-23 12:16:36,351: INFO: model_training: Rank 0, Epoch 7, Batch 0, Loss: 1.0652]
[2025-05-23 12:16:36,516: INFO: model_training: Rank 0, Epoch 7, Batch 1, Loss: 1.1074]
[2025-05-23 12:16:36,680: INFO: model_training: Rank 0, Epoch 7, Batch 2, Loss: 1.0076]
[2025-05-23 12:16:36,843: INFO: model_training: Rank 0, Epoch 7, Batch 3, Loss: 1.0489]
[2025-05-23 12:16:37,021: INFO: model_training: Rank 0, Epoch 7, Batch 4, Loss: 1.0164]
[2025-05-23 12:16:37,195: INFO: model_training: Rank 0, Epoch 8, Batch 0, Loss: 0.9538]
[2025-05-23 12:16:37,393: INFO: model_training: Rank 0, Epoch 8, Batch 1, Loss: 1.0116]
[2025-05-23 12:16:37,588: INFO: model_training: Rank 0, Epoch 8, Batch 2, Loss: 0.9242]
[2025-05-23 12:16:37,769: INFO: model_training: Rank 0, Epoch 8, Batch 3, Loss: 0.8849]
[2025-05-23 12:16:37,944: INFO: model_training: Rank 0, Epoch 8, Batch 4, Loss: 0.9261]
[2025-05-23 12:16:38,123: INFO: model_training: Rank 0, Epoch 9, Batch 0, Loss: 0.9128]
[2025-05-23 12:16:38,291: INFO: model_training: Rank 0, Epoch 9, Batch 1, Loss: 0.8811]
[2025-05-23 12:16:38,461: INFO: model_training: Rank 0, Epoch 9, Batch 2, Loss: 0.8370]
[2025-05-23 12:16:38,641: INFO: model_training: Rank 0, Epoch 9, Batch 3, Loss: 0.8034]
[2025-05-23 12:16:38,830: INFO: model_training: Rank 0, Epoch 9, Batch 4, Loss: 0.8365]
[2025-05-23 12:16:39,037: INFO: model_training: Rank 0, Epoch 10, Batch 0, Loss: 0.7856]
[2025-05-23 12:16:39,229: INFO: model_training: Rank 0, Epoch 10, Batch 1, Loss: 0.8003]
[2025-05-23 12:16:39,394: INFO: model_training: Rank 0, Epoch 10, Batch 2, Loss: 0.7851]
[2025-05-23 12:16:39,563: INFO: model_training: Rank 0, Epoch 10, Batch 3, Loss: 0.7385]
[2025-05-23 12:16:39,750: INFO: model_training: Rank 0, Epoch 10, Batch 4, Loss: 0.7851]
[2025-05-23 12:16:39,955: INFO: model_training: Rank 0, Epoch 11, Batch 0, Loss: 0.7694]
[2025-05-23 12:16:40,156: INFO: model_training: Rank 0, Epoch 11, Batch 1, Loss: 0.7078]
[2025-05-23 12:16:40,359: INFO: model_training: Rank 0, Epoch 11, Batch 2, Loss: 0.6525]
[2025-05-23 12:16:40,549: INFO: model_training: Rank 0, Epoch 11, Batch 3, Loss: 0.6832]
[2025-05-23 12:16:40,747: INFO: model_training: Rank 0, Epoch 11, Batch 4, Loss: 0.6541]
[2025-05-23 12:16:40,924: INFO: model_training: Rank 0, Epoch 12, Batch 0, Loss: 0.6804]
[2025-05-23 12:16:41,101: INFO: model_training: Rank 0, Epoch 12, Batch 1, Loss: 0.6382]
[2025-05-23 12:16:41,289: INFO: model_training: Rank 0, Epoch 12, Batch 2, Loss: 0.6076]
[2025-05-23 12:16:41,463: INFO: model_training: Rank 0, Epoch 12, Batch 3, Loss: 0.6440]
[2025-05-23 12:16:41,650: INFO: model_training: Rank 0, Epoch 12, Batch 4, Loss: 0.6211]
[2025-05-23 12:16:41,838: INFO: model_training: Rank 0, Epoch 13, Batch 0, Loss: 0.6303]
[2025-05-23 12:16:42,010: INFO: model_training: Rank 0, Epoch 13, Batch 1, Loss: 0.5408]
[2025-05-23 12:16:42,174: INFO: model_training: Rank 0, Epoch 13, Batch 2, Loss: 0.5846]
[2025-05-23 12:16:42,336: INFO: model_training: Rank 0, Epoch 13, Batch 3, Loss: 0.5799]
[2025-05-23 12:16:42,515: INFO: model_training: Rank 0, Epoch 13, Batch 4, Loss: 0.5214]
[2025-05-23 12:16:42,688: INFO: model_training: Rank 0, Epoch 14, Batch 0, Loss: 0.5305]
[2025-05-23 12:16:42,855: INFO: model_training: Rank 0, Epoch 14, Batch 1, Loss: 0.4775]
[2025-05-23 12:16:43,020: INFO: model_training: Rank 0, Epoch 14, Batch 2, Loss: 0.4958]
[2025-05-23 12:16:43,190: INFO: model_training: Rank 0, Epoch 14, Batch 3, Loss: 0.4717]
[2025-05-23 12:16:43,369: INFO: model_training: Rank 0, Epoch 14, Batch 4, Loss: 0.5108]
[2025-05-23 12:16:43,536: INFO: model_training: Rank 0, Epoch 15, Batch 0, Loss: 0.4879]
[2025-05-23 12:16:43,697: INFO: model_training: Rank 0, Epoch 15, Batch 1, Loss: 0.5165]
[2025-05-23 12:16:43,865: INFO: model_training: Rank 0, Epoch 15, Batch 2, Loss: 0.4457]
[2025-05-23 12:16:44,050: INFO: model_training: Rank 0, Epoch 15, Batch 3, Loss: 0.4182]
[2025-05-23 12:16:44,216: INFO: model_training: Rank 0, Epoch 15, Batch 4, Loss: 0.4307]
[2025-05-23 12:16:44,380: INFO: model_training: Rank 0, Epoch 16, Batch 0, Loss: 0.4447]
[2025-05-23 12:16:44,546: INFO: model_training: Rank 0, Epoch 16, Batch 1, Loss: 0.4751]
[2025-05-23 12:16:44,716: INFO: model_training: Rank 0, Epoch 16, Batch 2, Loss: 0.4171]
[2025-05-23 12:16:44,900: INFO: model_training: Rank 0, Epoch 16, Batch 3, Loss: 0.4003]
[2025-05-23 12:16:45,093: INFO: model_training: Rank 0, Epoch 16, Batch 4, Loss: 0.4396]
[2025-05-23 12:16:45,274: INFO: model_training: Rank 0, Epoch 17, Batch 0, Loss: 0.4346]
[2025-05-23 12:16:45,445: INFO: model_training: Rank 0, Epoch 17, Batch 1, Loss: 0.3657]
[2025-05-23 12:16:45,611: INFO: model_training: Rank 0, Epoch 17, Batch 2, Loss: 0.3799]
[2025-05-23 12:16:45,776: INFO: model_training: Rank 0, Epoch 17, Batch 3, Loss: 0.3246]
[2025-05-23 12:16:45,941: INFO: model_training: Rank 0, Epoch 17, Batch 4, Loss: 0.4073]
[2025-05-23 12:16:46,104: INFO: model_training: Rank 0, Epoch 18, Batch 0, Loss: 0.3771]
[2025-05-23 12:16:46,270: INFO: model_training: Rank 0, Epoch 18, Batch 1, Loss: 0.3580]
[2025-05-23 12:16:46,436: INFO: model_training: Rank 0, Epoch 18, Batch 2, Loss: 0.3197]
[2025-05-23 12:16:46,600: INFO: model_training: Rank 0, Epoch 18, Batch 3, Loss: 0.3408]
[2025-05-23 12:16:46,764: INFO: model_training: Rank 0, Epoch 18, Batch 4, Loss: 0.3000]
[2025-05-23 12:16:46,923: INFO: model_training: Rank 0, Epoch 19, Batch 0, Loss: 0.3120]
[2025-05-23 12:16:47,086: INFO: model_training: Rank 0, Epoch 19, Batch 1, Loss: 0.2825]
[2025-05-23 12:16:47,248: INFO: model_training: Rank 0, Epoch 19, Batch 2, Loss: 0.2868]
[2025-05-23 12:16:47,410: INFO: model_training: Rank 0, Epoch 19, Batch 3, Loss: 0.3060]
[2025-05-23 12:16:47,572: INFO: model_training: Rank 0, Epoch 19, Batch 4, Loss: 0.3400]
[2025-05-23 12:16:47,733: INFO: model_training: Rank 0, Epoch 20, Batch 0, Loss: 0.3179]
[2025-05-23 12:16:47,895: INFO: model_training: Rank 0, Epoch 20, Batch 1, Loss: 0.2787]
[2025-05-23 12:16:48,058: INFO: model_training: Rank 0, Epoch 20, Batch 2, Loss: 0.3079]
[2025-05-23 12:16:48,224: INFO: model_training: Rank 0, Epoch 20, Batch 3, Loss: 0.2535]
[2025-05-23 12:16:48,387: INFO: model_training: Rank 0, Epoch 20, Batch 4, Loss: 0.3183]
[2025-05-23 12:16:48,551: INFO: model_training: Rank 0, Epoch 21, Batch 0, Loss: 0.2783]
[2025-05-23 12:16:48,713: INFO: model_training: Rank 0, Epoch 21, Batch 1, Loss: 0.2821]
[2025-05-23 12:16:48,880: INFO: model_training: Rank 0, Epoch 21, Batch 2, Loss: 0.2545]
[2025-05-23 12:16:49,052: INFO: model_training: Rank 0, Epoch 21, Batch 3, Loss: 0.2992]
[2025-05-23 12:16:49,217: INFO: model_training: Rank 0, Epoch 21, Batch 4, Loss: 0.2084]
[2025-05-23 12:16:49,381: INFO: model_training: Rank 0, Epoch 22, Batch 0, Loss: 0.2312]
[2025-05-23 12:16:49,542: INFO: model_training: Rank 0, Epoch 22, Batch 1, Loss: 0.2801]
[2025-05-23 12:16:49,704: INFO: model_training: Rank 0, Epoch 22, Batch 2, Loss: 0.2355]
[2025-05-23 12:16:49,868: INFO: model_training: Rank 0, Epoch 22, Batch 3, Loss: 0.2719]
[2025-05-23 12:16:50,029: INFO: model_training: Rank 0, Epoch 22, Batch 4, Loss: 0.2295]
[2025-05-23 12:16:50,191: INFO: model_training: Rank 0, Epoch 23, Batch 0, Loss: 0.2181]
[2025-05-23 12:16:50,354: INFO: model_training: Rank 0, Epoch 23, Batch 1, Loss: 0.2182]
[2025-05-23 12:16:50,514: INFO: model_training: Rank 0, Epoch 23, Batch 2, Loss: 0.2416]
[2025-05-23 12:16:50,677: INFO: model_training: Rank 0, Epoch 23, Batch 3, Loss: 0.1556]
[2025-05-23 12:16:50,838: INFO: model_training: Rank 0, Epoch 23, Batch 4, Loss: 0.1521]
[2025-05-23 12:16:51,001: INFO: model_training: Rank 0, Epoch 24, Batch 0, Loss: 0.1821]
[2025-05-23 12:16:51,165: INFO: model_training: Rank 0, Epoch 24, Batch 1, Loss: 0.2195]
[2025-05-23 12:16:51,330: INFO: model_training: Rank 0, Epoch 24, Batch 2, Loss: 0.2120]
[2025-05-23 12:16:51,492: INFO: model_training: Rank 0, Epoch 24, Batch 3, Loss: 0.1952]
[2025-05-23 12:16:51,654: INFO: model_training: Rank 0, Epoch 24, Batch 4, Loss: 0.1759]
[2025-05-23 12:16:51,816: INFO: model_training: Rank 0, Epoch 25, Batch 0, Loss: 0.1299]
[2025-05-23 12:16:51,977: INFO: model_training: Rank 0, Epoch 25, Batch 1, Loss: 0.1729]
[2025-05-23 12:16:52,138: INFO: model_training: Rank 0, Epoch 25, Batch 2, Loss: 0.1419]
[2025-05-23 12:16:52,300: INFO: model_training: Rank 0, Epoch 25, Batch 3, Loss: 0.1990]
[2025-05-23 12:16:52,465: INFO: model_training: Rank 0, Epoch 25, Batch 4, Loss: 0.1904]
[2025-05-23 12:16:52,627: INFO: model_training: Rank 0, Epoch 26, Batch 0, Loss: 0.1927]
[2025-05-23 12:16:52,790: INFO: model_training: Rank 0, Epoch 26, Batch 1, Loss: 0.1931]
[2025-05-23 12:16:52,954: INFO: model_training: Rank 0, Epoch 26, Batch 2, Loss: 0.1926]
[2025-05-23 12:16:53,123: INFO: model_training: Rank 0, Epoch 26, Batch 3, Loss: 0.1556]
[2025-05-23 12:16:53,292: INFO: model_training: Rank 0, Epoch 26, Batch 4, Loss: 0.1081]
[2025-05-23 12:16:53,462: INFO: model_training: Rank 0, Epoch 27, Batch 0, Loss: 0.1583]
[2025-05-23 12:16:53,640: INFO: model_training: Rank 0, Epoch 27, Batch 1, Loss: 0.1586]
[2025-05-23 12:16:53,811: INFO: model_training: Rank 0, Epoch 27, Batch 2, Loss: 0.1151]
[2025-05-23 12:16:53,992: INFO: model_training: Rank 0, Epoch 27, Batch 3, Loss: 0.1463]
[2025-05-23 12:16:54,163: INFO: model_training: Rank 0, Epoch 27, Batch 4, Loss: 0.1654]
[2025-05-23 12:16:54,337: INFO: model_training: Rank 0, Epoch 28, Batch 0, Loss: 0.1321]
[2025-05-23 12:16:54,508: INFO: model_training: Rank 0, Epoch 28, Batch 1, Loss: 0.1292]
[2025-05-23 12:16:54,688: INFO: model_training: Rank 0, Epoch 28, Batch 2, Loss: 0.1466]
[2025-05-23 12:16:54,866: INFO: model_training: Rank 0, Epoch 28, Batch 3, Loss: 0.0928]
[2025-05-23 12:16:55,045: INFO: model_training: Rank 0, Epoch 28, Batch 4, Loss: 0.1031]
[2025-05-23 12:16:55,219: INFO: model_training: Rank 0, Epoch 29, Batch 0, Loss: 0.0771]
[2025-05-23 12:16:55,396: INFO: model_training: Rank 0, Epoch 29, Batch 1, Loss: 0.0792]
[2025-05-23 12:16:55,572: INFO: model_training: Rank 0, Epoch 29, Batch 2, Loss: 0.1533]
[2025-05-23 12:16:55,736: INFO: model_training: Rank 0, Epoch 29, Batch 3, Loss: 0.1445]
[2025-05-23 12:16:55,904: INFO: model_training: Rank 0, Epoch 29, Batch 4, Loss: 0.0722]
[2025-05-23 12:16:56,066: INFO: model_training: Rank 0, Epoch 30, Batch 0, Loss: 0.0905]
[2025-05-23 12:16:56,230: INFO: model_training: Rank 0, Epoch 30, Batch 1, Loss: 0.1345]
[2025-05-23 12:16:56,397: INFO: model_training: Rank 0, Epoch 30, Batch 2, Loss: 0.0959]
[2025-05-23 12:16:56,564: INFO: model_training: Rank 0, Epoch 30, Batch 3, Loss: 0.1266]
[2025-05-23 12:16:56,730: INFO: model_training: Rank 0, Epoch 30, Batch 4, Loss: 0.0986]
[2025-05-23 12:16:56,895: INFO: model_training: Rank 0, Epoch 31, Batch 0, Loss: 0.1023]
[2025-05-23 12:16:57,060: INFO: model_training: Rank 0, Epoch 31, Batch 1, Loss: 0.0957]
[2025-05-23 12:16:57,225: INFO: model_training: Rank 0, Epoch 31, Batch 2, Loss: 0.1078]
[2025-05-23 12:16:57,385: INFO: model_training: Rank 0, Epoch 31, Batch 3, Loss: 0.0679]
[2025-05-23 12:16:57,544: INFO: model_training: Rank 0, Epoch 31, Batch 4, Loss: 0.0879]
[2025-05-23 12:16:57,703: INFO: model_training: Rank 0, Epoch 32, Batch 0, Loss: 0.0500]
[2025-05-23 12:16:57,864: INFO: model_training: Rank 0, Epoch 32, Batch 1, Loss: 0.0591]
[2025-05-23 12:16:58,025: INFO: model_training: Rank 0, Epoch 32, Batch 2, Loss: 0.0705]
[2025-05-23 12:16:58,191: INFO: model_training: Rank 0, Epoch 32, Batch 3, Loss: 0.1225]
[2025-05-23 12:16:58,351: INFO: model_training: Rank 0, Epoch 32, Batch 4, Loss: 0.0500]
[2025-05-23 12:16:58,509: INFO: model_training: Rank 0, Epoch 33, Batch 0, Loss: 0.0642]
[2025-05-23 12:16:58,669: INFO: model_training: Rank 0, Epoch 33, Batch 1, Loss: 0.0836]
[2025-05-23 12:16:58,832: INFO: model_training: Rank 0, Epoch 33, Batch 2, Loss: 0.0500]
[2025-05-23 12:16:59,008: INFO: model_training: Rank 0, Epoch 33, Batch 3, Loss: 0.0750]
[2025-05-23 12:16:59,172: INFO: model_training: Rank 0, Epoch 33, Batch 4, Loss: 0.0703]
[2025-05-23 12:16:59,334: INFO: model_training: Rank 0, Epoch 34, Batch 0, Loss: 0.0963]
[2025-05-23 12:16:59,493: INFO: model_training: Rank 0, Epoch 34, Batch 1, Loss: 0.1164]
[2025-05-23 12:16:59,653: INFO: model_training: Rank 0, Epoch 34, Batch 2, Loss: 0.0974]
[2025-05-23 12:16:59,815: INFO: model_training: Rank 0, Epoch 34, Batch 3, Loss: 0.0534]
[2025-05-23 12:16:59,975: INFO: model_training: Rank 0, Epoch 34, Batch 4, Loss: 0.1057]
[2025-05-23 12:17:00,136: INFO: model_training: Rank 0, Epoch 35, Batch 0, Loss: 0.0925]
[2025-05-23 12:17:00,294: INFO: model_training: Rank 0, Epoch 35, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:00,455: INFO: model_training: Rank 0, Epoch 35, Batch 2, Loss: 0.0643]
[2025-05-23 12:17:00,621: INFO: model_training: Rank 0, Epoch 35, Batch 3, Loss: 0.1071]
[2025-05-23 12:17:00,786: INFO: model_training: Rank 0, Epoch 35, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:00,952: INFO: model_training: Rank 0, Epoch 36, Batch 0, Loss: 0.0824]
[2025-05-23 12:17:01,119: INFO: model_training: Rank 0, Epoch 36, Batch 1, Loss: 0.0963]
[2025-05-23 12:17:01,284: INFO: model_training: Rank 0, Epoch 36, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:01,450: INFO: model_training: Rank 0, Epoch 36, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:01,626: INFO: model_training: Rank 0, Epoch 36, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:01,787: INFO: model_training: Rank 0, Epoch 37, Batch 0, Loss: 0.0996]
[2025-05-23 12:17:01,947: INFO: model_training: Rank 0, Epoch 37, Batch 1, Loss: 0.0889]
[2025-05-23 12:17:02,113: INFO: model_training: Rank 0, Epoch 37, Batch 2, Loss: 0.0859]
[2025-05-23 12:17:02,276: INFO: model_training: Rank 0, Epoch 37, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:02,447: INFO: model_training: Rank 0, Epoch 37, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:02,642: INFO: model_training: Rank 0, Epoch 38, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:02,809: INFO: model_training: Rank 0, Epoch 38, Batch 1, Loss: 0.0711]
[2025-05-23 12:17:02,988: INFO: model_training: Rank 0, Epoch 38, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:03,195: INFO: model_training: Rank 0, Epoch 38, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:03,410: INFO: model_training: Rank 0, Epoch 38, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:03,616: INFO: model_training: Rank 0, Epoch 39, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:03,817: INFO: model_training: Rank 0, Epoch 39, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:04,015: INFO: model_training: Rank 0, Epoch 39, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:04,179: INFO: model_training: Rank 0, Epoch 39, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:04,345: INFO: model_training: Rank 0, Epoch 39, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:04,513: INFO: model_training: Rank 0, Epoch 40, Batch 0, Loss: 0.0873]
[2025-05-23 12:17:04,696: INFO: model_training: Rank 0, Epoch 40, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:04,872: INFO: model_training: Rank 0, Epoch 40, Batch 2, Loss: 0.0855]
[2025-05-23 12:17:05,058: INFO: model_training: Rank 0, Epoch 40, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:05,232: INFO: model_training: Rank 0, Epoch 40, Batch 4, Loss: 0.0801]
[2025-05-23 12:17:05,397: INFO: model_training: Rank 0, Epoch 41, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:05,585: INFO: model_training: Rank 0, Epoch 41, Batch 1, Loss: 0.0746]
[2025-05-23 12:17:05,765: INFO: model_training: Rank 0, Epoch 41, Batch 2, Loss: 0.0657]
[2025-05-23 12:17:05,933: INFO: model_training: Rank 0, Epoch 41, Batch 3, Loss: 0.0510]
[2025-05-23 12:17:06,113: INFO: model_training: Rank 0, Epoch 41, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:06,289: INFO: model_training: Rank 0, Epoch 42, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:06,462: INFO: model_training: Rank 0, Epoch 42, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:06,639: INFO: model_training: Rank 0, Epoch 42, Batch 2, Loss: 0.0555]
[2025-05-23 12:17:06,819: INFO: model_training: Rank 0, Epoch 42, Batch 3, Loss: 0.0517]
[2025-05-23 12:17:06,996: INFO: model_training: Rank 0, Epoch 42, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:07,171: INFO: model_training: Rank 0, Epoch 43, Batch 0, Loss: 0.0658]
[2025-05-23 12:17:07,343: INFO: model_training: Rank 0, Epoch 43, Batch 1, Loss: 0.0566]
[2025-05-23 12:17:07,504: INFO: model_training: Rank 0, Epoch 43, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:07,669: INFO: model_training: Rank 0, Epoch 43, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:07,827: INFO: model_training: Rank 0, Epoch 43, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:07,987: INFO: model_training: Rank 0, Epoch 44, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:08,152: INFO: model_training: Rank 0, Epoch 44, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:08,308: INFO: model_training: Rank 0, Epoch 44, Batch 2, Loss: 0.0576]
[2025-05-23 12:17:08,470: INFO: model_training: Rank 0, Epoch 44, Batch 3, Loss: 0.0645]
[2025-05-23 12:17:08,635: INFO: model_training: Rank 0, Epoch 44, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:08,796: INFO: model_training: Rank 0, Epoch 45, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:08,965: INFO: model_training: Rank 0, Epoch 45, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:09,141: INFO: model_training: Rank 0, Epoch 45, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:09,312: INFO: model_training: Rank 0, Epoch 45, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:09,483: INFO: model_training: Rank 0, Epoch 45, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:09,672: INFO: model_training: Rank 0, Epoch 46, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:09,863: INFO: model_training: Rank 0, Epoch 46, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:10,032: INFO: model_training: Rank 0, Epoch 46, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:10,230: INFO: model_training: Rank 0, Epoch 46, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:10,429: INFO: model_training: Rank 0, Epoch 46, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:10,623: INFO: model_training: Rank 0, Epoch 47, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:10,789: INFO: model_training: Rank 0, Epoch 47, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:10,974: INFO: model_training: Rank 0, Epoch 47, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:11,177: INFO: model_training: Rank 0, Epoch 47, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:11,368: INFO: model_training: Rank 0, Epoch 47, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:11,578: INFO: model_training: Rank 0, Epoch 48, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:11,767: INFO: model_training: Rank 0, Epoch 48, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:11,948: INFO: model_training: Rank 0, Epoch 48, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:12,116: INFO: model_training: Rank 0, Epoch 48, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:12,279: INFO: model_training: Rank 0, Epoch 48, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:12,439: INFO: model_training: Rank 0, Epoch 49, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:12,622: INFO: model_training: Rank 0, Epoch 49, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:12,830: INFO: model_training: Rank 0, Epoch 49, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:13,036: INFO: model_training: Rank 0, Epoch 49, Batch 3, Loss: 0.0550]
[2025-05-23 12:17:13,239: INFO: model_training: Rank 0, Epoch 49, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:13,443: INFO: model_training: Rank 0, Epoch 50, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:13,607: INFO: model_training: Rank 0, Epoch 50, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:13,766: INFO: model_training: Rank 0, Epoch 50, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:13,925: INFO: model_training: Rank 0, Epoch 50, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:14,110: INFO: model_training: Rank 0, Epoch 50, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:14,286: INFO: model_training: Rank 0, Epoch 51, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:14,460: INFO: model_training: Rank 0, Epoch 51, Batch 1, Loss: 0.0599]
[2025-05-23 12:17:14,632: INFO: model_training: Rank 0, Epoch 51, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:14,804: INFO: model_training: Rank 0, Epoch 51, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:14,972: INFO: model_training: Rank 0, Epoch 51, Batch 4, Loss: 0.0511]
[2025-05-23 12:17:15,153: INFO: model_training: Rank 0, Epoch 52, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:15,318: INFO: model_training: Rank 0, Epoch 52, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:15,481: INFO: model_training: Rank 0, Epoch 52, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:15,645: INFO: model_training: Rank 0, Epoch 52, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:15,806: INFO: model_training: Rank 0, Epoch 52, Batch 4, Loss: 0.0538]
[2025-05-23 12:17:15,968: INFO: model_training: Rank 0, Epoch 53, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:16,127: INFO: model_training: Rank 0, Epoch 53, Batch 1, Loss: 0.0602]
[2025-05-23 12:17:16,288: INFO: model_training: Rank 0, Epoch 53, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:16,450: INFO: model_training: Rank 0, Epoch 53, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:16,610: INFO: model_training: Rank 0, Epoch 53, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:16,776: INFO: model_training: Rank 0, Epoch 54, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:16,947: INFO: model_training: Rank 0, Epoch 54, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:17,115: INFO: model_training: Rank 0, Epoch 54, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:17,282: INFO: model_training: Rank 0, Epoch 54, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:17,450: INFO: model_training: Rank 0, Epoch 54, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:17,612: INFO: model_training: Rank 0, Epoch 55, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:17,786: INFO: model_training: Rank 0, Epoch 55, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:17,951: INFO: model_training: Rank 0, Epoch 55, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:18,119: INFO: model_training: Rank 0, Epoch 55, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:18,290: INFO: model_training: Rank 0, Epoch 55, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:18,554: INFO: model_training: Rank 0, Epoch 56, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:18,873: INFO: model_training: Rank 0, Epoch 56, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:19,109: INFO: model_training: Rank 0, Epoch 56, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:19,278: INFO: model_training: Rank 0, Epoch 56, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:19,441: INFO: model_training: Rank 0, Epoch 56, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:19,624: INFO: model_training: Rank 0, Epoch 57, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:19,789: INFO: model_training: Rank 0, Epoch 57, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:19,968: INFO: model_training: Rank 0, Epoch 57, Batch 2, Loss: 0.0538]
[2025-05-23 12:17:20,134: INFO: model_training: Rank 0, Epoch 57, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:20,301: INFO: model_training: Rank 0, Epoch 57, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:20,466: INFO: model_training: Rank 0, Epoch 58, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:20,632: INFO: model_training: Rank 0, Epoch 58, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:20,813: INFO: model_training: Rank 0, Epoch 58, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:20,980: INFO: model_training: Rank 0, Epoch 58, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:21,146: INFO: model_training: Rank 0, Epoch 58, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:21,308: INFO: model_training: Rank 0, Epoch 59, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:21,469: INFO: model_training: Rank 0, Epoch 59, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:21,631: INFO: model_training: Rank 0, Epoch 59, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:21,790: INFO: model_training: Rank 0, Epoch 59, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:21,951: INFO: model_training: Rank 0, Epoch 59, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:22,111: INFO: model_training: Rank 0, Epoch 60, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:22,274: INFO: model_training: Rank 0, Epoch 60, Batch 1, Loss: 0.0519]
[2025-05-23 12:17:22,435: INFO: model_training: Rank 0, Epoch 60, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:22,597: INFO: model_training: Rank 0, Epoch 60, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:22,755: INFO: model_training: Rank 0, Epoch 60, Batch 4, Loss: 0.0500]
[2025-05-23 12:17:22,916: INFO: model_training: Rank 0, Epoch 61, Batch 0, Loss: 0.0500]
[2025-05-23 12:17:23,084: INFO: model_training: Rank 0, Epoch 61, Batch 1, Loss: 0.0500]
[2025-05-23 12:17:23,248: INFO: model_training: Rank 0, Epoch 61, Batch 2, Loss: 0.0500]
[2025-05-23 12:17:23,415: INFO: model_training: Rank 0, Epoch 61, Batch 3, Loss: 0.0500]
[2025-05-23 12:17:23,579: INFO: model_training: Rank 0, Epoch 61, Batch 4, Loss: 0.0500]
